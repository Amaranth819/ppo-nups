/home/xulin/Documents/robust_rl_benchmarks/NUUS/src/policy_gradients/torch_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  return ch.tensor(t).float()
Logging in: /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805
Step 0
++++++++ Policy training ++++++++++
Current mean reward: 14.115466 | mean episode length: 19.653846
val_loss=46.69907
val_loss=50.95363
val_loss=34.43475
val_loss=40.82544
val_loss=27.60841
val_loss=42.30344
val_loss=25.00967
val_loss=35.24162
val_loss=14.10481
val_loss=20.78489
adv_loss= 1.71896
adv_loss= 1.04155
adv_loss= 0.64546
adv_loss= 1.06130
adv_loss= 0.50612
adv_loss= 0.71087
adv_loss= 0.73092
adv_loss= 0.43333
adv_loss= 1.33580
adv_loss= 0.67595
surrogate=-0.01652, entropy= 4.25817, loss=-0.01652
surrogate=-0.03339, entropy= 4.25645, loss=-0.03339
surrogate=-0.01429, entropy= 4.25375, loss=-0.01429
surrogate=-0.00716, entropy= 4.25415, loss=-0.00716
surrogate=-0.02205, entropy= 4.25195, loss=-0.02205
surrogate=-0.00500, entropy= 4.25161, loss=-0.00500
surrogate=-0.03788, entropy= 4.25153, loss=-0.03788
surrogate=-0.02305, entropy= 4.24994, loss=-0.02305
surrogate=-0.03303, entropy= 4.24917, loss=-0.03303
surrogate=-0.00164, entropy= 4.24787, loss=-0.00164
std_min= 0.98853, std_max= 1.00329, std_mean= 0.99705
val lr: [0.0002497438524590164], policy lr: [0.0002996926229508197]
Policy Loss: -0.0016381, | Entropy Bonus: -0, | Value Loss: 20.785, | Advantage Loss: 0.67595
Time elapsed (s): 1.7624945640563965
Agent stdevs: 0.9970517
--------------------------------------------------------------------------------

Step 1
++++++++ Policy training ++++++++++
Current mean reward: 26.134750 | mean episode length: 27.567568
val_loss=134.41618
val_loss=78.68857
val_loss=88.22820
val_loss=101.14162
val_loss=108.19107
val_loss=63.88052
val_loss=86.28825
val_loss=72.73415
val_loss=73.62420
val_loss=68.21392
adv_loss= 7.06900
adv_loss= 5.57887
adv_loss= 1.58474
adv_loss= 7.52214
adv_loss= 1.58707
adv_loss= 6.61363
adv_loss= 5.09787
adv_loss= 1.87902
adv_loss= 4.06312
adv_loss= 4.01967
surrogate=-0.00897, entropy= 4.24369, loss=-0.00897
surrogate=-0.01050, entropy= 4.23960, loss=-0.01050
surrogate=-0.02292, entropy= 4.23376, loss=-0.02292
surrogate=-0.02113, entropy= 4.22893, loss=-0.02113
surrogate=-0.06280, entropy= 4.22489, loss=-0.06280
surrogate=-0.01613, entropy= 4.22157, loss=-0.01613
surrogate=-0.01655, entropy= 4.21766, loss=-0.01655
surrogate=-0.02940, entropy= 4.21250, loss=-0.02940
surrogate=-0.02828, entropy= 4.21111, loss=-0.02828
surrogate=-0.05556, entropy= 4.20784, loss=-0.05556
std_min= 0.97459, std_max= 0.99466, std_mean= 0.98380
val lr: [0.0002494877049180328], policy lr: [0.0002993852459016393]
Policy Loss: -0.055564, | Entropy Bonus: -0, | Value Loss: 68.214, | Advantage Loss: 4.0197
Time elapsed (s): 1.7204835414886475
Agent stdevs: 0.9837971
--------------------------------------------------------------------------------

Step 2
++++++++ Policy training ++++++++++
Current mean reward: 56.596259 | mean episode length: 42.765957
val_loss=248.91090
val_loss=215.75230
val_loss=203.96178
val_loss=177.63269
val_loss=153.55231
val_loss=148.92059
val_loss=132.19754
val_loss=136.17274
val_loss=149.39355
val_loss=145.33357
adv_loss= 3.78614
adv_loss=25.82893
adv_loss= 2.32319
adv_loss=16.97877
adv_loss=15.87373
adv_loss= 3.28179
adv_loss= 5.29341
adv_loss= 3.71224
adv_loss= 8.56046
adv_loss= 3.59176
surrogate=-0.00741, entropy= 4.20713, loss=-0.00741
surrogate=-0.02272, entropy= 4.20429, loss=-0.02272
surrogate= 0.00330, entropy= 4.20498, loss= 0.00330
surrogate=-0.01697, entropy= 4.20590, loss=-0.01697
surrogate= 0.02457, entropy= 4.20483, loss= 0.02457
surrogate=-0.03139, entropy= 4.20428, loss=-0.03139
surrogate=-0.01633, entropy= 4.20368, loss=-0.01633
surrogate=-0.04303, entropy= 4.20347, loss=-0.04303
surrogate=-0.03328, entropy= 4.20097, loss=-0.03328
surrogate=-0.02862, entropy= 4.20107, loss=-0.02862
std_min= 0.96543, std_max= 0.99416, std_mean= 0.98168
val lr: [0.0002492315573770492], policy lr: [0.000299077868852459]
Policy Loss: -0.028619, | Entropy Bonus: -0, | Value Loss: 145.33, | Advantage Loss: 3.5918
Time elapsed (s): 1.6820719242095947
Agent stdevs: 0.98167896
--------------------------------------------------------------------------------

Step 3
++++++++ Policy training ++++++++++
Current mean reward: 85.987412 | mean episode length: 59.117647
val_loss=290.46463
val_loss=210.09898
val_loss=191.30042
val_loss=232.34781
val_loss=257.46378
val_loss=197.89752
val_loss=219.84486
val_loss=138.72725
val_loss=194.51323
val_loss=200.07480
adv_loss=17.31295
adv_loss=14.79251
adv_loss= 1.77995
adv_loss=32.90221
adv_loss=21.17787
adv_loss=16.07467
adv_loss=15.04028
adv_loss= 1.46268
adv_loss=39.93328
adv_loss= 0.73625
surrogate=-0.03077, entropy= 4.19392, loss=-0.03077
surrogate=-0.01097, entropy= 4.18796, loss=-0.01097
surrogate=-0.02736, entropy= 4.18518, loss=-0.02736
surrogate=-0.01554, entropy= 4.17920, loss=-0.01554
surrogate=-0.00503, entropy= 4.17503, loss=-0.00503
surrogate=-0.02115, entropy= 4.17337, loss=-0.02115
surrogate=-0.01701, entropy= 4.17077, loss=-0.01701
surrogate=-0.00509, entropy= 4.16969, loss=-0.00509
surrogate=-0.01465, entropy= 4.16630, loss=-0.01465
surrogate=-0.03392, entropy= 4.16520, loss=-0.03392
std_min= 0.96105, std_max= 0.98048, std_mean= 0.96993
val lr: [0.0002489754098360656], policy lr: [0.00029877049180327867]
Policy Loss: -0.033922, | Entropy Bonus: -0, | Value Loss: 200.07, | Advantage Loss: 0.73625
Time elapsed (s): 1.7345962524414062
Agent stdevs: 0.9699325
--------------------------------------------------------------------------------

Step 4
++++++++ Policy training ++++++++++
Current mean reward: 118.220682 | mean episode length: 66.433333
val_loss=487.23270
val_loss=366.47479
val_loss=374.68942
val_loss=358.29404
val_loss=321.12881
val_loss=344.63843
val_loss=257.33691
val_loss=298.42319
val_loss=316.54950
val_loss=248.83319
adv_loss=22.63196
adv_loss= 1.16507
adv_loss=22.03835
adv_loss= 3.41955
adv_loss=23.03270
adv_loss= 0.99295
adv_loss=43.32441
adv_loss= 0.91891
adv_loss= 2.04468
adv_loss=22.38649
surrogate=-0.00649, entropy= 4.15840, loss=-0.00649
surrogate=-0.00120, entropy= 4.15114, loss=-0.00120
surrogate=-0.02613, entropy= 4.14695, loss=-0.02613
surrogate=-0.02166, entropy= 4.14259, loss=-0.02166
surrogate= 0.00041, entropy= 4.13920, loss= 0.00041
surrogate=-0.00833, entropy= 4.13490, loss=-0.00833
surrogate=-0.00936, entropy= 4.13094, loss=-0.00936
surrogate=-0.01857, entropy= 4.12736, loss=-0.01857
surrogate=-0.02417, entropy= 4.12517, loss=-0.02417
surrogate=-0.00883, entropy= 4.12307, loss=-0.00883
std_min= 0.95018, std_max= 0.96565, std_mean= 0.95643
val lr: [0.000248719262295082], policy lr: [0.0002984631147540983]
Policy Loss: -0.0088296, | Entropy Bonus: -0, | Value Loss: 248.83, | Advantage Loss: 22.386
Time elapsed (s): 1.703117847442627
Agent stdevs: 0.9564348
--------------------------------------------------------------------------------

Step 5
++++++++ Policy training ++++++++++
Current mean reward: 126.737313 | mean episode length: 69.413793
val_loss=340.46600
val_loss=282.57999
val_loss=327.62561
val_loss=323.18399
val_loss=320.44919
val_loss=291.95859
val_loss=269.44510
val_loss=285.29202
val_loss=239.89455
val_loss=332.11530
adv_loss=27.76634
adv_loss= 1.09820
adv_loss= 1.05059
adv_loss=29.30676
adv_loss=28.49212
adv_loss= 1.12404
adv_loss=28.33080
adv_loss=26.74102
adv_loss=56.00530
adv_loss=27.44717
surrogate=-0.02992, entropy= 4.11870, loss=-0.02992
surrogate=-0.02508, entropy= 4.11940, loss=-0.02508
surrogate=-0.02148, entropy= 4.11491, loss=-0.02148
surrogate=-0.01094, entropy= 4.11337, loss=-0.01094
surrogate=-0.02288, entropy= 4.10891, loss=-0.02288
surrogate=-0.00207, entropy= 4.10656, loss=-0.00207
surrogate=-0.02187, entropy= 4.10479, loss=-0.02187
surrogate=-0.02841, entropy= 4.10361, loss=-0.02841
surrogate=-0.03145, entropy= 4.09879, loss=-0.03145
surrogate= 0.00676, entropy= 4.09772, loss= 0.00676
std_min= 0.93656, std_max= 0.95927, std_mean= 0.94838
val lr: [0.00024846311475409835], policy lr: [0.000298155737704918]
Policy Loss: 0.0067643, | Entropy Bonus: -0, | Value Loss: 332.12, | Advantage Loss: 27.447
Time elapsed (s): 1.7054479122161865
Agent stdevs: 0.9483803
--------------------------------------------------------------------------------

Step 6
++++++++ Policy training ++++++++++
Current mean reward: 165.916389 | mean episode length: 86.173913
val_loss=440.55899
val_loss=424.17688
val_loss=397.67288
val_loss=423.73688
val_loss=410.11771
val_loss=353.91803
val_loss=289.46921
val_loss=322.71384
val_loss=327.91199
val_loss=320.73898
adv_loss= 1.04388
adv_loss=37.97439
adv_loss= 1.33280
adv_loss= 1.03089
adv_loss= 1.07548
adv_loss=37.94522
adv_loss= 1.26350
adv_loss=37.05651
adv_loss= 1.73045
adv_loss=72.70489
surrogate= 0.00223, entropy= 4.09072, loss= 0.00223
surrogate=-0.01244, entropy= 4.08608, loss=-0.01244
surrogate=-0.00511, entropy= 4.07845, loss=-0.00511
surrogate=-0.03886, entropy= 4.07240, loss=-0.03886
surrogate=-0.02288, entropy= 4.06969, loss=-0.02288
surrogate=-0.02641, entropy= 4.06344, loss=-0.02641
surrogate= 0.00348, entropy= 4.06189, loss= 0.00348
surrogate=-0.01062, entropy= 4.05776, loss=-0.01062
surrogate= 0.00923, entropy= 4.05427, loss= 0.00923
surrogate=-0.01693, entropy= 4.05324, loss=-0.01693
std_min= 0.92596, std_max= 0.94415, std_mean= 0.93440
val lr: [0.0002482069672131148], policy lr: [0.00029784836065573767]
Policy Loss: -0.016934, | Entropy Bonus: -0, | Value Loss: 320.74, | Advantage Loss: 72.705
Time elapsed (s): 1.7105274200439453
Agent stdevs: 0.9343956
--------------------------------------------------------------------------------

Step 7
++++++++ Policy training ++++++++++
Current mean reward: 195.841010 | mean episode length: 98.350000
val_loss=576.75073
val_loss=503.60175
val_loss=535.07867
val_loss=411.69580
val_loss=399.07944
val_loss=373.90964
val_loss=356.93719
val_loss=311.31573
val_loss=332.25433
val_loss=382.29669
adv_loss= 0.79802
adv_loss= 1.66938
adv_loss= 0.66479
adv_loss=43.86646
adv_loss=48.40553
adv_loss=44.31564
adv_loss= 0.70570
adv_loss= 1.06080
adv_loss=45.55715
adv_loss= 1.08417
surrogate= 0.02078, entropy= 4.05378, loss= 0.02078
surrogate= 0.01369, entropy= 4.05431, loss= 0.01369
surrogate=-0.00674, entropy= 4.05448, loss=-0.00674
surrogate=-0.00742, entropy= 4.05502, loss=-0.00742
surrogate=-0.02128, entropy= 4.05597, loss=-0.02128
surrogate= 0.00166, entropy= 4.05654, loss= 0.00166
surrogate=-0.02899, entropy= 4.05860, loss=-0.02899
surrogate=-0.01366, entropy= 4.05615, loss=-0.01366
surrogate= 0.00520, entropy= 4.05756, loss= 0.00520
surrogate=-0.01143, entropy= 4.05833, loss=-0.01143
std_min= 0.91999, std_max= 0.95246, std_mean= 0.93611
val lr: [0.00024795081967213115], policy lr: [0.00029754098360655737]
Policy Loss: -0.011426, | Entropy Bonus: -0, | Value Loss: 382.3, | Advantage Loss: 1.0842
Time elapsed (s): 1.6977078914642334
Agent stdevs: 0.93611145
--------------------------------------------------------------------------------

Step 8
++++++++ Policy training ++++++++++
Current mean reward: 216.168236 | mean episode length: 103.105263
val_loss=538.96490
val_loss=436.39270
val_loss=467.49680
val_loss=450.53659
val_loss=405.19647
val_loss=458.78613
val_loss=380.73447
val_loss=472.56259
val_loss=442.88907
val_loss=373.52585
adv_loss=56.29169
adv_loss=110.11615
adv_loss=57.02975
adv_loss= 1.72236
adv_loss= 0.59531
adv_loss=54.16893
adv_loss= 2.26200
adv_loss= 1.60949
adv_loss= 1.18442
adv_loss= 1.59905
surrogate=-0.00991, entropy= 4.05661, loss=-0.00991
surrogate= 0.00176, entropy= 4.05530, loss= 0.00176
surrogate= 0.00943, entropy= 4.05348, loss= 0.00943
surrogate= 0.00418, entropy= 4.04976, loss= 0.00418
surrogate=-0.03596, entropy= 4.05005, loss=-0.03596
surrogate=-0.01740, entropy= 4.04771, loss=-0.01740
surrogate=-0.01202, entropy= 4.04912, loss=-0.01202
surrogate=-0.01597, entropy= 4.04812, loss=-0.01597
surrogate=-0.02106, entropy= 4.04619, loss=-0.02106
surrogate=-0.01191, entropy= 4.04757, loss=-0.01191
std_min= 0.91309, std_max= 0.95038, std_mean= 0.93275
val lr: [0.0002476946721311475], policy lr: [0.000297233606557377]
Policy Loss: -0.01191, | Entropy Bonus: -0, | Value Loss: 373.53, | Advantage Loss: 1.5991
Time elapsed (s): 1.7091569900512695
Agent stdevs: 0.9327471
--------------------------------------------------------------------------------

Step 9
++++++++ Policy training ++++++++++
Current mean reward: 248.586608 | mean episode length: 112.833333
val_loss=673.01013
val_loss=564.30707
val_loss=439.63712
val_loss=490.66766
val_loss=480.43918
val_loss=486.76071
val_loss=420.75851
val_loss=454.27802
val_loss=495.18085
val_loss=461.07822
adv_loss=59.35799
adv_loss=137.58376
adv_loss=65.63881
adv_loss=43.93730
adv_loss=132.62750
adv_loss= 2.05960
adv_loss=43.79267
adv_loss= 1.31116
adv_loss=43.90584
adv_loss= 2.61332
surrogate= 0.01398, entropy= 4.04782, loss= 0.01398
surrogate= 0.00179, entropy= 4.04857, loss= 0.00179
surrogate=-0.00785, entropy= 4.04818, loss=-0.00785
surrogate=-0.03161, entropy= 4.04983, loss=-0.03161
surrogate=-0.00572, entropy= 4.05002, loss=-0.00572
surrogate=-0.00496, entropy= 4.04871, loss=-0.00496
surrogate=-0.03708, entropy= 4.04863, loss=-0.03708
surrogate=-0.00618, entropy= 4.04899, loss=-0.00618
surrogate= 0.00040, entropy= 4.05059, loss= 0.00040
surrogate=-0.01367, entropy= 4.05025, loss=-0.01367
std_min= 0.91311, std_max= 0.94749, std_mean= 0.93359
val lr: [0.00024743852459016395], policy lr: [0.0002969262295081967]
Policy Loss: -0.01367, | Entropy Bonus: -0, | Value Loss: 461.08, | Advantage Loss: 2.6133
Time elapsed (s): 1.71742844581604
Agent stdevs: 0.93358827
--------------------------------------------------------------------------------

Step 10
++++++++ Policy training ++++++++++
Current mean reward: 271.422687 | mean episode length: 121.812500
val_loss=576.91571
val_loss=506.92374
val_loss=470.83569
val_loss=412.37424
val_loss=475.77563
val_loss=357.77637
val_loss=426.19302
val_loss=371.47443
val_loss=367.94244
val_loss=346.30035
adv_loss= 9.18430
adv_loss= 1.69413
adv_loss=25.54697
adv_loss= 1.61030
adv_loss= 0.52480
adv_loss= 1.10002
adv_loss= 8.27784
adv_loss= 2.89213
adv_loss= 9.05702
adv_loss= 8.30457
surrogate= 0.01284, entropy= 4.05437, loss= 0.01284
surrogate= 0.00046, entropy= 4.05696, loss= 0.00046
surrogate=-0.00192, entropy= 4.05832, loss=-0.00192
surrogate=-0.00164, entropy= 4.06082, loss=-0.00164
surrogate=-0.03817, entropy= 4.06111, loss=-0.03817
surrogate=-0.01805, entropy= 4.06277, loss=-0.01805
surrogate=-0.01542, entropy= 4.06421, loss=-0.01542
surrogate= 0.00217, entropy= 4.06391, loss= 0.00217
surrogate=-0.02793, entropy= 4.06456, loss=-0.02793
surrogate= 0.00098, entropy= 4.06445, loss= 0.00098
std_min= 0.92083, std_max= 0.95126, std_mean= 0.93797
val lr: [0.0002471823770491803], policy lr: [0.00029661885245901637]
Policy Loss: 0.00097549, | Entropy Bonus: -0, | Value Loss: 346.3, | Advantage Loss: 8.3046
Time elapsed (s): 1.7149746417999268
Agent stdevs: 0.93797237
--------------------------------------------------------------------------------

Step 11
++++++++ Policy training ++++++++++
Current mean reward: 277.901066 | mean episode length: 124.687500
val_loss=390.89401
val_loss=366.91281
val_loss=350.56696
val_loss=299.46835
val_loss=327.64374
val_loss=299.02917
val_loss=272.23004
val_loss=331.52863
val_loss=264.96835
val_loss=244.19589
adv_loss= 1.88200
adv_loss= 3.30207
adv_loss= 5.52266
adv_loss= 1.41953
adv_loss= 1.97948
adv_loss= 2.36354
adv_loss=13.04273
adv_loss=16.00436
adv_loss= 1.55321
adv_loss= 1.07545
surrogate=-0.01253, entropy= 4.06346, loss=-0.01253
surrogate= 0.01231, entropy= 4.06148, loss= 0.01231
surrogate=-0.01625, entropy= 4.06185, loss=-0.01625
surrogate=-0.00223, entropy= 4.06423, loss=-0.00223
surrogate=-0.00020, entropy= 4.06241, loss=-0.00020
surrogate=-0.00089, entropy= 4.06207, loss=-0.00089
surrogate=-0.03001, entropy= 4.06351, loss=-0.03001
surrogate=-0.00208, entropy= 4.06427, loss=-0.00208
surrogate=-0.02383, entropy= 4.06513, loss=-0.02383
surrogate=-0.01565, entropy= 4.06701, loss=-0.01565
std_min= 0.92267, std_max= 0.95810, std_mean= 0.93876
val lr: [0.00024692622950819675], policy lr: [0.00029631147540983607]
Policy Loss: -0.015655, | Entropy Bonus: -0, | Value Loss: 244.2, | Advantage Loss: 1.0754
Time elapsed (s): 1.6893951892852783
Agent stdevs: 0.9387596
--------------------------------------------------------------------------------

Step 12
++++++++ Policy training ++++++++++
Current mean reward: 265.465749 | mean episode length: 119.176471
val_loss=365.66394
val_loss=376.80811
val_loss=279.83176
val_loss=425.40161
val_loss=294.95914
val_loss=350.06796
val_loss=310.05786
val_loss=349.32919
val_loss=217.99792
val_loss=315.10706
adv_loss= 3.45766
adv_loss= 3.10228
adv_loss= 8.57576
adv_loss= 2.22947
adv_loss= 3.88245
adv_loss= 1.15606
adv_loss= 0.65244
adv_loss=15.07330
adv_loss= 2.44275
adv_loss= 3.21442
surrogate= 0.01252, entropy= 4.06304, loss= 0.01252
surrogate= 0.00184, entropy= 4.05948, loss= 0.00184
surrogate= 0.01012, entropy= 4.05500, loss= 0.01012
surrogate=-0.00305, entropy= 4.05099, loss=-0.00305
surrogate=-0.00077, entropy= 4.05011, loss=-0.00077
surrogate=-0.00987, entropy= 4.04557, loss=-0.00987
surrogate=-0.01130, entropy= 4.04327, loss=-0.01130
surrogate=-0.03139, entropy= 4.04063, loss=-0.03139
surrogate=-0.01908, entropy= 4.03926, loss=-0.01908
surrogate= 0.01988, entropy= 4.03798, loss= 0.01988
std_min= 0.90573, std_max= 0.95173, std_mean= 0.92982
val lr: [0.0002466700819672131], policy lr: [0.0002960040983606557]
Policy Loss: 0.019882, | Entropy Bonus: -0, | Value Loss: 315.11, | Advantage Loss: 3.2144
Time elapsed (s): 1.6870455741882324
Agent stdevs: 0.9298242
--------------------------------------------------------------------------------

Step 13
++++++++ Policy training ++++++++++
Current mean reward: 287.346642 | mean episode length: 124.125000
val_loss=261.47876
val_loss=394.25458
val_loss=333.72757
val_loss=300.74573
val_loss=406.30240
val_loss=288.68494
val_loss=265.06989
val_loss=245.84605
val_loss=237.39815
val_loss=232.02524
adv_loss= 1.34036
adv_loss= 1.68050
adv_loss= 1.90587
adv_loss= 1.22211
adv_loss= 0.72518
adv_loss=47.57608
adv_loss= 2.84621
adv_loss=48.30291
adv_loss=119.97628
adv_loss= 1.22583
surrogate=-0.00350, entropy= 4.03809, loss=-0.00350
surrogate=-0.01289, entropy= 4.03893, loss=-0.01289
surrogate=-0.03432, entropy= 4.04094, loss=-0.03432
surrogate=-0.00311, entropy= 4.04384, loss=-0.00311
surrogate=-0.02476, entropy= 4.04724, loss=-0.02476
surrogate=-0.01575, entropy= 4.04916, loss=-0.01575
surrogate=-0.00076, entropy= 4.05248, loss=-0.00076
surrogate= 0.01627, entropy= 4.05558, loss= 0.01627
surrogate= 0.00223, entropy= 4.05748, loss= 0.00223
surrogate=-0.00364, entropy= 4.05976, loss=-0.00364
std_min= 0.91809, std_max= 0.96456, std_mean= 0.93662
val lr: [0.0002464139344262295], policy lr: [0.00029569672131147536]
Policy Loss: -0.0036395, | Entropy Bonus: -0, | Value Loss: 232.03, | Advantage Loss: 1.2258
Time elapsed (s): 1.6852257251739502
Agent stdevs: 0.9366209
--------------------------------------------------------------------------------

Step 14
++++++++ Policy training ++++++++++
Current mean reward: 291.982243 | mean episode length: 123.312500
val_loss=322.40524
val_loss=319.90680
val_loss=316.78433
val_loss=255.83104
val_loss=263.18497
val_loss=232.62601
val_loss=207.97517
val_loss=230.87337
val_loss=239.86487
val_loss=241.40610
adv_loss= 3.49079
adv_loss= 3.24914
adv_loss= 1.87763
adv_loss= 2.23583
adv_loss= 4.50067
adv_loss= 2.33638
adv_loss= 2.81685
adv_loss= 2.25960
adv_loss= 1.86650
adv_loss= 3.25720
surrogate=-0.01693, entropy= 4.05893, loss=-0.01693
surrogate= 0.02356, entropy= 4.05952, loss= 0.02356
surrogate=-0.02972, entropy= 4.05970, loss=-0.02972
surrogate=-0.01134, entropy= 4.05990, loss=-0.01134
surrogate=-0.01573, entropy= 4.06078, loss=-0.01573
surrogate=-0.02099, entropy= 4.06035, loss=-0.02099
surrogate=-0.02521, entropy= 4.06081, loss=-0.02521
surrogate=-0.01952, entropy= 4.06141, loss=-0.01952
surrogate=-0.01288, entropy= 4.06479, loss=-0.01288
surrogate=-0.00020, entropy= 4.06425, loss=-0.00020
std_min= 0.90904, std_max= 0.96341, std_mean= 0.93808
val lr: [0.0002461577868852459], policy lr: [0.00029538934426229506]
Policy Loss: -0.00020269, | Entropy Bonus: -0, | Value Loss: 241.41, | Advantage Loss: 3.2572
Time elapsed (s): 1.695246934890747
Agent stdevs: 0.9380808
--------------------------------------------------------------------------------

Step 15
++++++++ Policy training ++++++++++
Current mean reward: 298.636583 | mean episode length: 125.812500
val_loss=251.49246
val_loss=266.08798
val_loss=256.69012
val_loss=248.56656
val_loss=312.45502
val_loss=257.99561
val_loss=232.68657
val_loss=205.91301
val_loss=194.48828
val_loss=174.12938
adv_loss= 2.43925
adv_loss= 2.43157
adv_loss=148.01962
adv_loss= 2.34512
adv_loss= 0.99328
adv_loss= 0.74284
adv_loss= 2.18615
adv_loss= 1.94547
adv_loss= 3.98910
adv_loss= 1.76922
surrogate=-0.01671, entropy= 4.06397, loss=-0.01671
surrogate=-0.00820, entropy= 4.06809, loss=-0.00820
surrogate=-0.01720, entropy= 4.07071, loss=-0.01720
surrogate= 0.01735, entropy= 4.06923, loss= 0.01735
surrogate=-0.02100, entropy= 4.06712, loss=-0.02100
surrogate=-0.00048, entropy= 4.06699, loss=-0.00048
surrogate=-0.02166, entropy= 4.06881, loss=-0.02166
surrogate=-0.00461, entropy= 4.06741, loss=-0.00461
surrogate= 0.00356, entropy= 4.06712, loss= 0.00356
surrogate=-0.02085, entropy= 4.06957, loss=-0.02085
std_min= 0.91978, std_max= 0.95506, std_mean= 0.93956
val lr: [0.0002459016393442623], policy lr: [0.0002950819672131147]
Policy Loss: -0.020852, | Entropy Bonus: -0, | Value Loss: 174.13, | Advantage Loss: 1.7692
Time elapsed (s): 1.7023804187774658
Agent stdevs: 0.93956023
--------------------------------------------------------------------------------

Step 16
++++++++ Policy training ++++++++++
Current mean reward: 305.631266 | mean episode length: 131.266667
val_loss=260.38013
val_loss=203.91248
val_loss=255.23645
val_loss=242.72813
val_loss=268.94394
val_loss=209.86914
val_loss=197.81557
val_loss=290.73276
val_loss=313.05396
val_loss=257.28156
adv_loss= 0.72349
adv_loss= 1.07016
adv_loss= 1.54829
adv_loss= 1.38881
adv_loss= 1.62315
adv_loss= 0.65819
adv_loss= 0.91723
adv_loss= 1.34451
adv_loss= 2.94097
adv_loss= 0.54782
surrogate= 0.00238, entropy= 4.06848, loss= 0.00238
surrogate=-0.00680, entropy= 4.06965, loss=-0.00680
surrogate=-0.00468, entropy= 4.06932, loss=-0.00468
surrogate=-0.00144, entropy= 4.07144, loss=-0.00144
surrogate=-0.02795, entropy= 4.07218, loss=-0.02795
surrogate=-0.03101, entropy= 4.07220, loss=-0.03101
surrogate=-0.02793, entropy= 4.07283, loss=-0.02793
surrogate=-0.03717, entropy= 4.07499, loss=-0.03717
surrogate=-0.02452, entropy= 4.07440, loss=-0.02452
surrogate=-0.01166, entropy= 4.07653, loss=-0.01166
std_min= 0.92608, std_max= 0.95740, std_mean= 0.94176
val lr: [0.00024564549180327867], policy lr: [0.0002947745901639344]
Policy Loss: -0.01166, | Entropy Bonus: -0, | Value Loss: 257.28, | Advantage Loss: 0.54782
Time elapsed (s): 1.70176362991333
Agent stdevs: 0.94176036
--------------------------------------------------------------------------------

Step 17
++++++++ Policy training ++++++++++
Current mean reward: 327.732751 | mean episode length: 137.214286
val_loss=245.39716
val_loss=240.56702
val_loss=233.61732
val_loss=217.09955
val_loss=185.29695
val_loss=167.56404
val_loss=190.97025
val_loss=166.62062
val_loss=171.41142
val_loss=140.78026
adv_loss=19.52535
adv_loss= 0.93683
adv_loss= 1.18920
adv_loss= 0.70027
adv_loss= 1.37645
adv_loss= 2.35536
adv_loss= 0.85096
adv_loss= 0.45853
adv_loss= 1.20647
adv_loss= 1.51834
surrogate= 0.00379, entropy= 4.06963, loss= 0.00379
surrogate= 0.00831, entropy= 4.07106, loss= 0.00831
surrogate= 0.01357, entropy= 4.06908, loss= 0.01357
surrogate= 0.00816, entropy= 4.06740, loss= 0.00816
surrogate=-0.00061, entropy= 4.06592, loss=-0.00061
surrogate=-0.01257, entropy= 4.06428, loss=-0.01257
surrogate=-0.00980, entropy= 4.06364, loss=-0.00980
surrogate=-0.01988, entropy= 4.06401, loss=-0.01988
surrogate= 0.01141, entropy= 4.06136, loss= 0.01141
surrogate=-0.02534, entropy= 4.06216, loss=-0.02534
std_min= 0.92913, std_max= 0.94360, std_mean= 0.93722
val lr: [0.0002453893442622951], policy lr: [0.00029446721311475406]
Policy Loss: -0.025336, | Entropy Bonus: -0, | Value Loss: 140.78, | Advantage Loss: 1.5183
Time elapsed (s): 1.684709072113037
Agent stdevs: 0.9372212
--------------------------------------------------------------------------------

Step 18
++++++++ Policy training ++++++++++
Current mean reward: 330.560645 | mean episode length: 136.466667
val_loss=300.05267
val_loss=252.72328
val_loss=192.94344
val_loss=197.74483
val_loss=198.18918
val_loss=178.24506
val_loss=140.79218
val_loss=138.48409
val_loss=181.43640
val_loss=141.55533
adv_loss= 2.40395
adv_loss= 0.86550
adv_loss= 1.32081
adv_loss=101.36906
adv_loss= 3.55813
adv_loss= 2.09736
adv_loss= 1.98598
adv_loss= 2.52038
adv_loss= 2.36019
adv_loss= 1.12525
surrogate= 0.02327, entropy= 4.06091, loss= 0.02327
surrogate=-0.01597, entropy= 4.06227, loss=-0.01597
surrogate= 0.00681, entropy= 4.06270, loss= 0.00681
surrogate=-0.01608, entropy= 4.06348, loss=-0.01608
surrogate=-0.01344, entropy= 4.06222, loss=-0.01344
surrogate=-0.00253, entropy= 4.06292, loss=-0.00253
surrogate=-0.01348, entropy= 4.06414, loss=-0.01348
surrogate=-0.00011, entropy= 4.06232, loss=-0.00011
surrogate=-0.03674, entropy= 4.06386, loss=-0.03674
surrogate=-0.03100, entropy= 4.06392, loss=-0.03100
std_min= 0.91978, std_max= 0.95365, std_mean= 0.93785
val lr: [0.00024513319672131147], policy lr: [0.00029415983606557376]
Policy Loss: -0.031004, | Entropy Bonus: -0, | Value Loss: 141.56, | Advantage Loss: 1.1252
Time elapsed (s): 1.6844391822814941
Agent stdevs: 0.9378471
--------------------------------------------------------------------------------

Step 19
++++++++ Policy training ++++++++++
Current mean reward: 328.857965 | mean episode length: 134.333333
val_loss=200.24988
val_loss=166.52557
val_loss=150.96527
val_loss=141.10484
val_loss=135.71663
val_loss=131.08763
val_loss=140.22308
val_loss=143.02850
val_loss=124.33517
val_loss=113.97370
adv_loss= 0.82182
adv_loss= 2.57875
adv_loss= 1.63536
adv_loss= 1.26876
adv_loss= 0.78926
adv_loss= 3.08518
adv_loss=10.01656
adv_loss= 2.04883
adv_loss= 1.58945
adv_loss= 1.13843
surrogate=-0.01109, entropy= 4.06567, loss=-0.01109
surrogate=-0.00965, entropy= 4.06686, loss=-0.00965
surrogate= 0.00604, entropy= 4.06771, loss= 0.00604
surrogate=-0.01979, entropy= 4.06916, loss=-0.01979
surrogate=-0.00845, entropy= 4.07106, loss=-0.00845
surrogate=-0.00308, entropy= 4.07074, loss=-0.00308
surrogate=-0.02851, entropy= 4.07339, loss=-0.02851
surrogate=-0.02114, entropy= 4.07289, loss=-0.02114
surrogate= 0.00974, entropy= 4.07577, loss= 0.00974
surrogate=-0.01123, entropy= 4.07550, loss=-0.01123
std_min= 0.91237, std_max= 0.97166, std_mean= 0.94173
val lr: [0.0002448770491803279], policy lr: [0.0002938524590163934]
Policy Loss: -0.011228, | Entropy Bonus: -0, | Value Loss: 113.97, | Advantage Loss: 1.1384
Time elapsed (s): 1.6771321296691895
Agent stdevs: 0.94173056
--------------------------------------------------------------------------------

Step 20
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 318.28
++++++++ Policy training ++++++++++
Current mean reward: 329.309899 | mean episode length: 135.571429
val_loss=172.11766
val_loss=182.46706
val_loss=135.10625
val_loss=141.91296
val_loss=157.71434
val_loss=128.61208
val_loss=130.25519
val_loss=118.60954
val_loss=97.00875
val_loss=124.22185
adv_loss= 1.74764
adv_loss= 0.65258
adv_loss= 1.59749
adv_loss= 2.16435
adv_loss= 1.80407
adv_loss= 3.15135
adv_loss= 4.31855
adv_loss= 1.31329
adv_loss= 0.79564
adv_loss= 2.06335
surrogate= 0.01936, entropy= 4.07165, loss= 0.01936
surrogate=-0.00211, entropy= 4.06423, loss=-0.00211
surrogate=-0.00062, entropy= 4.05796, loss=-0.00062
surrogate= 0.01368, entropy= 4.05081, loss= 0.01368
surrogate=-0.00665, entropy= 4.04255, loss=-0.00665
surrogate= 0.00621, entropy= 4.03890, loss= 0.00621
surrogate=-0.02096, entropy= 4.03515, loss=-0.02096
surrogate=-0.01681, entropy= 4.03097, loss=-0.01681
surrogate=-0.02560, entropy= 4.02883, loss=-0.02560
surrogate=-0.02903, entropy= 4.02666, loss=-0.02903
std_min= 0.90582, std_max= 0.95487, std_mean= 0.92635
val lr: [0.00024462090163934427], policy lr: [0.0002935450819672131]
Policy Loss: -0.029026, | Entropy Bonus: -0, | Value Loss: 124.22, | Advantage Loss: 2.0634
Time elapsed (s): 1.6933062076568604
Agent stdevs: 0.9263477
--------------------------------------------------------------------------------

Step 21
++++++++ Policy training ++++++++++
Current mean reward: 334.284638 | mean episode length: 135.333333
val_loss=208.23154
val_loss=319.25696
val_loss=180.98077
val_loss=156.04202
val_loss=151.68213
val_loss=157.11627
val_loss=174.93826
val_loss=139.27707
val_loss=148.13884
val_loss=114.65141
adv_loss= 5.39742
adv_loss= 7.16631
adv_loss= 1.99726
adv_loss=58.31992
adv_loss= 4.78174
adv_loss= 6.69971
adv_loss= 2.74300
adv_loss=29.77589
adv_loss= 1.48809
adv_loss= 4.83371
surrogate=-0.00139, entropy= 4.02202, loss=-0.00139
surrogate=-0.01611, entropy= 4.02150, loss=-0.01611
surrogate=-0.00032, entropy= 4.02123, loss=-0.00032
surrogate=-0.01960, entropy= 4.01931, loss=-0.01960
surrogate=-0.00314, entropy= 4.01718, loss=-0.00314
surrogate=-0.01328, entropy= 4.01901, loss=-0.01328
surrogate=-0.00745, entropy= 4.01723, loss=-0.00745
surrogate=-0.04012, entropy= 4.01706, loss=-0.04012
surrogate=-0.01116, entropy= 4.01757, loss=-0.01116
surrogate=-0.01720, entropy= 4.01538, loss=-0.01720
std_min= 0.90441, std_max= 0.95204, std_mean= 0.92288
val lr: [0.00024436475409836064], policy lr: [0.00029323770491803276]
Policy Loss: -0.017203, | Entropy Bonus: -0, | Value Loss: 114.65, | Advantage Loss: 4.8337
Time elapsed (s): 1.699324607849121
Agent stdevs: 0.9228785
--------------------------------------------------------------------------------

Step 22
++++++++ Policy training ++++++++++
Current mean reward: 317.630991 | mean episode length: 129.333333
val_loss=196.57037
val_loss=194.29543
val_loss=122.78831
val_loss=211.43399
val_loss=121.01103
val_loss=186.77512
val_loss=152.19910
val_loss=329.40445
val_loss=184.15784
val_loss=112.99506
adv_loss= 7.09275
adv_loss= 1.61462
adv_loss= 3.36558
adv_loss= 6.77091
adv_loss=12.29736
adv_loss= 4.73780
adv_loss= 2.81135
adv_loss=24.27851
adv_loss= 4.38409
adv_loss= 8.77982
surrogate=-0.02363, entropy= 4.01389, loss=-0.02363
surrogate= 0.00086, entropy= 4.00812, loss= 0.00086
surrogate=-0.01885, entropy= 4.00375, loss=-0.01885
surrogate=-0.01785, entropy= 4.00204, loss=-0.01785
surrogate= 0.02183, entropy= 3.99942, loss= 0.02183
surrogate=-0.00251, entropy= 3.99936, loss=-0.00251
surrogate=-0.02420, entropy= 3.99741, loss=-0.02420
surrogate= 0.00362, entropy= 3.99571, loss= 0.00362
surrogate=-0.00675, entropy= 3.99380, loss=-0.00675
surrogate=-0.01227, entropy= 3.99191, loss=-0.01227
std_min= 0.89928, std_max= 0.93807, std_mean= 0.91559
val lr: [0.00024410860655737704], policy lr: [0.0002929303278688524]
Policy Loss: -0.012271, | Entropy Bonus: -0, | Value Loss: 113, | Advantage Loss: 8.7798
Time elapsed (s): 1.6902472972869873
Agent stdevs: 0.91559
--------------------------------------------------------------------------------

Step 23
++++++++ Policy training ++++++++++
Current mean reward: 352.172219 | mean episode length: 140.642857
val_loss=144.53000
val_loss=117.52815
val_loss=119.40099
val_loss=123.95229
val_loss=110.24353
val_loss=112.31785
val_loss=91.11350
val_loss=90.33287
val_loss=72.71039
val_loss=65.82817
adv_loss= 3.22758
adv_loss= 5.35588
adv_loss= 1.23338
adv_loss= 3.06719
adv_loss= 3.93520
adv_loss= 3.08493
adv_loss= 1.98613
adv_loss= 1.08276
adv_loss= 1.35502
adv_loss= 2.88618
surrogate= 0.00985, entropy= 3.98714, loss= 0.00985
surrogate=-0.02079, entropy= 3.98132, loss=-0.02079
surrogate= 0.04012, entropy= 3.97631, loss= 0.04012
surrogate=-0.03908, entropy= 3.97311, loss=-0.03908
surrogate=-0.02484, entropy= 3.96882, loss=-0.02484
surrogate= 0.01850, entropy= 3.96482, loss= 0.01850
surrogate= 0.02332, entropy= 3.96174, loss= 0.02332
surrogate=-0.02958, entropy= 3.95493, loss=-0.02958
surrogate=-0.00948, entropy= 3.94974, loss=-0.00948
surrogate= 0.00277, entropy= 3.94691, loss= 0.00277
std_min= 0.88822, std_max= 0.91862, std_mean= 0.90190
val lr: [0.00024385245901639344], policy lr: [0.0002926229508196721]
Policy Loss: 0.0027658, | Entropy Bonus: -0, | Value Loss: 65.828, | Advantage Loss: 2.8862
Time elapsed (s): 1.679394245147705
Agent stdevs: 0.90189904
--------------------------------------------------------------------------------

Step 24
++++++++ Policy training ++++++++++
Current mean reward: 384.276855 | mean episode length: 144.214286
val_loss=335.92255
val_loss=172.50833
val_loss=340.29340
val_loss=271.94763
val_loss=141.45726
val_loss=173.40393
val_loss=137.57816
val_loss=103.55273
val_loss=71.13171
val_loss=190.51874
adv_loss= 3.82876
adv_loss= 1.30727
adv_loss= 1.37564
adv_loss= 0.67101
adv_loss= 1.46785
adv_loss= 5.36524
adv_loss= 4.23645
adv_loss= 1.47953
adv_loss= 1.12563
adv_loss= 1.54988
surrogate=-0.01840, entropy= 3.94330, loss=-0.01840
surrogate=-0.00204, entropy= 3.94151, loss=-0.00204
surrogate=-0.04872, entropy= 3.93480, loss=-0.04872
surrogate=-0.00053, entropy= 3.93157, loss=-0.00053
surrogate= 0.00542, entropy= 3.93119, loss= 0.00542
surrogate=-0.00691, entropy= 3.92794, loss=-0.00691
surrogate=-0.00396, entropy= 3.92348, loss=-0.00396
surrogate=-0.01763, entropy= 3.92053, loss=-0.01763
surrogate=-0.00993, entropy= 3.92069, loss=-0.00993
surrogate=-0.02338, entropy= 3.91916, loss=-0.02338
std_min= 0.87337, std_max= 0.90913, std_mean= 0.89364
val lr: [0.00024359631147540984], policy lr: [0.00029231557377049175]
Policy Loss: -0.023375, | Entropy Bonus: -0, | Value Loss: 190.52, | Advantage Loss: 1.5499
Time elapsed (s): 1.6774623394012451
Agent stdevs: 0.89364225
--------------------------------------------------------------------------------

Step 25
++++++++ Policy training ++++++++++
Current mean reward: 377.913794 | mean episode length: 142.785714
val_loss=357.03793
val_loss=139.95335
val_loss=227.77924
val_loss=200.77484
val_loss=172.47173
val_loss=116.86320
val_loss=164.69704
val_loss=262.48718
val_loss=143.84236
val_loss=372.71747
adv_loss= 1.73460
adv_loss=267.08334
adv_loss= 4.00880
adv_loss= 1.09062
adv_loss= 0.54651
adv_loss= 5.54541
adv_loss= 1.86264
adv_loss= 0.99922
adv_loss= 1.98886
adv_loss= 1.72396
surrogate=-0.00558, entropy= 3.91340, loss=-0.00558
surrogate= 0.02934, entropy= 3.91131, loss= 0.02934
surrogate=-0.00740, entropy= 3.91042, loss=-0.00740
surrogate=-0.01743, entropy= 3.90828, loss=-0.01743
surrogate=-0.01429, entropy= 3.90785, loss=-0.01429
surrogate=-0.01940, entropy= 3.90667, loss=-0.01940
surrogate= 0.00402, entropy= 3.90635, loss= 0.00402
surrogate=-0.01790, entropy= 3.90550, loss=-0.01790
surrogate= 0.00229, entropy= 3.90482, loss= 0.00229
surrogate=-0.01472, entropy= 3.90257, loss=-0.01472
std_min= 0.87089, std_max= 0.90086, std_mean= 0.88868
val lr: [0.00024334016393442624], policy lr: [0.00029200819672131145]
Policy Loss: -0.014722, | Entropy Bonus: -0, | Value Loss: 372.72, | Advantage Loss: 1.724
Time elapsed (s): 1.6756417751312256
Agent stdevs: 0.8886836
--------------------------------------------------------------------------------

Step 26
++++++++ Policy training ++++++++++
Current mean reward: 385.655305 | mean episode length: 144.500000
val_loss=117.84383
val_loss=139.16988
val_loss=110.79623
val_loss=142.23842
val_loss=156.73886
val_loss=95.73825
val_loss=115.25459
val_loss=108.19760
val_loss=83.05267
val_loss=96.42392
adv_loss= 3.37204
adv_loss= 1.97541
adv_loss= 1.79086
adv_loss= 3.88609
adv_loss= 2.16537
adv_loss= 2.67141
adv_loss= 2.06946
adv_loss= 4.85316
adv_loss= 3.89817
adv_loss= 2.32297
surrogate= 0.00711, entropy= 3.90317, loss= 0.00711
surrogate=-0.00529, entropy= 3.90012, loss=-0.00529
surrogate=-0.02541, entropy= 3.90100, loss=-0.02541
surrogate=-0.02508, entropy= 3.90268, loss=-0.02508
surrogate=-0.00943, entropy= 3.90122, loss=-0.00943
surrogate=-0.01340, entropy= 3.89959, loss=-0.01340
surrogate=-0.02603, entropy= 3.89967, loss=-0.02603
surrogate=-0.01322, entropy= 3.89972, loss=-0.01322
surrogate=-0.02737, entropy= 3.90091, loss=-0.02737
surrogate=-0.01191, entropy= 3.89726, loss=-0.01191
std_min= 0.87613, std_max= 0.89755, std_mean= 0.88708
val lr: [0.00024308401639344264], policy lr: [0.0002917008196721311]
Policy Loss: -0.01191, | Entropy Bonus: -0, | Value Loss: 96.424, | Advantage Loss: 2.323
Time elapsed (s): 1.6950960159301758
Agent stdevs: 0.8870754
--------------------------------------------------------------------------------

Step 27
++++++++ Policy training ++++++++++
Current mean reward: 408.423178 | mean episode length: 149.307692
val_loss=202.71375
val_loss=262.88983
val_loss=178.59662
val_loss=204.36775
val_loss=134.82922
val_loss=155.91763
val_loss=132.98038
val_loss=155.27448
val_loss=109.80138
val_loss=247.01180
adv_loss= 2.67942
adv_loss= 2.26845
adv_loss= 1.20926
adv_loss= 1.15998
adv_loss= 5.80122
adv_loss= 2.32047
adv_loss= 4.13082
adv_loss= 2.07545
adv_loss= 1.85804
adv_loss= 1.29454
surrogate=-0.00490, entropy= 3.89770, loss=-0.00490
surrogate=-0.02646, entropy= 3.89730, loss=-0.02646
surrogate=-0.02929, entropy= 3.89494, loss=-0.02929
surrogate= 0.02532, entropy= 3.89432, loss= 0.02532
surrogate=-0.03954, entropy= 3.89571, loss=-0.03954
surrogate=-0.02326, entropy= 3.89506, loss=-0.02326
surrogate=-0.01908, entropy= 3.89613, loss=-0.01908
surrogate=-0.01009, entropy= 3.89538, loss=-0.01009
surrogate=-0.01423, entropy= 3.89807, loss=-0.01423
surrogate= 0.00865, entropy= 3.89722, loss= 0.00865
std_min= 0.87374, std_max= 0.90105, std_mean= 0.88706
val lr: [0.00024282786885245904], policy lr: [0.0002913934426229508]
Policy Loss: 0.0086549, | Entropy Bonus: -0, | Value Loss: 247.01, | Advantage Loss: 1.2945
Time elapsed (s): 1.6971890926361084
Agent stdevs: 0.8870635
--------------------------------------------------------------------------------

Step 28
++++++++ Policy training ++++++++++
Current mean reward: 435.333409 | mean episode length: 159.083333
val_loss=300.84573
val_loss=209.23788
val_loss=309.75458
val_loss=146.93744
val_loss=143.79500
val_loss=135.17502
val_loss=168.32617
val_loss=286.43115
val_loss=144.78458
val_loss=125.98123
adv_loss= 0.91051
adv_loss= 3.21355
adv_loss= 6.90281
adv_loss= 2.50463
adv_loss=13.81449
adv_loss= 4.44637
adv_loss= 1.25204
adv_loss= 9.35566
adv_loss= 4.29944
adv_loss= 3.70371
surrogate= 0.00348, entropy= 3.89686, loss= 0.00348
surrogate=-0.02007, entropy= 3.89787, loss=-0.02007
surrogate=-0.01555, entropy= 3.89582, loss=-0.01555
surrogate= 0.00177, entropy= 3.89604, loss= 0.00177
surrogate=-0.01318, entropy= 3.89625, loss=-0.01318
surrogate=-0.02112, entropy= 3.89627, loss=-0.02112
surrogate=-0.03522, entropy= 3.89884, loss=-0.03522
surrogate=-0.00206, entropy= 3.89745, loss=-0.00206
surrogate=-0.03282, entropy= 3.89508, loss=-0.03282
surrogate= 0.01198, entropy= 3.89731, loss= 0.01198
std_min= 0.87622, std_max= 0.89892, std_mean= 0.88715
val lr: [0.0002425717213114754], policy lr: [0.00029108606557377045]
Policy Loss: 0.01198, | Entropy Bonus: -0, | Value Loss: 125.98, | Advantage Loss: 3.7037
Time elapsed (s): 1.692380428314209
Agent stdevs: 0.8871463
--------------------------------------------------------------------------------

Step 29
++++++++ Policy training ++++++++++
Current mean reward: 477.562130 | mean episode length: 165.750000
val_loss=214.29953
val_loss=237.26239
val_loss=230.63652
val_loss=199.15842
val_loss=170.38760
val_loss=150.56190
val_loss=120.79693
val_loss=160.45964
val_loss=164.74158
val_loss=172.15988
adv_loss= 6.89049
adv_loss=22.21097
adv_loss= 5.77427
adv_loss= 2.87776
adv_loss= 4.65791
adv_loss=20.44631
adv_loss= 2.45596
adv_loss= 3.50954
adv_loss=34.79216
adv_loss= 8.85752
surrogate= 0.00794, entropy= 3.89874, loss= 0.00794
surrogate=-0.00577, entropy= 3.89488, loss=-0.00577
surrogate=-0.03018, entropy= 3.89224, loss=-0.03018
surrogate=-0.00748, entropy= 3.89133, loss=-0.00748
surrogate=-0.02610, entropy= 3.88858, loss=-0.02610
surrogate=-0.02520, entropy= 3.88605, loss=-0.02520
surrogate=-0.01081, entropy= 3.88507, loss=-0.01081
surrogate=-0.02598, entropy= 3.88219, loss=-0.02598
surrogate=-0.00383, entropy= 3.88138, loss=-0.00383
surrogate=-0.02142, entropy= 3.87811, loss=-0.02142
std_min= 0.85660, std_max= 0.91073, std_mean= 0.88167
val lr: [0.0002423155737704918], policy lr: [0.00029077868852459015]
Policy Loss: -0.021416, | Entropy Bonus: -0, | Value Loss: 172.16, | Advantage Loss: 8.8575
Time elapsed (s): 1.6784741878509521
Agent stdevs: 0.8816748
--------------------------------------------------------------------------------

Step 30
++++++++ Policy training ++++++++++
Current mean reward: 490.068261 | mean episode length: 167.916667
val_loss=193.83385
val_loss=165.65666
val_loss=177.22034
val_loss=118.22821
val_loss=127.81989
val_loss=142.42227
val_loss=161.37547
val_loss=158.53087
val_loss=161.09450
val_loss=164.87802
adv_loss= 5.50368
adv_loss= 2.59897
adv_loss= 5.01946
adv_loss=10.27507
adv_loss= 8.75576
adv_loss= 3.44901
adv_loss= 3.08708
adv_loss=18.46603
adv_loss= 4.47912
adv_loss= 2.26807
surrogate= 0.01259, entropy= 3.87348, loss= 0.01259
surrogate=-0.00522, entropy= 3.86743, loss=-0.00522
surrogate=-0.02546, entropy= 3.86048, loss=-0.02546
surrogate=-0.02938, entropy= 3.85800, loss=-0.02938
surrogate=-0.01247, entropy= 3.85174, loss=-0.01247
surrogate=-0.03263, entropy= 3.84837, loss=-0.03263
surrogate=-0.01953, entropy= 3.84592, loss=-0.01953
surrogate= 0.00378, entropy= 3.84086, loss= 0.00378
surrogate= 0.00475, entropy= 3.83722, loss= 0.00475
surrogate=-0.03653, entropy= 3.83455, loss=-0.03653
std_min= 0.84642, std_max= 0.89875, std_mean= 0.86900
val lr: [0.00024205942622950818], policy lr: [0.0002904713114754098]
Policy Loss: -0.036531, | Entropy Bonus: -0, | Value Loss: 164.88, | Advantage Loss: 2.2681
Time elapsed (s): 1.6851928234100342
Agent stdevs: 0.86899596
--------------------------------------------------------------------------------

Step 31
++++++++ Policy training ++++++++++
Current mean reward: 431.199271 | mean episode length: 154.384615
val_loss=339.67575
val_loss=305.97473
val_loss=177.49036
val_loss=170.44934
val_loss=182.11617
val_loss=369.65387
val_loss=198.50555
val_loss=165.08702
val_loss=153.80063
val_loss=128.54138
adv_loss= 9.41932
adv_loss=11.69691
adv_loss=10.64796
adv_loss= 4.03627
adv_loss=153.19971
adv_loss=11.21404
adv_loss= 9.14303
adv_loss= 3.55589
adv_loss= 4.34334
adv_loss= 8.35747
surrogate= 0.02750, entropy= 3.83259, loss= 0.02750
surrogate= 0.00726, entropy= 3.83040, loss= 0.00726
surrogate=-0.02900, entropy= 3.82870, loss=-0.02900
surrogate=-0.01833, entropy= 3.82714, loss=-0.01833
surrogate=-0.02174, entropy= 3.82418, loss=-0.02174
surrogate=-0.03360, entropy= 3.82399, loss=-0.03360
surrogate=-0.00528, entropy= 3.82247, loss=-0.00528
surrogate=-0.02778, entropy= 3.82044, loss=-0.02778
surrogate=-0.01411, entropy= 3.82052, loss=-0.01411
surrogate=-0.00901, entropy= 3.82038, loss=-0.00901
std_min= 0.83947, std_max= 0.89440, std_mean= 0.86492
val lr: [0.00024180327868852458], policy lr: [0.00029016393442622945]
Policy Loss: -0.0090128, | Entropy Bonus: -0, | Value Loss: 128.54, | Advantage Loss: 8.3575
Time elapsed (s): 1.6932685375213623
Agent stdevs: 0.86491823
--------------------------------------------------------------------------------

Step 32
++++++++ Policy training ++++++++++
Current mean reward: 612.070046 | mean episode length: 202.444444
val_loss=269.31494
val_loss=256.97684
val_loss=211.65665
val_loss=244.12869
val_loss=229.50140
val_loss=163.79720
val_loss=173.41000
val_loss=163.20145
val_loss=152.56563
val_loss=148.10887
adv_loss= 2.88130
adv_loss= 1.87587
adv_loss= 5.13595
adv_loss= 2.36384
adv_loss= 3.15413
adv_loss= 1.03344
adv_loss= 4.51014
adv_loss= 2.34398
adv_loss= 1.72524
adv_loss= 1.40755
surrogate=-0.00566, entropy= 3.81849, loss=-0.00566
surrogate=-0.01053, entropy= 3.81508, loss=-0.01053
surrogate=-0.01700, entropy= 3.81248, loss=-0.01700
surrogate=-0.01088, entropy= 3.80891, loss=-0.01088
surrogate=-0.01495, entropy= 3.80772, loss=-0.01495
surrogate= 0.01717, entropy= 3.80541, loss= 0.01717
surrogate=-0.01423, entropy= 3.80290, loss=-0.01423
surrogate=-0.01478, entropy= 3.80275, loss=-0.01478
surrogate=-0.03862, entropy= 3.80121, loss=-0.03862
surrogate=-0.05914, entropy= 3.79747, loss=-0.05914
std_min= 0.82906, std_max= 0.89573, std_mean= 0.85845
val lr: [0.00024154713114754098], policy lr: [0.00028985655737704915]
Policy Loss: -0.059139, | Entropy Bonus: -0, | Value Loss: 148.11, | Advantage Loss: 1.4075
Time elapsed (s): 1.7126445770263672
Agent stdevs: 0.8584543
--------------------------------------------------------------------------------

Step 33
++++++++ Policy training ++++++++++
Current mean reward: 472.817064 | mean episode length: 162.416667
val_loss=435.41824
val_loss=383.14560
val_loss=246.48364
val_loss=593.12708
val_loss=286.32239
val_loss=208.39313
val_loss=322.96954
val_loss=187.18480
val_loss=292.05463
val_loss=178.52393
adv_loss= 7.48094
adv_loss=37.42028
adv_loss=11.46471
adv_loss=10.04514
adv_loss=46.12320
adv_loss=51.31772
adv_loss= 6.47088
adv_loss= 8.89890
adv_loss=13.56339
adv_loss=16.74117
surrogate=-0.01010, entropy= 3.79113, loss=-0.01010
surrogate=-0.00688, entropy= 3.78558, loss=-0.00688
surrogate=-0.00834, entropy= 3.77888, loss=-0.00834
surrogate=-0.00621, entropy= 3.77164, loss=-0.00621
surrogate=-0.03183, entropy= 3.76672, loss=-0.03183
surrogate= 0.00968, entropy= 3.76170, loss= 0.00968
surrogate=-0.02352, entropy= 3.75721, loss=-0.02352
surrogate= 0.01694, entropy= 3.75432, loss= 0.01694
surrogate= 0.00782, entropy= 3.74890, loss= 0.00782
surrogate=-0.02143, entropy= 3.74474, loss=-0.02143
std_min= 0.82093, std_max= 0.88365, std_mean= 0.84353
val lr: [0.00024129098360655738], policy lr: [0.00028954918032786885]
Policy Loss: -0.021435, | Entropy Bonus: -0, | Value Loss: 178.52, | Advantage Loss: 16.741
Time elapsed (s): 1.7089488506317139
Agent stdevs: 0.84353226
--------------------------------------------------------------------------------

Step 34
++++++++ Policy training ++++++++++
Current mean reward: 559.902508 | mean episode length: 183.000000
val_loss=271.58481
val_loss=240.56276
val_loss=212.88838
val_loss=186.21318
val_loss=224.00783
val_loss=210.16299
val_loss=237.18604
val_loss=171.88310
val_loss=205.56775
val_loss=142.99078
adv_loss= 5.77490
adv_loss= 2.40055
adv_loss=16.71382
adv_loss= 5.47843
adv_loss= 5.18557
adv_loss= 3.97859
adv_loss= 2.12623
adv_loss=17.49615
adv_loss= 2.85810
adv_loss= 4.67536
surrogate=-0.00622, entropy= 3.74470, loss=-0.00622
surrogate= 0.01053, entropy= 3.74476, loss= 0.01053
surrogate=-0.01028, entropy= 3.74394, loss=-0.01028
surrogate=-0.00584, entropy= 3.74576, loss=-0.00584
surrogate=-0.00675, entropy= 3.74465, loss=-0.00675
surrogate= 0.01494, entropy= 3.74375, loss= 0.01494
surrogate=-0.01811, entropy= 3.74396, loss=-0.01811
surrogate=-0.00022, entropy= 3.74448, loss=-0.00022
surrogate=-0.00275, entropy= 3.74567, loss=-0.00275
surrogate= 0.00364, entropy= 3.74601, loss= 0.00364
std_min= 0.81447, std_max= 0.87701, std_mean= 0.84380
val lr: [0.00024103483606557378], policy lr: [0.0002892418032786885]
Policy Loss: 0.0036365, | Entropy Bonus: -0, | Value Loss: 142.99, | Advantage Loss: 4.6754
Time elapsed (s): 1.6803052425384521
Agent stdevs: 0.8438039
--------------------------------------------------------------------------------

Step 35
++++++++ Policy training ++++++++++
Current mean reward: 565.811048 | mean episode length: 185.727273
val_loss=226.99698
val_loss=234.52888
val_loss=214.24394
val_loss=139.41293
val_loss=177.18579
val_loss=224.28146
val_loss=203.67242
val_loss=194.32001
val_loss=162.03979
val_loss=206.89313
adv_loss=17.12927
adv_loss=24.74692
adv_loss=17.12608
adv_loss= 2.93470
adv_loss= 7.09627
adv_loss=15.29167
adv_loss= 2.87709
adv_loss= 6.45157
adv_loss= 2.60931
adv_loss= 2.29836
surrogate=-0.01068, entropy= 3.74030, loss=-0.01068
surrogate= 0.00655, entropy= 3.73928, loss= 0.00655
surrogate=-0.00883, entropy= 3.73942, loss=-0.00883
surrogate=-0.01594, entropy= 3.74035, loss=-0.01594
surrogate=-0.01734, entropy= 3.73721, loss=-0.01734
surrogate=-0.03386, entropy= 3.73630, loss=-0.03386
surrogate=-0.00099, entropy= 3.73701, loss=-0.00099
surrogate=-0.01031, entropy= 3.73577, loss=-0.01031
surrogate=-0.01272, entropy= 3.73648, loss=-0.01272
surrogate=-0.02305, entropy= 3.73573, loss=-0.02305
std_min= 0.81651, std_max= 0.86552, std_mean= 0.84079
val lr: [0.00024077868852459018], policy lr: [0.0002889344262295082]
Policy Loss: -0.023049, | Entropy Bonus: -0, | Value Loss: 206.89, | Advantage Loss: 2.2984
Time elapsed (s): 1.6673316955566406
Agent stdevs: 0.84078693
--------------------------------------------------------------------------------

Step 36
++++++++ Policy training ++++++++++
Current mean reward: 514.549411 | mean episode length: 170.500000
val_loss=199.74217
val_loss=173.42944
val_loss=122.08629
val_loss=116.90114
val_loss=132.71628
val_loss=302.02271
val_loss=143.08299
val_loss=171.76636
val_loss=616.75665
val_loss=197.84683
adv_loss= 4.68072
adv_loss= 4.16670
adv_loss= 6.09988
adv_loss= 4.85299
adv_loss= 4.26208
adv_loss= 4.73769
adv_loss=15.68367
adv_loss= 2.73372
adv_loss= 4.52317
adv_loss=557.62317
surrogate=-0.02017, entropy= 3.73193, loss=-0.02017
surrogate=-0.00980, entropy= 3.72703, loss=-0.00980
surrogate= 0.04000, entropy= 3.72502, loss= 0.04000
surrogate=-0.00833, entropy= 3.71910, loss=-0.00833
surrogate=-0.02535, entropy= 3.71849, loss=-0.02535
surrogate=-0.01794, entropy= 3.71713, loss=-0.01794
surrogate= 0.00272, entropy= 3.71454, loss= 0.00272
surrogate=-0.02802, entropy= 3.71121, loss=-0.02802
surrogate=-0.00795, entropy= 3.70891, loss=-0.00795
surrogate=-0.00561, entropy= 3.70865, loss=-0.00561
std_min= 0.79910, std_max= 0.86909, std_mean= 0.83349
val lr: [0.00024052254098360656], policy lr: [0.00028862704918032785]
Policy Loss: -0.0056111, | Entropy Bonus: -0, | Value Loss: 197.85, | Advantage Loss: 557.62
Time elapsed (s): 1.681429147720337
Agent stdevs: 0.83349115
--------------------------------------------------------------------------------

Step 37
++++++++ Policy training ++++++++++
Current mean reward: 579.488082 | mean episode length: 184.100000
val_loss=226.16376
val_loss=134.80609
val_loss=157.13898
val_loss=177.24237
val_loss=150.93687
val_loss=172.71651
val_loss=163.87805
val_loss=132.06271
val_loss=153.49632
val_loss=140.27518
adv_loss= 4.51662
adv_loss= 2.52225
adv_loss= 2.31518
adv_loss= 2.28105
adv_loss= 2.46183
adv_loss= 8.83645
adv_loss= 2.58352
adv_loss= 2.34972
adv_loss= 1.49274
adv_loss= 3.57176
surrogate=-0.01043, entropy= 3.70283, loss=-0.01043
surrogate= 0.00518, entropy= 3.69531, loss= 0.00518
surrogate= 0.01517, entropy= 3.68863, loss= 0.01517
surrogate=-0.03319, entropy= 3.68236, loss=-0.03319
surrogate= 0.01732, entropy= 3.67670, loss= 0.01732
surrogate=-0.01260, entropy= 3.67204, loss=-0.01260
surrogate=-0.02213, entropy= 3.66786, loss=-0.02213
surrogate= 0.01449, entropy= 3.66255, loss= 0.01449
surrogate=-0.00288, entropy= 3.66149, loss=-0.00288
surrogate=-0.02378, entropy= 3.65571, loss=-0.02378
std_min= 0.78298, std_max= 0.85518, std_mean= 0.81894
val lr: [0.00024026639344262296], policy lr: [0.00028831967213114755]
Policy Loss: -0.023779, | Entropy Bonus: -0, | Value Loss: 140.28, | Advantage Loss: 3.5718
Time elapsed (s): 1.704742670059204
Agent stdevs: 0.81893873
--------------------------------------------------------------------------------

Step 38
++++++++ Policy training ++++++++++
Current mean reward: 564.107799 | mean episode length: 183.636364
val_loss=340.21118
val_loss=392.19241
val_loss=181.75453
val_loss=337.45059
val_loss=280.69827
val_loss=156.80273
val_loss=322.33618
val_loss=486.33267
val_loss=217.23732
val_loss=209.75992
adv_loss=14.86836
adv_loss= 7.19858
adv_loss=10.90733
adv_loss= 6.19232
adv_loss= 9.65522
adv_loss= 8.14135
adv_loss= 4.59813
adv_loss= 4.47269
adv_loss= 5.33934
adv_loss= 3.92866
surrogate=-0.01685, entropy= 3.65834, loss=-0.01685
surrogate= 0.00482, entropy= 3.66099, loss= 0.00482
surrogate=-0.00351, entropy= 3.66466, loss=-0.00351
surrogate=-0.02353, entropy= 3.66811, loss=-0.02353
surrogate=-0.01866, entropy= 3.66988, loss=-0.01866
surrogate= 0.01460, entropy= 3.67234, loss= 0.01460
surrogate=-0.02954, entropy= 3.67677, loss=-0.02954
surrogate=-0.02795, entropy= 3.67821, loss=-0.02795
surrogate=-0.00477, entropy= 3.67929, loss=-0.00477
surrogate=-0.00914, entropy= 3.68276, loss=-0.00914
std_min= 0.79040, std_max= 0.86147, std_mean= 0.82639
val lr: [0.00024001024590163933], policy lr: [0.0002880122950819672]
Policy Loss: -0.0091369, | Entropy Bonus: -0, | Value Loss: 209.76, | Advantage Loss: 3.9287
Time elapsed (s): 1.712968111038208
Agent stdevs: 0.8263867
--------------------------------------------------------------------------------

Step 39
++++++++ Policy training ++++++++++
Current mean reward: 567.270890 | mean episode length: 185.300000
val_loss=222.34944
val_loss=151.16360
val_loss=211.06674
val_loss=182.63722
val_loss=166.20068
val_loss=125.08493
val_loss=121.96944
val_loss=113.46014
val_loss=123.08656
val_loss=108.55231
adv_loss= 3.40982
adv_loss=16.10514
adv_loss= 5.67602
adv_loss= 4.66359
adv_loss= 2.43960
adv_loss= 3.02207
adv_loss= 3.18059
adv_loss= 2.98070
adv_loss= 5.53969
adv_loss= 4.50227
surrogate= 0.02325, entropy= 3.68236, loss= 0.02325
surrogate= 0.00140, entropy= 3.67738, loss= 0.00140
surrogate= 0.00850, entropy= 3.67329, loss= 0.00850
surrogate=-0.00681, entropy= 3.66799, loss=-0.00681
surrogate=-0.00101, entropy= 3.66439, loss=-0.00101
surrogate=-0.01872, entropy= 3.66379, loss=-0.01872
surrogate=-0.00120, entropy= 3.65963, loss=-0.00120
surrogate=-0.04069, entropy= 3.66044, loss=-0.04069
surrogate=-0.00137, entropy= 3.65908, loss=-0.00137
surrogate=-0.00430, entropy= 3.65712, loss=-0.00430
std_min= 0.78231, std_max= 0.84791, std_mean= 0.81927
val lr: [0.00023975409836065573], policy lr: [0.00028770491803278684]
Policy Loss: -0.0043048, | Entropy Bonus: -0, | Value Loss: 108.55, | Advantage Loss: 4.5023
Time elapsed (s): 1.693871259689331
Agent stdevs: 0.81926554
--------------------------------------------------------------------------------

Step 40
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 558.25
++++++++ Policy training ++++++++++
Current mean reward: 707.694025 | mean episode length: 223.222222
val_loss=238.42361
val_loss=255.85674
val_loss=202.51215
val_loss=169.17245
val_loss=223.14615
val_loss=147.39194
val_loss=180.95749
val_loss=149.27213
val_loss=155.41461
val_loss=142.45215
adv_loss= 3.14933
adv_loss= 8.89526
adv_loss= 4.46459
adv_loss= 6.15146
adv_loss= 2.78618
adv_loss= 5.44958
adv_loss= 1.99442
adv_loss= 2.02071
adv_loss= 3.17089
adv_loss= 2.87969
surrogate= 0.00173, entropy= 3.66295, loss= 0.00173
surrogate=-0.00175, entropy= 3.66840, loss=-0.00175
surrogate=-0.00958, entropy= 3.67219, loss=-0.00958
surrogate=-0.01954, entropy= 3.67534, loss=-0.01954
surrogate= 0.01950, entropy= 3.67980, loss= 0.01950
surrogate=-0.02214, entropy= 3.68257, loss=-0.02214
surrogate=-0.01563, entropy= 3.68756, loss=-0.01563
surrogate=-0.00905, entropy= 3.69043, loss=-0.00905
surrogate=-0.02525, entropy= 3.69161, loss=-0.02525
surrogate=-0.02550, entropy= 3.69412, loss=-0.02550
std_min= 0.79015, std_max= 0.86534, std_mean= 0.82958
val lr: [0.00023949795081967213], policy lr: [0.00028739754098360654]
Policy Loss: -0.025497, | Entropy Bonus: -0, | Value Loss: 142.45, | Advantage Loss: 2.8797
Time elapsed (s): 1.6688058376312256
Agent stdevs: 0.82958144
--------------------------------------------------------------------------------

Step 41
++++++++ Policy training ++++++++++
Current mean reward: 647.293715 | mean episode length: 209.000000
val_loss=592.16528
val_loss=176.30838
val_loss=179.25374
val_loss=279.38831
val_loss=233.56857
val_loss=240.23555
val_loss=354.67575
val_loss=130.91948
val_loss=336.38336
val_loss=504.49469
adv_loss= 1.25776
adv_loss=10.09472
adv_loss= 3.27954
adv_loss= 3.19612
adv_loss= 2.76673
adv_loss= 6.05344
adv_loss= 1.35197
adv_loss= 3.29813
adv_loss= 1.90860
adv_loss= 2.25222
surrogate= 0.00216, entropy= 3.69403, loss= 0.00216
surrogate=-0.00042, entropy= 3.69463, loss=-0.00042
surrogate= 0.00267, entropy= 3.69496, loss= 0.00267
surrogate= 0.00966, entropy= 3.69395, loss= 0.00966
surrogate= 0.00129, entropy= 3.69262, loss= 0.00129
surrogate=-0.02949, entropy= 3.69100, loss=-0.02949
surrogate=-0.01032, entropy= 3.69005, loss=-0.01032
surrogate=-0.01436, entropy= 3.68891, loss=-0.01436
surrogate=-0.02970, entropy= 3.68999, loss=-0.02970
surrogate=-0.04086, entropy= 3.69154, loss=-0.04086
std_min= 0.78124, std_max= 0.86690, std_mean= 0.82899
val lr: [0.00023924180327868853], policy lr: [0.0002870901639344262]
Policy Loss: -0.040856, | Entropy Bonus: -0, | Value Loss: 504.49, | Advantage Loss: 2.2522
Time elapsed (s): 1.6583986282348633
Agent stdevs: 0.8289916
--------------------------------------------------------------------------------

Step 42
++++++++ Policy training ++++++++++
Current mean reward: 593.796087 | mean episode length: 190.700000
val_loss=121.21742
val_loss=129.95590
val_loss=76.82240
val_loss=125.95292
val_loss=67.67091
val_loss=77.41448
val_loss=96.82514
val_loss=77.08902
val_loss=75.92220
val_loss=79.00946
adv_loss= 1.97846
adv_loss= 2.56504
adv_loss= 3.97601
adv_loss= 3.77606
adv_loss= 4.35904
adv_loss= 3.48216
adv_loss= 2.73011
adv_loss= 8.50053
adv_loss= 4.57967
adv_loss= 1.15697
surrogate=-0.00979, entropy= 3.68508, loss=-0.00979
surrogate=-0.00884, entropy= 3.67635, loss=-0.00884
surrogate= 0.00057, entropy= 3.66632, loss= 0.00057
surrogate=-0.01482, entropy= 3.66116, loss=-0.01482
surrogate=-0.03854, entropy= 3.65514, loss=-0.03854
surrogate=-0.00484, entropy= 3.64941, loss=-0.00484
surrogate=-0.03366, entropy= 3.64312, loss=-0.03366
surrogate=-0.02375, entropy= 3.64070, loss=-0.02375
surrogate=-0.02112, entropy= 3.63746, loss=-0.02112
surrogate=-0.00661, entropy= 3.63395, loss=-0.00661
std_min= 0.75920, std_max= 0.86011, std_mean= 0.81355
val lr: [0.00023898565573770493], policy lr: [0.0002867827868852459]
Policy Loss: -0.0066145, | Entropy Bonus: -0, | Value Loss: 79.009, | Advantage Loss: 1.157
Time elapsed (s): 1.6756811141967773
Agent stdevs: 0.8135531
--------------------------------------------------------------------------------

Step 43
++++++++ Policy training ++++++++++
Current mean reward: 691.517811 | mean episode length: 216.666667
val_loss=178.82892
val_loss=219.94774
val_loss=200.64294
val_loss=121.49419
val_loss=305.87820
val_loss=191.49693
val_loss=290.40823
val_loss=456.62024
val_loss=123.70516
val_loss=149.76620
adv_loss= 8.81517
adv_loss= 3.11099
adv_loss= 4.53601
adv_loss= 7.72745
adv_loss=11.05940
adv_loss= 4.87839
adv_loss= 4.58586
adv_loss= 3.95753
adv_loss= 7.95160
adv_loss= 3.17172
surrogate= 0.00210, entropy= 3.63089, loss= 0.00210
surrogate=-0.01089, entropy= 3.62789, loss=-0.01089
surrogate=-0.01397, entropy= 3.62757, loss=-0.01397
surrogate=-0.03068, entropy= 3.62549, loss=-0.03068
surrogate=-0.03182, entropy= 3.62418, loss=-0.03182
surrogate=-0.02522, entropy= 3.62249, loss=-0.02522
surrogate= 0.00711, entropy= 3.62014, loss= 0.00711
surrogate=-0.00466, entropy= 3.61833, loss=-0.00466
surrogate=-0.00828, entropy= 3.61825, loss=-0.00828
surrogate= 0.00202, entropy= 3.61734, loss= 0.00202
std_min= 0.75886, std_max= 0.85375, std_mean= 0.80897
val lr: [0.00023872950819672133], policy lr: [0.00028647540983606554]
Policy Loss: 0.0020151, | Entropy Bonus: -0, | Value Loss: 149.77, | Advantage Loss: 3.1717
Time elapsed (s): 1.6968097686767578
Agent stdevs: 0.808973
--------------------------------------------------------------------------------

Step 44
++++++++ Policy training ++++++++++
Current mean reward: 677.350716 | mean episode length: 221.125000
val_loss=153.99326
val_loss=176.78734
val_loss=200.37868
val_loss=123.22660
val_loss=89.45050
val_loss=195.05637
val_loss=121.34299
val_loss=117.66561
val_loss=95.57370
val_loss=158.99472
adv_loss= 6.64892
adv_loss= 4.10350
adv_loss= 3.76866
adv_loss= 4.62697
adv_loss=17.06908
adv_loss= 4.54469
adv_loss= 3.89999
adv_loss= 2.41760
adv_loss= 3.16234
adv_loss= 5.50987
surrogate=-0.00353, entropy= 3.61797, loss=-0.00353
surrogate=-0.01302, entropy= 3.61500, loss=-0.01302
surrogate= 0.00938, entropy= 3.61434, loss= 0.00938
surrogate= 0.02229, entropy= 3.61587, loss= 0.02229
surrogate= 0.02550, entropy= 3.61520, loss= 0.02550
surrogate= 0.01305, entropy= 3.61444, loss= 0.01305
surrogate=-0.01790, entropy= 3.61302, loss=-0.01790
surrogate=-0.00370, entropy= 3.61306, loss=-0.00370
surrogate=-0.02735, entropy= 3.61247, loss=-0.02735
surrogate= 0.00504, entropy= 3.61186, loss= 0.00504
std_min= 0.75649, std_max= 0.85315, std_mean= 0.80757
val lr: [0.0002384733606557377], policy lr: [0.00028616803278688524]
Policy Loss: 0.0050445, | Entropy Bonus: -0, | Value Loss: 158.99, | Advantage Loss: 5.5099
Time elapsed (s): 1.7263028621673584
Agent stdevs: 0.80757064
--------------------------------------------------------------------------------

Step 45
++++++++ Policy training ++++++++++
Current mean reward: 648.696241 | mean episode length: 207.000000
val_loss=254.07852
val_loss=248.38545
val_loss=134.32796
val_loss=217.54791
val_loss=152.48108
val_loss=249.98206
val_loss=166.99907
val_loss=144.01976
val_loss=211.02704
val_loss=185.77112
adv_loss= 7.59120
adv_loss= 5.30510
adv_loss= 8.94466
adv_loss= 3.92790
adv_loss= 4.19436
adv_loss= 4.54863
adv_loss= 3.65329
adv_loss=11.29674
adv_loss= 1.84851
adv_loss=17.08414
surrogate= 0.00173, entropy= 3.61482, loss= 0.00173
surrogate= 0.00745, entropy= 3.61245, loss= 0.00745
surrogate=-0.00209, entropy= 3.61550, loss=-0.00209
surrogate=-0.02716, entropy= 3.61417, loss=-0.02716
surrogate=-0.02040, entropy= 3.61497, loss=-0.02040
surrogate= 0.00515, entropy= 3.61433, loss= 0.00515
surrogate=-0.01869, entropy= 3.61661, loss=-0.01869
surrogate=-0.03580, entropy= 3.61608, loss=-0.03580
surrogate=-0.01741, entropy= 3.61565, loss=-0.01741
surrogate=-0.03676, entropy= 3.61423, loss=-0.03676
std_min= 0.76036, std_max= 0.86575, std_mean= 0.80839
val lr: [0.0002382172131147541], policy lr: [0.0002858606557377049]
Policy Loss: -0.036761, | Entropy Bonus: -0, | Value Loss: 185.77, | Advantage Loss: 17.084
Time elapsed (s): 1.7058048248291016
Agent stdevs: 0.80838776
--------------------------------------------------------------------------------

Step 46
++++++++ Policy training ++++++++++
Current mean reward: 693.194418 | mean episode length: 225.125000
val_loss=125.64784
val_loss=84.69020
val_loss=240.57283
val_loss=347.01447
val_loss=210.35812
val_loss=263.51672
val_loss=189.04019
val_loss=96.49899
val_loss=218.30348
val_loss=90.24634
adv_loss=13.44661
adv_loss=12.75031
adv_loss= 3.31848
adv_loss= 9.85485
adv_loss=10.43252
adv_loss= 9.62032
adv_loss=14.53430
adv_loss= 4.79264
adv_loss=16.34720
adv_loss=14.39182
surrogate=-0.02427, entropy= 3.61518, loss=-0.02427
surrogate=-0.00367, entropy= 3.61765, loss=-0.00367
surrogate=-0.02099, entropy= 3.61448, loss=-0.02099
surrogate=-0.00654, entropy= 3.61575, loss=-0.00654
surrogate= 0.02191, entropy= 3.61808, loss= 0.02191
surrogate=-0.00129, entropy= 3.61760, loss=-0.00129
surrogate=-0.01917, entropy= 3.61486, loss=-0.01917
surrogate=-0.03443, entropy= 3.61618, loss=-0.03443
surrogate=-0.00720, entropy= 3.61784, loss=-0.00720
surrogate= 0.00500, entropy= 3.61853, loss= 0.00500
std_min= 0.75926, std_max= 0.86952, std_mean= 0.80963
val lr: [0.0002379610655737705], policy lr: [0.0002855532786885246]
Policy Loss: 0.0050011, | Entropy Bonus: -0, | Value Loss: 90.246, | Advantage Loss: 14.392
Time elapsed (s): 1.6832318305969238
Agent stdevs: 0.8096256
--------------------------------------------------------------------------------

Step 47
++++++++ Policy training ++++++++++
Current mean reward: 682.606178 | mean episode length: 214.222222
val_loss=129.73999
val_loss=234.72649
val_loss=343.04791
val_loss=133.96609
val_loss=99.00618
val_loss=107.88203
val_loss=91.49632
val_loss=130.46596
val_loss=69.60431
val_loss=222.38914
adv_loss= 5.45124
adv_loss= 5.94139
adv_loss=13.24684
adv_loss= 7.57495
adv_loss= 4.94141
adv_loss=19.25433
adv_loss= 8.49068
adv_loss=16.72495
adv_loss=11.61413
adv_loss=15.51570
surrogate= 0.01775, entropy= 3.61642, loss= 0.01775
surrogate= 0.01058, entropy= 3.61531, loss= 0.01058
surrogate=-0.00975, entropy= 3.61353, loss=-0.00975
surrogate=-0.00968, entropy= 3.60999, loss=-0.00968
surrogate=-0.00632, entropy= 3.60659, loss=-0.00632
surrogate= 0.00135, entropy= 3.60615, loss= 0.00135
surrogate=-0.01428, entropy= 3.60493, loss=-0.01428
surrogate=-0.03518, entropy= 3.60468, loss=-0.03518
surrogate=-0.02647, entropy= 3.60321, loss=-0.02647
surrogate=-0.01927, entropy= 3.60200, loss=-0.01927
std_min= 0.75614, std_max= 0.85694, std_mean= 0.80498
val lr: [0.00023770491803278687], policy lr: [0.00028524590163934424]
Policy Loss: -0.019268, | Entropy Bonus: -0, | Value Loss: 222.39, | Advantage Loss: 15.516
Time elapsed (s): 1.6688623428344727
Agent stdevs: 0.8049827
--------------------------------------------------------------------------------

Step 48
++++++++ Policy training ++++++++++
Current mean reward: 674.277754 | mean episode length: 219.875000
val_loss=201.76337
val_loss=140.33473
val_loss=338.72626
val_loss=211.12378
val_loss=154.16232
val_loss=278.00027
val_loss=189.66008
val_loss=245.98264
val_loss=170.78500
val_loss=147.06874
adv_loss= 6.81702
adv_loss= 6.28873
adv_loss= 5.54571
adv_loss=11.14669
adv_loss= 3.50944
adv_loss= 2.44891
adv_loss= 4.23798
adv_loss= 3.37227
adv_loss= 2.36758
adv_loss= 5.34989
surrogate= 0.00805, entropy= 3.60310, loss= 0.00805
surrogate= 0.01549, entropy= 3.60365, loss= 0.01549
surrogate=-0.03412, entropy= 3.60397, loss=-0.03412
surrogate= 0.01473, entropy= 3.60377, loss= 0.01473
surrogate= 0.00125, entropy= 3.60621, loss= 0.00125
surrogate=-0.01501, entropy= 3.60432, loss=-0.01501
surrogate=-0.00911, entropy= 3.60430, loss=-0.00911
surrogate=-0.00534, entropy= 3.60354, loss=-0.00534
surrogate=-0.03607, entropy= 3.60456, loss=-0.03607
surrogate=-0.00253, entropy= 3.60448, loss=-0.00253
std_min= 0.75316, std_max= 0.86334, std_mean= 0.80584
val lr: [0.00023744877049180327], policy lr: [0.0002849385245901639]
Policy Loss: -0.0025294, | Entropy Bonus: -0, | Value Loss: 147.07, | Advantage Loss: 5.3499
Time elapsed (s): 1.6665573120117188
Agent stdevs: 0.80583566
--------------------------------------------------------------------------------

Step 49
++++++++ Policy training ++++++++++
Current mean reward: 689.693983 | mean episode length: 217.666667
val_loss=158.50873
val_loss=180.31972
val_loss=185.54477
val_loss=66.57978
val_loss=83.65063
val_loss=125.32546
val_loss=108.88213
val_loss=69.04908
val_loss=147.81097
val_loss=54.95251
adv_loss= 2.68949
adv_loss= 4.73022
adv_loss= 7.16247
adv_loss= 5.12431
adv_loss= 3.69746
adv_loss= 6.62297
adv_loss= 4.02952
adv_loss= 4.83377
adv_loss= 1.93155
adv_loss= 2.95330
surrogate= 0.00861, entropy= 3.60704, loss= 0.00861
surrogate=-0.01142, entropy= 3.60997, loss=-0.01142
surrogate= 0.01117, entropy= 3.61293, loss= 0.01117
surrogate=-0.03349, entropy= 3.61636, loss=-0.03349
surrogate= 0.00269, entropy= 3.61835, loss= 0.00269
surrogate= 0.01635, entropy= 3.61969, loss= 0.01635
surrogate= 0.01279, entropy= 3.62298, loss= 0.01279
surrogate=-0.01641, entropy= 3.62495, loss=-0.01641
surrogate= 0.00809, entropy= 3.62970, loss= 0.00809
surrogate=-0.00476, entropy= 3.62849, loss=-0.00476
std_min= 0.75857, std_max= 0.87891, std_mean= 0.81251
val lr: [0.00023719262295081967], policy lr: [0.0002846311475409836]
Policy Loss: -0.0047554, | Entropy Bonus: -0, | Value Loss: 54.953, | Advantage Loss: 2.9533
Time elapsed (s): 1.6743593215942383
Agent stdevs: 0.81250876
--------------------------------------------------------------------------------

Step 50
++++++++ Policy training ++++++++++
Current mean reward: 692.168766 | mean episode length: 214.666667
val_loss=116.61578
val_loss=79.43784
val_loss=108.63164
val_loss=101.11426
val_loss=145.42038
val_loss=80.63612
val_loss=92.27857
val_loss=170.15321
val_loss=148.84680
val_loss=133.06618
adv_loss= 7.12720
adv_loss= 9.71138
adv_loss= 5.15503
adv_loss= 5.25960
adv_loss= 2.54453
adv_loss= 5.04275
adv_loss= 2.51117
adv_loss= 4.84503
adv_loss=23.11932
adv_loss=15.30836
surrogate= 0.00322, entropy= 3.62593, loss= 0.00322
surrogate=-0.00009, entropy= 3.62426, loss=-0.00009
surrogate= 0.01250, entropy= 3.62265, loss= 0.01250
surrogate=-0.02079, entropy= 3.61952, loss=-0.02079
surrogate=-0.01802, entropy= 3.61924, loss=-0.01802
surrogate=-0.02095, entropy= 3.61835, loss=-0.02095
surrogate=-0.01956, entropy= 3.61917, loss=-0.01956
surrogate=-0.01804, entropy= 3.61810, loss=-0.01804
surrogate=-0.01461, entropy= 3.61667, loss=-0.01461
surrogate=-0.02632, entropy= 3.61743, loss=-0.02632
std_min= 0.75599, std_max= 0.86694, std_mean= 0.80930
val lr: [0.00023693647540983607], policy lr: [0.00028432377049180323]
Policy Loss: -0.026322, | Entropy Bonus: -0, | Value Loss: 133.07, | Advantage Loss: 15.308
Time elapsed (s): 1.7250440120697021
Agent stdevs: 0.8092978
--------------------------------------------------------------------------------

Step 51
++++++++ Policy training ++++++++++
Current mean reward: 862.334644 | mean episode length: 273.142857
val_loss=135.83507
val_loss=55.41830
val_loss=104.37377
val_loss=69.27981
val_loss=56.45882
val_loss=85.04549
val_loss=55.17579
val_loss=99.62610
val_loss=89.02364
val_loss=57.32081
adv_loss= 6.54415
adv_loss= 5.14058
adv_loss= 3.67186
adv_loss= 3.54374
adv_loss= 2.04538
adv_loss= 4.31413
adv_loss= 3.38792
adv_loss= 5.40914
adv_loss= 7.63990
adv_loss= 1.39130
surrogate=-0.00872, entropy= 3.61508, loss=-0.00872
surrogate=-0.02037, entropy= 3.61231, loss=-0.02037
surrogate=-0.00225, entropy= 3.61120, loss=-0.00225
surrogate=-0.01755, entropy= 3.60602, loss=-0.01755
surrogate=-0.02904, entropy= 3.60314, loss=-0.02904
surrogate= 0.01013, entropy= 3.60166, loss= 0.01013
surrogate=-0.00075, entropy= 3.59721, loss=-0.00075
surrogate= 0.00767, entropy= 3.59379, loss= 0.00767
surrogate= 0.00813, entropy= 3.59078, loss= 0.00813
surrogate=-0.02240, entropy= 3.58780, loss=-0.02240
std_min= 0.74730, std_max= 0.85760, std_mean= 0.80140
val lr: [0.00023668032786885247], policy lr: [0.00028401639344262294]
Policy Loss: -0.022403, | Entropy Bonus: -0, | Value Loss: 57.321, | Advantage Loss: 1.3913
Time elapsed (s): 1.6788246631622314
Agent stdevs: 0.8013973
--------------------------------------------------------------------------------

Step 52
++++++++ Policy training ++++++++++
Current mean reward: 625.830960 | mean episode length: 199.400000
val_loss=1374.46631
val_loss=57.91434
val_loss=221.49567
val_loss=77.41440
val_loss=81.17236
val_loss=153.24716
val_loss=111.89073
val_loss=60.77596
val_loss=1398.55518
val_loss=76.59209
adv_loss= 3.46230
adv_loss= 3.52987
adv_loss=933.26068
adv_loss= 3.13326
adv_loss= 5.97514
adv_loss= 4.68918
adv_loss= 2.88326
adv_loss= 4.56914
adv_loss= 5.38545
adv_loss= 5.24784
surrogate=-0.02761, entropy= 3.59168, loss=-0.02761
surrogate=-0.00778, entropy= 3.59598, loss=-0.00778
surrogate=-0.01150, entropy= 3.59867, loss=-0.01150
surrogate=-0.00128, entropy= 3.60200, loss=-0.00128
surrogate=-0.00740, entropy= 3.60421, loss=-0.00740
surrogate=-0.03899, entropy= 3.60720, loss=-0.03899
surrogate= 0.00494, entropy= 3.60783, loss= 0.00494
surrogate=-0.01880, entropy= 3.60861, loss=-0.01880
surrogate=-0.03603, entropy= 3.60963, loss=-0.03603
surrogate= 0.01199, entropy= 3.61057, loss= 0.01199
std_min= 0.74434, std_max= 0.86657, std_mean= 0.80778
val lr: [0.00023642418032786885], policy lr: [0.0002837090163934426]
Policy Loss: 0.011995, | Entropy Bonus: -0, | Value Loss: 76.592, | Advantage Loss: 5.2478
Time elapsed (s): 1.6809296607971191
Agent stdevs: 0.8077819
--------------------------------------------------------------------------------

Step 53
++++++++ Policy training ++++++++++
Current mean reward: 667.752136 | mean episode length: 218.333333
val_loss=248.63635
val_loss=166.46378
val_loss=204.78172
val_loss=370.78610
val_loss=187.97574
val_loss=359.77686
val_loss=183.79385
val_loss=255.81796
val_loss=336.58060
val_loss=174.81282
adv_loss= 6.73819
adv_loss= 8.52156
adv_loss=20.55213
adv_loss= 4.85613
adv_loss=11.17771
adv_loss=11.57334
adv_loss=135.59282
adv_loss= 4.07633
adv_loss= 2.60836
adv_loss=10.29310
surrogate= 0.02288, entropy= 3.60542, loss= 0.02288
surrogate=-0.01929, entropy= 3.60323, loss=-0.01929
surrogate=-0.00354, entropy= 3.60337, loss=-0.00354
surrogate=-0.01881, entropy= 3.60035, loss=-0.01881
surrogate=-0.00609, entropy= 3.59862, loss=-0.00609
surrogate=-0.01684, entropy= 3.59943, loss=-0.01684
surrogate=-0.04725, entropy= 3.59836, loss=-0.04725
surrogate=-0.01351, entropy= 3.59713, loss=-0.01351
surrogate=-0.00190, entropy= 3.59766, loss=-0.00190
surrogate=-0.03747, entropy= 3.59522, loss=-0.03747
std_min= 0.73055, std_max= 0.86793, std_mean= 0.80407
val lr: [0.00023616803278688525], policy lr: [0.0002834016393442623]
Policy Loss: -0.037465, | Entropy Bonus: -0, | Value Loss: 174.81, | Advantage Loss: 10.293
Time elapsed (s): 1.673645257949829
Agent stdevs: 0.80407333
--------------------------------------------------------------------------------

Step 54
++++++++ Policy training ++++++++++
Current mean reward: 836.379182 | mean episode length: 280.285714
val_loss=346.14224
val_loss=518.32184
val_loss=145.09102
val_loss=755.24750
val_loss=325.30951
val_loss=173.09113
val_loss=207.72827
val_loss=130.32666
val_loss=314.48288
val_loss=267.76700
adv_loss=41.10244
adv_loss=27.44968
adv_loss= 7.89581
adv_loss= 8.96027
adv_loss=23.38756
adv_loss= 7.93811
adv_loss=11.86551
adv_loss=15.11595
adv_loss=64.00452
adv_loss= 9.62598
surrogate=-0.01736, entropy= 3.59172, loss=-0.01736
surrogate= 0.00086, entropy= 3.59196, loss= 0.00086
surrogate=-0.00106, entropy= 3.59069, loss=-0.00106
surrogate= 0.00012, entropy= 3.59130, loss= 0.00012
surrogate= 0.01655, entropy= 3.59069, loss= 0.01655
surrogate=-0.01985, entropy= 3.59266, loss=-0.01985
surrogate=-0.01442, entropy= 3.59167, loss=-0.01442
surrogate=-0.02377, entropy= 3.59037, loss=-0.02377
surrogate=-0.01337, entropy= 3.59006, loss=-0.01337
surrogate=-0.02718, entropy= 3.58916, loss=-0.02718
std_min= 0.72870, std_max= 0.86588, std_mean= 0.80246
val lr: [0.00023591188524590165], policy lr: [0.00028309426229508193]
Policy Loss: -0.027175, | Entropy Bonus: -0, | Value Loss: 267.77, | Advantage Loss: 9.626
Time elapsed (s): 1.6676197052001953
Agent stdevs: 0.80246395
--------------------------------------------------------------------------------

Step 55
++++++++ Policy training ++++++++++
Current mean reward: 712.530479 | mean episode length: 226.111111
val_loss=51.75365
val_loss=160.84804
val_loss=131.69701
val_loss=299.07608
val_loss=185.37349
val_loss=49.60994
val_loss=148.83646
val_loss=119.05653
val_loss=197.69681
val_loss=181.56845
adv_loss=18.48451
adv_loss= 2.42862
adv_loss=10.18371
adv_loss=19.66346
adv_loss= 7.09671
adv_loss= 2.96481
adv_loss= 8.58283
adv_loss= 7.80315
adv_loss= 5.69526
adv_loss=16.24426
surrogate= 0.00796, entropy= 3.59064, loss= 0.00796
surrogate= 0.00249, entropy= 3.59203, loss= 0.00249
surrogate= 0.00346, entropy= 3.59333, loss= 0.00346
surrogate= 0.00670, entropy= 3.59379, loss= 0.00670
surrogate=-0.00084, entropy= 3.59289, loss=-0.00084
surrogate=-0.01335, entropy= 3.59265, loss=-0.01335
surrogate=-0.02401, entropy= 3.59382, loss=-0.02401
surrogate=-0.01183, entropy= 3.59562, loss=-0.01183
surrogate=-0.00326, entropy= 3.59437, loss=-0.00326
surrogate=-0.01886, entropy= 3.59384, loss=-0.01886
std_min= 0.73058, std_max= 0.86629, std_mean= 0.80372
val lr: [0.00023565573770491805], policy lr: [0.00028278688524590163]
Policy Loss: -0.018863, | Entropy Bonus: -0, | Value Loss: 181.57, | Advantage Loss: 16.244
Time elapsed (s): 1.67732834815979
Agent stdevs: 0.80371714
--------------------------------------------------------------------------------

Step 56
++++++++ Policy training ++++++++++
Current mean reward: 677.585890 | mean episode length: 218.222222
val_loss=330.82214
val_loss=299.88028
val_loss=133.97882
val_loss=133.41806
val_loss=145.28006
val_loss=92.13712
val_loss=171.35945
val_loss=198.98001
val_loss=161.30659
val_loss=125.28271
adv_loss= 5.34892
adv_loss=17.12255
adv_loss= 7.16989
adv_loss= 5.98494
adv_loss= 8.14164
adv_loss= 4.75740
adv_loss= 5.36331
adv_loss= 7.26500
adv_loss=30.05532
adv_loss=10.38140
surrogate=-0.01361, entropy= 3.59266, loss=-0.01361
surrogate= 0.00029, entropy= 3.59035, loss= 0.00029
surrogate=-0.03082, entropy= 3.58532, loss=-0.03082
surrogate=-0.00116, entropy= 3.58202, loss=-0.00116
surrogate=-0.02281, entropy= 3.57928, loss=-0.02281
surrogate= 0.00741, entropy= 3.57597, loss= 0.00741
surrogate=-0.02881, entropy= 3.57182, loss=-0.02881
surrogate=-0.03130, entropy= 3.57247, loss=-0.03130
surrogate=-0.01019, entropy= 3.56783, loss=-0.01019
surrogate=-0.03543, entropy= 3.56615, loss=-0.03543
std_min= 0.72210, std_max= 0.85267, std_mean= 0.79627
val lr: [0.00023539959016393442], policy lr: [0.0002824795081967213]
Policy Loss: -0.035432, | Entropy Bonus: -0, | Value Loss: 125.28, | Advantage Loss: 10.381
Time elapsed (s): 1.7051317691802979
Agent stdevs: 0.79627424
--------------------------------------------------------------------------------

Step 57
++++++++ Policy training ++++++++++
Current mean reward: 699.258030 | mean episode length: 221.777778
val_loss=191.25705
val_loss=207.98613
val_loss=176.81741
val_loss=164.51694
val_loss=165.45146
val_loss=91.75321
val_loss=97.44787
val_loss=141.66064
val_loss=91.14850
val_loss=91.44923
adv_loss= 4.37169
adv_loss= 7.66602
adv_loss= 3.31224
adv_loss=21.77803
adv_loss=15.46940
adv_loss=17.05310
adv_loss= 2.41673
adv_loss=13.24286
adv_loss= 3.43142
adv_loss= 6.10690
surrogate= 0.00844, entropy= 3.56234, loss= 0.00844
surrogate= 0.00623, entropy= 3.56010, loss= 0.00623
surrogate= 0.01416, entropy= 3.55620, loss= 0.01416
surrogate= 0.00482, entropy= 3.55175, loss= 0.00482
surrogate=-0.00519, entropy= 3.54772, loss=-0.00519
surrogate=-0.00538, entropy= 3.54440, loss=-0.00538
surrogate= 0.01567, entropy= 3.54273, loss= 0.01567
surrogate=-0.01553, entropy= 3.54105, loss=-0.01553
surrogate= 0.00079, entropy= 3.53970, loss= 0.00079
surrogate=-0.00576, entropy= 3.53630, loss=-0.00576
std_min= 0.71498, std_max= 0.84346, std_mean= 0.78836
val lr: [0.00023514344262295082], policy lr: [0.00028217213114754093]
Policy Loss: -0.0057561, | Entropy Bonus: -0, | Value Loss: 91.449, | Advantage Loss: 6.1069
Time elapsed (s): 1.6755378246307373
Agent stdevs: 0.7883566
--------------------------------------------------------------------------------

Step 58
++++++++ Policy training ++++++++++
Current mean reward: 912.157734 | mean episode length: 289.000000
val_loss=158.52939
val_loss=121.94789
val_loss=89.15604
val_loss=85.54837
val_loss=57.74649
val_loss=57.55035
val_loss=34.91849
val_loss=26.35288
val_loss=76.30643
val_loss=66.18353
adv_loss= 2.86017
adv_loss= 4.35280
adv_loss=12.47917
adv_loss= 9.91731
adv_loss= 3.58529
adv_loss= 4.38191
adv_loss= 4.41417
adv_loss= 7.63010
adv_loss= 3.87161
adv_loss= 4.82104
surrogate= 0.00640, entropy= 3.53389, loss= 0.00640
surrogate=-0.01443, entropy= 3.52992, loss=-0.01443
surrogate=-0.00793, entropy= 3.53142, loss=-0.00793
surrogate=-0.01926, entropy= 3.53054, loss=-0.01926
surrogate=-0.03169, entropy= 3.53199, loss=-0.03169
surrogate= 0.00594, entropy= 3.53188, loss= 0.00594
surrogate=-0.01687, entropy= 3.53503, loss=-0.01687
surrogate= 0.00795, entropy= 3.53566, loss= 0.00795
surrogate=-0.01235, entropy= 3.53538, loss=-0.01235
surrogate=-0.00718, entropy= 3.53472, loss=-0.00718
std_min= 0.72005, std_max= 0.84221, std_mean= 0.78774
val lr: [0.00023488729508196722], policy lr: [0.00028186475409836063]
Policy Loss: -0.0071813, | Entropy Bonus: -0, | Value Loss: 66.184, | Advantage Loss: 4.821
Time elapsed (s): 1.6707541942596436
Agent stdevs: 0.7877396
--------------------------------------------------------------------------------

Step 59
++++++++ Policy training ++++++++++
Current mean reward: 724.846781 | mean episode length: 230.625000
val_loss=252.09961
val_loss=866.24432
val_loss=296.57104
val_loss=710.17456
val_loss=118.17641
val_loss=249.53287
val_loss=153.87936
val_loss=375.08282
val_loss=127.64817
val_loss=232.86749
adv_loss=13.60819
adv_loss= 7.68424
adv_loss= 4.12784
adv_loss=10.20564
adv_loss=11.86725
adv_loss= 5.22343
adv_loss= 6.93428
adv_loss= 4.98381
adv_loss= 7.03337
adv_loss= 5.29984
surrogate=-0.01104, entropy= 3.53092, loss=-0.01104
surrogate=-0.00829, entropy= 3.52236, loss=-0.00829
surrogate=-0.00817, entropy= 3.51566, loss=-0.00817
surrogate=-0.03264, entropy= 3.51357, loss=-0.03264
surrogate=-0.02512, entropy= 3.50920, loss=-0.02512
surrogate=-0.03468, entropy= 3.50683, loss=-0.03468
surrogate=-0.03396, entropy= 3.50311, loss=-0.03396
surrogate=-0.00811, entropy= 3.49851, loss=-0.00811
surrogate=-0.02268, entropy= 3.49521, loss=-0.02268
surrogate=-0.02315, entropy= 3.49061, loss=-0.02315
std_min= 0.70749, std_max= 0.83126, std_mean= 0.77633
val lr: [0.00023463114754098362], policy lr: [0.0002815573770491803]
Policy Loss: -0.023148, | Entropy Bonus: -0, | Value Loss: 232.87, | Advantage Loss: 5.2998
Time elapsed (s): 1.6735074520111084
Agent stdevs: 0.77632993
--------------------------------------------------------------------------------

Step 60
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 745.28
++++++++ Policy training ++++++++++
Current mean reward: 684.542188 | mean episode length: 219.222222
val_loss=206.21863
val_loss=137.66241
val_loss=361.04837
val_loss=247.97809
val_loss=937.96667
val_loss=108.99352
val_loss=832.89166
val_loss=350.59076
val_loss=467.45712
val_loss=112.93240
adv_loss= 3.72882
adv_loss= 9.92140
adv_loss= 2.98352
adv_loss= 7.81508
adv_loss=1048.83374
adv_loss= 7.09059
adv_loss=23.22826
adv_loss=10.41340
adv_loss= 3.38461
adv_loss= 6.56720
surrogate=-0.02188, entropy= 3.48675, loss=-0.02188
surrogate= 0.02097, entropy= 3.48490, loss= 0.02097
surrogate= 0.04212, entropy= 3.48425, loss= 0.04212
surrogate=-0.01085, entropy= 3.48131, loss=-0.01085
surrogate=-0.01537, entropy= 3.47733, loss=-0.01537
surrogate= 0.00275, entropy= 3.47483, loss= 0.00275
surrogate= 0.03207, entropy= 3.47379, loss= 0.03207
surrogate=-0.01504, entropy= 3.47085, loss=-0.01504
surrogate= 0.02836, entropy= 3.47003, loss= 0.02836
surrogate=-0.01723, entropy= 3.46575, loss=-0.01723
std_min= 0.70428, std_max= 0.82090, std_mean= 0.76977
val lr: [0.000234375], policy lr: [0.00028125]
Policy Loss: -0.017228, | Entropy Bonus: -0, | Value Loss: 112.93, | Advantage Loss: 6.5672
Time elapsed (s): 1.6778368949890137
Agent stdevs: 0.7697719
--------------------------------------------------------------------------------

Step 61
++++++++ Policy training ++++++++++
Current mean reward: 658.872991 | mean episode length: 212.333333
val_loss=168.17648
val_loss=245.39182
val_loss=263.24365
val_loss=155.56339
val_loss=180.30562
val_loss=102.06580
val_loss=127.41071
val_loss=110.67639
val_loss=113.26958
val_loss=174.92508
adv_loss=11.78300
adv_loss=20.23931
adv_loss= 2.43015
adv_loss= 6.44488
adv_loss= 3.84307
adv_loss= 3.78934
adv_loss= 4.48076
adv_loss= 6.26712
adv_loss= 7.63978
adv_loss= 5.08861
surrogate=-0.00941, entropy= 3.46128, loss=-0.00941
surrogate=-0.00393, entropy= 3.45633, loss=-0.00393
surrogate=-0.00032, entropy= 3.45225, loss=-0.00032
surrogate=-0.01139, entropy= 3.45113, loss=-0.01139
surrogate=-0.01310, entropy= 3.44658, loss=-0.01310
surrogate=-0.02256, entropy= 3.44147, loss=-0.02256
surrogate=-0.02295, entropy= 3.43599, loss=-0.02295
surrogate=-0.02431, entropy= 3.43401, loss=-0.02431
surrogate=-0.02543, entropy= 3.43126, loss=-0.02543
surrogate=-0.00434, entropy= 3.42872, loss=-0.00434
std_min= 0.69875, std_max= 0.80566, std_mean= 0.76007
val lr: [0.0002341188524590164], policy lr: [0.0002809426229508196]
Policy Loss: -0.0043415, | Entropy Bonus: -0, | Value Loss: 174.93, | Advantage Loss: 5.0886
Time elapsed (s): 1.6588091850280762
Agent stdevs: 0.7600735
--------------------------------------------------------------------------------

Step 62
++++++++ Policy training ++++++++++
Current mean reward: 754.978383 | mean episode length: 236.375000
val_loss=234.62732
val_loss=205.61900
val_loss=92.31844
val_loss=98.54395
val_loss=100.52987
val_loss=93.22269
val_loss=118.00324
val_loss=140.94591
val_loss=74.76441
val_loss=93.46915
adv_loss=17.17746
adv_loss= 6.73558
adv_loss= 3.84049
adv_loss= 6.54877
adv_loss= 8.72705
adv_loss= 6.87719
adv_loss= 4.38757
adv_loss= 3.58988
adv_loss= 1.25504
adv_loss= 6.48017
surrogate=-0.01218, entropy= 3.42392, loss=-0.01218
surrogate= 0.00757, entropy= 3.42181, loss= 0.00757
surrogate=-0.00831, entropy= 3.42167, loss=-0.00831
surrogate=-0.00421, entropy= 3.42131, loss=-0.00421
surrogate=-0.00520, entropy= 3.41981, loss=-0.00520
surrogate=-0.01095, entropy= 3.41952, loss=-0.01095
surrogate=-0.01750, entropy= 3.42225, loss=-0.01750
surrogate=-0.02163, entropy= 3.42318, loss=-0.02163
surrogate=-0.02576, entropy= 3.42361, loss=-0.02576
surrogate= 0.00893, entropy= 3.42510, loss= 0.00893
std_min= 0.70021, std_max= 0.79492, std_mean= 0.75906
val lr: [0.0002338627049180328], policy lr: [0.00028063524590163933]
Policy Loss: 0.0089327, | Entropy Bonus: -0, | Value Loss: 93.469, | Advantage Loss: 6.4802
Time elapsed (s): 1.7048840522766113
Agent stdevs: 0.75905925
--------------------------------------------------------------------------------

Step 63
++++++++ Policy training ++++++++++
Current mean reward: 710.282835 | mean episode length: 226.444444
val_loss=146.31441
val_loss=197.81747
val_loss=264.48407
val_loss=205.20499
val_loss=187.66417
val_loss=88.74693
val_loss=108.01159
val_loss=223.82533
val_loss=182.80511
val_loss=145.64401
adv_loss= 5.11826
adv_loss=13.01933
adv_loss= 3.45547
adv_loss= 3.29442
adv_loss= 2.63463
adv_loss=20.17210
adv_loss= 4.87595
adv_loss= 7.80559
adv_loss=14.09953
adv_loss= 4.51543
surrogate= 0.00697, entropy= 3.42398, loss= 0.00697
surrogate=-0.05520, entropy= 3.42734, loss=-0.05520
surrogate=-0.00874, entropy= 3.42897, loss=-0.00874
surrogate=-0.00223, entropy= 3.43121, loss=-0.00223
surrogate=-0.01665, entropy= 3.43252, loss=-0.01665
surrogate=-0.03482, entropy= 3.43457, loss=-0.03482
surrogate=-0.00595, entropy= 3.43318, loss=-0.00595
surrogate=-0.00283, entropy= 3.43300, loss=-0.00283
surrogate=-0.00631, entropy= 3.43204, loss=-0.00631
surrogate=-0.03258, entropy= 3.43292, loss=-0.03258
std_min= 0.70629, std_max= 0.79703, std_mean= 0.76088
val lr: [0.0002336065573770492], policy lr: [0.00028032786885245903]
Policy Loss: -0.032579, | Entropy Bonus: -0, | Value Loss: 145.64, | Advantage Loss: 4.5154
Time elapsed (s): 1.6937878131866455
Agent stdevs: 0.76088357
--------------------------------------------------------------------------------

Step 64
++++++++ Policy training ++++++++++
Current mean reward: 628.705798 | mean episode length: 204.777778
val_loss=179.38885
val_loss=175.31114
val_loss=161.44585
val_loss=116.75040
val_loss=135.18430
val_loss=108.09796
val_loss=97.95570
val_loss=100.06488
val_loss=75.38562
val_loss=105.38496
adv_loss= 8.20464
adv_loss= 5.37440
adv_loss= 5.77302
adv_loss=13.64565
adv_loss= 7.79951
adv_loss=11.06483
adv_loss= 8.46611
adv_loss=34.30222
adv_loss= 7.20957
adv_loss=20.10712
surrogate=-0.01406, entropy= 3.43067, loss=-0.01406
surrogate= 0.01026, entropy= 3.43169, loss= 0.01026
surrogate= 0.00961, entropy= 3.43147, loss= 0.00961
surrogate= 0.00254, entropy= 3.43138, loss= 0.00254
surrogate= 0.02738, entropy= 3.43077, loss= 0.02738
surrogate=-0.01203, entropy= 3.42914, loss=-0.01203
surrogate=-0.02231, entropy= 3.42834, loss=-0.02231
surrogate=-0.01152, entropy= 3.42954, loss=-0.01152
surrogate=-0.00531, entropy= 3.42773, loss=-0.00531
surrogate=-0.01315, entropy= 3.42673, loss=-0.01315
std_min= 0.70635, std_max= 0.80109, std_mean= 0.75931
val lr: [0.0002333504098360656], policy lr: [0.0002800204918032787]
Policy Loss: -0.013149, | Entropy Bonus: -0, | Value Loss: 105.38, | Advantage Loss: 20.107
Time elapsed (s): 1.66617751121521
Agent stdevs: 0.7593073
--------------------------------------------------------------------------------

Step 65
++++++++ Policy training ++++++++++
Current mean reward: 782.895485 | mean episode length: 250.750000
val_loss=70.75449
val_loss=111.50328
val_loss=63.74201
val_loss=119.19580
val_loss=36.67513
val_loss=54.85394
val_loss=87.45905
val_loss=62.04074
val_loss=212.24715
val_loss=115.55245
adv_loss= 3.99870
adv_loss= 7.81124
adv_loss= 4.08102
adv_loss= 4.77913
adv_loss= 8.48359
adv_loss= 8.38712
adv_loss= 8.63696
adv_loss= 5.38320
adv_loss= 5.17557
adv_loss= 2.19808
surrogate=-0.01026, entropy= 3.42286, loss=-0.01026
surrogate=-0.00125, entropy= 3.41804, loss=-0.00125
surrogate=-0.00158, entropy= 3.41572, loss=-0.00158
surrogate=-0.02792, entropy= 3.41636, loss=-0.02792
surrogate=-0.01593, entropy= 3.41185, loss=-0.01593
surrogate=-0.04160, entropy= 3.40876, loss=-0.04160
surrogate=-0.01935, entropy= 3.40338, loss=-0.01935
surrogate=-0.02545, entropy= 3.40313, loss=-0.02545
surrogate=-0.01309, entropy= 3.40060, loss=-0.01309
surrogate= 0.00515, entropy= 3.39964, loss= 0.00515
std_min= 0.70088, std_max= 0.78596, std_mean= 0.75235
val lr: [0.00023309426229508196], policy lr: [0.0002797131147540983]
Policy Loss: 0.0051549, | Entropy Bonus: -0, | Value Loss: 115.55, | Advantage Loss: 2.1981
Time elapsed (s): 1.6585862636566162
Agent stdevs: 0.7523486
--------------------------------------------------------------------------------

Step 66
++++++++ Policy training ++++++++++
Current mean reward: 799.511456 | mean episode length: 268.142857
val_loss=131.63441
val_loss=102.66519
val_loss=171.19330
val_loss=119.47957
val_loss=143.42506
val_loss=131.65799
val_loss=160.16005
val_loss=190.75279
val_loss=507.99509
val_loss=493.72900
adv_loss=11.50322
adv_loss= 8.99422
adv_loss= 8.12389
adv_loss=12.76097
adv_loss= 4.93317
adv_loss= 8.49439
adv_loss= 8.38841
adv_loss=12.66290
adv_loss= 8.02396
adv_loss=12.01259
surrogate=-0.02497, entropy= 3.39400, loss=-0.02497
surrogate=-0.01749, entropy= 3.39294, loss=-0.01749
surrogate=-0.00016, entropy= 3.39403, loss=-0.00016
surrogate=-0.02605, entropy= 3.39324, loss=-0.02605
surrogate= 0.00972, entropy= 3.39173, loss= 0.00972
surrogate=-0.00744, entropy= 3.39174, loss=-0.00744
surrogate=-0.01625, entropy= 3.39197, loss=-0.01625
surrogate=-0.01188, entropy= 3.39101, loss=-0.01188
surrogate=-0.03153, entropy= 3.39021, loss=-0.03153
surrogate=-0.02517, entropy= 3.38847, loss=-0.02517
std_min= 0.70184, std_max= 0.78354, std_mean= 0.74953
val lr: [0.00023283811475409836], policy lr: [0.00027940573770491797]
Policy Loss: -0.025174, | Entropy Bonus: -0, | Value Loss: 493.73, | Advantage Loss: 12.013
Time elapsed (s): 1.6605844497680664
Agent stdevs: 0.7495323
--------------------------------------------------------------------------------

Step 67
++++++++ Policy training ++++++++++
Current mean reward: 805.803359 | mean episode length: 253.250000
val_loss=124.77116
val_loss=75.98213
val_loss=86.95995
val_loss=62.92276
val_loss=77.40024
val_loss=42.08505
val_loss=81.33763
val_loss=67.05077
val_loss=72.57001
val_loss=43.08562
adv_loss= 4.01917
adv_loss=14.41023
adv_loss= 6.79933
adv_loss= 6.05846
adv_loss=11.19671
adv_loss= 7.52172
adv_loss=10.87716
adv_loss= 3.95850
adv_loss= 8.23519
adv_loss= 5.16301
surrogate= 0.00523, entropy= 3.38801, loss= 0.00523
surrogate=-0.03102, entropy= 3.38874, loss=-0.03102
surrogate=-0.01160, entropy= 3.38702, loss=-0.01160
surrogate=-0.01224, entropy= 3.38502, loss=-0.01224
surrogate=-0.00791, entropy= 3.38051, loss=-0.00791
surrogate=-0.02729, entropy= 3.37777, loss=-0.02729
surrogate=-0.01482, entropy= 3.37559, loss=-0.01482
surrogate=-0.02676, entropy= 3.37432, loss=-0.02676
surrogate=-0.03410, entropy= 3.37461, loss=-0.03410
surrogate=-0.01128, entropy= 3.37189, loss=-0.01128
std_min= 0.69049, std_max= 0.77756, std_mean= 0.74561
val lr: [0.00023258196721311476], policy lr: [0.00027909836065573767]
Policy Loss: -0.011279, | Entropy Bonus: -0, | Value Loss: 43.086, | Advantage Loss: 5.163
Time elapsed (s): 1.6758325099945068
Agent stdevs: 0.74560636
--------------------------------------------------------------------------------

Step 68
++++++++ Policy training ++++++++++
Current mean reward: 1086.925211 | mean episode length: 373.000000
val_loss=101.44000
val_loss=126.42924
val_loss=78.78857
val_loss=160.83578
val_loss=166.92145
val_loss=59.51317
val_loss=123.38799
val_loss=173.84369
val_loss=141.77899
val_loss=54.26860
adv_loss=12.57192
adv_loss= 3.67568
adv_loss= 3.72571
adv_loss= 6.62791
adv_loss= 1.38300
adv_loss= 4.32454
adv_loss= 5.48903
adv_loss= 3.36512
adv_loss= 7.22746
adv_loss= 8.72078
surrogate= 0.00078, entropy= 3.37099, loss= 0.00078
surrogate=-0.01395, entropy= 3.37198, loss=-0.01395
surrogate= 0.01264, entropy= 3.37070, loss= 0.01264
surrogate= 0.00017, entropy= 3.37141, loss= 0.00017
surrogate= 0.00249, entropy= 3.37240, loss= 0.00249
surrogate=-0.01811, entropy= 3.37085, loss=-0.01811
surrogate=-0.00018, entropy= 3.37005, loss=-0.00018
surrogate= 0.01276, entropy= 3.36990, loss= 0.01276
surrogate=-0.03656, entropy= 3.36853, loss=-0.03656
surrogate=-0.00957, entropy= 3.36819, loss=-0.00957
std_min= 0.69389, std_max= 0.77724, std_mean= 0.74455
val lr: [0.00023232581967213116], policy lr: [0.0002787909836065574]
Policy Loss: -0.0095713, | Entropy Bonus: -0, | Value Loss: 54.269, | Advantage Loss: 8.7208
Time elapsed (s): 1.728245496749878
Agent stdevs: 0.7445496
--------------------------------------------------------------------------------

Step 69
++++++++ Policy training ++++++++++
Current mean reward: 952.114658 | mean episode length: 317.200000
val_loss=102.88091
val_loss=77.51481
val_loss=120.32426
val_loss=79.00232
val_loss=133.30779
val_loss=74.91519
val_loss=34.00155
val_loss=96.84518
val_loss=73.86535
val_loss=93.61043
adv_loss=14.73165
adv_loss=13.39302
adv_loss= 5.42675
adv_loss= 7.35089
adv_loss=14.92084
adv_loss=12.12365
adv_loss=18.34706
adv_loss=18.63988
adv_loss= 9.83016
adv_loss= 5.73821
surrogate= 0.01089, entropy= 3.36420, loss= 0.01089
surrogate= 0.01868, entropy= 3.36010, loss= 0.01868
surrogate=-0.01363, entropy= 3.35594, loss=-0.01363
surrogate=-0.02251, entropy= 3.35492, loss=-0.02251
surrogate=-0.01609, entropy= 3.35206, loss=-0.01609
surrogate= 0.00035, entropy= 3.35080, loss= 0.00035
surrogate=-0.04309, entropy= 3.34957, loss=-0.04309
surrogate=-0.01659, entropy= 3.34775, loss=-0.01659
surrogate=-0.01643, entropy= 3.34615, loss=-0.01643
surrogate=-0.03525, entropy= 3.34473, loss=-0.03525
std_min= 0.67517, std_max= 0.79055, std_mean= 0.73943
val lr: [0.00023206967213114754], policy lr: [0.000278483606557377]
Policy Loss: -0.035255, | Entropy Bonus: -0, | Value Loss: 93.61, | Advantage Loss: 5.7382
Time elapsed (s): 1.686354398727417
Agent stdevs: 0.73942655
--------------------------------------------------------------------------------

Step 70
++++++++ Policy training ++++++++++
Current mean reward: 558.217875 | mean episode length: 181.444444
val_loss=421.54181
val_loss=403.82288
val_loss=314.57098
val_loss=177.22543
val_loss=292.57135
val_loss=366.11453
val_loss=301.90604
val_loss=160.12820
val_loss=441.05045
val_loss=251.40784
adv_loss=16.20827
adv_loss=11.58711
adv_loss=13.35285
adv_loss=12.20749
adv_loss=241.74689
adv_loss=17.03462
adv_loss= 5.83022
adv_loss=30.79568
adv_loss=12.84435
adv_loss=40.37365
surrogate=-0.00117, entropy= 3.34841, loss=-0.00117
surrogate= 0.00962, entropy= 3.35576, loss= 0.00962
surrogate= 0.00086, entropy= 3.35637, loss= 0.00086
surrogate=-0.01292, entropy= 3.36100, loss=-0.01292
surrogate=-0.00123, entropy= 3.36155, loss=-0.00123
surrogate=-0.00331, entropy= 3.36212, loss=-0.00331
surrogate=-0.01817, entropy= 3.36262, loss=-0.01817
surrogate= 0.01650, entropy= 3.36178, loss= 0.01650
surrogate=-0.00496, entropy= 3.36153, loss=-0.00496
surrogate=-0.01005, entropy= 3.36305, loss=-0.01005
std_min= 0.68244, std_max= 0.79344, std_mean= 0.74382
val lr: [0.00023181352459016394], policy lr: [0.0002781762295081967]
Policy Loss: -0.010049, | Entropy Bonus: -0, | Value Loss: 251.41, | Advantage Loss: 40.374
Time elapsed (s): 1.6529467105865479
Agent stdevs: 0.7438185
--------------------------------------------------------------------------------

Step 71
++++++++ Policy training ++++++++++
Current mean reward: 798.763705 | mean episode length: 263.285714
val_loss=100.69738
val_loss=81.26408
val_loss=102.46713
val_loss=110.77738
val_loss=86.46685
val_loss=63.34131
val_loss=85.19841
val_loss=93.88522
val_loss=99.11855
val_loss=96.06557
adv_loss=12.41004
adv_loss=11.07960
adv_loss=25.21469
adv_loss= 3.89583
adv_loss= 5.48270
adv_loss=12.89374
adv_loss= 4.74965
adv_loss=15.34213
adv_loss= 9.95774
adv_loss= 8.66056
surrogate=-0.01716, entropy= 3.36097, loss=-0.01716
surrogate= 0.00071, entropy= 3.35670, loss= 0.00071
surrogate=-0.01400, entropy= 3.35148, loss=-0.01400
surrogate= 0.00919, entropy= 3.34820, loss= 0.00919
surrogate=-0.00327, entropy= 3.34729, loss=-0.00327
surrogate=-0.01253, entropy= 3.34364, loss=-0.01253
surrogate=-0.01812, entropy= 3.33986, loss=-0.01812
surrogate=-0.03593, entropy= 3.33623, loss=-0.03593
surrogate=-0.01880, entropy= 3.33465, loss=-0.01880
surrogate=-0.03456, entropy= 3.33260, loss=-0.03456
std_min= 0.68391, std_max= 0.78010, std_mean= 0.73598
val lr: [0.00023155737704918034], policy lr: [0.00027786885245901637]
Policy Loss: -0.034558, | Entropy Bonus: -0, | Value Loss: 96.066, | Advantage Loss: 8.6606
Time elapsed (s): 1.6598896980285645
Agent stdevs: 0.7359764
--------------------------------------------------------------------------------

Step 72
++++++++ Policy training ++++++++++
Current mean reward: 755.302641 | mean episode length: 244.625000
val_loss=401.95590
val_loss=122.27835
val_loss=217.35117
val_loss=196.73840
val_loss=84.16860
val_loss=316.66989
val_loss=145.90414
val_loss=118.98427
val_loss=141.89934
val_loss=143.79559
adv_loss= 7.65221
adv_loss= 8.51588
adv_loss= 9.72480
adv_loss=30.16977
adv_loss=17.59405
adv_loss=50.80787
adv_loss= 5.48783
adv_loss=12.55901
adv_loss= 6.58324
adv_loss=12.87547
surrogate=-0.00089, entropy= 3.33519, loss=-0.00089
surrogate=-0.02250, entropy= 3.33715, loss=-0.02250
surrogate=-0.00925, entropy= 3.33817, loss=-0.00925
surrogate=-0.01778, entropy= 3.33907, loss=-0.01778
surrogate= 0.01024, entropy= 3.34029, loss= 0.01024
surrogate=-0.03084, entropy= 3.34336, loss=-0.03084
surrogate= 0.00785, entropy= 3.34454, loss= 0.00785
surrogate=-0.02246, entropy= 3.34589, loss=-0.02246
surrogate= 0.01047, entropy= 3.34838, loss= 0.01047
surrogate=-0.03038, entropy= 3.34988, loss=-0.03038
std_min= 0.68624, std_max= 0.78139, std_mean= 0.74018
val lr: [0.00023130122950819674], policy lr: [0.00027756147540983607]
Policy Loss: -0.030378, | Entropy Bonus: -0, | Value Loss: 143.8, | Advantage Loss: 12.875
Time elapsed (s): 1.661384105682373
Agent stdevs: 0.7401809
--------------------------------------------------------------------------------

Step 73
++++++++ Policy training ++++++++++
Current mean reward: 666.268290 | mean episode length: 213.428571
val_loss=335.82971
val_loss=117.84865
val_loss=280.89249
val_loss=180.82806
val_loss=92.19096
val_loss=212.35544
val_loss=126.87046
val_loss=118.05836
val_loss=185.98331
val_loss=87.81258
adv_loss=14.06983
adv_loss= 6.71692
adv_loss= 7.65066
adv_loss=14.40095
adv_loss=12.70898
adv_loss= 5.87352
adv_loss=12.69042
adv_loss=13.89527
adv_loss=17.98860
adv_loss= 9.01331
surrogate=-0.00036, entropy= 3.35095, loss=-0.00036
surrogate=-0.03382, entropy= 3.35544, loss=-0.03382
surrogate=-0.00561, entropy= 3.35746, loss=-0.00561
surrogate=-0.01152, entropy= 3.35869, loss=-0.01152
surrogate=-0.02442, entropy= 3.35959, loss=-0.02442
surrogate=-0.00604, entropy= 3.36439, loss=-0.00604
surrogate=-0.02914, entropy= 3.36477, loss=-0.02914
surrogate=-0.00100, entropy= 3.36369, loss=-0.00100
surrogate=-0.01303, entropy= 3.36585, loss=-0.01303
surrogate= 0.00337, entropy= 3.36548, loss= 0.00337
std_min= 0.68691, std_max= 0.78932, std_mean= 0.74422
val lr: [0.00023104508196721314], policy lr: [0.0002772540983606557]
Policy Loss: 0.0033684, | Entropy Bonus: -0, | Value Loss: 87.813, | Advantage Loss: 9.0133
Time elapsed (s): 1.6581265926361084
Agent stdevs: 0.7442224
--------------------------------------------------------------------------------

Step 74
++++++++ Policy training ++++++++++
Current mean reward: 966.077882 | mean episode length: 316.833333
val_loss=74.54926
val_loss=93.11668
val_loss=44.03115
val_loss=96.33305
val_loss=51.55014
val_loss=137.88451
val_loss=210.63547
val_loss=43.15704
val_loss=68.12189
val_loss=95.81364
adv_loss= 9.76591
adv_loss= 5.43816
adv_loss= 8.33998
adv_loss= 8.58356
adv_loss=10.32327
adv_loss= 8.79832
adv_loss= 3.79021
adv_loss= 6.26053
adv_loss=11.32534
adv_loss=14.29601
surrogate= 0.01344, entropy= 3.36305, loss= 0.01344
surrogate= 0.00245, entropy= 3.35733, loss= 0.00245
surrogate= 0.00611, entropy= 3.35482, loss= 0.00611
surrogate= 0.01743, entropy= 3.34939, loss= 0.01743
surrogate=-0.00296, entropy= 3.34519, loss=-0.00296
surrogate=-0.00012, entropy= 3.34357, loss=-0.00012
surrogate=-0.00860, entropy= 3.33902, loss=-0.00860
surrogate= 0.02172, entropy= 3.33596, loss= 0.02172
surrogate= 0.01138, entropy= 3.33122, loss= 0.01138
surrogate= 0.01271, entropy= 3.32758, loss= 0.01271
std_min= 0.66851, std_max= 0.79025, std_mean= 0.73534
val lr: [0.0002307889344262295], policy lr: [0.00027694672131147537]
Policy Loss: 0.012715, | Entropy Bonus: -0, | Value Loss: 95.814, | Advantage Loss: 14.296
Time elapsed (s): 1.727973461151123
Agent stdevs: 0.735344
--------------------------------------------------------------------------------

Step 75
++++++++ Policy training ++++++++++
Current mean reward: 795.073614 | mean episode length: 259.285714
val_loss=106.77252
val_loss=79.46664
val_loss=77.15945
val_loss=93.31915
val_loss=102.45276
val_loss=73.19824
val_loss=93.90781
val_loss=43.34876
val_loss=78.57027
val_loss=59.79508
adv_loss=12.38139
adv_loss= 3.88325
adv_loss= 4.67291
adv_loss=11.42722
adv_loss=12.12570
adv_loss= 9.83496
adv_loss=17.30813
adv_loss= 9.71085
adv_loss=25.87174
adv_loss= 3.66845
surrogate= 0.00907, entropy= 3.32782, loss= 0.00907
surrogate=-0.00555, entropy= 3.32742, loss=-0.00555
surrogate=-0.01719, entropy= 3.32876, loss=-0.01719
surrogate=-0.02488, entropy= 3.32669, loss=-0.02488
surrogate= 0.00702, entropy= 3.32676, loss= 0.00702
surrogate=-0.02168, entropy= 3.32481, loss=-0.02168
surrogate=-0.02798, entropy= 3.32509, loss=-0.02798
surrogate=-0.01878, entropy= 3.32494, loss=-0.01878
surrogate=-0.02709, entropy= 3.32540, loss=-0.02709
surrogate=-0.00528, entropy= 3.32393, loss=-0.00528
std_min= 0.66723, std_max= 0.79857, std_mean= 0.73470
val lr: [0.0002305327868852459], policy lr: [0.00027663934426229507]
Policy Loss: -0.0052751, | Entropy Bonus: -0, | Value Loss: 59.795, | Advantage Loss: 3.6685
Time elapsed (s): 1.6720690727233887
Agent stdevs: 0.73470277
--------------------------------------------------------------------------------

Step 76
++++++++ Policy training ++++++++++
Current mean reward: 718.688946 | mean episode length: 229.125000
val_loss=191.32320
val_loss=112.77392
val_loss=137.32726
val_loss=138.56825
val_loss=41.26794
val_loss=65.02605
val_loss=39.18382
val_loss=129.80508
val_loss=80.91297
val_loss=164.92050
adv_loss= 8.90687
adv_loss=56.90986
adv_loss=11.38683
adv_loss=10.16774
adv_loss=16.33224
adv_loss= 6.18371
adv_loss= 5.67468
adv_loss=16.69563
adv_loss=14.36307
adv_loss= 7.30647
surrogate= 0.00460, entropy= 3.32372, loss= 0.00460
surrogate=-0.01538, entropy= 3.32109, loss=-0.01538
surrogate=-0.00588, entropy= 3.31928, loss=-0.00588
surrogate= 0.00414, entropy= 3.32071, loss= 0.00414
surrogate=-0.02356, entropy= 3.31944, loss=-0.02356
surrogate= 0.01607, entropy= 3.31831, loss= 0.01607
surrogate=-0.02310, entropy= 3.31862, loss=-0.02310
surrogate=-0.01325, entropy= 3.31736, loss=-0.01325
surrogate= 0.00032, entropy= 3.31710, loss= 0.00032
surrogate=-0.00663, entropy= 3.31770, loss=-0.00663
std_min= 0.66950, std_max= 0.79234, std_mean= 0.73294
val lr: [0.0002302766393442623], policy lr: [0.0002763319672131147]
Policy Loss: -0.0066333, | Entropy Bonus: -0, | Value Loss: 164.92, | Advantage Loss: 7.3065
Time elapsed (s): 1.6633925437927246
Agent stdevs: 0.73293906
--------------------------------------------------------------------------------

Step 77
++++++++ Policy training ++++++++++
Current mean reward: 775.096765 | mean episode length: 244.428571
val_loss=157.13850
val_loss=98.19974
val_loss=101.74435
val_loss=109.97860
val_loss=129.69666
val_loss=99.69292
val_loss=45.44612
val_loss=105.65817
val_loss=73.68594
val_loss=110.09514
adv_loss=23.27142
adv_loss= 9.32142
adv_loss= 7.16353
adv_loss=12.02887
adv_loss= 6.03233
adv_loss= 6.88733
adv_loss= 9.30402
adv_loss= 2.36157
adv_loss= 6.62803
adv_loss= 9.91917
surrogate=-0.01465, entropy= 3.31276, loss=-0.01465
surrogate= 0.02279, entropy= 3.31075, loss= 0.02279
surrogate= 0.01287, entropy= 3.30726, loss= 0.01287
surrogate=-0.00544, entropy= 3.30233, loss=-0.00544
surrogate=-0.00482, entropy= 3.29844, loss=-0.00482
surrogate=-0.02614, entropy= 3.29409, loss=-0.02614
surrogate=-0.01695, entropy= 3.29228, loss=-0.01695
surrogate= 0.01698, entropy= 3.28808, loss= 0.01698
surrogate= 0.00394, entropy= 3.28784, loss= 0.00394
surrogate=-0.02748, entropy= 3.28658, loss=-0.02748
std_min= 0.65668, std_max= 0.78799, std_mean= 0.72567
val lr: [0.00023002049180327868], policy lr: [0.0002760245901639344]
Policy Loss: -0.027481, | Entropy Bonus: -0, | Value Loss: 110.1, | Advantage Loss: 9.9192
Time elapsed (s): 1.6628901958465576
Agent stdevs: 0.72567004
--------------------------------------------------------------------------------

Step 78
++++++++ Policy training ++++++++++
Current mean reward: 970.369608 | mean episode length: 313.833333
val_loss=44.97442
val_loss=41.93380
val_loss=33.88317
val_loss=47.66708
val_loss=58.56241
val_loss=21.48027
val_loss=56.40743
val_loss=27.36733
val_loss=38.17747
val_loss=36.24303
adv_loss=10.35178
adv_loss= 7.71499
adv_loss= 9.70505
adv_loss= 3.00499
adv_loss=16.52632
adv_loss= 5.31220
adv_loss= 5.82778
adv_loss= 1.49468
adv_loss= 6.09769
adv_loss= 5.47073
surrogate= 0.01727, entropy= 3.28485, loss= 0.01727
surrogate=-0.00753, entropy= 3.28527, loss=-0.00753
surrogate= 0.00490, entropy= 3.28695, loss= 0.00490
surrogate=-0.01142, entropy= 3.28781, loss=-0.01142
surrogate=-0.00963, entropy= 3.28805, loss=-0.00963
surrogate=-0.00509, entropy= 3.28911, loss=-0.00509
surrogate=-0.01371, entropy= 3.29126, loss=-0.01371
surrogate= 0.00196, entropy= 3.29006, loss= 0.00196
surrogate=-0.03221, entropy= 3.29224, loss=-0.03221
surrogate=-0.00991, entropy= 3.29225, loss=-0.00991
std_min= 0.66575, std_max= 0.78691, std_mean= 0.72671
val lr: [0.00022976434426229508], policy lr: [0.00027571721311475406]
Policy Loss: -0.0099114, | Entropy Bonus: -0, | Value Loss: 36.243, | Advantage Loss: 5.4707
Time elapsed (s): 1.6670148372650146
Agent stdevs: 0.7267105
--------------------------------------------------------------------------------

Step 79
++++++++ Policy training ++++++++++
Current mean reward: 1113.780201 | mean episode length: 356.600000
val_loss=30.60257
val_loss=35.12642
val_loss=19.94515
val_loss=58.47635
val_loss=63.42873
val_loss=48.75593
val_loss=47.28628
val_loss=41.14533
val_loss=25.29150
val_loss=40.27308
adv_loss= 6.23950
adv_loss= 8.54213
adv_loss= 5.03513
adv_loss= 4.45538
adv_loss= 7.88489
adv_loss=15.30622
adv_loss= 3.64144
adv_loss= 4.01976
adv_loss=11.45857
adv_loss= 5.15707
surrogate=-0.00807, entropy= 3.29257, loss=-0.00807
surrogate=-0.02106, entropy= 3.29631, loss=-0.02106
surrogate=-0.01263, entropy= 3.29885, loss=-0.01263
surrogate= 0.01125, entropy= 3.29977, loss= 0.01125
surrogate=-0.03019, entropy= 3.30260, loss=-0.03019
surrogate= 0.00898, entropy= 3.30575, loss= 0.00898
surrogate=-0.00690, entropy= 3.30609, loss=-0.00690
surrogate= 0.00620, entropy= 3.30631, loss= 0.00620
surrogate=-0.00755, entropy= 3.30583, loss=-0.00755
surrogate= 0.04074, entropy= 3.30618, loss= 0.04074
std_min= 0.66452, std_max= 0.78841, std_mean= 0.73021
val lr: [0.00022950819672131148], policy lr: [0.00027540983606557377]
Policy Loss: 0.040736, | Entropy Bonus: -0, | Value Loss: 40.273, | Advantage Loss: 5.1571
Time elapsed (s): 1.6860430240631104
Agent stdevs: 0.73020846
--------------------------------------------------------------------------------

Step 80
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 874.6
++++++++ Policy training ++++++++++
Current mean reward: 1096.575706 | mean episode length: 350.400000
val_loss=61.94904
val_loss=117.99738
val_loss=39.66773
val_loss=82.93520
val_loss=51.58232
val_loss=32.86478
val_loss=29.49668
val_loss=15.53960
val_loss=22.12613
val_loss=16.59188
adv_loss= 2.85176
adv_loss= 8.10234
adv_loss= 2.98270
adv_loss= 4.22038
adv_loss= 4.29507
adv_loss= 1.99060
adv_loss= 1.61088
adv_loss= 2.99120
adv_loss= 5.93849
adv_loss= 8.59217
surrogate=-0.02305, entropy= 3.30182, loss=-0.02305
surrogate= 0.00735, entropy= 3.29800, loss= 0.00735
surrogate=-0.01430, entropy= 3.29288, loss=-0.01430
surrogate=-0.02970, entropy= 3.29026, loss=-0.02970
surrogate= 0.00215, entropy= 3.28642, loss= 0.00215
surrogate=-0.01259, entropy= 3.28287, loss=-0.01259
surrogate= 0.01093, entropy= 3.27735, loss= 0.01093
surrogate= 0.02310, entropy= 3.27562, loss= 0.02310
surrogate=-0.03172, entropy= 3.27357, loss=-0.03172
surrogate=-0.02102, entropy= 3.27201, loss=-0.02102
std_min= 0.65544, std_max= 0.77465, std_mean= 0.72188
val lr: [0.00022925204918032788], policy lr: [0.0002751024590163934]
Policy Loss: -0.02102, | Entropy Bonus: -0, | Value Loss: 16.592, | Advantage Loss: 8.5922
Time elapsed (s): 1.7112858295440674
Agent stdevs: 0.72187966
--------------------------------------------------------------------------------

Step 81
++++++++ Policy training ++++++++++
Current mean reward: 1243.737904 | mean episode length: 404.000000
val_loss=129.59233
val_loss=106.82882
val_loss=124.27418
val_loss=64.55251
val_loss=17.19854
val_loss=37.99502
val_loss=37.13315
val_loss=69.39184
val_loss=68.08593
val_loss=72.96895
adv_loss= 2.07848
adv_loss= 8.42004
adv_loss= 9.73998
adv_loss= 3.76535
adv_loss= 7.75002
adv_loss= 8.58277
adv_loss= 3.11366
adv_loss= 3.36560
adv_loss= 3.59041
adv_loss= 2.31102
surrogate= 0.00092, entropy= 3.26858, loss= 0.00092
surrogate=-0.00855, entropy= 3.26362, loss=-0.00855
surrogate= 0.01046, entropy= 3.25854, loss= 0.01046
surrogate=-0.02818, entropy= 3.25377, loss=-0.02818
surrogate=-0.01747, entropy= 3.24994, loss=-0.01747
surrogate=-0.00747, entropy= 3.24814, loss=-0.00747
surrogate=-0.02929, entropy= 3.24903, loss=-0.02929
surrogate=-0.03761, entropy= 3.24706, loss=-0.03761
surrogate=-0.02417, entropy= 3.24689, loss=-0.02417
surrogate=-0.03500, entropy= 3.24563, loss=-0.03500
std_min= 0.64217, std_max= 0.77470, std_mean= 0.71601
val lr: [0.00022899590163934428], policy lr: [0.0002747950819672131]
Policy Loss: -0.035, | Entropy Bonus: -0, | Value Loss: 72.969, | Advantage Loss: 2.311
Time elapsed (s): 1.682884931564331
Agent stdevs: 0.71600944
--------------------------------------------------------------------------------

Step 82
++++++++ Policy training ++++++++++
Current mean reward: 970.681646 | mean episode length: 312.333333
val_loss=46.73413
val_loss=686.68231
val_loss=344.08044
val_loss=364.65155
val_loss=86.78139
val_loss=589.27008
val_loss=61.94005
val_loss=208.62276
val_loss=58.46856
val_loss=249.82950
adv_loss= 2.99191
adv_loss=10.18177
adv_loss= 6.17427
adv_loss= 4.60403
adv_loss= 4.00680
adv_loss= 3.38289
adv_loss= 6.67178
adv_loss=997.87109
adv_loss= 3.76936
adv_loss= 3.83867
surrogate= 0.03346, entropy= 3.24319, loss= 0.03346
surrogate=-0.03307, entropy= 3.24197, loss=-0.03307
surrogate= 0.01860, entropy= 3.23785, loss= 0.01860
surrogate= 0.02404, entropy= 3.23276, loss= 0.02404
surrogate=-0.01125, entropy= 3.22944, loss=-0.01125
surrogate=-0.01457, entropy= 3.22689, loss=-0.01457
surrogate=-0.01058, entropy= 3.22234, loss=-0.01058
surrogate=-0.01638, entropy= 3.22167, loss=-0.01638
surrogate= 0.02281, entropy= 3.22148, loss= 0.02281
surrogate=-0.03132, entropy= 3.21898, loss=-0.03132
std_min= 0.63590, std_max= 0.77074, std_mean= 0.70979
val lr: [0.00022873975409836068], policy lr: [0.00027448770491803276]
Policy Loss: -0.031316, | Entropy Bonus: -0, | Value Loss: 249.83, | Advantage Loss: 3.8387
Time elapsed (s): 1.6830730438232422
Agent stdevs: 0.70979285
--------------------------------------------------------------------------------

Step 83
++++++++ Policy training ++++++++++
Current mean reward: 1201.885456 | mean episode length: 376.400000
val_loss=98.60126
val_loss=44.46089
val_loss=61.11040
val_loss=51.98617
val_loss=25.67505
val_loss=72.85560
val_loss=35.52159
val_loss=53.00643
val_loss=36.42648
val_loss=87.62277
adv_loss= 2.49070
adv_loss= 3.92811
adv_loss= 4.50595
adv_loss= 5.12980
adv_loss= 4.10051
adv_loss= 5.17746
adv_loss= 4.40061
adv_loss= 7.95075
adv_loss= 4.27974
adv_loss= 5.58841
surrogate= 0.03166, entropy= 3.21712, loss= 0.03166
surrogate=-0.00746, entropy= 3.21461, loss=-0.00746
surrogate= 0.01289, entropy= 3.21114, loss= 0.01289
surrogate=-0.01734, entropy= 3.21029, loss=-0.01734
surrogate=-0.01733, entropy= 3.20779, loss=-0.01733
surrogate=-0.03341, entropy= 3.20779, loss=-0.03341
surrogate=-0.00771, entropy= 3.20490, loss=-0.00771
surrogate=-0.04608, entropy= 3.20141, loss=-0.04608
surrogate= 0.00533, entropy= 3.20102, loss= 0.00533
surrogate=-0.01979, entropy= 3.19891, loss=-0.01979
std_min= 0.62355, std_max= 0.78223, std_mean= 0.70584
val lr: [0.00022848360655737705], policy lr: [0.0002741803278688524]
Policy Loss: -0.019794, | Entropy Bonus: -0, | Value Loss: 87.623, | Advantage Loss: 5.5884
Time elapsed (s): 1.662520408630371
Agent stdevs: 0.705839
--------------------------------------------------------------------------------

Step 84
++++++++ Policy training ++++++++++
Current mean reward: 915.451955 | mean episode length: 295.000000
val_loss=29.64145
val_loss=37.82144
val_loss=137.40643
val_loss=137.65678
val_loss=26.77743
val_loss=59.10507
val_loss=47.52560
val_loss=47.52169
val_loss=35.96801
val_loss=39.69837
adv_loss= 8.83783
adv_loss= 6.34026
adv_loss= 6.15705
adv_loss= 3.29251
adv_loss=29.67511
adv_loss= 6.03393
adv_loss= 9.40625
adv_loss=11.33544
adv_loss=13.78740
adv_loss=12.06990
surrogate= 0.01546, entropy= 3.19719, loss= 0.01546
surrogate= 0.01457, entropy= 3.19847, loss= 0.01457
surrogate=-0.02234, entropy= 3.19930, loss=-0.02234
surrogate=-0.02127, entropy= 3.19998, loss=-0.02127
surrogate=-0.03568, entropy= 3.20013, loss=-0.03568
surrogate=-0.00767, entropy= 3.19910, loss=-0.00767
surrogate=-0.01245, entropy= 3.19705, loss=-0.01245
surrogate=-0.03199, entropy= 3.19629, loss=-0.03199
surrogate=-0.02186, entropy= 3.19578, loss=-0.02186
surrogate=-0.02718, entropy= 3.19621, loss=-0.02718
std_min= 0.62277, std_max= 0.79047, std_mean= 0.70556
val lr: [0.00022822745901639345], policy lr: [0.0002738729508196721]
Policy Loss: -0.027182, | Entropy Bonus: -0, | Value Loss: 39.698, | Advantage Loss: 12.07
Time elapsed (s): 1.694821834564209
Agent stdevs: 0.7055587
--------------------------------------------------------------------------------

Step 85
++++++++ Policy training ++++++++++
Current mean reward: 982.071768 | mean episode length: 303.666667
val_loss=61.01997
val_loss=56.99509
val_loss=41.95657
val_loss=107.91087
val_loss=84.47744
val_loss=45.79097
val_loss=57.02256
val_loss=68.61464
val_loss=142.30017
val_loss=48.08906
adv_loss=10.60540
adv_loss= 7.70181
adv_loss= 3.91945
adv_loss= 4.50008
adv_loss= 6.43325
adv_loss= 5.08058
adv_loss= 9.40225
adv_loss= 9.53043
adv_loss= 6.34523
adv_loss= 9.66484
surrogate=-0.02204, entropy= 3.19434, loss=-0.02204
surrogate= 0.00059, entropy= 3.19211, loss= 0.00059
surrogate=-0.01338, entropy= 3.18712, loss=-0.01338
surrogate= 0.03084, entropy= 3.18403, loss= 0.03084
surrogate= 0.01578, entropy= 3.18002, loss= 0.01578
surrogate=-0.01716, entropy= 3.17693, loss=-0.01716
surrogate=-0.03026, entropy= 3.17338, loss=-0.03026
surrogate=-0.00417, entropy= 3.17280, loss=-0.00417
surrogate=-0.01958, entropy= 3.16923, loss=-0.01958
surrogate=-0.01654, entropy= 3.16678, loss=-0.01654
std_min= 0.61786, std_max= 0.78009, std_mean= 0.69851
val lr: [0.00022797131147540983], policy lr: [0.00027356557377049176]
Policy Loss: -0.016539, | Entropy Bonus: -0, | Value Loss: 48.089, | Advantage Loss: 9.6648
Time elapsed (s): 1.6947786808013916
Agent stdevs: 0.69850636
--------------------------------------------------------------------------------

Step 86
++++++++ Policy training ++++++++++
Current mean reward: 1394.880639 | mean episode length: 430.250000
val_loss=64.84923
val_loss=121.68646
val_loss=152.84538
val_loss=125.71425
val_loss=49.45398
val_loss=39.73896
val_loss=40.08998
val_loss=22.15206
val_loss=61.07981
val_loss=26.91707
adv_loss= 6.55359
adv_loss= 4.01721
adv_loss= 4.31403
adv_loss= 2.86239
adv_loss= 6.24348
adv_loss= 5.36008
adv_loss= 6.03834
adv_loss= 5.40906
adv_loss= 5.56774
adv_loss= 8.83218
surrogate=-0.00223, entropy= 3.16717, loss=-0.00223
surrogate=-0.01794, entropy= 3.16279, loss=-0.01794
surrogate= 0.01579, entropy= 3.16180, loss= 0.01579
surrogate=-0.03766, entropy= 3.15904, loss=-0.03766
surrogate= 0.00686, entropy= 3.15624, loss= 0.00686
surrogate=-0.00792, entropy= 3.15318, loss=-0.00792
surrogate=-0.00063, entropy= 3.14949, loss=-0.00063
surrogate=-0.03718, entropy= 3.14706, loss=-0.03718
surrogate=-0.00851, entropy= 3.14443, loss=-0.00851
surrogate= 0.01160, entropy= 3.14445, loss= 0.01160
std_min= 0.61803, std_max= 0.76215, std_mean= 0.69270
val lr: [0.00022771516393442623], policy lr: [0.00027325819672131146]
Policy Loss: 0.011604, | Entropy Bonus: -0, | Value Loss: 26.917, | Advantage Loss: 8.8322
Time elapsed (s): 1.7346382141113281
Agent stdevs: 0.69269794
--------------------------------------------------------------------------------

Step 87
++++++++ Policy training ++++++++++
Current mean reward: 1227.262548 | mean episode length: 389.600000
val_loss=262.59201
val_loss=327.57535
val_loss=331.82047
val_loss=70.92126
val_loss=116.00817
val_loss=630.32886
val_loss=203.35858
val_loss=440.02289
val_loss=606.16052
val_loss=448.09436
adv_loss=10.88689
adv_loss=13.11553
adv_loss= 4.03011
adv_loss= 8.09616
adv_loss= 3.82196
adv_loss=14.43822
adv_loss= 7.91633
adv_loss=26.72799
adv_loss= 8.90182
adv_loss= 2.51610
surrogate=-0.00497, entropy= 3.14404, loss=-0.00497
surrogate=-0.01931, entropy= 3.14111, loss=-0.01931
surrogate= 0.00153, entropy= 3.14039, loss= 0.00153
surrogate=-0.01121, entropy= 3.13986, loss=-0.01121
surrogate=-0.03112, entropy= 3.14041, loss=-0.03112
surrogate=-0.02547, entropy= 3.13991, loss=-0.02547
surrogate=-0.01905, entropy= 3.13977, loss=-0.01905
surrogate=-0.00844, entropy= 3.13776, loss=-0.00844
surrogate= 0.00747, entropy= 3.13746, loss= 0.00747
surrogate=-0.04025, entropy= 3.13710, loss=-0.04025
std_min= 0.61188, std_max= 0.77033, std_mean= 0.69156
val lr: [0.00022745901639344263], policy lr: [0.0002729508196721311]
Policy Loss: -0.040252, | Entropy Bonus: -0, | Value Loss: 448.09, | Advantage Loss: 2.5161
Time elapsed (s): 1.6827099323272705
Agent stdevs: 0.691561
--------------------------------------------------------------------------------

Step 88
++++++++ Policy training ++++++++++
Current mean reward: 1215.744420 | mean episode length: 395.000000
val_loss=20.70086
val_loss=47.90873
val_loss=33.72259
val_loss=26.13029
val_loss=54.77448
val_loss=51.54231
val_loss=40.81741
val_loss=32.46170
val_loss=30.08080
val_loss=13.94687
adv_loss= 4.71414
adv_loss= 6.45590
adv_loss= 9.24051
adv_loss= 3.97978
adv_loss= 5.84641
adv_loss= 5.73935
adv_loss= 3.97853
adv_loss= 1.51281
adv_loss= 5.30982
adv_loss= 2.44611
surrogate=-0.00033, entropy= 3.13939, loss=-0.00033
surrogate= 0.00531, entropy= 3.13904, loss= 0.00531
surrogate=-0.02576, entropy= 3.13898, loss=-0.02576
surrogate=-0.01701, entropy= 3.13706, loss=-0.01701
surrogate=-0.00666, entropy= 3.13979, loss=-0.00666
surrogate=-0.04520, entropy= 3.13913, loss=-0.04520
surrogate=-0.03341, entropy= 3.13789, loss=-0.03341
surrogate=-0.02466, entropy= 3.13690, loss=-0.02466
surrogate=-0.02898, entropy= 3.14002, loss=-0.02898
surrogate=-0.01157, entropy= 3.13956, loss=-0.01157
std_min= 0.61041, std_max= 0.77233, std_mean= 0.69225
val lr: [0.00022720286885245903], policy lr: [0.0002726434426229508]
Policy Loss: -0.011566, | Entropy Bonus: -0, | Value Loss: 13.947, | Advantage Loss: 2.4461
Time elapsed (s): 1.6640844345092773
Agent stdevs: 0.6922545
--------------------------------------------------------------------------------

Step 89
++++++++ Policy training ++++++++++
Current mean reward: 1267.625227 | mean episode length: 396.000000
val_loss=27.61083
val_loss=31.31343
val_loss=39.77021
val_loss=35.55700
val_loss=29.85281
val_loss=30.15071
val_loss=19.99208
val_loss=17.72909
val_loss=20.68336
val_loss=26.19456
adv_loss= 1.76077
adv_loss= 4.64221
adv_loss= 2.78586
adv_loss= 2.79422
adv_loss= 5.23144
adv_loss= 4.61534
adv_loss= 2.30277
adv_loss= 3.82678
adv_loss= 7.53355
adv_loss= 5.38629
surrogate=-0.00951, entropy= 3.13710, loss=-0.00951
surrogate= 0.01545, entropy= 3.13855, loss= 0.01545
surrogate=-0.01088, entropy= 3.13512, loss=-0.01088
surrogate=-0.00218, entropy= 3.13543, loss=-0.00218
surrogate=-0.02813, entropy= 3.13298, loss=-0.02813
surrogate=-0.03282, entropy= 3.13169, loss=-0.03282
surrogate=-0.01632, entropy= 3.13227, loss=-0.01632
surrogate=-0.03184, entropy= 3.12966, loss=-0.03184
surrogate=-0.00280, entropy= 3.12804, loss=-0.00280
surrogate=-0.04629, entropy= 3.12565, loss=-0.04629
std_min= 0.59932, std_max= 0.77693, std_mean= 0.68975
val lr: [0.00022694672131147543], policy lr: [0.00027233606557377046]
Policy Loss: -0.046292, | Entropy Bonus: -0, | Value Loss: 26.195, | Advantage Loss: 5.3863
Time elapsed (s): 1.6668636798858643
Agent stdevs: 0.6897516
--------------------------------------------------------------------------------

Step 90
++++++++ Policy training ++++++++++
Current mean reward: 1088.590669 | mean episode length: 339.000000
val_loss=41.22669
val_loss=29.47653
val_loss=64.98412
val_loss=44.99612
val_loss=65.09608
val_loss=38.66131
val_loss=36.77410
val_loss=59.77130
val_loss=112.70032
val_loss=23.60793
adv_loss= 5.03242
adv_loss= 4.24509
adv_loss=12.18890
adv_loss= 2.25703
adv_loss= 4.70706
adv_loss= 3.67460
adv_loss=10.64758
adv_loss= 5.89110
adv_loss= 4.93587
adv_loss=11.28721
surrogate=-0.00256, entropy= 3.12525, loss=-0.00256
surrogate= 0.00012, entropy= 3.12208, loss= 0.00012
surrogate=-0.01697, entropy= 3.12114, loss=-0.01697
surrogate= 0.01762, entropy= 3.11967, loss= 0.01762
surrogate= 0.00397, entropy= 3.11959, loss= 0.00397
surrogate=-0.01963, entropy= 3.11647, loss=-0.01963
surrogate=-0.02467, entropy= 3.11551, loss=-0.02467
surrogate=-0.00659, entropy= 3.11569, loss=-0.00659
surrogate= 0.00021, entropy= 3.11586, loss= 0.00021
surrogate= 0.00064, entropy= 3.11414, loss= 0.00064
std_min= 0.59137, std_max= 0.77750, std_mean= 0.68749
val lr: [0.0002266905737704918], policy lr: [0.0002720286885245901]
Policy Loss: 0.00064456, | Entropy Bonus: -0, | Value Loss: 23.608, | Advantage Loss: 11.287
Time elapsed (s): 1.702082633972168
Agent stdevs: 0.68749446
--------------------------------------------------------------------------------

Step 91
++++++++ Policy training ++++++++++
Current mean reward: 1239.841932 | mean episode length: 383.200000
val_loss=100.60664
val_loss=51.25822
val_loss=76.97831
val_loss=157.17624
val_loss=43.19302
val_loss=56.22578
val_loss=86.33861
val_loss=27.28626
val_loss=68.62691
val_loss=71.09019
adv_loss= 6.98745
adv_loss= 6.26870
adv_loss=11.06834
adv_loss= 4.32473
adv_loss= 2.40881
adv_loss=10.64975
adv_loss= 7.07220
adv_loss= 8.51426
adv_loss= 2.66911
adv_loss=14.79703
surrogate=-0.02263, entropy= 3.10885, loss=-0.02263
surrogate=-0.01127, entropy= 3.10447, loss=-0.01127
surrogate= 0.01942, entropy= 3.09921, loss= 0.01942
surrogate= 0.03687, entropy= 3.09685, loss= 0.03687
surrogate=-0.01363, entropy= 3.09222, loss=-0.01363
surrogate=-0.02526, entropy= 3.08672, loss=-0.02526
surrogate= 0.01156, entropy= 3.08234, loss= 0.01156
surrogate=-0.05928, entropy= 3.07957, loss=-0.05928
surrogate=-0.01829, entropy= 3.07492, loss=-0.01829
surrogate=-0.03366, entropy= 3.07417, loss=-0.03366
std_min= 0.58527, std_max= 0.76835, std_mean= 0.67835
val lr: [0.00022643442622950822], policy lr: [0.0002717213114754098]
Policy Loss: -0.033658, | Entropy Bonus: -0, | Value Loss: 71.09, | Advantage Loss: 14.797
Time elapsed (s): 1.6862962245941162
Agent stdevs: 0.6783517
--------------------------------------------------------------------------------

Step 92
++++++++ Policy training ++++++++++
Current mean reward: 1465.718081 | mean episode length: 453.750000
val_loss=31.62572
val_loss=43.64700
val_loss=22.62039
val_loss=25.77397
val_loss=25.89177
val_loss=20.11946
val_loss=32.71799
val_loss=27.36785
val_loss=24.34712
val_loss=42.95805
adv_loss= 4.92039
adv_loss= 3.96056
adv_loss= 3.52030
adv_loss= 4.91010
adv_loss= 3.16197
adv_loss= 1.83236
adv_loss= 2.01882
adv_loss= 2.75489
adv_loss= 1.66008
adv_loss= 5.00486
surrogate=-0.01130, entropy= 3.07622, loss=-0.01130
surrogate=-0.02109, entropy= 3.07616, loss=-0.02109
surrogate=-0.02517, entropy= 3.07445, loss=-0.02517
surrogate= 0.03238, entropy= 3.07238, loss= 0.03238
surrogate=-0.01199, entropy= 3.06936, loss=-0.01199
surrogate=-0.02669, entropy= 3.06770, loss=-0.02669
surrogate=-0.01991, entropy= 3.06682, loss=-0.01991
surrogate=-0.00984, entropy= 3.06374, loss=-0.00984
surrogate=-0.00411, entropy= 3.06163, loss=-0.00411
surrogate=-0.04784, entropy= 3.06228, loss=-0.04784
std_min= 0.58454, std_max= 0.77672, std_mean= 0.67610
val lr: [0.0002261782786885246], policy lr: [0.00027141393442622945]
Policy Loss: -0.047836, | Entropy Bonus: -0, | Value Loss: 42.958, | Advantage Loss: 5.0049
Time elapsed (s): 1.7189090251922607
Agent stdevs: 0.6760974
--------------------------------------------------------------------------------

Step 93
++++++++ Policy training ++++++++++
Current mean reward: 1089.674352 | mean episode length: 329.800000
val_loss=57.49206
val_loss=23.40727
val_loss=48.98357
val_loss=40.43864
val_loss=17.63758
val_loss=25.05874
val_loss=22.05026
val_loss=29.48476
val_loss=33.40987
val_loss=25.22684
adv_loss= 6.20330
adv_loss= 9.89723
adv_loss= 9.02842
adv_loss= 3.68969
adv_loss= 3.29854
adv_loss= 2.69917
adv_loss= 2.88967
adv_loss= 2.88402
adv_loss= 2.03478
adv_loss= 4.01927
surrogate=-0.02647, entropy= 3.06482, loss=-0.02647
surrogate= 0.01412, entropy= 3.06777, loss= 0.01412
surrogate=-0.00801, entropy= 3.06953, loss=-0.00801
surrogate=-0.00306, entropy= 3.07040, loss=-0.00306
surrogate=-0.00729, entropy= 3.07258, loss=-0.00729
surrogate=-0.01767, entropy= 3.07432, loss=-0.01767
surrogate=-0.01092, entropy= 3.07653, loss=-0.01092
surrogate=-0.01441, entropy= 3.07674, loss=-0.01441
surrogate= 0.05367, entropy= 3.07903, loss= 0.05367
surrogate=-0.01642, entropy= 3.08036, loss=-0.01642
std_min= 0.57998, std_max= 0.78030, std_mean= 0.68056
val lr: [0.00022592213114754097], policy lr: [0.00027110655737704915]
Policy Loss: -0.016421, | Entropy Bonus: -0, | Value Loss: 25.227, | Advantage Loss: 4.0193
Time elapsed (s): 1.6857678890228271
Agent stdevs: 0.6805611
--------------------------------------------------------------------------------

Step 94
++++++++ Policy training ++++++++++
Current mean reward: 1658.050656 | mean episode length: 519.333333
val_loss=29.43780
val_loss=21.97707
val_loss=22.33563
val_loss=32.98059
val_loss=18.62709
val_loss=34.64519
val_loss=23.91896
val_loss=25.54322
val_loss=23.86312
val_loss=17.63130
adv_loss= 3.61405
adv_loss= 3.70593
adv_loss=11.10976
adv_loss= 3.72128
adv_loss= 2.40056
adv_loss= 1.52678
adv_loss= 9.52815
adv_loss= 3.23026
adv_loss= 1.32984
adv_loss= 2.90132
surrogate= 0.00364, entropy= 3.07929, loss= 0.00364
surrogate=-0.02836, entropy= 3.08067, loss=-0.02836
surrogate= 0.01042, entropy= 3.08105, loss= 0.01042
surrogate=-0.00209, entropy= 3.08094, loss=-0.00209
surrogate= 0.00903, entropy= 3.07946, loss= 0.00903
surrogate= 0.00467, entropy= 3.08280, loss= 0.00467
surrogate=-0.00866, entropy= 3.08455, loss=-0.00866
surrogate=-0.02861, entropy= 3.08415, loss=-0.02861
surrogate= 0.02348, entropy= 3.08315, loss= 0.02348
surrogate=-0.01623, entropy= 3.08173, loss=-0.01623
std_min= 0.57826, std_max= 0.78679, std_mean= 0.68123
val lr: [0.00022566598360655737], policy lr: [0.0002707991803278688]
Policy Loss: -0.016233, | Entropy Bonus: -0, | Value Loss: 17.631, | Advantage Loss: 2.9013
Time elapsed (s): 1.6783554553985596
Agent stdevs: 0.6812317
--------------------------------------------------------------------------------

Step 95
++++++++ Policy training ++++++++++
Current mean reward: 1090.887997 | mean episode length: 337.800000
val_loss=61.61045
val_loss=42.33271
val_loss=23.01731
val_loss=31.04009
val_loss=40.56294
val_loss=18.91632
val_loss=29.56191
val_loss=28.91180
val_loss=22.23919
val_loss=24.96211
adv_loss= 2.38738
adv_loss= 1.32514
adv_loss= 1.89025
adv_loss= 3.19403
adv_loss= 2.50299
adv_loss= 4.46056
adv_loss= 3.25353
adv_loss= 0.84732
adv_loss= 8.42550
adv_loss= 1.59268
surrogate=-0.03303, entropy= 3.08058, loss=-0.03303
surrogate= 0.02014, entropy= 3.08147, loss= 0.02014
surrogate=-0.00524, entropy= 3.08317, loss=-0.00524
surrogate=-0.00958, entropy= 3.08150, loss=-0.00958
surrogate=-0.01145, entropy= 3.08174, loss=-0.01145
surrogate=-0.02165, entropy= 3.08230, loss=-0.02165
surrogate=-0.01482, entropy= 3.08190, loss=-0.01482
surrogate= 0.00162, entropy= 3.08088, loss= 0.00162
surrogate=-0.01502, entropy= 3.08016, loss=-0.01502
surrogate=-0.04277, entropy= 3.08059, loss=-0.04277
std_min= 0.58561, std_max= 0.77855, std_mean= 0.68023
val lr: [0.00022540983606557377], policy lr: [0.0002704918032786885]
Policy Loss: -0.042773, | Entropy Bonus: -0, | Value Loss: 24.962, | Advantage Loss: 1.5927
Time elapsed (s): 1.7096378803253174
Agent stdevs: 0.68023056
--------------------------------------------------------------------------------

Step 96
++++++++ Policy training ++++++++++
Current mean reward: 1014.891435 | mean episode length: 309.666667
val_loss=49.34387
val_loss=39.69547
val_loss=57.99027
val_loss=38.25423
val_loss=38.80420
val_loss=24.59138
val_loss=48.53363
val_loss=33.02700
val_loss=24.42741
val_loss=28.00282
adv_loss= 3.85533
adv_loss= 3.12230
adv_loss= 2.40401
adv_loss=11.94090
adv_loss= 3.12406
adv_loss= 2.93336
adv_loss= 1.75433
adv_loss= 7.54302
adv_loss= 2.18709
adv_loss= 2.33279
surrogate=-0.00363, entropy= 3.08210, loss=-0.00363
surrogate= 0.03241, entropy= 3.07966, loss= 0.03241
surrogate=-0.02610, entropy= 3.07907, loss=-0.02610
surrogate= 0.00187, entropy= 3.07891, loss= 0.00187
surrogate=-0.02216, entropy= 3.07546, loss=-0.02216
surrogate=-0.00839, entropy= 3.07558, loss=-0.00839
surrogate=-0.01961, entropy= 3.07611, loss=-0.01961
surrogate=-0.00000, entropy= 3.07396, loss=-0.00000
surrogate= 0.00030, entropy= 3.07393, loss= 0.00030
surrogate=-0.03967, entropy= 3.07500, loss=-0.03967
std_min= 0.58332, std_max= 0.77540, std_mean= 0.67892
val lr: [0.00022515368852459017], policy lr: [0.0002701844262295082]
Policy Loss: -0.039672, | Entropy Bonus: -0, | Value Loss: 28.003, | Advantage Loss: 2.3328
Time elapsed (s): 1.6889398097991943
Agent stdevs: 0.67892295
--------------------------------------------------------------------------------

Step 97
++++++++ Policy training ++++++++++
Current mean reward: 1258.765346 | mean episode length: 381.000000
val_loss=56.52260
val_loss=37.03732
val_loss=49.25409
val_loss=45.31889
val_loss=27.15438
val_loss=30.83266
val_loss=21.17864
val_loss=55.71008
val_loss=17.47157
val_loss= 9.92799
adv_loss= 2.47423
adv_loss= 1.73726
adv_loss= 2.38513
adv_loss= 4.43922
adv_loss= 2.95784
adv_loss=11.91406
adv_loss=10.53659
adv_loss= 2.41387
adv_loss= 3.26600
adv_loss= 2.74472
surrogate=-0.00785, entropy= 3.07415, loss=-0.00785
surrogate=-0.00061, entropy= 3.07220, loss=-0.00061
surrogate= 0.01341, entropy= 3.07246, loss= 0.01341
surrogate=-0.01290, entropy= 3.07215, loss=-0.01290
surrogate=-0.01459, entropy= 3.07091, loss=-0.01459
surrogate=-0.03175, entropy= 3.06902, loss=-0.03175
surrogate=-0.03298, entropy= 3.06745, loss=-0.03298
surrogate=-0.05370, entropy= 3.06672, loss=-0.05370
surrogate= 0.01683, entropy= 3.06760, loss= 0.01683
surrogate= 0.01301, entropy= 3.06404, loss= 0.01301
std_min= 0.57489, std_max= 0.77763, std_mean= 0.67703
val lr: [0.00022489754098360657], policy lr: [0.00026987704918032785]
Policy Loss: 0.013015, | Entropy Bonus: -0, | Value Loss: 9.928, | Advantage Loss: 2.7447
Time elapsed (s): 1.7037992477416992
Agent stdevs: 0.67702824
--------------------------------------------------------------------------------

Step 98
++++++++ Policy training ++++++++++
Current mean reward: 1494.139851 | mean episode length: 452.500000
val_loss=27.96934
val_loss=36.96895
val_loss=17.32990
val_loss=14.80199
val_loss=27.29322
val_loss=18.19803
val_loss=39.17601
val_loss=27.62825
val_loss=32.71914
val_loss=34.53075
adv_loss= 2.36638
adv_loss= 3.41111
adv_loss= 1.84338
adv_loss= 3.39110
adv_loss= 2.11982
adv_loss= 0.89926
adv_loss= 4.01853
adv_loss= 1.55890
adv_loss= 3.02584
adv_loss= 2.58293
surrogate=-0.01954, entropy= 3.06060, loss=-0.01954
surrogate=-0.00342, entropy= 3.05403, loss=-0.00342
surrogate=-0.00127, entropy= 3.04769, loss=-0.00127
surrogate= 0.00284, entropy= 3.04414, loss= 0.00284
surrogate=-0.02061, entropy= 3.03989, loss=-0.02061
surrogate=-0.03574, entropy= 3.03524, loss=-0.03574
surrogate=-0.01340, entropy= 3.03229, loss=-0.01340
surrogate= 0.01206, entropy= 3.02741, loss= 0.01206
surrogate=-0.02357, entropy= 3.02411, loss=-0.02357
surrogate=-0.03177, entropy= 3.02160, loss=-0.03177
std_min= 0.56657, std_max= 0.76241, std_mean= 0.66733
val lr: [0.00022464139344262297], policy lr: [0.00026956967213114755]
Policy Loss: -0.031766, | Entropy Bonus: -0, | Value Loss: 34.531, | Advantage Loss: 2.5829
Time elapsed (s): 1.7417817115783691
Agent stdevs: 0.66732574
--------------------------------------------------------------------------------

Step 99
++++++++ Policy training ++++++++++
Current mean reward: 1188.560780 | mean episode length: 368.200000
val_loss=58.86152
val_loss=63.13980
val_loss=21.62211
val_loss=23.53189
val_loss=14.11317
val_loss=44.73492
val_loss=41.18667
val_loss=18.11656
val_loss=34.59609
val_loss=27.73598
adv_loss= 8.07852
adv_loss= 0.80426
adv_loss= 1.00350
adv_loss= 8.46784
adv_loss= 2.03377
adv_loss= 1.75673
adv_loss= 1.99980
adv_loss= 1.60563
adv_loss= 5.07018
adv_loss= 8.70737
surrogate=-0.00183, entropy= 3.01998, loss=-0.00183
surrogate= 0.00580, entropy= 3.01725, loss= 0.00580
surrogate=-0.00675, entropy= 3.01687, loss=-0.00675
surrogate= 0.05901, entropy= 3.01813, loss= 0.05901
surrogate=-0.00269, entropy= 3.01655, loss=-0.00269
surrogate=-0.00221, entropy= 3.01399, loss=-0.00221
surrogate= 0.01060, entropy= 3.01204, loss= 0.01060
surrogate=-0.04089, entropy= 3.01147, loss=-0.04089
surrogate=-0.01082, entropy= 3.00965, loss=-0.01082
surrogate=-0.00177, entropy= 3.00909, loss=-0.00177
std_min= 0.56914, std_max= 0.74960, std_mean= 0.66391
val lr: [0.00022438524590163934], policy lr: [0.00026926229508196715]
Policy Loss: -0.0017699, | Entropy Bonus: -0, | Value Loss: 27.736, | Advantage Loss: 8.7074
Time elapsed (s): 1.7216882705688477
Agent stdevs: 0.6639101
--------------------------------------------------------------------------------

Step 100
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1209.4
++++++++ Policy training ++++++++++
Current mean reward: 1222.353386 | mean episode length: 367.800000
val_loss=41.56844
val_loss=25.86688
val_loss=23.56277
val_loss=20.37703
val_loss=18.18408
val_loss=27.36171
val_loss=17.50353
val_loss=24.86762
val_loss=18.24104
val_loss=16.36637
adv_loss= 2.12585
adv_loss=11.40880
adv_loss= 4.24637
adv_loss= 2.87869
adv_loss= 7.22063
adv_loss= 1.51853
adv_loss= 1.69735
adv_loss= 3.36613
adv_loss=12.73411
adv_loss=10.13723
surrogate=-0.00040, entropy= 3.00578, loss=-0.00040
surrogate=-0.00864, entropy= 3.00326, loss=-0.00864
surrogate= 0.01085, entropy= 3.00271, loss= 0.01085
surrogate=-0.02598, entropy= 3.00063, loss=-0.02598
surrogate=-0.01301, entropy= 2.99856, loss=-0.01301
surrogate=-0.02158, entropy= 2.99553, loss=-0.02158
surrogate=-0.00002, entropy= 2.99630, loss=-0.00002
surrogate=-0.02146, entropy= 2.99230, loss=-0.02146
surrogate= 0.01983, entropy= 2.99140, loss= 0.01983
surrogate=-0.01909, entropy= 2.98971, loss=-0.01909
std_min= 0.56400, std_max= 0.73937, std_mean= 0.65961
val lr: [0.00022412909836065574], policy lr: [0.00026895491803278685]
Policy Loss: -0.019089, | Entropy Bonus: -0, | Value Loss: 16.366, | Advantage Loss: 10.137
Time elapsed (s): 1.6794545650482178
Agent stdevs: 0.65960544
--------------------------------------------------------------------------------

Step 101
++++++++ Policy training ++++++++++
Current mean reward: 1100.673419 | mean episode length: 347.000000
val_loss=51.23311
val_loss=60.24165
val_loss=104.85152
val_loss=33.42488
val_loss=71.25967
val_loss=61.62238
val_loss=67.82607
val_loss=44.10175
val_loss=63.52676
val_loss=83.14755
adv_loss=21.25645
adv_loss= 9.19496
adv_loss= 7.57581
adv_loss= 2.70774
adv_loss= 7.97141
adv_loss= 6.88170
adv_loss= 6.86326
adv_loss= 5.95269
adv_loss= 5.67234
adv_loss= 8.49327
surrogate= 0.03084, entropy= 2.98881, loss= 0.03084
surrogate= 0.01590, entropy= 2.98849, loss= 0.01590
surrogate=-0.01259, entropy= 2.99111, loss=-0.01259
surrogate= 0.00835, entropy= 2.98738, loss= 0.00835
surrogate= 0.00013, entropy= 2.98469, loss= 0.00013
surrogate=-0.00322, entropy= 2.98192, loss=-0.00322
surrogate=-0.00384, entropy= 2.98142, loss=-0.00384
surrogate=-0.02258, entropy= 2.97933, loss=-0.02258
surrogate= 0.00511, entropy= 2.97880, loss= 0.00511
surrogate=-0.03955, entropy= 2.97820, loss=-0.03955
std_min= 0.56201, std_max= 0.72903, std_mean= 0.65685
val lr: [0.00022387295081967212], policy lr: [0.00026864754098360655]
Policy Loss: -0.039549, | Entropy Bonus: -0, | Value Loss: 83.148, | Advantage Loss: 8.4933
Time elapsed (s): 1.6903960704803467
Agent stdevs: 0.6568539
--------------------------------------------------------------------------------

Step 102
++++++++ Policy training ++++++++++
Current mean reward: 1224.994563 | mean episode length: 401.200000
val_loss=184.73248
val_loss=62.76791
val_loss=282.19461
val_loss=80.78326
val_loss=91.80720
val_loss=55.14363
val_loss=313.84805
val_loss=75.49995
val_loss=49.22241
val_loss=68.11073
adv_loss= 6.19019
adv_loss=15.89753
adv_loss= 4.18141
adv_loss= 7.64773
adv_loss= 3.88426
adv_loss= 4.43056
adv_loss=13.45574
adv_loss= 6.10095
adv_loss= 5.04325
adv_loss= 8.07221
surrogate= 0.03053, entropy= 2.97916, loss= 0.03053
surrogate=-0.01502, entropy= 2.97856, loss=-0.01502
surrogate=-0.00003, entropy= 2.97937, loss=-0.00003
surrogate=-0.00580, entropy= 2.98145, loss=-0.00580
surrogate=-0.00915, entropy= 2.98264, loss=-0.00915
surrogate=-0.02934, entropy= 2.98515, loss=-0.02934
surrogate=-0.02308, entropy= 2.98764, loss=-0.02308
surrogate=-0.02257, entropy= 2.98815, loss=-0.02257
surrogate= 0.00694, entropy= 2.98976, loss= 0.00694
surrogate=-0.00367, entropy= 2.99228, loss=-0.00367
std_min= 0.56621, std_max= 0.73110, std_mean= 0.65985
val lr: [0.00022361680327868852], policy lr: [0.0002683401639344262]
Policy Loss: -0.0036701, | Entropy Bonus: -0, | Value Loss: 68.111, | Advantage Loss: 8.0722
Time elapsed (s): 1.687680721282959
Agent stdevs: 0.6598469
--------------------------------------------------------------------------------

Step 103
++++++++ Policy training ++++++++++
Current mean reward: 1377.933481 | mean episode length: 427.500000
val_loss=83.31169
val_loss=93.09018
val_loss=72.51689
val_loss=34.42884
val_loss=157.48575
val_loss=110.98682
val_loss=57.48057
val_loss=48.53244
val_loss=78.48640
val_loss=34.48329
adv_loss= 6.42506
adv_loss= 5.26228
adv_loss= 4.44305
adv_loss=25.00790
adv_loss= 4.94858
adv_loss= 3.72303
adv_loss= 4.01014
adv_loss= 4.40361
adv_loss= 7.44522
adv_loss= 7.67621
surrogate=-0.02855, entropy= 2.99686, loss=-0.02855
surrogate=-0.00094, entropy= 3.00209, loss=-0.00094
surrogate=-0.02357, entropy= 3.00616, loss=-0.02357
surrogate=-0.01614, entropy= 3.00828, loss=-0.01614
surrogate=-0.01615, entropy= 3.01086, loss=-0.01615
surrogate=-0.01733, entropy= 3.01481, loss=-0.01733
surrogate= 0.01237, entropy= 3.01805, loss= 0.01237
surrogate=-0.01407, entropy= 3.02039, loss=-0.01407
surrogate=-0.03748, entropy= 3.02224, loss=-0.03748
surrogate=-0.01942, entropy= 3.02393, loss=-0.01942
std_min= 0.56936, std_max= 0.73799, std_mean= 0.66701
val lr: [0.00022336065573770492], policy lr: [0.0002680327868852459]
Policy Loss: -0.019425, | Entropy Bonus: -0, | Value Loss: 34.483, | Advantage Loss: 7.6762
Time elapsed (s): 1.702610969543457
Agent stdevs: 0.6670094
--------------------------------------------------------------------------------

Step 104
++++++++ Policy training ++++++++++
Current mean reward: 1365.690064 | mean episode length: 405.400000
val_loss=46.55349
val_loss=71.49351
val_loss=97.37350
val_loss=74.47810
val_loss=76.34491
val_loss=65.52837
val_loss=50.11604
val_loss=61.81478
val_loss=60.97456
val_loss=57.84766
adv_loss= 6.24943
adv_loss= 5.16806
adv_loss=13.17025
adv_loss=19.62272
adv_loss= 4.69742
adv_loss= 2.54812
adv_loss=10.69263
adv_loss= 4.92752
adv_loss= 8.66322
adv_loss= 4.42414
surrogate=-0.01609, entropy= 3.02483, loss=-0.01609
surrogate=-0.01351, entropy= 3.02617, loss=-0.01351
surrogate=-0.02478, entropy= 3.02879, loss=-0.02478
surrogate= 0.03934, entropy= 3.03267, loss= 0.03934
surrogate=-0.01373, entropy= 3.03747, loss=-0.01373
surrogate=-0.01384, entropy= 3.03984, loss=-0.01384
surrogate=-0.02104, entropy= 3.03905, loss=-0.02104
surrogate=-0.01852, entropy= 3.04115, loss=-0.01852
surrogate=-0.02215, entropy= 3.04324, loss=-0.02215
surrogate=-0.02400, entropy= 3.04386, loss=-0.02400
std_min= 0.57104, std_max= 0.74471, std_mean= 0.67167
val lr: [0.00022310450819672132], policy lr: [0.00026772540983606555]
Policy Loss: -0.024005, | Entropy Bonus: -0, | Value Loss: 57.848, | Advantage Loss: 4.4241
Time elapsed (s): 1.7084708213806152
Agent stdevs: 0.6716724
--------------------------------------------------------------------------------

Step 105
++++++++ Policy training ++++++++++
Current mean reward: 1381.588591 | mean episode length: 438.500000
val_loss=73.40067
val_loss=30.07986
val_loss=163.34967
val_loss=155.18323
val_loss=55.36099
val_loss=138.94391
val_loss=30.83847
val_loss=68.94262
val_loss=39.42733
val_loss=97.61033
adv_loss= 5.06992
adv_loss= 3.45484
adv_loss= 4.42407
adv_loss= 7.61297
adv_loss=11.07476
adv_loss=12.14158
adv_loss= 5.39288
adv_loss= 5.44102
adv_loss= 3.46206
adv_loss= 3.16350
surrogate= 0.00292, entropy= 3.04346, loss= 0.00292
surrogate=-0.00486, entropy= 3.04466, loss=-0.00486
surrogate=-0.00617, entropy= 3.04349, loss=-0.00617
surrogate=-0.01363, entropy= 3.04267, loss=-0.01363
surrogate=-0.00775, entropy= 3.04175, loss=-0.00775
surrogate=-0.00714, entropy= 3.03877, loss=-0.00714
surrogate=-0.00595, entropy= 3.03943, loss=-0.00595
surrogate=-0.02837, entropy= 3.03755, loss=-0.02837
surrogate=-0.02536, entropy= 3.03830, loss=-0.02536
surrogate= 0.02042, entropy= 3.03495, loss= 0.02042
std_min= 0.57147, std_max= 0.73491, std_mean= 0.66935
val lr: [0.00022284836065573771], policy lr: [0.00026741803278688525]
Policy Loss: 0.020417, | Entropy Bonus: -0, | Value Loss: 97.61, | Advantage Loss: 3.1635
Time elapsed (s): 1.6947412490844727
Agent stdevs: 0.669349
--------------------------------------------------------------------------------

Step 106
++++++++ Policy training ++++++++++
Current mean reward: 1451.743107 | mean episode length: 440.000000
val_loss=75.27206
val_loss=106.06197
val_loss=24.32624
val_loss=46.44994
val_loss=65.69186
val_loss=91.57894
val_loss=62.37848
val_loss=25.98431
val_loss=38.13209
val_loss=51.82506
adv_loss=13.86050
adv_loss= 9.04103
adv_loss= 5.93319
adv_loss= 2.31143
adv_loss=12.05950
adv_loss= 3.67938
adv_loss= 5.10457
adv_loss= 3.57178
adv_loss= 7.01332
adv_loss= 2.92433
surrogate= 0.00423, entropy= 3.03531, loss= 0.00423
surrogate= 0.01423, entropy= 3.03277, loss= 0.01423
surrogate=-0.02589, entropy= 3.03470, loss=-0.02589
surrogate= 0.00929, entropy= 3.03604, loss= 0.00929
surrogate=-0.02065, entropy= 3.03702, loss=-0.02065
surrogate=-0.00877, entropy= 3.03906, loss=-0.00877
surrogate=-0.00215, entropy= 3.03642, loss=-0.00215
surrogate=-0.03016, entropy= 3.03596, loss=-0.03016
surrogate=-0.03012, entropy= 3.03748, loss=-0.03012
surrogate=-0.03064, entropy= 3.03768, loss=-0.03064
std_min= 0.56719, std_max= 0.74728, std_mean= 0.67057
val lr: [0.00022259221311475411], policy lr: [0.0002671106557377049]
Policy Loss: -0.030642, | Entropy Bonus: -0, | Value Loss: 51.825, | Advantage Loss: 2.9243
Time elapsed (s): 1.6704955101013184
Agent stdevs: 0.6705653
--------------------------------------------------------------------------------

Step 107
++++++++ Policy training ++++++++++
Current mean reward: 1848.818992 | mean episode length: 603.333333
val_loss=519.35858
val_loss=197.66365
val_loss=96.53472
val_loss=143.12064
val_loss=122.10248
val_loss=479.36554
val_loss=95.03127
val_loss=61.61476
val_loss=59.79015
val_loss=241.69492
adv_loss= 7.01293
adv_loss=23.33841
adv_loss=495.15039
adv_loss=14.91054
adv_loss= 7.11987
adv_loss=10.15482
adv_loss= 9.92828
adv_loss=14.53879
adv_loss= 4.62094
adv_loss= 3.40092
surrogate=-0.01487, entropy= 3.03975, loss=-0.01487
surrogate=-0.03048, entropy= 3.03990, loss=-0.03048
surrogate=-0.00458, entropy= 3.04081, loss=-0.00458
surrogate= 0.00606, entropy= 3.04180, loss= 0.00606
surrogate=-0.00091, entropy= 3.04147, loss=-0.00091
surrogate=-0.00197, entropy= 3.04211, loss=-0.00197
surrogate=-0.02288, entropy= 3.04074, loss=-0.02288
surrogate=-0.03590, entropy= 3.04220, loss=-0.03590
surrogate=-0.02971, entropy= 3.04048, loss=-0.02971
surrogate=-0.00407, entropy= 3.03944, loss=-0.00407
std_min= 0.56523, std_max= 0.74029, std_mean= 0.67097
val lr: [0.00022233606557377051], policy lr: [0.0002668032786885246]
Policy Loss: -0.0040651, | Entropy Bonus: -0, | Value Loss: 241.69, | Advantage Loss: 3.4009
Time elapsed (s): 1.6840531826019287
Agent stdevs: 0.6709747
--------------------------------------------------------------------------------

Step 108
++++++++ Policy training ++++++++++
Current mean reward: 1296.364246 | mean episode length: 387.000000
val_loss=59.18713
val_loss=52.74571
val_loss=37.72075
val_loss=46.38427
val_loss=43.46609
val_loss=42.03530
val_loss=25.67908
val_loss=22.60039
val_loss=25.69859
val_loss=28.11139
adv_loss= 4.95191
adv_loss= 5.87750
adv_loss= 6.18834
adv_loss= 4.14755
adv_loss= 3.56652
adv_loss= 8.65871
adv_loss= 3.72641
adv_loss= 9.30733
adv_loss= 8.26382
adv_loss= 8.96845
surrogate=-0.01339, entropy= 3.03978, loss=-0.01339
surrogate= 0.00174, entropy= 3.04246, loss= 0.00174
surrogate=-0.02409, entropy= 3.04145, loss=-0.02409
surrogate=-0.01049, entropy= 3.04089, loss=-0.01049
surrogate=-0.00539, entropy= 3.03848, loss=-0.00539
surrogate=-0.04165, entropy= 3.03910, loss=-0.04165
surrogate=-0.04949, entropy= 3.03853, loss=-0.04949
surrogate=-0.00051, entropy= 3.03745, loss=-0.00051
surrogate= 0.00892, entropy= 3.03813, loss= 0.00892
surrogate=-0.02363, entropy= 3.03935, loss=-0.02363
std_min= 0.56280, std_max= 0.73914, std_mean= 0.67114
val lr: [0.0002220799180327869], policy lr: [0.00026649590163934424]
Policy Loss: -0.023626, | Entropy Bonus: -0, | Value Loss: 28.111, | Advantage Loss: 8.9685
Time elapsed (s): 1.6641101837158203
Agent stdevs: 0.67113644
--------------------------------------------------------------------------------

Step 109
++++++++ Policy training ++++++++++
Current mean reward: 999.949793 | mean episode length: 290.250000
val_loss=31.35631
val_loss=29.44618
val_loss=32.38833
val_loss=29.09739
val_loss=22.28499
val_loss=14.47568
val_loss=16.16748
val_loss=37.04302
val_loss=30.34910
val_loss=15.82366
adv_loss= 3.19884
adv_loss= 2.20255
adv_loss= 3.25512
adv_loss=13.70758
adv_loss= 2.33930
adv_loss= 2.80230
adv_loss= 3.04722
adv_loss= 5.12292
adv_loss= 3.44765
adv_loss= 1.93396
surrogate= 0.01136, entropy= 3.04243, loss= 0.01136
surrogate= 0.02057, entropy= 3.04172, loss= 0.02057
surrogate= 0.01488, entropy= 3.04198, loss= 0.01488
surrogate=-0.02123, entropy= 3.04326, loss=-0.02123
surrogate=-0.00733, entropy= 3.04305, loss=-0.00733
surrogate=-0.02312, entropy= 3.04366, loss=-0.02312
surrogate=-0.01509, entropy= 3.04204, loss=-0.01509
surrogate=-0.02913, entropy= 3.03940, loss=-0.02913
surrogate= 0.00581, entropy= 3.03705, loss= 0.00581
surrogate=-0.00871, entropy= 3.03750, loss=-0.00871
std_min= 0.56394, std_max= 0.74015, std_mean= 0.67062
val lr: [0.00022182377049180326], policy lr: [0.0002661885245901639]
Policy Loss: -0.0087052, | Entropy Bonus: -0, | Value Loss: 15.824, | Advantage Loss: 1.934
Time elapsed (s): 1.6796882152557373
Agent stdevs: 0.67062026
--------------------------------------------------------------------------------

Step 110
++++++++ Policy training ++++++++++
Current mean reward: 1035.856358 | mean episode length: 324.333333
val_loss=36.92547
val_loss=55.73602
val_loss=165.98933
val_loss=384.95187
val_loss=42.00305
val_loss=33.09372
val_loss=769.50726
val_loss=142.88458
val_loss=426.03238
val_loss=156.09138
adv_loss= 8.63765
adv_loss= 4.18895
adv_loss= 5.68883
adv_loss= 7.25816
adv_loss= 2.92741
adv_loss= 4.27821
adv_loss= 7.90063
adv_loss=1208.57068
adv_loss= 7.49586
adv_loss= 6.94986
surrogate=-0.02985, entropy= 3.03925, loss=-0.02985
surrogate=-0.01365, entropy= 3.03920, loss=-0.01365
surrogate= 0.02050, entropy= 3.03714, loss= 0.02050
surrogate= 0.00973, entropy= 3.03598, loss= 0.00973
surrogate=-0.00765, entropy= 3.03473, loss=-0.00765
surrogate=-0.00485, entropy= 3.03297, loss=-0.00485
surrogate=-0.00405, entropy= 3.03262, loss=-0.00405
surrogate=-0.04974, entropy= 3.03353, loss=-0.04974
surrogate=-0.02535, entropy= 3.03279, loss=-0.02535
surrogate=-0.01811, entropy= 3.03196, loss=-0.01811
std_min= 0.56681, std_max= 0.73181, std_mean= 0.66896
val lr: [0.00022156762295081966], policy lr: [0.0002658811475409836]
Policy Loss: -0.018109, | Entropy Bonus: -0, | Value Loss: 156.09, | Advantage Loss: 6.9499
Time elapsed (s): 1.7094135284423828
Agent stdevs: 0.66896296
--------------------------------------------------------------------------------

Step 111
++++++++ Policy training ++++++++++
Current mean reward: 1223.902346 | mean episode length: 374.800000
val_loss=46.89536
val_loss=25.94516
val_loss=25.83973
val_loss=51.60366
val_loss=65.73682
val_loss=34.50190
val_loss=19.83815
val_loss=41.58268
val_loss=12.85282
val_loss=21.61650
adv_loss= 4.20471
adv_loss= 5.98738
adv_loss= 6.26181
adv_loss= 2.45450
adv_loss= 4.48842
adv_loss= 4.57021
adv_loss= 7.22639
adv_loss= 2.71982
adv_loss= 7.27678
adv_loss= 5.74859
surrogate= 0.00008, entropy= 3.03322, loss= 0.00008
surrogate= 0.01273, entropy= 3.03324, loss= 0.01273
surrogate= 0.03690, entropy= 3.03354, loss= 0.03690
surrogate=-0.01893, entropy= 3.03571, loss=-0.01893
surrogate= 0.00039, entropy= 3.03536, loss= 0.00039
surrogate=-0.01279, entropy= 3.03761, loss=-0.01279
surrogate=-0.01202, entropy= 3.03757, loss=-0.01202
surrogate=-0.00727, entropy= 3.03499, loss=-0.00727
surrogate=-0.00058, entropy= 3.03523, loss=-0.00058
surrogate=-0.00089, entropy= 3.03366, loss=-0.00089
std_min= 0.56143, std_max= 0.73021, std_mean= 0.66984
val lr: [0.00022131147540983606], policy lr: [0.00026557377049180324]
Policy Loss: -0.00089092, | Entropy Bonus: -0, | Value Loss: 21.616, | Advantage Loss: 5.7486
Time elapsed (s): 1.6714282035827637
Agent stdevs: 0.66984135
--------------------------------------------------------------------------------

Step 112
++++++++ Policy training ++++++++++
Current mean reward: 1533.952352 | mean episode length: 485.000000
val_loss=57.88460
val_loss=57.99804
val_loss=43.04480
val_loss=72.84355
val_loss=22.49458
val_loss=24.18297
val_loss=29.41036
val_loss=39.89702
val_loss=49.17737
val_loss=32.66021
adv_loss= 9.47057
adv_loss= 8.63133
adv_loss= 3.57309
adv_loss= 2.73061
adv_loss= 2.86086
adv_loss= 3.26264
adv_loss= 6.72208
adv_loss= 5.32490
adv_loss= 6.15647
adv_loss= 1.73748
surrogate=-0.00053, entropy= 3.03450, loss=-0.00053
surrogate=-0.01311, entropy= 3.03368, loss=-0.01311
surrogate=-0.01798, entropy= 3.02933, loss=-0.01798
surrogate=-0.01159, entropy= 3.02623, loss=-0.01159
surrogate= 0.01167, entropy= 3.02448, loss= 0.01167
surrogate= 0.01629, entropy= 3.02307, loss= 0.01629
surrogate=-0.02684, entropy= 3.01915, loss=-0.02684
surrogate=-0.02570, entropy= 3.01479, loss=-0.02570
surrogate=-0.02232, entropy= 3.01349, loss=-0.02232
surrogate=-0.00025, entropy= 3.01121, loss=-0.00025
std_min= 0.55303, std_max= 0.72565, std_mean= 0.66521
val lr: [0.00022105532786885246], policy lr: [0.00026526639344262294]
Policy Loss: -0.00024513, | Entropy Bonus: -0, | Value Loss: 32.66, | Advantage Loss: 1.7375
Time elapsed (s): 1.6723241806030273
Agent stdevs: 0.66520566
--------------------------------------------------------------------------------

Step 113
++++++++ Policy training ++++++++++
Current mean reward: 1981.065951 | mean episode length: 607.000000
val_loss=67.86713
val_loss=26.33629
val_loss=26.62033
val_loss=38.12938
val_loss=33.52535
val_loss=41.08007
val_loss=24.07226
val_loss=38.11162
val_loss=14.86662
val_loss=14.95595
adv_loss= 2.69093
adv_loss= 3.93007
adv_loss= 7.56291
adv_loss= 2.85435
adv_loss= 5.91303
adv_loss= 5.00729
adv_loss= 2.84313
adv_loss= 5.13804
adv_loss= 1.53146
adv_loss= 3.45806
surrogate= 0.01339, entropy= 3.00463, loss= 0.01339
surrogate=-0.00844, entropy= 2.99904, loss=-0.00844
surrogate=-0.02846, entropy= 2.99483, loss=-0.02846
surrogate=-0.00539, entropy= 2.99292, loss=-0.00539
surrogate=-0.01187, entropy= 2.98960, loss=-0.01187
surrogate=-0.01260, entropy= 2.98427, loss=-0.01260
surrogate=-0.02474, entropy= 2.98232, loss=-0.02474
surrogate= 0.00582, entropy= 2.98068, loss= 0.00582
surrogate=-0.01062, entropy= 2.97855, loss=-0.01062
surrogate=-0.01324, entropy= 2.97704, loss=-0.01324
std_min= 0.53958, std_max= 0.72028, std_mean= 0.65847
val lr: [0.00022079918032786886], policy lr: [0.0002649590163934426]
Policy Loss: -0.013236, | Entropy Bonus: -0, | Value Loss: 14.956, | Advantage Loss: 3.4581
Time elapsed (s): 1.6989085674285889
Agent stdevs: 0.65846586
--------------------------------------------------------------------------------

Step 114
++++++++ Policy training ++++++++++
Current mean reward: 1742.531459 | mean episode length: 589.000000
val_loss=138.18230
val_loss=50.05486
val_loss=67.70428
val_loss=304.69540
val_loss=52.22513
val_loss=390.58194
val_loss=98.28753
val_loss=48.41111
val_loss=30.21180
val_loss=795.71204
adv_loss= 2.54452
adv_loss= 3.91227
adv_loss= 4.56264
adv_loss= 3.27287
adv_loss= 3.07069
adv_loss= 2.84259
adv_loss= 5.32562
adv_loss= 2.39416
adv_loss= 3.59225
adv_loss= 2.07128
surrogate=-0.02371, entropy= 2.97099, loss=-0.02371
surrogate= 0.01451, entropy= 2.97191, loss= 0.01451
surrogate= 0.02092, entropy= 2.96789, loss= 0.02092
surrogate=-0.01905, entropy= 2.96772, loss=-0.01905
surrogate=-0.00414, entropy= 2.96847, loss=-0.00414
surrogate=-0.02457, entropy= 2.96952, loss=-0.02457
surrogate= 0.03575, entropy= 2.97003, loss= 0.03575
surrogate= 0.01859, entropy= 2.97005, loss= 0.01859
surrogate=-0.00395, entropy= 2.96940, loss=-0.00395
surrogate= 0.01491, entropy= 2.97064, loss= 0.01491
std_min= 0.54341, std_max= 0.71720, std_mean= 0.65657
val lr: [0.00022054303278688526], policy lr: [0.0002646516393442623]
Policy Loss: 0.014909, | Entropy Bonus: -0, | Value Loss: 795.71, | Advantage Loss: 2.0713
Time elapsed (s): 1.6604723930358887
Agent stdevs: 0.65656906
--------------------------------------------------------------------------------

Step 115
++++++++ Policy training ++++++++++
Current mean reward: 1091.254032 | mean episode length: 322.500000
val_loss=56.34218
val_loss=28.61319
val_loss=34.12183
val_loss=29.41421
val_loss=50.99081
val_loss=39.45933
val_loss=38.05368
val_loss=32.77235
val_loss=34.38243
val_loss=25.73457
adv_loss= 4.70030
adv_loss=10.16844
adv_loss=11.76872
adv_loss= 5.37113
adv_loss= 3.40770
adv_loss=10.53012
adv_loss= 4.38770
adv_loss=18.56657
adv_loss= 4.77811
adv_loss= 3.82279
surrogate= 0.00979, entropy= 2.97194, loss= 0.00979
surrogate= 0.01397, entropy= 2.97075, loss= 0.01397
surrogate=-0.01400, entropy= 2.97055, loss=-0.01400
surrogate=-0.02034, entropy= 2.96942, loss=-0.02034
surrogate= 0.00198, entropy= 2.96755, loss= 0.00198
surrogate= 0.00144, entropy= 2.96633, loss= 0.00144
surrogate= 0.03280, entropy= 2.96685, loss= 0.03280
surrogate=-0.03706, entropy= 2.96765, loss=-0.03706
surrogate=-0.02850, entropy= 2.96491, loss=-0.02850
surrogate= 0.01229, entropy= 2.96511, loss= 0.01229
std_min= 0.54552, std_max= 0.71392, std_mean= 0.65498
val lr: [0.00022028688524590166], policy lr: [0.00026434426229508194]
Policy Loss: 0.012293, | Entropy Bonus: -0, | Value Loss: 25.735, | Advantage Loss: 3.8228
Time elapsed (s): 1.6881847381591797
Agent stdevs: 0.6549795
--------------------------------------------------------------------------------

Step 116
++++++++ Policy training ++++++++++
Current mean reward: 1712.559327 | mean episode length: 509.000000
val_loss=45.19872
val_loss=36.84692
val_loss=51.66394
val_loss=21.32508
val_loss=29.38265
val_loss=17.25674
val_loss=25.54101
val_loss=17.27907
val_loss=31.28749
val_loss=14.21691
adv_loss= 3.64848
adv_loss= 2.49625
adv_loss= 4.31562
adv_loss= 1.83836
adv_loss=12.93338
adv_loss= 1.89080
adv_loss= 2.49629
adv_loss= 2.40728
adv_loss= 1.32654
adv_loss= 2.82521
surrogate= 0.01365, entropy= 2.96120, loss= 0.01365
surrogate= 0.00237, entropy= 2.96118, loss= 0.00237
surrogate=-0.03331, entropy= 2.95896, loss=-0.03331
surrogate=-0.02674, entropy= 2.95773, loss=-0.02674
surrogate=-0.01017, entropy= 2.95694, loss=-0.01017
surrogate=-0.00763, entropy= 2.95349, loss=-0.00763
surrogate=-0.01712, entropy= 2.95185, loss=-0.01712
surrogate=-0.02319, entropy= 2.94886, loss=-0.02319
surrogate=-0.02849, entropy= 2.94905, loss=-0.02849
surrogate=-0.02919, entropy= 2.94582, loss=-0.02919
std_min= 0.54950, std_max= 0.70788, std_mean= 0.65014
val lr: [0.00022003073770491806], policy lr: [0.00026403688524590164]
Policy Loss: -0.029189, | Entropy Bonus: -0, | Value Loss: 14.217, | Advantage Loss: 2.8252
Time elapsed (s): 1.703172206878662
Agent stdevs: 0.6501385
--------------------------------------------------------------------------------

Step 117
++++++++ Policy training ++++++++++
Current mean reward: 1601.963732 | mean episode length: 466.000000
val_loss=23.00301
val_loss=20.67397
val_loss=19.26529
val_loss=21.30399
val_loss=19.85816
val_loss=22.09102
val_loss=13.55787
val_loss=20.51579
val_loss=19.06480
val_loss=17.65667
adv_loss=10.94117
adv_loss= 2.13747
adv_loss= 6.64067
adv_loss= 3.10236
adv_loss= 4.56348
adv_loss= 5.24966
adv_loss= 5.58197
adv_loss= 3.79021
adv_loss= 4.47931
adv_loss= 3.40333
surrogate= 0.00666, entropy= 2.94423, loss= 0.00666
surrogate= 0.01979, entropy= 2.94305, loss= 0.01979
surrogate= 0.01584, entropy= 2.93961, loss= 0.01584
surrogate= 0.00147, entropy= 2.93784, loss= 0.00147
surrogate= 0.00654, entropy= 2.93581, loss= 0.00654
surrogate= 0.00315, entropy= 2.93328, loss= 0.00315
surrogate=-0.01612, entropy= 2.93075, loss=-0.01612
surrogate=-0.02616, entropy= 2.92967, loss=-0.02616
surrogate=-0.01820, entropy= 2.92655, loss=-0.01820
surrogate=-0.02765, entropy= 2.92584, loss=-0.02765
std_min= 0.54708, std_max= 0.70307, std_mean= 0.64569
val lr: [0.0002197745901639344], policy lr: [0.0002637295081967213]
Policy Loss: -0.027645, | Entropy Bonus: -0, | Value Loss: 17.657, | Advantage Loss: 3.4033
Time elapsed (s): 1.6709458827972412
Agent stdevs: 0.6456855
--------------------------------------------------------------------------------

Step 118
++++++++ Policy training ++++++++++
Current mean reward: 1037.144918 | mean episode length: 312.666667
val_loss=83.00923
val_loss=85.21827
val_loss=71.11509
val_loss=94.54855
val_loss=220.11444
val_loss=124.39030
val_loss=165.32497
val_loss=183.23169
val_loss=233.24020
val_loss=48.30116
adv_loss=388.40314
adv_loss=10.50118
adv_loss= 7.08980
adv_loss= 9.07410
adv_loss= 5.89866
adv_loss= 4.90448
adv_loss=23.06186
adv_loss=18.01680
adv_loss= 5.63126
adv_loss=10.55808
surrogate= 0.01896, entropy= 2.92778, loss= 0.01896
surrogate=-0.02685, entropy= 2.92746, loss=-0.02685
surrogate= 0.01339, entropy= 2.92915, loss= 0.01339
surrogate= 0.00190, entropy= 2.92793, loss= 0.00190
surrogate= 0.01508, entropy= 2.92810, loss= 0.01508
surrogate=-0.00885, entropy= 2.93058, loss=-0.00885
surrogate=-0.02773, entropy= 2.93181, loss=-0.02773
surrogate=-0.01410, entropy= 2.93202, loss=-0.01410
surrogate=-0.03219, entropy= 2.93391, loss=-0.03219
surrogate=-0.02409, entropy= 2.93600, loss=-0.02409
std_min= 0.54084, std_max= 0.70842, std_mean= 0.64867
val lr: [0.0002195184426229508], policy lr: [0.00026342213114754093]
Policy Loss: -0.02409, | Entropy Bonus: -0, | Value Loss: 48.301, | Advantage Loss: 10.558
Time elapsed (s): 1.6817529201507568
Agent stdevs: 0.64866805
--------------------------------------------------------------------------------

Step 119
++++++++ Policy training ++++++++++
Current mean reward: 1218.038821 | mean episode length: 374.000000
val_loss=61.73238
val_loss=36.17708
val_loss=30.43447
val_loss=70.69180
val_loss=24.65700
val_loss=49.48338
val_loss=61.31955
val_loss=46.93185
val_loss=60.17976
val_loss=59.22952
adv_loss= 4.86846
adv_loss= 8.38470
adv_loss= 2.62362
adv_loss= 2.17205
adv_loss= 4.31685
adv_loss=19.38506
adv_loss=17.53745
adv_loss= 4.15875
adv_loss= 7.74651
adv_loss= 4.86161
surrogate= 0.00534, entropy= 2.93587, loss= 0.00534
surrogate= 0.00182, entropy= 2.93452, loss= 0.00182
surrogate=-0.02257, entropy= 2.93420, loss=-0.02257
surrogate=-0.01129, entropy= 2.93204, loss=-0.01129
surrogate= 0.00440, entropy= 2.93107, loss= 0.00440
surrogate=-0.01435, entropy= 2.93026, loss=-0.01435
surrogate=-0.01864, entropy= 2.92816, loss=-0.01864
surrogate=-0.02688, entropy= 2.92750, loss=-0.02688
surrogate=-0.02833, entropy= 2.92546, loss=-0.02833
surrogate=-0.00852, entropy= 2.92347, loss=-0.00852
std_min= 0.53572, std_max= 0.71746, std_mean= 0.64631
val lr: [0.0002192622950819672], policy lr: [0.00026311475409836063]
Policy Loss: -0.0085201, | Entropy Bonus: -0, | Value Loss: 59.23, | Advantage Loss: 4.8616
Time elapsed (s): 1.6711392402648926
Agent stdevs: 0.64631397
--------------------------------------------------------------------------------

Step 120
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1332.2
++++++++ Policy training ++++++++++
Current mean reward: 944.363562 | mean episode length: 289.714286
val_loss=241.84653
val_loss=442.80139
val_loss=272.92184
val_loss=304.02176
val_loss=128.97563
val_loss=259.14160
val_loss=85.47726
val_loss=85.38260
val_loss=121.72523
val_loss=100.84480
adv_loss=14.66793
adv_loss=12.37973
adv_loss= 6.86439
adv_loss=12.47893
adv_loss=11.92167
adv_loss=19.57771
adv_loss= 6.04019
adv_loss= 4.25789
adv_loss=16.69918
adv_loss= 6.18769
surrogate=-0.01746, entropy= 2.92122, loss=-0.01746
surrogate=-0.01267, entropy= 2.91923, loss=-0.01267
surrogate=-0.02086, entropy= 2.91884, loss=-0.02086
surrogate=-0.03445, entropy= 2.91815, loss=-0.03445
surrogate= 0.00539, entropy= 2.92075, loss= 0.00539
surrogate=-0.00942, entropy= 2.91981, loss=-0.00942
surrogate=-0.02772, entropy= 2.91979, loss=-0.02772
surrogate=-0.04093, entropy= 2.92033, loss=-0.04093
surrogate=-0.03109, entropy= 2.91825, loss=-0.03109
surrogate=-0.01842, entropy= 2.91982, loss=-0.01842
std_min= 0.53992, std_max= 0.72115, std_mean= 0.64520
val lr: [0.0002190061475409836], policy lr: [0.0002628073770491803]
Policy Loss: -0.018423, | Entropy Bonus: -0, | Value Loss: 100.84, | Advantage Loss: 6.1877
Time elapsed (s): 1.6623461246490479
Agent stdevs: 0.64519674
--------------------------------------------------------------------------------

Step 121
++++++++ Policy training ++++++++++
Current mean reward: 1815.130566 | mean episode length: 537.666667
val_loss=21.39084
val_loss=23.85642
val_loss=25.90101
val_loss=28.73139
val_loss=17.37060
val_loss=19.40195
val_loss=14.90286
val_loss=17.27258
val_loss=23.82575
val_loss=18.00148
adv_loss= 3.29228
adv_loss= 2.45016
adv_loss= 1.08095
adv_loss= 1.80929
adv_loss= 3.02338
adv_loss= 1.68684
adv_loss= 4.08503
adv_loss= 8.74783
adv_loss= 3.03752
adv_loss= 1.51310
surrogate= 0.02719, entropy= 2.91741, loss= 0.02719
surrogate= 0.00742, entropy= 2.91598, loss= 0.00742
surrogate=-0.01068, entropy= 2.91127, loss=-0.01068
surrogate= 0.00504, entropy= 2.91125, loss= 0.00504
surrogate= 0.00739, entropy= 2.90625, loss= 0.00739
surrogate=-0.01442, entropy= 2.90400, loss=-0.01442
surrogate=-0.00499, entropy= 2.90340, loss=-0.00499
surrogate= 0.00287, entropy= 2.89972, loss= 0.00287
surrogate=-0.02901, entropy= 2.89736, loss=-0.02901
surrogate=-0.00401, entropy= 2.89387, loss=-0.00401
std_min= 0.54077, std_max= 0.71114, std_mean= 0.63911
val lr: [0.00021875], policy lr: [0.0002625]
Policy Loss: -0.0040117, | Entropy Bonus: -0, | Value Loss: 18.001, | Advantage Loss: 1.5131
Time elapsed (s): 1.6871497631072998
Agent stdevs: 0.6391055
--------------------------------------------------------------------------------

Step 122
++++++++ Policy training ++++++++++
Current mean reward: 1772.751763 | mean episode length: 552.000000
val_loss=24.13923
val_loss=38.19363
val_loss=22.71596
val_loss=23.80772
val_loss=41.85656
val_loss=28.53060
val_loss=32.14015
val_loss=10.70557
val_loss=26.30686
val_loss=37.46114
adv_loss= 4.41153
adv_loss= 1.38751
adv_loss= 2.04880
adv_loss= 1.83946
adv_loss=12.44429
adv_loss= 8.48603
adv_loss= 2.16487
adv_loss= 2.27751
adv_loss= 1.92522
adv_loss= 3.72984
surrogate=-0.00988, entropy= 2.89203, loss=-0.00988
surrogate=-0.00335, entropy= 2.89237, loss=-0.00335
surrogate=-0.01815, entropy= 2.89275, loss=-0.01815
surrogate= 0.01920, entropy= 2.89400, loss= 0.01920
surrogate=-0.01218, entropy= 2.89555, loss=-0.01218
surrogate=-0.02397, entropy= 2.89511, loss=-0.02397
surrogate= 0.00224, entropy= 2.89353, loss= 0.00224
surrogate=-0.03529, entropy= 2.89066, loss=-0.03529
surrogate=-0.01307, entropy= 2.89111, loss=-0.01307
surrogate= 0.00374, entropy= 2.89227, loss= 0.00374
std_min= 0.53965, std_max= 0.71229, std_mean= 0.63888
val lr: [0.0002184938524590164], policy lr: [0.00026219262295081963]
Policy Loss: 0.0037405, | Entropy Bonus: -0, | Value Loss: 37.461, | Advantage Loss: 3.7298
Time elapsed (s): 1.6914787292480469
Agent stdevs: 0.63888264
--------------------------------------------------------------------------------

Step 123
++++++++ Policy training ++++++++++
Current mean reward: 1443.063908 | mean episode length: 436.500000
val_loss=87.21124
val_loss=53.43609
val_loss=114.87846
val_loss=39.46687
val_loss=106.68091
val_loss=39.15694
val_loss=78.37095
val_loss=33.25629
val_loss=76.01256
val_loss=53.22853
adv_loss= 2.07555
adv_loss= 2.68072
adv_loss= 3.75360
adv_loss= 2.36478
adv_loss= 8.23333
adv_loss= 3.50049
adv_loss= 2.50091
adv_loss= 5.97647
adv_loss= 4.16815
adv_loss= 1.98325
surrogate= 0.01638, entropy= 2.88800, loss= 0.01638
surrogate=-0.00598, entropy= 2.88097, loss=-0.00598
surrogate=-0.00292, entropy= 2.87809, loss=-0.00292
surrogate= 0.00030, entropy= 2.87456, loss= 0.00030
surrogate=-0.00525, entropy= 2.87329, loss=-0.00525
surrogate= 0.01548, entropy= 2.87004, loss= 0.01548
surrogate=-0.02002, entropy= 2.86736, loss=-0.02002
surrogate=-0.03162, entropy= 2.86700, loss=-0.03162
surrogate=-0.02431, entropy= 2.86467, loss=-0.02431
surrogate= 0.00972, entropy= 2.86514, loss= 0.00972
std_min= 0.53184, std_max= 0.70305, std_mean= 0.63328
val lr: [0.0002182377049180328], policy lr: [0.00026188524590163933]
Policy Loss: 0.0097168, | Entropy Bonus: -0, | Value Loss: 53.229, | Advantage Loss: 1.9833
Time elapsed (s): 1.6610748767852783
Agent stdevs: 0.6332752
--------------------------------------------------------------------------------

Step 124
++++++++ Policy training ++++++++++
Current mean reward: 1374.943666 | mean episode length: 405.800000
val_loss=189.30780
val_loss=207.72464
val_loss=100.98682
val_loss=69.69325
val_loss=60.44750
val_loss=71.61671
val_loss=87.74159
val_loss=55.91565
val_loss=65.65574
val_loss=37.10796
adv_loss= 8.11959
adv_loss= 3.82395
adv_loss= 4.47536
adv_loss= 5.45802
adv_loss= 7.50169
adv_loss= 8.86180
adv_loss= 5.35452
adv_loss= 3.25618
adv_loss= 9.36333
adv_loss= 3.93689
surrogate= 0.04098, entropy= 2.86391, loss= 0.04098
surrogate=-0.02494, entropy= 2.86496, loss=-0.02494
surrogate=-0.00576, entropy= 2.86198, loss=-0.00576
surrogate= 0.03613, entropy= 2.86235, loss= 0.03613
surrogate=-0.02724, entropy= 2.86395, loss=-0.02724
surrogate=-0.02580, entropy= 2.86393, loss=-0.02580
surrogate=-0.01764, entropy= 2.86503, loss=-0.01764
surrogate= 0.00364, entropy= 2.86441, loss= 0.00364
surrogate=-0.02281, entropy= 2.86561, loss=-0.02281
surrogate=-0.02061, entropy= 2.86652, loss=-0.02061
std_min= 0.54005, std_max= 0.69771, std_mean= 0.63288
val lr: [0.0002179815573770492], policy lr: [0.000261577868852459]
Policy Loss: -0.020608, | Entropy Bonus: -0, | Value Loss: 37.108, | Advantage Loss: 3.9369
Time elapsed (s): 1.6722054481506348
Agent stdevs: 0.63288045
--------------------------------------------------------------------------------

Step 125
++++++++ Policy training ++++++++++
Current mean reward: 1501.255909 | mean episode length: 448.500000
val_loss=92.38052
val_loss=45.65269
val_loss=96.30206
val_loss=153.52927
val_loss=29.96805
val_loss=44.74554
val_loss=62.52459
val_loss=55.85433
val_loss=62.04513
val_loss=38.46436
adv_loss= 3.52210
adv_loss= 5.35804
adv_loss= 7.31679
adv_loss= 1.91259
adv_loss=12.32473
adv_loss= 1.76607
adv_loss= 6.39744
adv_loss= 9.72338
adv_loss= 4.42731
adv_loss= 2.84300
surrogate= 0.00018, entropy= 2.86521, loss= 0.00018
surrogate= 0.00385, entropy= 2.86256, loss= 0.00385
surrogate=-0.01145, entropy= 2.86076, loss=-0.01145
surrogate= 0.00102, entropy= 2.86067, loss= 0.00102
surrogate=-0.01267, entropy= 2.85933, loss=-0.01267
surrogate=-0.00999, entropy= 2.85844, loss=-0.00999
surrogate=-0.01534, entropy= 2.85735, loss=-0.01534
surrogate= 0.01186, entropy= 2.85408, loss= 0.01186
surrogate=-0.01672, entropy= 2.85327, loss=-0.01672
surrogate=-0.02130, entropy= 2.85209, loss=-0.02130
std_min= 0.54041, std_max= 0.69451, std_mean= 0.62961
val lr: [0.0002177254098360656], policy lr: [0.0002612704918032787]
Policy Loss: -0.021302, | Entropy Bonus: -0, | Value Loss: 38.464, | Advantage Loss: 2.843
Time elapsed (s): 1.6671876907348633
Agent stdevs: 0.62961
--------------------------------------------------------------------------------

Step 126
++++++++ Policy training ++++++++++
Current mean reward: 1948.142883 | mean episode length: 623.666667
val_loss=102.32176
val_loss=65.71864
val_loss=191.00229
val_loss=237.02937
val_loss=442.31247
val_loss=50.73639
val_loss=53.41657
val_loss=818.44061
val_loss=111.41495
val_loss=578.19995
adv_loss= 6.45348
adv_loss= 2.38678
adv_loss= 3.89985
adv_loss=13.13550
adv_loss= 3.35566
adv_loss= 3.14680
adv_loss= 2.01132
adv_loss= 2.44010
adv_loss= 2.53954
adv_loss= 9.52814
surrogate=-0.01854, entropy= 2.85076, loss=-0.01854
surrogate=-0.00257, entropy= 2.84775, loss=-0.00257
surrogate= 0.01610, entropy= 2.84664, loss= 0.01610
surrogate= 0.00590, entropy= 2.84435, loss= 0.00590
surrogate=-0.03221, entropy= 2.84568, loss=-0.03221
surrogate=-0.00157, entropy= 2.84564, loss=-0.00157
surrogate= 0.00681, entropy= 2.84593, loss= 0.00681
surrogate= 0.01227, entropy= 2.84411, loss= 0.01227
surrogate=-0.03367, entropy= 2.84321, loss=-0.03367
surrogate=-0.02731, entropy= 2.84326, loss=-0.02731
std_min= 0.54560, std_max= 0.67914, std_mean= 0.62707
val lr: [0.00021746926229508195], policy lr: [0.00026096311475409833]
Policy Loss: -0.027313, | Entropy Bonus: -0, | Value Loss: 578.2, | Advantage Loss: 9.5281
Time elapsed (s): 1.6740682125091553
Agent stdevs: 0.62706906
--------------------------------------------------------------------------------

Step 127
++++++++ Policy training ++++++++++
Current mean reward: 1530.329533 | mean episode length: 476.750000
val_loss=62.37051
val_loss=41.98928
val_loss=58.64198
val_loss=121.62766
val_loss=37.10538
val_loss=111.75068
val_loss=78.56021
val_loss=46.03484
val_loss=30.47559
val_loss=37.85879
adv_loss= 3.53175
adv_loss= 2.34988
adv_loss=14.95776
adv_loss= 3.10665
adv_loss= 5.52459
adv_loss=12.40818
adv_loss= 5.21458
adv_loss= 4.47217
adv_loss=10.43225
adv_loss=91.13235
surrogate= 0.00325, entropy= 2.84055, loss= 0.00325
surrogate= 0.00702, entropy= 2.83469, loss= 0.00702
surrogate= 0.00134, entropy= 2.82918, loss= 0.00134
surrogate=-0.00472, entropy= 2.82532, loss=-0.00472
surrogate=-0.04086, entropy= 2.82178, loss=-0.04086
surrogate=-0.00085, entropy= 2.81684, loss=-0.00085
surrogate=-0.02859, entropy= 2.81482, loss=-0.02859
surrogate=-0.01628, entropy= 2.81211, loss=-0.01628
surrogate=-0.02891, entropy= 2.81329, loss=-0.02891
surrogate=-0.03808, entropy= 2.81036, loss=-0.03808
std_min= 0.53388, std_max= 0.67947, std_mean= 0.62078
val lr: [0.00021721311475409835], policy lr: [0.000260655737704918]
Policy Loss: -0.038077, | Entropy Bonus: -0, | Value Loss: 37.859, | Advantage Loss: 91.132
Time elapsed (s): 1.7343416213989258
Agent stdevs: 0.62077785
--------------------------------------------------------------------------------

Step 128
++++++++ Policy training ++++++++++
Current mean reward: 2652.412491 | mean episode length: 915.500000
val_loss=77.78286
val_loss=1086.89465
val_loss=152.60464
val_loss=629.78290
val_loss=612.31079
val_loss=786.35541
val_loss=57.48895
val_loss=124.51965
val_loss=65.01900
val_loss=42.15717
adv_loss= 3.69281
adv_loss= 6.43744
adv_loss= 7.78160
adv_loss= 5.84724
adv_loss= 4.77658
adv_loss= 3.59627
adv_loss= 5.87558
adv_loss= 3.59091
adv_loss= 4.42643
adv_loss= 4.87774
surrogate=-0.01132, entropy= 2.81207, loss=-0.01132
surrogate=-0.00064, entropy= 2.81060, loss=-0.00064
surrogate=-0.00151, entropy= 2.81025, loss=-0.00151
surrogate=-0.01507, entropy= 2.80899, loss=-0.01507
surrogate=-0.01458, entropy= 2.80935, loss=-0.01458
surrogate= 0.01305, entropy= 2.80786, loss= 0.01305
surrogate=-0.02165, entropy= 2.80951, loss=-0.02165
surrogate=-0.02228, entropy= 2.80855, loss=-0.02228
surrogate= 0.00569, entropy= 2.80761, loss= 0.00569
surrogate=-0.03184, entropy= 2.80870, loss=-0.03184
std_min= 0.53687, std_max= 0.67919, std_mean= 0.62020
val lr: [0.00021695696721311475], policy lr: [0.0002603483606557377]
Policy Loss: -0.03184, | Entropy Bonus: -0, | Value Loss: 42.157, | Advantage Loss: 4.8777
Time elapsed (s): 1.7049996852874756
Agent stdevs: 0.62019724
--------------------------------------------------------------------------------

Step 129
++++++++ Policy training ++++++++++
Current mean reward: 1357.243022 | mean episode length: 419.333333
val_loss=57.22059
val_loss=71.34587
val_loss=109.16259
val_loss=72.54921
val_loss=57.65267
val_loss=38.26841
val_loss=44.21788
val_loss=45.22078
val_loss=54.72948
val_loss=84.29742
adv_loss= 4.59020
adv_loss= 3.17261
adv_loss= 6.13902
adv_loss=10.16558
adv_loss= 3.08260
adv_loss= 4.21796
adv_loss=13.41829
adv_loss= 2.25585
adv_loss= 2.69935
adv_loss=12.05858
surrogate= 0.01046, entropy= 2.81044, loss= 0.01046
surrogate=-0.00833, entropy= 2.80957, loss=-0.00833
surrogate=-0.00602, entropy= 2.81093, loss=-0.00602
surrogate=-0.03844, entropy= 2.81287, loss=-0.03844
surrogate=-0.00580, entropy= 2.81336, loss=-0.00580
surrogate=-0.03491, entropy= 2.81252, loss=-0.03491
surrogate=-0.01495, entropy= 2.81334, loss=-0.01495
surrogate=-0.03832, entropy= 2.81485, loss=-0.03832
surrogate=-0.01842, entropy= 2.81319, loss=-0.01842
surrogate=-0.03680, entropy= 2.81423, loss=-0.03680
std_min= 0.54045, std_max= 0.67850, std_mean= 0.62113
val lr: [0.00021670081967213115], policy lr: [0.0002600409836065573]
Policy Loss: -0.036801, | Entropy Bonus: -0, | Value Loss: 84.297, | Advantage Loss: 12.059
Time elapsed (s): 1.6663939952850342
Agent stdevs: 0.62112546
--------------------------------------------------------------------------------

Step 130
++++++++ Policy training ++++++++++
Current mean reward: 1737.792274 | mean episode length: 562.666667
val_loss=102.60330
val_loss=54.22281
val_loss=41.84458
val_loss=659.21735
val_loss=210.00162
val_loss=189.05524
val_loss=410.00464
val_loss=835.44482
val_loss=43.55726
val_loss=207.37682
adv_loss= 3.12545
adv_loss= 3.83229
adv_loss= 4.50187
adv_loss= 2.88506
adv_loss= 3.66602
adv_loss=1334.96045
adv_loss= 3.96628
adv_loss= 3.95951
adv_loss= 2.50959
adv_loss= 4.17654
surrogate=-0.00560, entropy= 2.81041, loss=-0.00560
surrogate= 0.01451, entropy= 2.80734, loss= 0.01451
surrogate=-0.01555, entropy= 2.80675, loss=-0.01555
surrogate= 0.01492, entropy= 2.80625, loss= 0.01492
surrogate=-0.03438, entropy= 2.80638, loss=-0.03438
surrogate=-0.02796, entropy= 2.80821, loss=-0.02796
surrogate=-0.01835, entropy= 2.80849, loss=-0.01835
surrogate=-0.03073, entropy= 2.81003, loss=-0.03073
surrogate=-0.03151, entropy= 2.81285, loss=-0.03151
surrogate=-0.03055, entropy= 2.81207, loss=-0.03055
std_min= 0.54298, std_max= 0.67563, std_mean= 0.62050
val lr: [0.00021644467213114755], policy lr: [0.000259733606557377]
Policy Loss: -0.030549, | Entropy Bonus: -0, | Value Loss: 207.38, | Advantage Loss: 4.1765
Time elapsed (s): 1.6809306144714355
Agent stdevs: 0.62050194
--------------------------------------------------------------------------------

Step 131
++++++++ Policy training ++++++++++
Current mean reward: 779.094654 | mean episode length: 234.000000
val_loss=143.39037
val_loss=108.38258
val_loss=183.78853
val_loss=84.51436
val_loss=301.02896
val_loss=138.07426
val_loss=18.79169
val_loss=98.70865
val_loss=221.75055
val_loss=35.27149
adv_loss=12.14155
adv_loss= 8.96374
adv_loss= 7.65269
adv_loss=14.29178
adv_loss= 5.76826
adv_loss= 4.25372
adv_loss= 7.10529
adv_loss= 4.24217
adv_loss=235.88937
adv_loss=17.71754
surrogate=-0.01762, entropy= 2.81089, loss=-0.01762
surrogate=-0.01055, entropy= 2.80979, loss=-0.01055
surrogate= 0.03677, entropy= 2.80985, loss= 0.03677
surrogate=-0.01085, entropy= 2.80735, loss=-0.01085
surrogate=-0.01536, entropy= 2.80161, loss=-0.01536
surrogate=-0.00180, entropy= 2.79963, loss=-0.00180
surrogate= 0.00848, entropy= 2.79961, loss= 0.00848
surrogate=-0.04878, entropy= 2.79731, loss=-0.04878
surrogate=-0.03293, entropy= 2.79358, loss=-0.03293
surrogate=-0.01546, entropy= 2.79417, loss=-0.01546
std_min= 0.53551, std_max= 0.66633, std_mean= 0.61696
val lr: [0.00021618852459016395], policy lr: [0.00025942622950819673]
Policy Loss: -0.015463, | Entropy Bonus: -0, | Value Loss: 35.271, | Advantage Loss: 17.718
Time elapsed (s): 1.7013421058654785
Agent stdevs: 0.6169646
--------------------------------------------------------------------------------

Step 132
++++++++ Policy training ++++++++++
Current mean reward: 1763.208005 | mean episode length: 578.333333
val_loss=864.56018
val_loss=704.67236
val_loss=70.31066
val_loss=44.18636
val_loss=587.74725
val_loss=109.43594
val_loss=713.95135
val_loss=2082.67627
val_loss=32.77002
val_loss=98.39075
adv_loss= 3.47152
adv_loss= 5.99691
adv_loss= 3.45061
adv_loss= 2.96427
adv_loss= 2.00642
adv_loss= 2.86508
adv_loss= 2.75255
adv_loss= 5.73221
adv_loss= 3.14201
adv_loss= 2.50955
surrogate= 0.00280, entropy= 2.79120, loss= 0.00280
surrogate= 0.01144, entropy= 2.79342, loss= 0.01144
surrogate=-0.02728, entropy= 2.78850, loss=-0.02728
surrogate=-0.00718, entropy= 2.78860, loss=-0.00718
surrogate= 0.00675, entropy= 2.78803, loss= 0.00675
surrogate= 0.00456, entropy= 2.78652, loss= 0.00456
surrogate=-0.00704, entropy= 2.78830, loss=-0.00704
surrogate=-0.01799, entropy= 2.78740, loss=-0.01799
surrogate=-0.02153, entropy= 2.78766, loss=-0.02153
surrogate= 0.00075, entropy= 2.78710, loss= 0.00075
std_min= 0.54081, std_max= 0.66060, std_mean= 0.61505
val lr: [0.00021593237704918035], policy lr: [0.0002591188524590164]
Policy Loss: 0.00075298, | Entropy Bonus: -0, | Value Loss: 98.391, | Advantage Loss: 2.5095
Time elapsed (s): 1.6798286437988281
Agent stdevs: 0.6150541
--------------------------------------------------------------------------------

Step 133
++++++++ Policy training ++++++++++
Current mean reward: 951.242306 | mean episode length: 284.285714
val_loss=33.10602
val_loss=34.03007
val_loss=22.73977
val_loss=39.06993
val_loss=51.69377
val_loss=22.99979
val_loss=45.54778
val_loss=29.61278
val_loss=35.77716
val_loss=34.47303
adv_loss= 2.95648
adv_loss= 8.99773
adv_loss= 7.19719
adv_loss=27.73510
adv_loss=11.44400
adv_loss= 2.93669
adv_loss= 7.75930
adv_loss=11.41636
adv_loss= 3.27941
adv_loss= 6.33504
surrogate= 0.03059, entropy= 2.78929, loss= 0.03059
surrogate= 0.00609, entropy= 2.79006, loss= 0.00609
surrogate=-0.01017, entropy= 2.78807, loss=-0.01017
surrogate=-0.00981, entropy= 2.78987, loss=-0.00981
surrogate=-0.02714, entropy= 2.78842, loss=-0.02714
surrogate=-0.02195, entropy= 2.78649, loss=-0.02195
surrogate=-0.02056, entropy= 2.78668, loss=-0.02056
surrogate= 0.00443, entropy= 2.78408, loss= 0.00443
surrogate=-0.02556, entropy= 2.78481, loss=-0.02556
surrogate=-0.00546, entropy= 2.78565, loss=-0.00546
std_min= 0.53301, std_max= 0.66981, std_mean= 0.61536
val lr: [0.00021567622950819672], policy lr: [0.000258811475409836]
Policy Loss: -0.0054618, | Entropy Bonus: -0, | Value Loss: 34.473, | Advantage Loss: 6.335
Time elapsed (s): 1.730071783065796
Agent stdevs: 0.6153632
--------------------------------------------------------------------------------

Step 134
++++++++ Policy training ++++++++++
Current mean reward: 1183.294390 | mean episode length: 358.200000
val_loss=334.88892
val_loss=90.60970
val_loss=239.84865
val_loss=79.55569
val_loss=113.64137
val_loss=130.48050
val_loss=54.32785
val_loss=117.87123
val_loss=77.27882
val_loss=102.87520
adv_loss=332.34857
adv_loss=12.16088
adv_loss=14.15456
adv_loss= 7.79655
adv_loss=265.85101
adv_loss= 4.45334
adv_loss= 8.94552
adv_loss= 4.79955
adv_loss= 7.27651
adv_loss= 6.15994
surrogate=-0.00640, entropy= 2.78363, loss=-0.00640
surrogate= 0.00172, entropy= 2.78109, loss= 0.00172
surrogate=-0.01944, entropy= 2.78209, loss=-0.01944
surrogate=-0.02562, entropy= 2.78039, loss=-0.02562
surrogate= 0.00198, entropy= 2.77911, loss= 0.00198
surrogate=-0.01025, entropy= 2.77883, loss=-0.01025
surrogate=-0.02894, entropy= 2.77996, loss=-0.02894
surrogate=-0.00304, entropy= 2.78039, loss=-0.00304
surrogate= 0.01304, entropy= 2.78016, loss= 0.01304
surrogate=-0.01793, entropy= 2.78145, loss=-0.01793
std_min= 0.52959, std_max= 0.67044, std_mean= 0.61472
val lr: [0.00021542008196721312], policy lr: [0.0002585040983606557]
Policy Loss: -0.017932, | Entropy Bonus: -0, | Value Loss: 102.88, | Advantage Loss: 6.1599
Time elapsed (s): 1.6979529857635498
Agent stdevs: 0.61472285
--------------------------------------------------------------------------------

Step 135
++++++++ Policy training ++++++++++
Current mean reward: 1425.508287 | mean episode length: 447.750000
val_loss=228.06335
val_loss=77.63631
val_loss=118.24709
val_loss=1038.83435
val_loss=1399.55518
val_loss=356.18121
val_loss=45.17570
val_loss=83.18864
val_loss=306.20416
val_loss=34.36540
adv_loss= 6.50530
adv_loss=10.76446
adv_loss= 4.80622
adv_loss= 2.80182
adv_loss= 2.02635
adv_loss= 3.86729
adv_loss=11.66733
adv_loss= 3.32137
adv_loss=1365.38831
adv_loss= 4.24248
surrogate= 0.05519, entropy= 2.78268, loss= 0.05519
surrogate=-0.02214, entropy= 2.78027, loss=-0.02214
surrogate=-0.02474, entropy= 2.77880, loss=-0.02474
surrogate=-0.02590, entropy= 2.77748, loss=-0.02590
surrogate=-0.01045, entropy= 2.77402, loss=-0.01045
surrogate=-0.00799, entropy= 2.77143, loss=-0.00799
surrogate= 0.02107, entropy= 2.76837, loss= 0.02107
surrogate=-0.03304, entropy= 2.76763, loss=-0.03304
surrogate= 0.01124, entropy= 2.76728, loss= 0.01124
surrogate= 0.02777, entropy= 2.76394, loss= 0.02777
std_min= 0.53008, std_max= 0.66184, std_mean= 0.61081
val lr: [0.0002151639344262295], policy lr: [0.00025819672131147537]
Policy Loss: 0.027772, | Entropy Bonus: -0, | Value Loss: 34.365, | Advantage Loss: 4.2425
Time elapsed (s): 1.6714465618133545
Agent stdevs: 0.61081004
--------------------------------------------------------------------------------

Step 136
++++++++ Policy training ++++++++++
Current mean reward: 948.298933 | mean episode length: 292.166667
val_loss=128.33781
val_loss=844.55731
val_loss=58.09887
val_loss=107.74329
val_loss=61.52054
val_loss=432.23923
val_loss=170.13316
val_loss=751.26202
val_loss=351.33063
val_loss=127.17065
adv_loss= 4.10728
adv_loss= 7.87478
adv_loss= 7.92558
adv_loss= 5.60412
adv_loss= 6.24936
adv_loss= 7.10305
adv_loss= 2.82850
adv_loss= 2.99041
adv_loss= 4.61354
adv_loss= 9.21967
surrogate= 0.00236, entropy= 2.75988, loss= 0.00236
surrogate=-0.03337, entropy= 2.75778, loss=-0.03337
surrogate= 0.03270, entropy= 2.75610, loss= 0.03270
surrogate= 0.01165, entropy= 2.75581, loss= 0.01165
surrogate= 0.01931, entropy= 2.75483, loss= 0.01931
surrogate=-0.01053, entropy= 2.75575, loss=-0.01053
surrogate=-0.04513, entropy= 2.75354, loss=-0.04513
surrogate= 0.00122, entropy= 2.75318, loss= 0.00122
surrogate=-0.00832, entropy= 2.74988, loss=-0.00832
surrogate=-0.02144, entropy= 2.75005, loss=-0.02144
std_min= 0.53172, std_max= 0.65729, std_mean= 0.60770
val lr: [0.0002149077868852459], policy lr: [0.0002578893442622951]
Policy Loss: -0.021442, | Entropy Bonus: -0, | Value Loss: 127.17, | Advantage Loss: 9.2197
Time elapsed (s): 1.6766149997711182
Agent stdevs: 0.607703
--------------------------------------------------------------------------------

Step 137
++++++++ Policy training ++++++++++
Current mean reward: 1068.463229 | mean episode length: 324.833333
val_loss=44.11273
val_loss=266.08408
val_loss=507.51019
val_loss=45.61473
val_loss=509.35315
val_loss=48.28450
val_loss=178.35913
val_loss=690.63171
val_loss=46.03734
val_loss=272.90872
adv_loss=11.39284
adv_loss= 5.67377
adv_loss= 4.52296
adv_loss= 5.83735
adv_loss= 3.67785
adv_loss= 4.00344
adv_loss= 6.58377
adv_loss=663.08173
adv_loss= 6.79276
adv_loss= 3.31605
surrogate=-0.01322, entropy= 2.75071, loss=-0.01322
surrogate= 0.02776, entropy= 2.74975, loss= 0.02776
surrogate= 0.01641, entropy= 2.74894, loss= 0.01641
surrogate=-0.02951, entropy= 2.74953, loss=-0.02951
surrogate= 0.00409, entropy= 2.75054, loss= 0.00409
surrogate=-0.00246, entropy= 2.75140, loss=-0.00246
surrogate=-0.00945, entropy= 2.75265, loss=-0.00945
surrogate=-0.01767, entropy= 2.75191, loss=-0.01767
surrogate=-0.01153, entropy= 2.75240, loss=-0.01153
surrogate=-0.03727, entropy= 2.75318, loss=-0.03727
std_min= 0.53449, std_max= 0.65792, std_mean= 0.60820
val lr: [0.0002146516393442623], policy lr: [0.0002575819672131147]
Policy Loss: -0.03727, | Entropy Bonus: -0, | Value Loss: 272.91, | Advantage Loss: 3.3161
Time elapsed (s): 1.6996023654937744
Agent stdevs: 0.60820055
--------------------------------------------------------------------------------

Step 138
++++++++ Policy training ++++++++++
Current mean reward: 1098.181087 | mean episode length: 329.000000
val_loss=53.74124
val_loss=133.80147
val_loss=47.54825
val_loss=54.94408
val_loss=45.82087
val_loss=22.43588
val_loss=35.17284
val_loss=19.37033
val_loss=15.24634
val_loss=20.77890
adv_loss= 2.22166
adv_loss= 7.17355
adv_loss= 5.93344
adv_loss= 3.77164
adv_loss= 5.84587
adv_loss= 3.16114
adv_loss= 1.60295
adv_loss= 5.47348
adv_loss=12.91514
adv_loss= 3.77601
surrogate= 0.01040, entropy= 2.75590, loss= 0.01040
surrogate= 0.00534, entropy= 2.75862, loss= 0.00534
surrogate= 0.03684, entropy= 2.76128, loss= 0.03684
surrogate=-0.00570, entropy= 2.76221, loss=-0.00570
surrogate=-0.01522, entropy= 2.76340, loss=-0.01522
surrogate=-0.01066, entropy= 2.76375, loss=-0.01066
surrogate=-0.00893, entropy= 2.76444, loss=-0.00893
surrogate=-0.00241, entropy= 2.76759, loss=-0.00241
surrogate=-0.02054, entropy= 2.76903, loss=-0.02054
surrogate=-0.02728, entropy= 2.76740, loss=-0.02728
std_min= 0.53650, std_max= 0.65487, std_mean= 0.61109
val lr: [0.0002143954918032787], policy lr: [0.0002572745901639344]
Policy Loss: -0.027284, | Entropy Bonus: -0, | Value Loss: 20.779, | Advantage Loss: 3.776
Time elapsed (s): 1.6858117580413818
Agent stdevs: 0.6110938
--------------------------------------------------------------------------------

Step 139
++++++++ Policy training ++++++++++
Current mean reward: 1150.197534 | mean episode length: 348.000000
val_loss=77.59853
val_loss=45.49379
val_loss=16.95610
val_loss=25.82958
val_loss=25.91795
val_loss=33.62709
val_loss=28.79754
val_loss=20.12669
val_loss=16.69947
val_loss=45.59187
adv_loss= 3.93291
adv_loss=10.44189
adv_loss= 6.42273
adv_loss= 2.34161
adv_loss= 4.19830
adv_loss= 4.59136
adv_loss= 6.02735
adv_loss= 1.66501
adv_loss= 5.15665
adv_loss= 3.84979
surrogate= 0.06396, entropy= 2.76411, loss= 0.06396
surrogate=-0.02165, entropy= 2.76046, loss=-0.02165
surrogate= 0.00908, entropy= 2.75653, loss= 0.00908
surrogate=-0.03484, entropy= 2.75212, loss=-0.03484
surrogate=-0.00069, entropy= 2.74882, loss=-0.00069
surrogate=-0.01332, entropy= 2.74378, loss=-0.01332
surrogate= 0.00127, entropy= 2.74070, loss= 0.00127
surrogate=-0.01993, entropy= 2.73764, loss=-0.01993
surrogate=-0.00672, entropy= 2.73741, loss=-0.00672
surrogate=-0.01414, entropy= 2.73649, loss=-0.01414
std_min= 0.53049, std_max= 0.65548, std_mean= 0.60491
val lr: [0.0002141393442622951], policy lr: [0.00025696721311475407]
Policy Loss: -0.014137, | Entropy Bonus: -0, | Value Loss: 45.592, | Advantage Loss: 3.8498
Time elapsed (s): 1.750495195388794
Agent stdevs: 0.60490966
--------------------------------------------------------------------------------

Step 140
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1138.1
++++++++ Policy training ++++++++++
Current mean reward: 1923.356094 | mean episode length: 596.666667
val_loss=17.69901
val_loss=15.09831
val_loss=19.29879
val_loss=13.61679
val_loss=19.16061
val_loss=19.04590
val_loss=17.32627
val_loss=32.76698
val_loss=31.63226
val_loss=15.44456
adv_loss= 2.24866
adv_loss= 2.36003
adv_loss= 2.56628
adv_loss= 2.45238
adv_loss= 2.35006
adv_loss= 7.37875
adv_loss= 5.50940
adv_loss= 8.24703
adv_loss= 6.11397
adv_loss= 2.72260
surrogate=-0.01267, entropy= 2.73800, loss=-0.01267
surrogate=-0.00136, entropy= 2.73442, loss=-0.00136
surrogate=-0.00041, entropy= 2.73386, loss=-0.00041
surrogate= 0.00053, entropy= 2.73072, loss= 0.00053
surrogate=-0.03176, entropy= 2.73200, loss=-0.03176
surrogate=-0.00804, entropy= 2.73247, loss=-0.00804
surrogate=-0.00631, entropy= 2.72887, loss=-0.00631
surrogate=-0.03264, entropy= 2.72695, loss=-0.03264
surrogate=-0.01630, entropy= 2.72384, loss=-0.01630
surrogate=-0.00397, entropy= 2.72386, loss=-0.00397
std_min= 0.52751, std_max= 0.65274, std_mean= 0.60241
val lr: [0.0002138831967213115], policy lr: [0.00025665983606557377]
Policy Loss: -0.0039735, | Entropy Bonus: -0, | Value Loss: 15.445, | Advantage Loss: 2.7226
Time elapsed (s): 1.7029695510864258
Agent stdevs: 0.602413
--------------------------------------------------------------------------------

Step 141
++++++++ Policy training ++++++++++
Current mean reward: 1213.530105 | mean episode length: 378.000000
val_loss=35.10676
val_loss=19.57616
val_loss=19.26413
val_loss=15.75720
val_loss=21.85997
val_loss=18.13670
val_loss=16.96495
val_loss=14.69735
val_loss=13.61641
val_loss= 8.17348
adv_loss= 3.07804
adv_loss= 7.60166
adv_loss= 2.84493
adv_loss= 1.96633
adv_loss= 1.64943
adv_loss= 7.88479
adv_loss= 2.20213
adv_loss= 2.96200
adv_loss= 1.86068
adv_loss= 1.48658
surrogate= 0.01706, entropy= 2.72423, loss= 0.01706
surrogate=-0.01291, entropy= 2.72559, loss=-0.01291
surrogate=-0.02168, entropy= 2.72403, loss=-0.02168
surrogate=-0.00604, entropy= 2.72378, loss=-0.00604
surrogate= 0.01048, entropy= 2.72381, loss= 0.01048
surrogate= 0.00313, entropy= 2.72193, loss= 0.00313
surrogate=-0.00560, entropy= 2.71992, loss=-0.00560
surrogate=-0.01220, entropy= 2.72039, loss=-0.01220
surrogate=-0.01261, entropy= 2.72274, loss=-0.01261
surrogate=-0.02151, entropy= 2.72380, loss=-0.02151
std_min= 0.53390, std_max= 0.65588, std_mean= 0.60209
val lr: [0.0002136270491803279], policy lr: [0.0002563524590163934]
Policy Loss: -0.02151, | Entropy Bonus: -0, | Value Loss: 8.1735, | Advantage Loss: 1.4866
Time elapsed (s): 1.7043707370758057
Agent stdevs: 0.60209113
--------------------------------------------------------------------------------

Step 142
++++++++ Policy training ++++++++++
Current mean reward: 937.054648 | mean episode length: 282.857143
val_loss=50.50002
val_loss=39.90676
val_loss=44.82497
val_loss=80.42925
val_loss=48.14881
val_loss=32.33691
val_loss=41.13984
val_loss=22.62619
val_loss=42.66489
val_loss=35.59554
adv_loss= 8.67315
adv_loss= 2.66021
adv_loss= 6.92237
adv_loss= 3.50433
adv_loss=22.10616
adv_loss= 4.65511
adv_loss= 5.09545
adv_loss= 6.42148
adv_loss=38.69293
adv_loss= 2.00859
surrogate=-0.00772, entropy= 2.72337, loss=-0.00772
surrogate=-0.00423, entropy= 2.72366, loss=-0.00423
surrogate=-0.00718, entropy= 2.72125, loss=-0.00718
surrogate=-0.02527, entropy= 2.72307, loss=-0.02527
surrogate=-0.00801, entropy= 2.72162, loss=-0.00801
surrogate= 0.02658, entropy= 2.72094, loss= 0.02658
surrogate=-0.04080, entropy= 2.71764, loss=-0.04080
surrogate=-0.04918, entropy= 2.71954, loss=-0.04918
surrogate=-0.03497, entropy= 2.71764, loss=-0.03497
surrogate=-0.01133, entropy= 2.71349, loss=-0.01133
std_min= 0.53279, std_max= 0.65340, std_mean= 0.59998
val lr: [0.00021337090163934424], policy lr: [0.00025604508196721307]
Policy Loss: -0.011335, | Entropy Bonus: -0, | Value Loss: 35.596, | Advantage Loss: 2.0086
Time elapsed (s): 1.7143833637237549
Agent stdevs: 0.59998304
--------------------------------------------------------------------------------

Step 143
++++++++ Policy training ++++++++++
Current mean reward: 953.421416 | mean episode length: 288.571429
val_loss=50.11437
val_loss=33.32763
val_loss=26.13695
val_loss=68.08536
val_loss=17.99671
val_loss=19.87356
val_loss=46.00132
val_loss=46.34541
val_loss=16.88685
val_loss=35.11740
adv_loss= 4.25195
adv_loss=21.73618
adv_loss= 2.21427
adv_loss= 3.08436
adv_loss= 4.24086
adv_loss=12.75034
adv_loss= 3.62009
adv_loss= 4.13408
adv_loss= 2.65750
adv_loss=13.30849
surrogate=-0.00121, entropy= 2.71519, loss=-0.00121
surrogate= 0.01280, entropy= 2.71498, loss= 0.01280
surrogate=-0.02758, entropy= 2.71549, loss=-0.02758
surrogate=-0.02036, entropy= 2.71647, loss=-0.02036
surrogate=-0.00168, entropy= 2.71807, loss=-0.00168
surrogate=-0.00328, entropy= 2.71538, loss=-0.00328
surrogate=-0.04350, entropy= 2.71656, loss=-0.04350
surrogate= 0.00774, entropy= 2.71455, loss= 0.00774
surrogate=-0.00819, entropy= 2.71422, loss=-0.00819
surrogate=-0.02938, entropy= 2.71684, loss=-0.02938
std_min= 0.53581, std_max= 0.64701, std_mean= 0.60040
val lr: [0.00021311475409836067], policy lr: [0.00025573770491803277]
Policy Loss: -0.029381, | Entropy Bonus: -0, | Value Loss: 35.117, | Advantage Loss: 13.308
Time elapsed (s): 1.7070598602294922
Agent stdevs: 0.60039854
--------------------------------------------------------------------------------

Step 144
++++++++ Policy training ++++++++++
Current mean reward: 890.763467 | mean episode length: 265.428571
val_loss=33.78579
val_loss=41.93269
val_loss=55.46222
val_loss=29.79335
val_loss=41.48186
val_loss=56.54732
val_loss=30.29405
val_loss=42.47018
val_loss=76.72158
val_loss=47.89312
adv_loss= 1.82249
adv_loss=40.97419
adv_loss= 4.46064
adv_loss=11.14582
adv_loss= 4.42065
adv_loss= 8.04240
adv_loss= 3.95872
adv_loss= 2.93864
adv_loss= 4.68068
adv_loss= 4.78657
surrogate=-0.01535, entropy= 2.71492, loss=-0.01535
surrogate=-0.01497, entropy= 2.71292, loss=-0.01497
surrogate=-0.00404, entropy= 2.71062, loss=-0.00404
surrogate=-0.01211, entropy= 2.70844, loss=-0.01211
surrogate=-0.01262, entropy= 2.70554, loss=-0.01262
surrogate=-0.00368, entropy= 2.70480, loss=-0.00368
surrogate= 0.01521, entropy= 2.70403, loss= 0.01521
surrogate=-0.04319, entropy= 2.70257, loss=-0.04319
surrogate=-0.02302, entropy= 2.70056, loss=-0.02302
surrogate=-0.00899, entropy= 2.69638, loss=-0.00899
std_min= 0.53046, std_max= 0.64782, std_mean= 0.59647
val lr: [0.00021285860655737704], policy lr: [0.0002554303278688524]
Policy Loss: -0.0089856, | Entropy Bonus: -0, | Value Loss: 47.893, | Advantage Loss: 4.7866
Time elapsed (s): 1.682769536972046
Agent stdevs: 0.59646577
--------------------------------------------------------------------------------

Step 145
++++++++ Policy training ++++++++++
Current mean reward: 940.048812 | mean episode length: 283.714286
val_loss=76.45315
val_loss=60.75660
val_loss=126.60601
val_loss=654.17908
val_loss=234.97382
val_loss=73.29174
val_loss=171.72612
val_loss=51.74523
val_loss=44.60922
val_loss=78.47057
adv_loss=783.79761
adv_loss= 6.75557
adv_loss= 1.99378
adv_loss= 6.04339
adv_loss= 2.01854
adv_loss= 3.35127
adv_loss= 2.09010
adv_loss= 2.76109
adv_loss= 4.91788
adv_loss= 4.96798
surrogate=-0.02554, entropy= 2.69226, loss=-0.02554
surrogate= 0.00879, entropy= 2.68956, loss= 0.00879
surrogate= 0.00659, entropy= 2.68658, loss= 0.00659
surrogate=-0.00356, entropy= 2.68369, loss=-0.00356
surrogate= 0.00484, entropy= 2.68456, loss= 0.00484
surrogate=-0.01427, entropy= 2.68360, loss=-0.01427
surrogate= 0.00511, entropy= 2.68170, loss= 0.00511
surrogate=-0.01668, entropy= 2.68105, loss=-0.01668
surrogate=-0.02014, entropy= 2.67978, loss=-0.02014
surrogate=-0.00770, entropy= 2.67710, loss=-0.00770
std_min= 0.53378, std_max= 0.64162, std_mean= 0.59230
val lr: [0.00021260245901639344], policy lr: [0.0002551229508196721]
Policy Loss: -0.0077043, | Entropy Bonus: -0, | Value Loss: 78.471, | Advantage Loss: 4.968
Time elapsed (s): 1.7752528190612793
Agent stdevs: 0.59230167
--------------------------------------------------------------------------------

Step 146
++++++++ Policy training ++++++++++
Current mean reward: 1001.556624 | mean episode length: 309.200000
val_loss=29.03162
val_loss=25.34817
val_loss=31.97626
val_loss=28.30178
val_loss=18.92546
val_loss=44.58475
val_loss=30.88023
val_loss=32.92715
val_loss=19.26824
val_loss=22.37027
adv_loss= 5.88653
adv_loss= 5.08722
adv_loss= 3.10294
adv_loss= 3.43198
adv_loss= 2.33430
adv_loss= 3.78987
adv_loss= 1.13951
adv_loss= 6.98579
adv_loss= 6.73792
adv_loss= 4.70402
surrogate=-0.01361, entropy= 2.67126, loss=-0.01361
surrogate=-0.01618, entropy= 2.66882, loss=-0.01618
surrogate=-0.01501, entropy= 2.66822, loss=-0.01501
surrogate=-0.01075, entropy= 2.66227, loss=-0.01075
surrogate=-0.02122, entropy= 2.65962, loss=-0.02122
surrogate=-0.01461, entropy= 2.65628, loss=-0.01461
surrogate=-0.00359, entropy= 2.65276, loss=-0.00359
surrogate= 0.00258, entropy= 2.65001, loss= 0.00258
surrogate=-0.03625, entropy= 2.64683, loss=-0.03625
surrogate=-0.01250, entropy= 2.64365, loss=-0.01250
std_min= 0.52931, std_max= 0.63580, std_mean= 0.58571
val lr: [0.00021234631147540984], policy lr: [0.00025481557377049176]
Policy Loss: -0.012498, | Entropy Bonus: -0, | Value Loss: 22.37, | Advantage Loss: 4.704
Time elapsed (s): 1.6963391304016113
Agent stdevs: 0.5857129
--------------------------------------------------------------------------------

Step 147
++++++++ Policy training ++++++++++
Current mean reward: 1224.951772 | mean episode length: 363.400000
val_loss=31.38802
val_loss=19.37464
val_loss=20.37012
val_loss=15.21155
val_loss= 8.45933
val_loss=16.16678
val_loss=15.17601
val_loss= 8.92185
val_loss=15.47026
val_loss=34.70179
adv_loss= 3.30041
adv_loss= 2.33803
adv_loss=12.72984
adv_loss= 1.86079
adv_loss= 1.75200
adv_loss= 2.66472
adv_loss= 8.63597
adv_loss= 5.05384
adv_loss= 5.39256
adv_loss= 2.96843
surrogate= 0.03869, entropy= 2.64299, loss= 0.03869
surrogate= 0.00332, entropy= 2.64314, loss= 0.00332
surrogate=-0.01933, entropy= 2.64355, loss=-0.01933
surrogate= 0.00726, entropy= 2.64234, loss= 0.00726
surrogate=-0.02195, entropy= 2.64159, loss=-0.02195
surrogate= 0.00642, entropy= 2.63996, loss= 0.00642
surrogate=-0.01625, entropy= 2.63984, loss=-0.01625
surrogate=-0.01909, entropy= 2.63871, loss=-0.01909
surrogate= 0.00664, entropy= 2.63806, loss= 0.00664
surrogate=-0.01534, entropy= 2.63726, loss=-0.01534
std_min= 0.53353, std_max= 0.62215, std_mean= 0.58401
val lr: [0.00021209016393442624], policy lr: [0.00025450819672131146]
Policy Loss: -0.015341, | Entropy Bonus: -0, | Value Loss: 34.702, | Advantage Loss: 2.9684
Time elapsed (s): 1.7093753814697266
Agent stdevs: 0.584012
--------------------------------------------------------------------------------

Step 148
++++++++ Policy training ++++++++++
Current mean reward: 1050.616825 | mean episode length: 309.833333
val_loss=34.78382
val_loss=24.94454
val_loss=23.99944
val_loss=18.04143
val_loss=27.60066
val_loss=44.90840
val_loss=16.15519
val_loss=25.04049
val_loss=21.25070
val_loss=20.15159
adv_loss= 3.06826
adv_loss= 1.96379
adv_loss= 1.54831
adv_loss= 2.39458
adv_loss= 1.91645
adv_loss= 3.72110
adv_loss= 3.06778
adv_loss= 2.20104
adv_loss= 2.89828
adv_loss= 2.15613
surrogate=-0.01420, entropy= 2.63801, loss=-0.01420
surrogate= 0.01060, entropy= 2.63695, loss= 0.01060
surrogate= 0.00727, entropy= 2.63586, loss= 0.00727
surrogate= 0.02179, entropy= 2.63657, loss= 0.02179
surrogate= 0.00728, entropy= 2.63682, loss= 0.00728
surrogate= 0.00654, entropy= 2.63684, loss= 0.00654
surrogate=-0.01138, entropy= 2.63576, loss=-0.01138
surrogate=-0.00487, entropy= 2.63714, loss=-0.00487
surrogate=-0.00972, entropy= 2.63513, loss=-0.00972
surrogate=-0.02624, entropy= 2.63422, loss=-0.02624
std_min= 0.53022, std_max= 0.62694, std_mean= 0.58366
val lr: [0.00021183401639344264], policy lr: [0.0002542008196721311]
Policy Loss: -0.026237, | Entropy Bonus: -0, | Value Loss: 20.152, | Advantage Loss: 2.1561
Time elapsed (s): 1.7188374996185303
Agent stdevs: 0.5836599
--------------------------------------------------------------------------------

Step 149
++++++++ Policy training ++++++++++
Current mean reward: 1449.715994 | mean episode length: 437.500000
val_loss=28.35216
val_loss=14.71696
val_loss=16.45557
val_loss=19.90560
val_loss=12.08442
val_loss=13.69358
val_loss=12.90366
val_loss=20.41256
val_loss=12.17149
val_loss= 9.60230
adv_loss= 1.77408
adv_loss= 5.21077
adv_loss= 2.83388
adv_loss= 5.48936
adv_loss= 2.34211
adv_loss= 2.29678
adv_loss= 4.41521
adv_loss= 1.81429
adv_loss= 3.53138
adv_loss= 4.13411
surrogate=-0.00007, entropy= 2.63415, loss=-0.00007
surrogate=-0.00778, entropy= 2.63561, loss=-0.00778
surrogate= 0.00690, entropy= 2.63508, loss= 0.00690
surrogate= 0.02248, entropy= 2.63580, loss= 0.02248
surrogate=-0.00242, entropy= 2.63596, loss=-0.00242
surrogate=-0.01604, entropy= 2.63467, loss=-0.01604
surrogate=-0.00076, entropy= 2.63508, loss=-0.00076
surrogate=-0.01164, entropy= 2.63605, loss=-0.01164
surrogate=-0.01826, entropy= 2.63504, loss=-0.01826
surrogate=-0.00707, entropy= 2.63313, loss=-0.00707
std_min= 0.53248, std_max= 0.62318, std_mean= 0.58328
val lr: [0.00021157786885245904], policy lr: [0.0002538934426229508]
Policy Loss: -0.0070725, | Entropy Bonus: -0, | Value Loss: 9.6023, | Advantage Loss: 4.1341
Time elapsed (s): 1.7077958583831787
Agent stdevs: 0.58328193
--------------------------------------------------------------------------------

Step 150
++++++++ Policy training ++++++++++
Current mean reward: 1401.131220 | mean episode length: 433.000000
val_loss=37.39063
val_loss=155.30910
val_loss=313.42639
val_loss=29.96648
val_loss=63.95454
val_loss=90.25871
val_loss=55.05497
val_loss=37.45708
val_loss=157.40683
val_loss=1072.86047
adv_loss= 2.70544
adv_loss=13.12658
adv_loss= 5.31827
adv_loss= 3.05104
adv_loss= 3.47129
adv_loss= 6.58571
adv_loss= 4.33927
adv_loss= 4.67469
adv_loss= 5.70948
adv_loss= 2.49597
surrogate=-0.00584, entropy= 2.62903, loss=-0.00584
surrogate=-0.00029, entropy= 2.62445, loss=-0.00029
surrogate= 0.02528, entropy= 2.62109, loss= 0.02528
surrogate= 0.00386, entropy= 2.61911, loss= 0.00386
surrogate=-0.03097, entropy= 2.61780, loss=-0.03097
surrogate=-0.02105, entropy= 2.61809, loss=-0.02105
surrogate= 0.01518, entropy= 2.61509, loss= 0.01518
surrogate=-0.02518, entropy= 2.61571, loss=-0.02518
surrogate=-0.00159, entropy= 2.61736, loss=-0.00159
surrogate=-0.02068, entropy= 2.61529, loss=-0.02068
std_min= 0.53115, std_max= 0.61285, std_mean= 0.57965
val lr: [0.0002113217213114754], policy lr: [0.00025358606557377046]
Policy Loss: -0.020677, | Entropy Bonus: -0, | Value Loss: 1072.9, | Advantage Loss: 2.496
Time elapsed (s): 1.6997368335723877
Agent stdevs: 0.5796511
--------------------------------------------------------------------------------

Step 151
++++++++ Policy training ++++++++++
Current mean reward: 987.621378 | mean episode length: 290.500000
val_loss=58.36730
val_loss=52.75222
val_loss=51.44845
val_loss=21.53915
val_loss=30.02786
val_loss=28.42786
val_loss=39.01469
val_loss=31.08150
val_loss=28.26916
val_loss=24.10860
adv_loss= 6.02834
adv_loss= 6.07080
adv_loss= 7.77805
adv_loss= 8.50873
adv_loss= 3.37623
adv_loss= 3.75518
adv_loss= 3.66915
adv_loss= 5.28167
adv_loss= 4.54095
adv_loss= 3.31435
surrogate= 0.04317, entropy= 2.61335, loss= 0.04317
surrogate=-0.00505, entropy= 2.61198, loss=-0.00505
surrogate=-0.00355, entropy= 2.60887, loss=-0.00355
surrogate=-0.00008, entropy= 2.60653, loss=-0.00008
surrogate= 0.01156, entropy= 2.60693, loss= 0.01156
surrogate=-0.01733, entropy= 2.60663, loss=-0.01733
surrogate=-0.00915, entropy= 2.60542, loss=-0.00915
surrogate=-0.03160, entropy= 2.60431, loss=-0.03160
surrogate=-0.00812, entropy= 2.60405, loss=-0.00812
surrogate=-0.02848, entropy= 2.60586, loss=-0.02848
std_min= 0.53394, std_max= 0.60859, std_mean= 0.57764
val lr: [0.00021106557377049178], policy lr: [0.0002532786885245901]
Policy Loss: -0.028475, | Entropy Bonus: -0, | Value Loss: 24.109, | Advantage Loss: 3.3143
Time elapsed (s): 1.7767770290374756
Agent stdevs: 0.57763773
--------------------------------------------------------------------------------

Step 152
++++++++ Policy training ++++++++++
Current mean reward: 946.456154 | mean episode length: 293.333333
val_loss=417.06366
val_loss=122.16283
val_loss=464.05362
val_loss=56.09002
val_loss=198.47380
val_loss=367.75714
val_loss=156.55313
val_loss=42.41739
val_loss=640.68494
val_loss=139.54716
adv_loss= 8.69335
adv_loss= 6.83288
adv_loss= 5.96436
adv_loss= 3.63780
adv_loss= 5.93921
adv_loss= 4.66139
adv_loss=399.26221
adv_loss= 4.27003
adv_loss=10.26729
adv_loss= 6.94166
surrogate=-0.02384, entropy= 2.60720, loss=-0.02384
surrogate=-0.04098, entropy= 2.60553, loss=-0.04098
surrogate=-0.01982, entropy= 2.60383, loss=-0.01982
surrogate=-0.00151, entropy= 2.60363, loss=-0.00151
surrogate=-0.01960, entropy= 2.60514, loss=-0.01960
surrogate=-0.03228, entropy= 2.60427, loss=-0.03228
surrogate=-0.02026, entropy= 2.60451, loss=-0.02026
surrogate=-0.02573, entropy= 2.60472, loss=-0.02573
surrogate=-0.04482, entropy= 2.60481, loss=-0.04482
surrogate=-0.01512, entropy= 2.60483, loss=-0.01512
std_min= 0.53571, std_max= 0.60333, std_mean= 0.57733
val lr: [0.0002108094262295082], policy lr: [0.0002529713114754098]
Policy Loss: -0.015116, | Entropy Bonus: -0, | Value Loss: 139.55, | Advantage Loss: 6.9417
Time elapsed (s): 1.6940622329711914
Agent stdevs: 0.57733136
--------------------------------------------------------------------------------

Step 153
++++++++ Policy training ++++++++++
Current mean reward: 1355.657395 | mean episode length: 408.000000
val_loss=33.64246
val_loss=12.96000
val_loss=17.29701
val_loss=18.73435
val_loss=24.28734
val_loss=11.03057
val_loss= 9.45201
val_loss=11.70484
val_loss=12.67423
val_loss=15.41015
adv_loss= 3.73683
adv_loss= 3.01084
adv_loss= 3.67804
adv_loss= 8.64042
adv_loss= 2.02305
adv_loss= 3.34029
adv_loss= 5.90009
adv_loss= 7.89094
adv_loss=16.85250
adv_loss= 4.01645
surrogate=-0.00294, entropy= 2.60139, loss=-0.00294
surrogate= 0.02243, entropy= 2.59990, loss= 0.02243
surrogate=-0.01048, entropy= 2.59601, loss=-0.01048
surrogate=-0.00846, entropy= 2.59568, loss=-0.00846
surrogate=-0.01045, entropy= 2.59203, loss=-0.01045
surrogate=-0.04103, entropy= 2.58903, loss=-0.04103
surrogate=-0.01345, entropy= 2.59063, loss=-0.01345
surrogate=-0.02266, entropy= 2.58971, loss=-0.02266
surrogate=-0.02079, entropy= 2.58847, loss=-0.02079
surrogate= 0.00325, entropy= 2.58954, loss= 0.00325
std_min= 0.52698, std_max= 0.60515, std_mean= 0.57464
val lr: [0.00021055327868852458], policy lr: [0.00025266393442622946]
Policy Loss: 0.003249, | Entropy Bonus: -0, | Value Loss: 15.41, | Advantage Loss: 4.0164
Time elapsed (s): 1.7149510383605957
Agent stdevs: 0.5746439
--------------------------------------------------------------------------------

Step 154
++++++++ Policy training ++++++++++
Current mean reward: 962.240356 | mean episode length: 279.142857
val_loss=78.72996
val_loss=23.99080
val_loss=16.61004
val_loss=33.72701
val_loss=52.62345
val_loss=37.55724
val_loss=26.97587
val_loss=42.67925
val_loss=25.26243
val_loss=19.83555
adv_loss=11.11020
adv_loss= 5.79913
adv_loss= 4.19641
adv_loss= 6.17685
adv_loss= 3.35524
adv_loss= 5.00643
adv_loss= 7.62175
adv_loss= 1.74287
adv_loss= 6.44777
adv_loss= 4.23347
surrogate= 0.00790, entropy= 2.58813, loss= 0.00790
surrogate= 0.01234, entropy= 2.58523, loss= 0.01234
surrogate= 0.01070, entropy= 2.58391, loss= 0.01070
surrogate=-0.02507, entropy= 2.58274, loss=-0.02507
surrogate= 0.00430, entropy= 2.58248, loss= 0.00430
surrogate=-0.04356, entropy= 2.58170, loss=-0.04356
surrogate=-0.00691, entropy= 2.58171, loss=-0.00691
surrogate=-0.01197, entropy= 2.58154, loss=-0.01197
surrogate= 0.00088, entropy= 2.58104, loss= 0.00088
surrogate=-0.00206, entropy= 2.57798, loss=-0.00206
std_min= 0.52126, std_max= 0.60711, std_mean= 0.57268
val lr: [0.00021029713114754098], policy lr: [0.00025235655737704916]
Policy Loss: -0.002064, | Entropy Bonus: -0, | Value Loss: 19.836, | Advantage Loss: 4.2335
Time elapsed (s): 1.692054271697998
Agent stdevs: 0.5726822
--------------------------------------------------------------------------------

Step 155
++++++++ Policy training ++++++++++
Current mean reward: 924.709560 | mean episode length: 267.750000
val_loss=19.82464
val_loss=17.99477
val_loss=19.91819
val_loss=17.46152
val_loss=24.57321
val_loss=20.55623
val_loss=18.59481
val_loss=18.83457
val_loss=12.91868
val_loss=11.33721
adv_loss= 3.91278
adv_loss= 2.32345
adv_loss= 1.83001
adv_loss= 1.45457
adv_loss= 1.68274
adv_loss= 1.38462
adv_loss= 1.36496
adv_loss= 1.62662
adv_loss= 4.35230
adv_loss= 1.27771
surrogate=-0.00741, entropy= 2.57700, loss=-0.00741
surrogate= 0.01036, entropy= 2.57616, loss= 0.01036
surrogate= 0.01036, entropy= 2.57218, loss= 0.01036
surrogate=-0.01380, entropy= 2.56777, loss=-0.01380
surrogate=-0.01266, entropy= 2.56371, loss=-0.01266
surrogate=-0.00751, entropy= 2.56045, loss=-0.00751
surrogate= 0.00140, entropy= 2.55904, loss= 0.00140
surrogate=-0.03969, entropy= 2.55882, loss=-0.03969
surrogate=-0.01701, entropy= 2.55695, loss=-0.01701
surrogate=-0.03962, entropy= 2.55493, loss=-0.03962
std_min= 0.51704, std_max= 0.60316, std_mean= 0.56829
val lr: [0.00021004098360655738], policy lr: [0.0002520491803278688]
Policy Loss: -0.039619, | Entropy Bonus: -0, | Value Loss: 11.337, | Advantage Loss: 1.2777
Time elapsed (s): 1.6596672534942627
Agent stdevs: 0.56828827
--------------------------------------------------------------------------------

Step 156
++++++++ Policy training ++++++++++
Current mean reward: 1115.142089 | mean episode length: 338.666667
val_loss=290.74066
val_loss=34.33415
val_loss=68.48942
val_loss=264.71506
val_loss=29.25623
val_loss=35.11407
val_loss=21.83817
val_loss=20.46145
val_loss=75.17693
val_loss=34.56574
adv_loss= 5.68052
adv_loss= 7.60559
adv_loss= 2.18252
adv_loss= 5.28180
adv_loss= 2.68132
adv_loss= 6.61049
adv_loss= 8.23997
adv_loss= 7.01254
adv_loss= 3.94246
adv_loss= 6.91904
surrogate= 0.01187, entropy= 2.55104, loss= 0.01187
surrogate=-0.01790, entropy= 2.54653, loss=-0.01790
surrogate= 0.01118, entropy= 2.54474, loss= 0.01118
surrogate=-0.03297, entropy= 2.54175, loss=-0.03297
surrogate=-0.01909, entropy= 2.54063, loss=-0.01909
surrogate= 0.01140, entropy= 2.53899, loss= 0.01140
surrogate=-0.03275, entropy= 2.53760, loss=-0.03275
surrogate=-0.01421, entropy= 2.53340, loss=-0.01421
surrogate=-0.01266, entropy= 2.53332, loss=-0.01266
surrogate=-0.02815, entropy= 2.53444, loss=-0.02815
std_min= 0.51278, std_max= 0.60086, std_mean= 0.56448
val lr: [0.00020978483606557378], policy lr: [0.0002517418032786885]
Policy Loss: -0.028153, | Entropy Bonus: -0, | Value Loss: 34.566, | Advantage Loss: 6.919
Time elapsed (s): 1.6666805744171143
Agent stdevs: 0.5644786
--------------------------------------------------------------------------------

Step 157
++++++++ Policy training ++++++++++
Current mean reward: 1058.343576 | mean episode length: 308.166667
val_loss=43.88889
val_loss=32.43127
val_loss=82.04184
val_loss=67.64099
val_loss=30.23845
val_loss=61.46402
val_loss=24.49392
val_loss=33.50434
val_loss=37.10968
val_loss=24.90874
adv_loss= 3.53958
adv_loss= 4.19197
adv_loss= 3.24060
adv_loss= 2.27773
adv_loss= 9.23156
adv_loss= 2.84921
adv_loss= 2.82505
adv_loss= 2.27905
adv_loss= 3.96580
adv_loss= 8.45966
surrogate=-0.01187, entropy= 2.53328, loss=-0.01187
surrogate= 0.09477, entropy= 2.52982, loss= 0.09477
surrogate= 0.01041, entropy= 2.52679, loss= 0.01041
surrogate=-0.03147, entropy= 2.52715, loss=-0.03147
surrogate=-0.01903, entropy= 2.52915, loss=-0.01903
surrogate=-0.02797, entropy= 2.52800, loss=-0.02797
surrogate=-0.01539, entropy= 2.52526, loss=-0.01539
surrogate=-0.00662, entropy= 2.52361, loss=-0.00662
surrogate=-0.03006, entropy= 2.52460, loss=-0.03006
surrogate=-0.01593, entropy= 2.52483, loss=-0.01593
std_min= 0.51025, std_max= 0.60190, std_mean= 0.56277
val lr: [0.00020952868852459018], policy lr: [0.00025143442622950816]
Policy Loss: -0.015926, | Entropy Bonus: -0, | Value Loss: 24.909, | Advantage Loss: 8.4597
Time elapsed (s): 1.701387882232666
Agent stdevs: 0.5627728
--------------------------------------------------------------------------------

Step 158
++++++++ Policy training ++++++++++
Current mean reward: 1100.589029 | mean episode length: 327.000000
val_loss=40.12258
val_loss=19.83342
val_loss=29.82247
val_loss=20.06227
val_loss=19.71652
val_loss=27.24941
val_loss=23.33412
val_loss=17.40776
val_loss=10.28163
val_loss=22.03217
adv_loss= 1.31361
adv_loss= 2.28514
adv_loss= 1.96427
adv_loss= 3.27026
adv_loss= 2.53434
adv_loss= 4.30907
adv_loss=11.86609
adv_loss= 3.61745
adv_loss= 1.36733
adv_loss= 2.11352
surrogate= 0.00470, entropy= 2.52174, loss= 0.00470
surrogate=-0.01728, entropy= 2.51640, loss=-0.01728
surrogate=-0.00061, entropy= 2.51264, loss=-0.00061
surrogate=-0.00907, entropy= 2.51042, loss=-0.00907
surrogate= 0.01938, entropy= 2.50893, loss= 0.01938
surrogate= 0.02380, entropy= 2.50807, loss= 0.02380
surrogate=-0.01130, entropy= 2.50324, loss=-0.01130
surrogate=-0.00283, entropy= 2.50187, loss=-0.00283
surrogate=-0.01257, entropy= 2.49911, loss=-0.01257
surrogate=-0.01379, entropy= 2.49695, loss=-0.01379
std_min= 0.50723, std_max= 0.59370, std_mean= 0.55742
val lr: [0.00020927254098360656], policy lr: [0.00025112704918032786]
Policy Loss: -0.013788, | Entropy Bonus: -0, | Value Loss: 22.032, | Advantage Loss: 2.1135
Time elapsed (s): 1.6419939994812012
Agent stdevs: 0.55742294
--------------------------------------------------------------------------------

Step 159
++++++++ Policy training ++++++++++
Current mean reward: 774.745115 | mean episode length: 231.750000
val_loss=37.86821
val_loss=33.02340
val_loss=55.57138
val_loss=19.76369
val_loss=29.89226
val_loss=31.73717
val_loss=26.87178
val_loss=36.76473
val_loss=29.65774
val_loss=26.71235
adv_loss= 6.82050
adv_loss= 2.32644
adv_loss= 3.60565
adv_loss= 3.03219
adv_loss= 6.92783
adv_loss= 3.10664
adv_loss= 3.09794
adv_loss= 4.17604
adv_loss= 6.65363
adv_loss= 9.42692
surrogate= 0.02271, entropy= 2.49332, loss= 0.02271
surrogate= 0.02863, entropy= 2.48973, loss= 0.02863
surrogate=-0.01875, entropy= 2.48753, loss=-0.01875
surrogate=-0.02171, entropy= 2.48157, loss=-0.02171
surrogate=-0.02658, entropy= 2.47893, loss=-0.02658
surrogate=-0.03281, entropy= 2.47441, loss=-0.03281
surrogate=-0.00690, entropy= 2.47267, loss=-0.00690
surrogate=-0.01732, entropy= 2.47109, loss=-0.01732
surrogate= 0.00139, entropy= 2.46885, loss= 0.00139
surrogate=-0.01171, entropy= 2.46619, loss=-0.01171
std_min= 0.50045, std_max= 0.58273, std_mean= 0.55177
val lr: [0.00020901639344262296], policy lr: [0.00025081967213114756]
Policy Loss: -0.011713, | Entropy Bonus: -0, | Value Loss: 26.712, | Advantage Loss: 9.4269
Time elapsed (s): 1.6484885215759277
Agent stdevs: 0.55176944
--------------------------------------------------------------------------------

Step 160
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 994.71
++++++++ Policy training ++++++++++
Current mean reward: 1166.009951 | mean episode length: 370.400000
val_loss=92.58747
val_loss=64.36598
val_loss=70.81845
val_loss=75.67390
val_loss=80.60179
val_loss=31.65884
val_loss=36.21010
val_loss=27.77652
val_loss=37.47680
val_loss=55.15210
adv_loss= 6.81187
adv_loss= 8.61593
adv_loss=13.41841
adv_loss= 2.42324
adv_loss= 3.85344
adv_loss= 4.39593
adv_loss=14.46566
adv_loss= 5.75652
adv_loss= 3.72217
adv_loss= 4.67083
surrogate= 0.00044, entropy= 2.46530, loss= 0.00044
surrogate=-0.00517, entropy= 2.46215, loss=-0.00517
surrogate=-0.00972, entropy= 2.45992, loss=-0.00972
surrogate= 0.00311, entropy= 2.45916, loss= 0.00311
surrogate= 0.00496, entropy= 2.45809, loss= 0.00496
surrogate= 0.01401, entropy= 2.45431, loss= 0.01401
surrogate=-0.02217, entropy= 2.45253, loss=-0.02217
surrogate= 0.01253, entropy= 2.45178, loss= 0.01253
surrogate=-0.02266, entropy= 2.45039, loss=-0.02266
surrogate=-0.01731, entropy= 2.44819, loss=-0.01731
std_min= 0.50079, std_max= 0.57749, std_mean= 0.54830
val lr: [0.00020876024590163933], policy lr: [0.00025051229508196715]
Policy Loss: -0.01731, | Entropy Bonus: -0, | Value Loss: 55.152, | Advantage Loss: 4.6708
Time elapsed (s): 1.693570852279663
Agent stdevs: 0.548301
--------------------------------------------------------------------------------

Step 161
++++++++ Policy training ++++++++++
Current mean reward: 1260.949273 | mean episode length: 395.400000
val_loss=36.24669
val_loss=144.54173
val_loss=1068.63037
val_loss=50.24753
val_loss=1285.11121
val_loss=476.20618
val_loss=188.57924
val_loss=90.16936
val_loss=91.21138
val_loss=83.92083
adv_loss= 3.37501
adv_loss= 9.50513
adv_loss= 8.71979
adv_loss= 3.47467
adv_loss= 4.88157
adv_loss= 9.01984
adv_loss= 4.95712
adv_loss= 3.58288
adv_loss= 3.35933
adv_loss= 5.75723
surrogate=-0.01806, entropy= 2.44529, loss=-0.01806
surrogate=-0.02947, entropy= 2.44350, loss=-0.02947
surrogate= 0.00858, entropy= 2.44289, loss= 0.00858
surrogate=-0.02040, entropy= 2.44295, loss=-0.02040
surrogate=-0.02447, entropy= 2.44023, loss=-0.02447
surrogate= 0.01239, entropy= 2.43942, loss= 0.01239
surrogate= 0.00606, entropy= 2.43779, loss= 0.00606
surrogate= 0.00376, entropy= 2.43594, loss= 0.00376
surrogate=-0.00930, entropy= 2.43551, loss=-0.00930
surrogate=-0.01757, entropy= 2.43425, loss=-0.01757
std_min= 0.50463, std_max= 0.57335, std_mean= 0.54553
val lr: [0.00020850409836065576], policy lr: [0.0002502049180327869]
Policy Loss: -0.017568, | Entropy Bonus: -0, | Value Loss: 83.921, | Advantage Loss: 5.7572
Time elapsed (s): 1.6866512298583984
Agent stdevs: 0.5455269
--------------------------------------------------------------------------------

Step 162
++++++++ Policy training ++++++++++
Current mean reward: 777.833558 | mean episode length: 231.125000
val_loss=131.31215
val_loss=75.05920
val_loss=76.75515
val_loss=57.89698
val_loss=73.80820
val_loss=83.89144
val_loss=58.42043
val_loss=41.93737
val_loss=30.66514
val_loss=35.98118
adv_loss=10.56516
adv_loss= 6.03682
adv_loss= 5.76495
adv_loss= 2.82875
adv_loss=10.63742
adv_loss= 7.05840
adv_loss=13.50550
adv_loss= 6.65283
adv_loss= 4.73784
adv_loss= 4.95135
surrogate=-0.00888, entropy= 2.42912, loss=-0.00888
surrogate=-0.00850, entropy= 2.42563, loss=-0.00850
surrogate=-0.01052, entropy= 2.42074, loss=-0.01052
surrogate=-0.01750, entropy= 2.41676, loss=-0.01750
surrogate= 0.00183, entropy= 2.41403, loss= 0.00183
surrogate=-0.02300, entropy= 2.41367, loss=-0.02300
surrogate=-0.01503, entropy= 2.40789, loss=-0.01503
surrogate=-0.02737, entropy= 2.40387, loss=-0.02737
surrogate=-0.00121, entropy= 2.40061, loss=-0.00121
surrogate=-0.04007, entropy= 2.39888, loss=-0.04007
std_min= 0.49567, std_max= 0.56391, std_mean= 0.53921
val lr: [0.00020824795081967213], policy lr: [0.0002498975409836065]
Policy Loss: -0.040067, | Entropy Bonus: -0, | Value Loss: 35.981, | Advantage Loss: 4.9513
Time elapsed (s): 1.7969608306884766
Agent stdevs: 0.53920573
--------------------------------------------------------------------------------

Step 163
++++++++ Policy training ++++++++++
Current mean reward: 849.783301 | mean episode length: 248.000000
val_loss=30.08440
val_loss=25.60021
val_loss=54.18616
val_loss=21.37725
val_loss=28.84523
val_loss=27.84504
val_loss=21.82536
val_loss=26.90483
val_loss=22.39598
val_loss=21.82719
adv_loss= 2.60527
adv_loss=12.94110
adv_loss= 2.75863
adv_loss= 8.38740
adv_loss= 8.33666
adv_loss= 5.05477
adv_loss= 6.21906
adv_loss= 3.52355
adv_loss= 5.54899
adv_loss= 1.92682
surrogate= 0.00220, entropy= 2.39789, loss= 0.00220
surrogate= 0.00541, entropy= 2.39415, loss= 0.00541
surrogate=-0.01428, entropy= 2.39466, loss=-0.01428
surrogate=-0.02022, entropy= 2.39323, loss=-0.02022
surrogate= 0.01702, entropy= 2.39296, loss= 0.01702
surrogate=-0.02468, entropy= 2.39098, loss=-0.02468
surrogate=-0.00828, entropy= 2.38892, loss=-0.00828
surrogate=-0.00585, entropy= 2.38773, loss=-0.00585
surrogate=-0.02207, entropy= 2.38676, loss=-0.02207
surrogate=-0.00122, entropy= 2.38489, loss=-0.00122
std_min= 0.49386, std_max= 0.55857, std_mean= 0.53667
val lr: [0.00020799180327868853], policy lr: [0.0002495901639344262]
Policy Loss: -0.0012239, | Entropy Bonus: -0, | Value Loss: 21.827, | Advantage Loss: 1.9268
Time elapsed (s): 1.7477200031280518
Agent stdevs: 0.5366695
--------------------------------------------------------------------------------

Step 164
++++++++ Policy training ++++++++++
Current mean reward: 1618.557746 | mean episode length: 495.333333
val_loss=134.59818
val_loss=48.84893
val_loss=406.24741
val_loss=628.03143
val_loss=39.32420
val_loss=244.11922
val_loss=703.30237
val_loss=108.46038
val_loss=1010.30359
val_loss=81.05843
adv_loss= 3.64748
adv_loss= 2.94241
adv_loss= 3.02515
adv_loss= 2.79774
adv_loss= 3.79960
adv_loss= 3.81809
adv_loss= 2.25812
adv_loss= 4.60663
adv_loss= 4.03605
adv_loss= 3.14861
surrogate= 0.00404, entropy= 2.38372, loss= 0.00404
surrogate=-0.00569, entropy= 2.38237, loss=-0.00569
surrogate=-0.02023, entropy= 2.38379, loss=-0.02023
surrogate=-0.02444, entropy= 2.38533, loss=-0.02444
surrogate=-0.02067, entropy= 2.38571, loss=-0.02067
surrogate= 0.02636, entropy= 2.38412, loss= 0.02636
surrogate=-0.01382, entropy= 2.38232, loss=-0.01382
surrogate=-0.02344, entropy= 2.38217, loss=-0.02344
surrogate=-0.01799, entropy= 2.38098, loss=-0.01799
surrogate=-0.00974, entropy= 2.38388, loss=-0.00974
std_min= 0.49369, std_max= 0.55987, std_mean= 0.53651
val lr: [0.00020773565573770493], policy lr: [0.0002492827868852459]
Policy Loss: -0.0097368, | Entropy Bonus: -0, | Value Loss: 81.058, | Advantage Loss: 3.1486
Time elapsed (s): 1.6852805614471436
Agent stdevs: 0.53650635
--------------------------------------------------------------------------------

Step 165
++++++++ Policy training ++++++++++
Current mean reward: 1940.099341 | mean episode length: 638.500000
val_loss=98.64516
val_loss=158.05183
val_loss=173.85495
val_loss=40.17530
val_loss=103.17091
val_loss=98.55097
val_loss=778.60645
val_loss=100.71016
val_loss=70.60030
val_loss=968.80536
adv_loss= 3.29409
adv_loss= 5.07191
adv_loss= 3.11336
adv_loss= 2.38936
adv_loss= 6.26448
adv_loss= 4.97424
adv_loss= 2.88476
adv_loss= 4.64696
adv_loss= 4.54549
adv_loss=1061.50354
surrogate=-0.01014, entropy= 2.38017, loss=-0.01014
surrogate=-0.00396, entropy= 2.37382, loss=-0.00396
surrogate=-0.00111, entropy= 2.37067, loss=-0.00111
surrogate=-0.00784, entropy= 2.36895, loss=-0.00784
surrogate= 0.01920, entropy= 2.37032, loss= 0.01920
surrogate= 0.00182, entropy= 2.37095, loss= 0.00182
surrogate= 0.00198, entropy= 2.36942, loss= 0.00198
surrogate=-0.01038, entropy= 2.36729, loss=-0.01038
surrogate=-0.01158, entropy= 2.36630, loss=-0.01158
surrogate=-0.00255, entropy= 2.36513, loss=-0.00255
std_min= 0.49045, std_max= 0.55851, std_mean= 0.53316
val lr: [0.00020747950819672133], policy lr: [0.00024897540983606555]
Policy Loss: -0.0025546, | Entropy Bonus: -0, | Value Loss: 968.81, | Advantage Loss: 1061.5
Time elapsed (s): 1.688244342803955
Agent stdevs: 0.53316075
--------------------------------------------------------------------------------

Step 166
++++++++ Policy training ++++++++++
Current mean reward: 974.754956 | mean episode length: 288.142857
val_loss=56.38670
val_loss=46.47813
val_loss=34.44561
val_loss=84.33076
val_loss=54.66109
val_loss=57.88848
val_loss=84.00619
val_loss=47.39376
val_loss=84.48826
val_loss=61.69527
adv_loss= 9.38376
adv_loss= 4.46281
adv_loss= 5.05613
adv_loss= 6.44191
adv_loss=11.25862
adv_loss= 8.00154
adv_loss=14.05434
adv_loss= 8.77875
adv_loss= 2.48977
adv_loss= 2.35217
surrogate=-0.00877, entropy= 2.36135, loss=-0.00877
surrogate=-0.01946, entropy= 2.36168, loss=-0.01946
surrogate= 0.00452, entropy= 2.36047, loss= 0.00452
surrogate=-0.02488, entropy= 2.35850, loss=-0.02488
surrogate=-0.00379, entropy= 2.35970, loss=-0.00379
surrogate=-0.02459, entropy= 2.35896, loss=-0.02459
surrogate=-0.02527, entropy= 2.35875, loss=-0.02527
surrogate=-0.03187, entropy= 2.35782, loss=-0.03187
surrogate=-0.03659, entropy= 2.35657, loss=-0.03659
surrogate=-0.00707, entropy= 2.35614, loss=-0.00707
std_min= 0.48783, std_max= 0.55846, std_mean= 0.53165
val lr: [0.0002072233606557377], policy lr: [0.00024866803278688525]
Policy Loss: -0.007071, | Entropy Bonus: -0, | Value Loss: 61.695, | Advantage Loss: 2.3522
Time elapsed (s): 1.7293994426727295
Agent stdevs: 0.5316504
--------------------------------------------------------------------------------

Step 167
++++++++ Policy training ++++++++++
Current mean reward: 1068.160699 | mean episode length: 310.833333
val_loss=33.82317
val_loss=26.97281
val_loss=25.85468
val_loss=23.21305
val_loss=17.01860
val_loss=42.88969
val_loss=34.65514
val_loss=21.17142
val_loss=27.61089
val_loss=19.00835
adv_loss= 3.32951
adv_loss= 3.44370
adv_loss= 3.17079
adv_loss= 4.31046
adv_loss= 2.64347
adv_loss= 4.10752
adv_loss= 4.08484
adv_loss= 2.24271
adv_loss= 4.70605
adv_loss= 4.54873
surrogate= 0.01599, entropy= 2.35553, loss= 0.01599
surrogate=-0.03380, entropy= 2.35233, loss=-0.03380
surrogate= 0.00724, entropy= 2.35248, loss= 0.00724
surrogate= 0.01420, entropy= 2.35124, loss= 0.01420
surrogate= 0.00860, entropy= 2.34847, loss= 0.00860
surrogate=-0.00137, entropy= 2.34730, loss=-0.00137
surrogate=-0.01543, entropy= 2.34709, loss=-0.01543
surrogate= 0.00996, entropy= 2.34606, loss= 0.00996
surrogate=-0.02153, entropy= 2.34482, loss=-0.02153
surrogate=-0.04111, entropy= 2.34266, loss=-0.04111
std_min= 0.48841, std_max= 0.55580, std_mean= 0.52914
val lr: [0.0002069672131147541], policy lr: [0.0002483606557377049]
Policy Loss: -0.041107, | Entropy Bonus: -0, | Value Loss: 19.008, | Advantage Loss: 4.5487
Time elapsed (s): 1.6889512538909912
Agent stdevs: 0.5291397
--------------------------------------------------------------------------------

Step 168
++++++++ Policy training ++++++++++
Current mean reward: 1071.765301 | mean episode length: 312.333333
val_loss=17.04947
val_loss=29.36104
val_loss=19.71326
val_loss=20.23800
val_loss=23.82577
val_loss=24.01030
val_loss=31.05075
val_loss=15.52582
val_loss=20.55218
val_loss=24.50329
adv_loss= 3.54569
adv_loss= 2.41548
adv_loss= 1.10259
adv_loss= 2.33318
adv_loss= 1.13499
adv_loss= 7.63545
adv_loss= 3.46826
adv_loss= 4.01566
adv_loss= 2.36803
adv_loss= 9.30819
surrogate=-0.00351, entropy= 2.34517, loss=-0.00351
surrogate= 0.03423, entropy= 2.34684, loss= 0.03423
surrogate= 0.00625, entropy= 2.34913, loss= 0.00625
surrogate=-0.02662, entropy= 2.34971, loss=-0.02662
surrogate=-0.03269, entropy= 2.34955, loss=-0.03269
surrogate=-0.01006, entropy= 2.34992, loss=-0.01006
surrogate=-0.01433, entropy= 2.35058, loss=-0.01433
surrogate=-0.01363, entropy= 2.35119, loss=-0.01363
surrogate=-0.02099, entropy= 2.34983, loss=-0.02099
surrogate= 0.00748, entropy= 2.34882, loss= 0.00748
std_min= 0.48974, std_max= 0.55502, std_mean= 0.53023
val lr: [0.0002067110655737705], policy lr: [0.0002480532786885246]
Policy Loss: 0.0074796, | Entropy Bonus: -0, | Value Loss: 24.503, | Advantage Loss: 9.3082
Time elapsed (s): 1.725567102432251
Agent stdevs: 0.5302337
--------------------------------------------------------------------------------

Step 169
++++++++ Policy training ++++++++++
Current mean reward: 1458.676977 | mean episode length: 455.250000
val_loss=29.74975
val_loss=48.18594
val_loss=78.23092
val_loss=33.56396
val_loss=1184.39087
val_loss=1317.09949
val_loss=68.78759
val_loss=204.36943
val_loss=1431.41687
val_loss=32.20641
adv_loss=10.98708
adv_loss= 4.71848
adv_loss= 2.68686
adv_loss= 2.51242
adv_loss= 5.45161
adv_loss= 6.26172
adv_loss= 6.61058
adv_loss= 2.35576
adv_loss= 1.89345
adv_loss= 1.87207
surrogate= 0.02170, entropy= 2.35108, loss= 0.02170
surrogate=-0.00081, entropy= 2.35060, loss=-0.00081
surrogate= 0.00559, entropy= 2.35425, loss= 0.00559
surrogate=-0.01157, entropy= 2.35721, loss=-0.01157
surrogate=-0.01088, entropy= 2.35607, loss=-0.01088
surrogate=-0.03240, entropy= 2.35737, loss=-0.03240
surrogate=-0.00323, entropy= 2.36027, loss=-0.00323
surrogate= 0.04568, entropy= 2.36271, loss= 0.04568
surrogate= 0.01069, entropy= 2.36341, loss= 0.01069
surrogate=-0.00299, entropy= 2.36298, loss=-0.00299
std_min= 0.49114, std_max= 0.55872, std_mean= 0.53276
val lr: [0.00020645491803278687], policy lr: [0.00024774590163934425]
Policy Loss: -0.0029909, | Entropy Bonus: -0, | Value Loss: 32.206, | Advantage Loss: 1.8721
Time elapsed (s): 1.6723971366882324
Agent stdevs: 0.53275764
--------------------------------------------------------------------------------

Step 170
++++++++ Policy training ++++++++++
Current mean reward: 1096.371166 | mean episode length: 332.666667
val_loss=430.24844
val_loss=155.75488
val_loss=1308.71069
val_loss=55.56562
val_loss=56.27800
val_loss=54.89960
val_loss=35.60259
val_loss=239.74712
val_loss=1417.58667
val_loss=588.16833
adv_loss= 7.15552
adv_loss= 3.46077
adv_loss= 8.00571
adv_loss=1417.75806
adv_loss= 4.41308
adv_loss= 4.55054
adv_loss= 9.13410
adv_loss=1416.54321
adv_loss= 5.14889
adv_loss= 3.28864
surrogate= 0.00883, entropy= 2.36230, loss= 0.00883
surrogate=-0.01795, entropy= 2.36189, loss=-0.01795
surrogate=-0.00694, entropy= 2.36246, loss=-0.00694
surrogate= 0.02144, entropy= 2.36432, loss= 0.02144
surrogate=-0.02740, entropy= 2.36727, loss=-0.02740
surrogate= 0.00099, entropy= 2.36980, loss= 0.00099
surrogate=-0.01776, entropy= 2.37159, loss=-0.01776
surrogate= 0.01056, entropy= 2.37460, loss= 0.01056
surrogate=-0.01262, entropy= 2.37346, loss=-0.01262
surrogate=-0.01242, entropy= 2.37445, loss=-0.01242
std_min= 0.49541, std_max= 0.55913, std_mean= 0.53470
val lr: [0.0002061987704918033], policy lr: [0.00024743852459016395]
Policy Loss: -0.01242, | Entropy Bonus: -0, | Value Loss: 588.17, | Advantage Loss: 3.2886
Time elapsed (s): 1.645418405532837
Agent stdevs: 0.5347032
--------------------------------------------------------------------------------

Step 171
++++++++ Policy training ++++++++++
Current mean reward: 854.597944 | mean episode length: 252.000000
val_loss=46.20228
val_loss=41.70570
val_loss=29.34651
val_loss=29.67526
val_loss=39.46025
val_loss=39.70180
val_loss=43.07410
val_loss=21.02738
val_loss=24.89974
val_loss=14.31691
adv_loss= 2.39614
adv_loss= 3.17747
adv_loss= 4.04126
adv_loss= 5.42133
adv_loss= 3.04949
adv_loss= 8.33188
adv_loss= 2.46590
adv_loss= 5.14214
adv_loss= 1.86854
adv_loss= 3.75040
surrogate=-0.00993, entropy= 2.37235, loss=-0.00993
surrogate= 0.00137, entropy= 2.37125, loss= 0.00137
surrogate=-0.03110, entropy= 2.37163, loss=-0.03110
surrogate= 0.00541, entropy= 2.36960, loss= 0.00541
surrogate= 0.04043, entropy= 2.36902, loss= 0.04043
surrogate= 0.01355, entropy= 2.36866, loss= 0.01355
surrogate= 0.00132, entropy= 2.36824, loss= 0.00132
surrogate=-0.01614, entropy= 2.36522, loss=-0.01614
surrogate=-0.01737, entropy= 2.36347, loss=-0.01737
surrogate= 0.00170, entropy= 2.36268, loss= 0.00170
std_min= 0.49050, std_max= 0.55391, std_mean= 0.53271
val lr: [0.00020594262295081967], policy lr: [0.0002471311475409836]
Policy Loss: 0.001697, | Entropy Bonus: -0, | Value Loss: 14.317, | Advantage Loss: 3.7504
Time elapsed (s): 1.6562933921813965
Agent stdevs: 0.5327144
--------------------------------------------------------------------------------

Step 172
++++++++ Policy training ++++++++++
Current mean reward: 2978.532622 | mean episode length: 1000.000000
val_loss=70.12777
val_loss=1597.42456
val_loss=85.92909
val_loss=867.84821
val_loss=346.75177
val_loss=1179.62537
val_loss=60.32287
val_loss=641.88324
val_loss=1182.46899
val_loss=550.46545
adv_loss= 3.01951
adv_loss= 2.37445
adv_loss= 3.08044
adv_loss= 5.19738
adv_loss= 3.71196
adv_loss= 2.89737
adv_loss= 4.09612
adv_loss= 6.14853
adv_loss= 5.14329
adv_loss= 4.23743
surrogate=-0.00406, entropy= 2.36225, loss=-0.00406
surrogate= 0.03817, entropy= 2.36285, loss= 0.03817
surrogate=-0.00879, entropy= 2.36545, loss=-0.00879
surrogate= 0.02841, entropy= 2.36568, loss= 0.02841
surrogate= 0.00049, entropy= 2.36445, loss= 0.00049
surrogate=-0.02188, entropy= 2.36432, loss=-0.02188
surrogate=-0.00484, entropy= 2.36557, loss=-0.00484
surrogate= 0.01216, entropy= 2.36654, loss= 0.01216
surrogate=-0.01090, entropy= 2.36659, loss=-0.01090
surrogate= 0.03842, entropy= 2.36619, loss= 0.03842
std_min= 0.48335, std_max= 0.56183, std_mean= 0.53371
val lr: [0.00020568647540983607], policy lr: [0.00024682377049180324]
Policy Loss: 0.038415, | Entropy Bonus: -0, | Value Loss: 550.47, | Advantage Loss: 4.2374
Time elapsed (s): 1.6699614524841309
Agent stdevs: 0.53371054
--------------------------------------------------------------------------------

Step 173
++++++++ Policy training ++++++++++
Current mean reward: 1127.590565 | mean episode length: 328.750000
val_loss=28.44346
val_loss=11.64799
val_loss=25.98298
val_loss=17.59619
val_loss=13.48009
val_loss=14.59596
val_loss=19.49794
val_loss=14.29898
val_loss=14.45629
val_loss=14.83544
adv_loss= 1.96378
adv_loss= 2.40548
adv_loss= 1.33213
adv_loss= 3.44571
adv_loss= 3.66093
adv_loss= 1.64929
adv_loss= 2.01576
adv_loss= 1.67131
adv_loss= 1.13497
adv_loss= 2.09753
surrogate=-0.02012, entropy= 2.36658, loss=-0.02012
surrogate=-0.00803, entropy= 2.36470, loss=-0.00803
surrogate= 0.04759, entropy= 2.36850, loss= 0.04759
surrogate=-0.03668, entropy= 2.36992, loss=-0.03668
surrogate= 0.00926, entropy= 2.37151, loss= 0.00926
surrogate=-0.00233, entropy= 2.37417, loss=-0.00233
surrogate=-0.02461, entropy= 2.37576, loss=-0.02461
surrogate=-0.00893, entropy= 2.37733, loss=-0.00893
surrogate=-0.00921, entropy= 2.37949, loss=-0.00921
surrogate=-0.01632, entropy= 2.37780, loss=-0.01632
std_min= 0.48279, std_max= 0.56555, std_mean= 0.53595
val lr: [0.00020543032786885247], policy lr: [0.00024651639344262295]
Policy Loss: -0.016319, | Entropy Bonus: -0, | Value Loss: 14.835, | Advantage Loss: 2.0975
Time elapsed (s): 1.6590394973754883
Agent stdevs: 0.53594744
--------------------------------------------------------------------------------

Step 174
++++++++ Policy training ++++++++++
Current mean reward: 799.023705 | mean episode length: 241.250000
val_loss=95.88242
val_loss=29.75143
val_loss=33.04266
val_loss=80.27393
val_loss=96.22481
val_loss=36.92379
val_loss=143.56361
val_loss=83.98302
val_loss=40.10510
val_loss=47.75903
adv_loss=10.75568
adv_loss= 2.82593
adv_loss= 6.55339
adv_loss=38.96241
adv_loss= 5.66140
adv_loss= 7.08183
adv_loss= 6.99626
adv_loss= 3.19876
adv_loss= 3.91069
adv_loss= 8.17348
surrogate= 0.00035, entropy= 2.38166, loss= 0.00035
surrogate=-0.00489, entropy= 2.38458, loss=-0.00489
surrogate= 0.01626, entropy= 2.38718, loss= 0.01626
surrogate=-0.01143, entropy= 2.38992, loss=-0.01143
surrogate=-0.02521, entropy= 2.39392, loss=-0.02521
surrogate=-0.01557, entropy= 2.39656, loss=-0.01557
surrogate= 0.01807, entropy= 2.39996, loss= 0.01807
surrogate=-0.02191, entropy= 2.40148, loss=-0.02191
surrogate=-0.01804, entropy= 2.40333, loss=-0.01804
surrogate=-0.01143, entropy= 2.40401, loss=-0.01143
std_min= 0.48394, std_max= 0.57156, std_mean= 0.54080
val lr: [0.00020517418032786885], policy lr: [0.0002462090163934426]
Policy Loss: -0.011429, | Entropy Bonus: -0, | Value Loss: 47.759, | Advantage Loss: 8.1735
Time elapsed (s): 1.6988327503204346
Agent stdevs: 0.54080194
--------------------------------------------------------------------------------

Step 175
++++++++ Policy training ++++++++++
Current mean reward: 756.912965 | mean episode length: 227.714286
val_loss=31.30588
val_loss=36.85479
val_loss=69.66845
val_loss=56.79015
val_loss=30.56455
val_loss=31.05105
val_loss=51.75732
val_loss=27.52278
val_loss=18.35643
val_loss=22.00893
adv_loss= 4.98975
adv_loss= 5.42521
adv_loss= 2.32127
adv_loss= 2.37843
adv_loss= 3.20546
adv_loss= 1.92899
adv_loss= 6.47387
adv_loss= 1.44812
adv_loss= 3.10214
adv_loss=24.97889
surrogate= 0.02052, entropy= 2.40523, loss= 0.02052
surrogate= 0.00126, entropy= 2.40268, loss= 0.00126
surrogate= 0.00481, entropy= 2.40092, loss= 0.00481
surrogate=-0.01894, entropy= 2.39858, loss=-0.01894
surrogate=-0.02388, entropy= 2.39733, loss=-0.02388
surrogate= 0.00917, entropy= 2.39734, loss= 0.00917
surrogate=-0.02701, entropy= 2.39454, loss=-0.02701
surrogate=-0.03217, entropy= 2.39309, loss=-0.03217
surrogate= 0.00349, entropy= 2.39053, loss= 0.00349
surrogate=-0.01913, entropy= 2.38825, loss=-0.01913
std_min= 0.48060, std_max= 0.56828, std_mean= 0.53798
val lr: [0.00020491803278688525], policy lr: [0.0002459016393442623]
Policy Loss: -0.019134, | Entropy Bonus: -0, | Value Loss: 22.009, | Advantage Loss: 24.979
Time elapsed (s): 1.6806204319000244
Agent stdevs: 0.53798
--------------------------------------------------------------------------------

Step 176
++++++++ Policy training ++++++++++
Current mean reward: 916.145511 | mean episode length: 273.714286
val_loss=1289.63782
val_loss=1063.90759
val_loss=1019.63647
val_loss=302.58176
val_loss=85.04621
val_loss=105.54298
val_loss=66.27510
val_loss=49.79294
val_loss=57.15855
val_loss=138.71927
adv_loss= 4.75880
adv_loss=11.45476
adv_loss=12.60774
adv_loss= 4.45284
adv_loss= 2.67602
adv_loss=12.97216
adv_loss= 6.44663
adv_loss= 4.73158
adv_loss=1278.99707
adv_loss= 9.43960
surrogate= 0.03074, entropy= 2.38626, loss= 0.03074
surrogate= 0.00232, entropy= 2.38632, loss= 0.00232
surrogate= 0.04654, entropy= 2.38488, loss= 0.04654
surrogate=-0.02353, entropy= 2.38545, loss=-0.02353
surrogate=-0.01697, entropy= 2.38537, loss=-0.01697
surrogate= 0.00084, entropy= 2.38255, loss= 0.00084
surrogate=-0.02233, entropy= 2.38414, loss=-0.02233
surrogate=-0.00935, entropy= 2.38298, loss=-0.00935
surrogate=-0.01765, entropy= 2.38130, loss=-0.01765
surrogate= 0.00137, entropy= 2.37967, loss= 0.00137
std_min= 0.48094, std_max= 0.56560, std_mean= 0.53638
val lr: [0.00020466188524590165], policy lr: [0.00024559426229508194]
Policy Loss: 0.0013721, | Entropy Bonus: -0, | Value Loss: 138.72, | Advantage Loss: 9.4396
Time elapsed (s): 1.6541368961334229
Agent stdevs: 0.53638446
--------------------------------------------------------------------------------

Step 177
++++++++ Policy training ++++++++++
Current mean reward: 974.989450 | mean episode length: 291.833333
val_loss=33.68196
val_loss=45.32434
val_loss=55.41181
val_loss=20.61327
val_loss=45.79629
val_loss=30.57422
val_loss=19.24836
val_loss=27.45192
val_loss=20.49698
val_loss=40.56223
adv_loss= 2.35544
adv_loss= 4.07746
adv_loss=10.55027
adv_loss= 4.01394
adv_loss= 2.70640
adv_loss= 1.29446
adv_loss= 4.27854
adv_loss= 6.93559
adv_loss= 5.26180
adv_loss= 3.68300
surrogate= 0.01977, entropy= 2.37648, loss= 0.01977
surrogate=-0.03674, entropy= 2.37001, loss=-0.03674
surrogate= 0.02023, entropy= 2.36625, loss= 0.02023
surrogate=-0.02833, entropy= 2.36181, loss=-0.02833
surrogate=-0.01080, entropy= 2.35783, loss=-0.01080
surrogate=-0.00808, entropy= 2.35206, loss=-0.00808
surrogate=-0.02545, entropy= 2.34684, loss=-0.02545
surrogate=-0.01938, entropy= 2.34143, loss=-0.01938
surrogate=-0.01689, entropy= 2.34096, loss=-0.01689
surrogate=-0.00405, entropy= 2.33923, loss=-0.00405
std_min= 0.47508, std_max= 0.55759, std_mean= 0.52913
val lr: [0.00020440573770491805], policy lr: [0.00024528688524590164]
Policy Loss: -0.0040497, | Entropy Bonus: -0, | Value Loss: 40.562, | Advantage Loss: 3.683
Time elapsed (s): 1.6639363765716553
Agent stdevs: 0.5291262
--------------------------------------------------------------------------------

Step 178
++++++++ Policy training ++++++++++
Current mean reward: 999.826722 | mean episode length: 296.333333
val_loss=33.33739
val_loss=21.81106
val_loss=37.64331
val_loss=20.66003
val_loss=80.46994
val_loss=25.14130
val_loss=13.06176
val_loss=50.25056
val_loss=22.46717
val_loss=48.72548
adv_loss= 4.32286
adv_loss= 4.60498
adv_loss= 4.47737
adv_loss= 2.82450
adv_loss= 3.82504
adv_loss= 2.00962
adv_loss= 5.56448
adv_loss= 5.22202
adv_loss=13.51965
adv_loss= 6.15264
surrogate= 0.03676, entropy= 2.33488, loss= 0.03676
surrogate= 0.02154, entropy= 2.33110, loss= 0.02154
surrogate= 0.04452, entropy= 2.32857, loss= 0.04452
surrogate=-0.00652, entropy= 2.32688, loss=-0.00652
surrogate=-0.00173, entropy= 2.32270, loss=-0.00173
surrogate=-0.03366, entropy= 2.31937, loss=-0.03366
surrogate= 0.01124, entropy= 2.31615, loss= 0.01124
surrogate= 0.04061, entropy= 2.31446, loss= 0.04061
surrogate= 0.02104, entropy= 2.31272, loss= 0.02104
surrogate=-0.00141, entropy= 2.30973, loss=-0.00141
std_min= 0.47564, std_max= 0.54795, std_mean= 0.52366
val lr: [0.00020414959016393442], policy lr: [0.0002449795081967213]
Policy Loss: -0.0014103, | Entropy Bonus: -0, | Value Loss: 48.725, | Advantage Loss: 6.1526
Time elapsed (s): 1.6400120258331299
Agent stdevs: 0.52366203
--------------------------------------------------------------------------------

Step 179
++++++++ Policy training ++++++++++
Current mean reward: 1010.695486 | mean episode length: 294.800000
val_loss=29.85352
val_loss=31.97929
val_loss=24.47995
val_loss=28.49367
val_loss=23.77473
val_loss=31.04381
val_loss=18.89285
val_loss=26.83849
val_loss=30.46234
val_loss=25.64731
adv_loss= 1.60983
adv_loss= 6.04225
adv_loss= 6.30915
adv_loss= 3.15866
adv_loss= 6.12166
adv_loss= 4.50445
adv_loss= 2.55605
adv_loss= 2.72460
adv_loss= 4.70561
adv_loss= 1.83718
surrogate= 0.00664, entropy= 2.30375, loss= 0.00664
surrogate=-0.00712, entropy= 2.30020, loss=-0.00712
surrogate= 0.00409, entropy= 2.29614, loss= 0.00409
surrogate=-0.03229, entropy= 2.29122, loss=-0.03229
surrogate=-0.03347, entropy= 2.28830, loss=-0.03347
surrogate=-0.02003, entropy= 2.28351, loss=-0.02003
surrogate=-0.00324, entropy= 2.28091, loss=-0.00324
surrogate=-0.03099, entropy= 2.27907, loss=-0.03099
surrogate=-0.01380, entropy= 2.27679, loss=-0.01380
surrogate=-0.03379, entropy= 2.27447, loss=-0.03379
std_min= 0.47309, std_max= 0.54072, std_mean= 0.51739
val lr: [0.00020389344262295082], policy lr: [0.00024467213114754094]
Policy Loss: -0.033791, | Entropy Bonus: -0, | Value Loss: 25.647, | Advantage Loss: 1.8372
Time elapsed (s): 1.6647119522094727
Agent stdevs: 0.51739085
--------------------------------------------------------------------------------

Step 180
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 931.71
++++++++ Policy training ++++++++++
Current mean reward: 1591.913318 | mean episode length: 522.000000
val_loss=423.86118
val_loss=21.59034
val_loss=55.69659
val_loss=183.51073
val_loss=495.25470
val_loss=29.60972
val_loss=52.76228
val_loss=42.77821
val_loss=369.42474
val_loss=638.17010
adv_loss= 8.45413
adv_loss= 2.14040
adv_loss= 2.42208
adv_loss= 2.96774
adv_loss= 1.77424
adv_loss= 2.67704
adv_loss= 1.08452
adv_loss= 4.93681
adv_loss= 8.53442
adv_loss= 2.44056
surrogate=-0.02816, entropy= 2.27328, loss=-0.02816
surrogate=-0.02822, entropy= 2.27572, loss=-0.02822
surrogate= 0.02648, entropy= 2.27875, loss= 0.02648
surrogate= 0.01813, entropy= 2.28120, loss= 0.01813
surrogate=-0.01083, entropy= 2.28245, loss=-0.01083
surrogate= 0.00473, entropy= 2.28343, loss= 0.00473
surrogate=-0.02572, entropy= 2.28395, loss=-0.02572
surrogate= 0.01612, entropy= 2.28688, loss= 0.01612
surrogate=-0.03594, entropy= 2.28975, loss=-0.03594
surrogate=-0.01021, entropy= 2.29091, loss=-0.01021
std_min= 0.47341, std_max= 0.54438, std_mean= 0.52039
val lr: [0.00020363729508196722], policy lr: [0.00024436475409836064]
Policy Loss: -0.010206, | Entropy Bonus: -0, | Value Loss: 638.17, | Advantage Loss: 2.4406
Time elapsed (s): 1.7186288833618164
Agent stdevs: 0.5203899
--------------------------------------------------------------------------------

Step 181
++++++++ Policy training ++++++++++
Current mean reward: 1821.451199 | mean episode length: 613.000000
val_loss=490.44162
val_loss=55.25143
val_loss=89.96523
val_loss=25.47193
val_loss=910.71271
val_loss=41.68522
val_loss=416.26056
val_loss=47.95488
val_loss=52.33498
val_loss=70.88548
adv_loss= 4.86626
adv_loss= 2.85852
adv_loss= 2.49919
adv_loss= 3.11472
adv_loss= 1.92336
adv_loss= 3.05734
adv_loss= 3.63985
adv_loss= 3.11609
adv_loss= 4.59325
adv_loss= 2.46103
surrogate= 0.01104, entropy= 2.29140, loss= 0.01104
surrogate=-0.01871, entropy= 2.29441, loss=-0.01871
surrogate= 0.00854, entropy= 2.29658, loss= 0.00854
surrogate=-0.01975, entropy= 2.30065, loss=-0.01975
surrogate= 0.01968, entropy= 2.30425, loss= 0.01968
surrogate= 0.01367, entropy= 2.30447, loss= 0.01367
surrogate=-0.01075, entropy= 2.30466, loss=-0.01075
surrogate=-0.01687, entropy= 2.30597, loss=-0.01687
surrogate=-0.00266, entropy= 2.30801, loss=-0.00266
surrogate=-0.01622, entropy= 2.31109, loss=-0.01622
std_min= 0.48230, std_max= 0.54517, std_mean= 0.52365
val lr: [0.00020338114754098362], policy lr: [0.00024405737704918031]
Policy Loss: -0.016219, | Entropy Bonus: -0, | Value Loss: 70.885, | Advantage Loss: 2.461
Time elapsed (s): 1.7350091934204102
Agent stdevs: 0.5236452
--------------------------------------------------------------------------------

Step 182
++++++++ Policy training ++++++++++
Current mean reward: 1291.799831 | mean episode length: 378.600000
val_loss=113.14601
val_loss=34.38345
val_loss=36.95516
val_loss=30.73060
val_loss=42.74167
val_loss=26.92678
val_loss=43.89935
val_loss=55.36194
val_loss=24.93561
val_loss=18.61472
adv_loss= 2.76632
adv_loss= 8.33623
adv_loss= 1.82212
adv_loss= 2.86066
adv_loss= 4.90639
adv_loss= 5.72210
adv_loss= 9.37012
adv_loss= 7.87009
adv_loss= 4.30473
adv_loss= 5.44363
surrogate= 0.01267, entropy= 2.31255, loss= 0.01267
surrogate=-0.02278, entropy= 2.31173, loss=-0.02278
surrogate=-0.02421, entropy= 2.31112, loss=-0.02421
surrogate= 0.00368, entropy= 2.31188, loss= 0.00368
surrogate=-0.00447, entropy= 2.31146, loss=-0.00447
surrogate=-0.03336, entropy= 2.31164, loss=-0.03336
surrogate=-0.02158, entropy= 2.30879, loss=-0.02158
surrogate=-0.04193, entropy= 2.31017, loss=-0.04193
surrogate=-0.01549, entropy= 2.30844, loss=-0.01549
surrogate= 0.01889, entropy= 2.30919, loss= 0.01889
std_min= 0.48653, std_max= 0.54655, std_mean= 0.52311
val lr: [0.00020312500000000002], policy lr: [0.00024375]
Policy Loss: 0.018892, | Entropy Bonus: -0, | Value Loss: 18.615, | Advantage Loss: 5.4436
Time elapsed (s): 1.6721351146697998
Agent stdevs: 0.5231117
--------------------------------------------------------------------------------

Step 183
++++++++ Policy training ++++++++++
Current mean reward: 1136.243783 | mean episode length: 371.400000
val_loss=406.83389
val_loss=1153.72485
val_loss=126.33702
val_loss=455.25360
val_loss=187.21910
val_loss=673.91754
val_loss=709.22345
val_loss=159.01813
val_loss=113.57032
val_loss=51.05096
adv_loss= 5.37470
adv_loss=1076.17847
adv_loss= 8.72678
adv_loss= 6.12818
adv_loss= 9.94531
adv_loss= 4.69493
adv_loss=10.31165
adv_loss= 3.92550
adv_loss= 2.13955
adv_loss= 5.19703
surrogate=-0.02688, entropy= 2.30572, loss=-0.02688
surrogate=-0.02066, entropy= 2.30394, loss=-0.02066
surrogate=-0.02228, entropy= 2.29985, loss=-0.02228
surrogate= 0.01220, entropy= 2.29862, loss= 0.01220
surrogate= 0.01063, entropy= 2.29371, loss= 0.01063
surrogate=-0.00570, entropy= 2.29183, loss=-0.00570
surrogate=-0.03272, entropy= 2.29203, loss=-0.03272
surrogate=-0.01696, entropy= 2.29027, loss=-0.01696
surrogate= 0.00571, entropy= 2.28970, loss= 0.00571
surrogate=-0.00332, entropy= 2.28754, loss=-0.00332
std_min= 0.48054, std_max= 0.54654, std_mean= 0.51947
val lr: [0.0002028688524590164], policy lr: [0.00024344262295081966]
Policy Loss: -0.0033155, | Entropy Bonus: -0, | Value Loss: 51.051, | Advantage Loss: 5.197
Time elapsed (s): 1.7101433277130127
Agent stdevs: 0.5194735
--------------------------------------------------------------------------------

Step 184
++++++++ Policy training ++++++++++
Current mean reward: 1120.554503 | mean episode length: 333.000000
val_loss=53.45076
val_loss=21.41579
val_loss=23.35554
val_loss=50.04271
val_loss=50.94869
val_loss=47.88380
val_loss=25.82154
val_loss=45.48851
val_loss=30.04015
val_loss=36.63891
adv_loss= 9.47881
adv_loss= 4.89213
adv_loss=12.34060
adv_loss= 8.98876
adv_loss= 2.09558
adv_loss= 6.99912
adv_loss= 8.03815
adv_loss= 4.63564
adv_loss= 5.26369
adv_loss=14.18596
surrogate=-0.01914, entropy= 2.28650, loss=-0.01914
surrogate= 0.00138, entropy= 2.28589, loss= 0.00138
surrogate= 0.03086, entropy= 2.28577, loss= 0.03086
surrogate=-0.01747, entropy= 2.28675, loss=-0.01747
surrogate=-0.00595, entropy= 2.28621, loss=-0.00595
surrogate=-0.03868, entropy= 2.28679, loss=-0.03868
surrogate=-0.02510, entropy= 2.28466, loss=-0.02510
surrogate= 0.01419, entropy= 2.28657, loss= 0.01419
surrogate=-0.01341, entropy= 2.28592, loss=-0.01341
surrogate=-0.01322, entropy= 2.28554, loss=-0.01322
std_min= 0.47837, std_max= 0.54717, std_mean= 0.51921
val lr: [0.0002026127049180328], policy lr: [0.00024313524590163934]
Policy Loss: -0.013225, | Entropy Bonus: -0, | Value Loss: 36.639, | Advantage Loss: 14.186
Time elapsed (s): 1.6737422943115234
Agent stdevs: 0.5192147
--------------------------------------------------------------------------------

Step 185
++++++++ Policy training ++++++++++
Current mean reward: 1469.040720 | mean episode length: 462.250000
val_loss=313.06879
val_loss=825.65167
val_loss=230.77156
val_loss=299.77429
val_loss=56.40722
val_loss=45.64797
val_loss=35.67560
val_loss=48.64130
val_loss=27.87732
val_loss=390.70892
adv_loss= 1.77220
adv_loss= 2.21385
adv_loss=10.70763
adv_loss= 2.31755
adv_loss= 4.04900
adv_loss= 3.59558
adv_loss= 3.63470
adv_loss= 1.87138
adv_loss= 3.12279
adv_loss= 1.81517
surrogate= 0.02158, entropy= 2.28465, loss= 0.02158
surrogate=-0.01420, entropy= 2.28120, loss=-0.01420
surrogate= 0.00226, entropy= 2.27828, loss= 0.00226
surrogate=-0.01990, entropy= 2.27317, loss=-0.01990
surrogate=-0.00430, entropy= 2.26865, loss=-0.00430
surrogate= 0.02703, entropy= 2.26262, loss= 0.02703
surrogate=-0.03499, entropy= 2.26095, loss=-0.03499
surrogate=-0.01044, entropy= 2.25874, loss=-0.01044
surrogate=-0.01782, entropy= 2.25632, loss=-0.01782
surrogate=-0.02288, entropy= 2.25309, loss=-0.02288
std_min= 0.47243, std_max= 0.54194, std_mean= 0.51364
val lr: [0.0002023565573770492], policy lr: [0.000242827868852459]
Policy Loss: -0.022884, | Entropy Bonus: -0, | Value Loss: 390.71, | Advantage Loss: 1.8152
Time elapsed (s): 1.6687886714935303
Agent stdevs: 0.51364
--------------------------------------------------------------------------------

Step 186
++++++++ Policy training ++++++++++
Current mean reward: 1295.677182 | mean episode length: 372.400000
val_loss=33.88535
val_loss=24.01057
val_loss=16.02475
val_loss=18.97537
val_loss=21.97349
val_loss=18.30617
val_loss=22.27005
val_loss=24.37185
val_loss=15.02943
val_loss=22.51742
adv_loss= 1.12939
adv_loss= 1.47967
adv_loss= 2.95203
adv_loss= 2.07074
adv_loss= 2.59993
adv_loss= 2.20542
adv_loss= 1.90783
adv_loss= 1.94026
adv_loss= 2.24471
adv_loss= 5.03182
surrogate= 0.00051, entropy= 2.25294, loss= 0.00051
surrogate=-0.02163, entropy= 2.25393, loss=-0.02163
surrogate=-0.00956, entropy= 2.25438, loss=-0.00956
surrogate=-0.02078, entropy= 2.25622, loss=-0.02078
surrogate= 0.00545, entropy= 2.25599, loss= 0.00545
surrogate=-0.01638, entropy= 2.25856, loss=-0.01638
surrogate=-0.03846, entropy= 2.25939, loss=-0.03846
surrogate= 0.00270, entropy= 2.25959, loss= 0.00270
surrogate=-0.01413, entropy= 2.26189, loss=-0.01413
surrogate= 0.00626, entropy= 2.26346, loss= 0.00626
std_min= 0.46533, std_max= 0.54675, std_mean= 0.51585
val lr: [0.0002021004098360656], policy lr: [0.0002425204918032787]
Policy Loss: 0.0062586, | Entropy Bonus: -0, | Value Loss: 22.517, | Advantage Loss: 5.0318
Time elapsed (s): 1.7373478412628174
Agent stdevs: 0.51585025
--------------------------------------------------------------------------------

Step 187
++++++++ Policy training ++++++++++
Current mean reward: 1687.104881 | mean episode length: 569.333333
val_loss=831.46533
val_loss=191.13309
val_loss=198.64528
val_loss=450.14276
val_loss=817.26233
val_loss=78.57332
val_loss=65.38405
val_loss=447.09805
val_loss=246.94922
val_loss=993.39685
adv_loss= 5.50877
adv_loss= 5.49881
adv_loss= 5.80993
adv_loss= 3.85457
adv_loss= 3.56477
adv_loss= 3.36320
adv_loss= 4.27687
adv_loss= 3.83803
adv_loss= 5.80138
adv_loss= 5.19767
surrogate=-0.00875, entropy= 2.26303, loss=-0.00875
surrogate=-0.02893, entropy= 2.26331, loss=-0.02893
surrogate=-0.00627, entropy= 2.26550, loss=-0.00627
surrogate=-0.01518, entropy= 2.26753, loss=-0.01518
surrogate=-0.02120, entropy= 2.26893, loss=-0.02120
surrogate=-0.02502, entropy= 2.26790, loss=-0.02502
surrogate= 0.00562, entropy= 2.26710, loss= 0.00562
surrogate=-0.01930, entropy= 2.26792, loss=-0.01930
surrogate=-0.00153, entropy= 2.26902, loss=-0.00153
surrogate=-0.03022, entropy= 2.26839, loss=-0.03022
std_min= 0.46570, std_max= 0.54843, std_mean= 0.51674
val lr: [0.00020184426229508196], policy lr: [0.00024221311475409833]
Policy Loss: -0.030225, | Entropy Bonus: -0, | Value Loss: 993.4, | Advantage Loss: 5.1977
Time elapsed (s): 1.7064454555511475
Agent stdevs: 0.5167352
--------------------------------------------------------------------------------

Step 188
++++++++ Policy training ++++++++++
Current mean reward: 2880.682241 | mean episode length: 1000.000000
val_loss=84.89383
val_loss=349.85742
val_loss=1160.66931
val_loss=342.56006
val_loss=88.99527
val_loss=133.22496
val_loss=405.05310
val_loss=745.02203
val_loss=109.99948
val_loss=78.45107
adv_loss= 3.79792
adv_loss= 3.84799
adv_loss= 3.17594
adv_loss=1202.99561
adv_loss=16.49369
adv_loss= 2.83745
adv_loss= 3.60102
adv_loss= 2.61161
adv_loss= 5.40204
adv_loss= 5.17583
surrogate= 0.01686, entropy= 2.27115, loss= 0.01686
surrogate=-0.01522, entropy= 2.27304, loss=-0.01522
surrogate=-0.01965, entropy= 2.27691, loss=-0.01965
surrogate= 0.05116, entropy= 2.28209, loss= 0.05116
surrogate=-0.00732, entropy= 2.28344, loss=-0.00732
surrogate=-0.03631, entropy= 2.28671, loss=-0.03631
surrogate=-0.02691, entropy= 2.28780, loss=-0.02691
surrogate=-0.01832, entropy= 2.29150, loss=-0.01832
surrogate=-0.00619, entropy= 2.29222, loss=-0.00619
surrogate= 0.00628, entropy= 2.29283, loss= 0.00628
std_min= 0.46995, std_max= 0.55096, std_mean= 0.52094
val lr: [0.00020158811475409836], policy lr: [0.000241905737704918]
Policy Loss: 0.0062752, | Entropy Bonus: -0, | Value Loss: 78.451, | Advantage Loss: 5.1758
Time elapsed (s): 1.6903717517852783
Agent stdevs: 0.5209359
--------------------------------------------------------------------------------

Step 189
++++++++ Policy training ++++++++++
Current mean reward: 2120.995008 | mean episode length: 646.000000
val_loss=640.91968
val_loss=74.91220
val_loss=194.15137
val_loss=1657.79858
val_loss=1184.73462
val_loss=39.75576
val_loss=264.23770
val_loss=785.78381
val_loss=43.87972
val_loss=888.33997
adv_loss= 4.50371
adv_loss= 2.23457
adv_loss= 5.28931
adv_loss= 1.75792
adv_loss= 7.64836
adv_loss= 2.08598
adv_loss= 2.17926
adv_loss= 4.43017
adv_loss= 2.54015
adv_loss= 4.84664
surrogate=-0.00269, entropy= 2.29098, loss=-0.00269
surrogate=-0.00723, entropy= 2.29112, loss=-0.00723
surrogate=-0.00139, entropy= 2.29259, loss=-0.00139
surrogate= 0.02534, entropy= 2.29142, loss= 0.02534
surrogate= 0.00597, entropy= 2.28974, loss= 0.00597
surrogate= 0.02784, entropy= 2.28733, loss= 0.02784
surrogate= 0.00034, entropy= 2.28519, loss= 0.00034
surrogate=-0.03138, entropy= 2.28537, loss=-0.03138
surrogate= 0.00570, entropy= 2.28439, loss= 0.00570
surrogate=-0.02875, entropy= 2.28491, loss=-0.02875
std_min= 0.46865, std_max= 0.54741, std_mean= 0.51955
val lr: [0.00020133196721311476], policy lr: [0.00024159836065573768]
Policy Loss: -0.028752, | Entropy Bonus: -0, | Value Loss: 888.34, | Advantage Loss: 4.8466
Time elapsed (s): 1.6943368911743164
Agent stdevs: 0.5195461
--------------------------------------------------------------------------------

Step 190
++++++++ Policy training ++++++++++
Current mean reward: 1222.241264 | mean episode length: 361.800000
val_loss=73.20211
val_loss=149.09143
val_loss=167.03880
val_loss=59.94436
val_loss=32.75910
val_loss=114.91052
val_loss=132.75301
val_loss=134.64178
val_loss=136.89821
val_loss=33.95867
adv_loss=18.43963
adv_loss= 5.96351
adv_loss= 5.12799
adv_loss=10.25777
adv_loss= 1.66791
adv_loss= 5.04424
adv_loss= 8.51798
adv_loss= 6.94172
adv_loss= 2.90018
adv_loss= 2.13208
surrogate=-0.00688, entropy= 2.28063, loss=-0.00688
surrogate= 0.02239, entropy= 2.27934, loss= 0.02239
surrogate=-0.00817, entropy= 2.27711, loss=-0.00817
surrogate= 0.00406, entropy= 2.27504, loss= 0.00406
surrogate=-0.02029, entropy= 2.27299, loss=-0.02029
surrogate=-0.02102, entropy= 2.26952, loss=-0.02102
surrogate=-0.00967, entropy= 2.26675, loss=-0.00967
surrogate=-0.06078, entropy= 2.26528, loss=-0.06078
surrogate=-0.01218, entropy= 2.26289, loss=-0.01218
surrogate=-0.03596, entropy= 2.26103, loss=-0.03596
std_min= 0.46118, std_max= 0.54328, std_mean= 0.51562
val lr: [0.00020107581967213116], policy lr: [0.00024129098360655736]
Policy Loss: -0.035964, | Entropy Bonus: -0, | Value Loss: 33.959, | Advantage Loss: 2.1321
Time elapsed (s): 1.6823303699493408
Agent stdevs: 0.51562095
--------------------------------------------------------------------------------

Step 191
++++++++ Policy training ++++++++++
Current mean reward: 1375.917332 | mean episode length: 408.000000
val_loss=31.67206
val_loss=21.10299
val_loss=32.59794
val_loss=28.59062
val_loss=59.25885
val_loss=31.06966
val_loss=28.23882
val_loss=38.18952
val_loss=22.11239
val_loss=26.99373
adv_loss= 3.97056
adv_loss= 4.23839
adv_loss= 2.56724
adv_loss= 2.38730
adv_loss= 3.14088
adv_loss= 5.18857
adv_loss= 2.94508
adv_loss= 8.56589
adv_loss= 1.83163
adv_loss= 2.17744
surrogate=-0.00152, entropy= 2.25895, loss=-0.00152
surrogate=-0.01101, entropy= 2.25520, loss=-0.01101
surrogate= 0.01390, entropy= 2.25374, loss= 0.01390
surrogate=-0.00626, entropy= 2.25194, loss=-0.00626
surrogate= 0.02924, entropy= 2.24961, loss= 0.02924
surrogate=-0.01993, entropy= 2.24672, loss=-0.01993
surrogate=-0.01777, entropy= 2.24352, loss=-0.01777
surrogate=-0.02583, entropy= 2.24229, loss=-0.02583
surrogate=-0.01854, entropy= 2.24163, loss=-0.01854
surrogate=-0.03209, entropy= 2.23942, loss=-0.03209
std_min= 0.45420, std_max= 0.54483, std_mean= 0.51215
val lr: [0.00020081967213114754], policy lr: [0.00024098360655737703]
Policy Loss: -0.032092, | Entropy Bonus: -0, | Value Loss: 26.994, | Advantage Loss: 2.1774
Time elapsed (s): 1.675835132598877
Agent stdevs: 0.51215464
--------------------------------------------------------------------------------

Step 192
++++++++ Policy training ++++++++++
Current mean reward: 1323.024079 | mean episode length: 385.400000
val_loss=19.82166
val_loss=295.26038
val_loss=64.73096
val_loss=24.83627
val_loss=1665.16895
val_loss=766.47656
val_loss=57.91502
val_loss=49.39529
val_loss=292.64496
val_loss=43.85884
adv_loss= 4.04044
adv_loss= 3.94094
adv_loss= 1.50521
adv_loss= 2.10124
adv_loss= 1.55580
adv_loss= 2.86125
adv_loss= 2.91416
adv_loss=1458.63428
adv_loss= 7.66288
adv_loss= 2.74801
surrogate= 0.04638, entropy= 2.23811, loss= 0.04638
surrogate=-0.02555, entropy= 2.23718, loss=-0.02555
surrogate= 0.00555, entropy= 2.23912, loss= 0.00555
surrogate= 0.02040, entropy= 2.23853, loss= 0.02040
surrogate=-0.01366, entropy= 2.23745, loss=-0.01366
surrogate= 0.05078, entropy= 2.23630, loss= 0.05078
surrogate=-0.00791, entropy= 2.23757, loss=-0.00791
surrogate=-0.03870, entropy= 2.23647, loss=-0.03870
surrogate=-0.00102, entropy= 2.23495, loss=-0.00102
surrogate= 0.02502, entropy= 2.23592, loss= 0.02502
std_min= 0.45563, std_max= 0.54331, std_mean= 0.51144
val lr: [0.00020056352459016394], policy lr: [0.0002406762295081967]
Policy Loss: 0.02502, | Entropy Bonus: -0, | Value Loss: 43.859, | Advantage Loss: 2.748
Time elapsed (s): 1.7231028079986572
Agent stdevs: 0.5114434
--------------------------------------------------------------------------------

Step 193
++++++++ Policy training ++++++++++
Current mean reward: 956.104900 | mean episode length: 279.400000
val_loss=37.42126
val_loss=75.97625
val_loss=20.81220
val_loss=38.31751
val_loss=48.90246
val_loss=22.25538
val_loss=38.17570
val_loss=17.35267
val_loss=40.73919
val_loss=35.92841
adv_loss= 1.17347
adv_loss= 2.80611
adv_loss= 2.87222
adv_loss= 4.38241
adv_loss= 1.99097
adv_loss= 5.76289
adv_loss= 3.73962
adv_loss= 3.70630
adv_loss= 3.85345
adv_loss= 1.97292
surrogate= 0.00761, entropy= 2.23601, loss= 0.00761
surrogate=-0.02035, entropy= 2.23371, loss=-0.02035
surrogate=-0.02534, entropy= 2.23181, loss=-0.02534
surrogate=-0.04671, entropy= 2.22973, loss=-0.04671
surrogate=-0.01411, entropy= 2.22679, loss=-0.01411
surrogate=-0.01231, entropy= 2.22769, loss=-0.01231
surrogate=-0.01096, entropy= 2.22775, loss=-0.01096
surrogate=-0.02770, entropy= 2.22580, loss=-0.02770
surrogate= 0.00876, entropy= 2.22399, loss= 0.00876
surrogate= 0.00212, entropy= 2.22328, loss= 0.00212
std_min= 0.45749, std_max= 0.54640, std_mean= 0.50912
val lr: [0.00020030737704918034], policy lr: [0.00024036885245901638]
Policy Loss: 0.0021213, | Entropy Bonus: -0, | Value Loss: 35.928, | Advantage Loss: 1.9729
Time elapsed (s): 1.7012372016906738
Agent stdevs: 0.50912184
--------------------------------------------------------------------------------

Step 194
++++++++ Policy training ++++++++++
Current mean reward: 1043.837828 | mean episode length: 303.833333
val_loss=27.91346
val_loss=24.73450
val_loss=38.77016
val_loss=15.40408
val_loss=17.79334
val_loss=19.48726
val_loss=12.78574
val_loss=18.59397
val_loss= 8.11940
val_loss=17.81977
adv_loss= 1.39855
adv_loss= 1.71843
adv_loss= 2.94305
adv_loss= 3.30024
adv_loss= 2.80909
adv_loss= 3.92576
adv_loss= 1.86600
adv_loss= 3.46632
adv_loss= 3.19351
adv_loss= 2.37453
surrogate=-0.00248, entropy= 2.22084, loss=-0.00248
surrogate= 0.00881, entropy= 2.21877, loss= 0.00881
surrogate=-0.03390, entropy= 2.21686, loss=-0.03390
surrogate= 0.00275, entropy= 2.21596, loss= 0.00275
surrogate=-0.00759, entropy= 2.21301, loss=-0.00759
surrogate=-0.00340, entropy= 2.21131, loss=-0.00340
surrogate=-0.01140, entropy= 2.20905, loss=-0.01140
surrogate= 0.00211, entropy= 2.20675, loss= 0.00211
surrogate=-0.00430, entropy= 2.20612, loss=-0.00430
surrogate=-0.03580, entropy= 2.20388, loss=-0.03580
std_min= 0.45344, std_max= 0.54522, std_mean= 0.50595
val lr: [0.0002000512295081967], policy lr: [0.00024006147540983603]
Policy Loss: -0.035805, | Entropy Bonus: -0, | Value Loss: 17.82, | Advantage Loss: 2.3745
Time elapsed (s): 1.6830389499664307
Agent stdevs: 0.5059496
--------------------------------------------------------------------------------

Step 195
++++++++ Policy training ++++++++++
Current mean reward: 1032.884540 | mean episode length: 302.800000
val_loss=37.87898
val_loss=30.87311
val_loss=13.62754
val_loss=37.02509
val_loss=22.11547
val_loss=22.64429
val_loss=11.10004
val_loss=12.71381
val_loss=20.47597
val_loss=18.28006
adv_loss= 1.78569
adv_loss= 4.77449
adv_loss= 4.58116
adv_loss= 4.24378
adv_loss= 2.42730
adv_loss= 2.02933
adv_loss= 9.77505
adv_loss= 3.39974
adv_loss= 3.31892
adv_loss= 3.06135
surrogate= 0.00288, entropy= 2.20378, loss= 0.00288
surrogate=-0.01786, entropy= 2.20360, loss=-0.01786
surrogate=-0.01280, entropy= 2.20421, loss=-0.01280
surrogate=-0.00541, entropy= 2.20531, loss=-0.00541
surrogate=-0.02181, entropy= 2.20471, loss=-0.02181
surrogate=-0.02803, entropy= 2.20732, loss=-0.02803
surrogate=-0.03442, entropy= 2.20677, loss=-0.03442
surrogate=-0.02718, entropy= 2.20544, loss=-0.02718
surrogate=-0.02168, entropy= 2.20535, loss=-0.02168
surrogate=-0.02921, entropy= 2.20508, loss=-0.02921
std_min= 0.45432, std_max= 0.55047, std_mean= 0.50624
val lr: [0.00019979508196721313], policy lr: [0.00023975409836065573]
Policy Loss: -0.029212, | Entropy Bonus: -0, | Value Loss: 18.28, | Advantage Loss: 3.0614
Time elapsed (s): 1.7242991924285889
Agent stdevs: 0.5062376
--------------------------------------------------------------------------------

Step 196
++++++++ Policy training ++++++++++
Current mean reward: 1001.366994 | mean episode length: 289.857143
val_loss=42.49462
val_loss=15.33109
val_loss=22.46268
val_loss=16.75700
val_loss=12.54158
val_loss=13.48766
val_loss=12.13169
val_loss=15.48558
val_loss=20.08209
val_loss=19.28406
adv_loss= 1.63073
adv_loss= 3.36092
adv_loss= 3.25541
adv_loss= 6.59380
adv_loss=11.60128
adv_loss= 1.92786
adv_loss= 1.37977
adv_loss= 9.56634
adv_loss= 2.54524
adv_loss= 7.03009
surrogate= 0.05759, entropy= 2.20134, loss= 0.05759
surrogate=-0.02411, entropy= 2.19651, loss=-0.02411
surrogate= 0.00544, entropy= 2.19288, loss= 0.00544
surrogate=-0.02835, entropy= 2.18887, loss=-0.02835
surrogate=-0.03191, entropy= 2.18439, loss=-0.03191
surrogate=-0.01583, entropy= 2.18100, loss=-0.01583
surrogate= 0.00434, entropy= 2.17734, loss= 0.00434
surrogate=-0.00753, entropy= 2.17156, loss=-0.00753
surrogate=-0.00131, entropy= 2.16804, loss=-0.00131
surrogate=-0.04575, entropy= 2.16350, loss=-0.04575
std_min= 0.44761, std_max= 0.54367, std_mean= 0.49927
val lr: [0.0001995389344262295], policy lr: [0.00023944672131147538]
Policy Loss: -0.045754, | Entropy Bonus: -0, | Value Loss: 19.284, | Advantage Loss: 7.0301
Time elapsed (s): 1.6904921531677246
Agent stdevs: 0.49926782
--------------------------------------------------------------------------------

Step 197
++++++++ Policy training ++++++++++
Current mean reward: 1171.382970 | mean episode length: 339.666667
val_loss=32.40735
val_loss=22.64302
val_loss=24.40100
val_loss=21.06186
val_loss=27.27299
val_loss=26.18813
val_loss=20.44849
val_loss=23.76717
val_loss=26.73899
val_loss=18.67782
adv_loss= 1.97602
adv_loss= 2.05640
adv_loss= 1.95587
adv_loss= 3.61652
adv_loss= 4.03202
adv_loss= 1.99326
adv_loss= 3.59438
adv_loss= 3.29506
adv_loss= 2.72603
adv_loss= 0.89641
surrogate= 0.02163, entropy= 2.16338, loss= 0.02163
surrogate= 0.00337, entropy= 2.15917, loss= 0.00337
surrogate=-0.02707, entropy= 2.15961, loss=-0.02707
surrogate= 0.02827, entropy= 2.15980, loss= 0.02827
surrogate=-0.00386, entropy= 2.16061, loss=-0.00386
surrogate=-0.04657, entropy= 2.15900, loss=-0.04657
surrogate=-0.01711, entropy= 2.15783, loss=-0.01711
surrogate=-0.03538, entropy= 2.15743, loss=-0.03538
surrogate=-0.00659, entropy= 2.15589, loss=-0.00659
surrogate=-0.00403, entropy= 2.15599, loss=-0.00403
std_min= 0.44708, std_max= 0.54432, std_mean= 0.49806
val lr: [0.0001992827868852459], policy lr: [0.00023913934426229505]
Policy Loss: -0.0040267, | Entropy Bonus: -0, | Value Loss: 18.678, | Advantage Loss: 0.89641
Time elapsed (s): 1.6900525093078613
Agent stdevs: 0.49806234
--------------------------------------------------------------------------------

Step 198
++++++++ Policy training ++++++++++
Current mean reward: 3167.920890 | mean episode length: 1000.000000
val_loss=199.23331
val_loss=114.45808
val_loss=127.10845
val_loss=226.38000
val_loss=83.16455
val_loss=540.87573
val_loss=1387.78723
val_loss=208.84146
val_loss=57.22366
val_loss=329.56049
adv_loss= 3.35725
adv_loss= 4.43577
adv_loss= 3.32820
adv_loss= 2.77340
adv_loss= 1.86046
adv_loss= 2.44234
adv_loss= 2.80501
adv_loss= 2.51280
adv_loss= 1.55410
adv_loss= 2.28435
surrogate= 0.02834, entropy= 2.16056, loss= 0.02834
surrogate= 0.00047, entropy= 2.16313, loss= 0.00047
surrogate=-0.00223, entropy= 2.16547, loss=-0.00223
surrogate=-0.00741, entropy= 2.16424, loss=-0.00741
surrogate=-0.00794, entropy= 2.16612, loss=-0.00794
surrogate=-0.00383, entropy= 2.16843, loss=-0.00383
surrogate=-0.02217, entropy= 2.17216, loss=-0.02217
surrogate= 0.00991, entropy= 2.17270, loss= 0.00991
surrogate= 0.01425, entropy= 2.17541, loss= 0.01425
surrogate=-0.01450, entropy= 2.17783, loss=-0.01450
std_min= 0.45181, std_max= 0.54746, std_mean= 0.50163
val lr: [0.0001990266393442623], policy lr: [0.00023883196721311473]
Policy Loss: -0.014497, | Entropy Bonus: -0, | Value Loss: 329.56, | Advantage Loss: 2.2844
Time elapsed (s): 1.7443714141845703
Agent stdevs: 0.50163406
--------------------------------------------------------------------------------

Step 199
++++++++ Policy training ++++++++++
Current mean reward: 1353.259875 | mean episode length: 392.800000
val_loss=66.43504
val_loss=65.59585
val_loss=91.98433
val_loss=33.60926
val_loss=38.63034
val_loss=48.52314
val_loss=21.47408
val_loss=64.07375
val_loss=31.33728
val_loss=26.14952
adv_loss=11.39361
adv_loss= 7.61295
adv_loss= 4.55014
adv_loss= 8.88804
adv_loss= 8.84110
adv_loss= 2.55876
adv_loss= 3.19863
adv_loss= 8.46903
adv_loss= 2.07649
adv_loss= 2.65163
surrogate= 0.02249, entropy= 2.17761, loss= 0.02249
surrogate=-0.00868, entropy= 2.17466, loss=-0.00868
surrogate=-0.01036, entropy= 2.17199, loss=-0.01036
surrogate= 0.00450, entropy= 2.17306, loss= 0.00450
surrogate= 0.00576, entropy= 2.17025, loss= 0.00576
surrogate=-0.00890, entropy= 2.16822, loss=-0.00890
surrogate=-0.01517, entropy= 2.16621, loss=-0.01517
surrogate= 0.02613, entropy= 2.16440, loss= 0.02613
surrogate= 0.00588, entropy= 2.16248, loss= 0.00588
surrogate=-0.00817, entropy= 2.15904, loss=-0.00817
std_min= 0.45087, std_max= 0.54618, std_mean= 0.49848
val lr: [0.00019877049180327868], policy lr: [0.0002385245901639344]
Policy Loss: -0.0081686, | Entropy Bonus: -0, | Value Loss: 26.15, | Advantage Loss: 2.6516
Time elapsed (s): 1.7055799961090088
Agent stdevs: 0.49847785
--------------------------------------------------------------------------------

Step 200
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1545.4
++++++++ Policy training ++++++++++
Current mean reward: 885.994283 | mean episode length: 258.285714
val_loss=89.08560
val_loss=62.53016
val_loss=31.86416
val_loss=42.70885
val_loss=43.50889
val_loss=36.25354
val_loss=67.11539
val_loss=28.45314
val_loss=60.17881
val_loss=22.55032
adv_loss= 4.06180
adv_loss=14.71940
adv_loss= 8.34949
adv_loss= 7.68429
adv_loss= 5.27212
adv_loss= 5.76285
adv_loss= 6.44219
adv_loss=19.42522
adv_loss= 6.86201
adv_loss= 7.60189
surrogate=-0.01815, entropy= 2.16082, loss=-0.01815
surrogate= 0.01838, entropy= 2.15982, loss= 0.01838
surrogate=-0.01500, entropy= 2.15956, loss=-0.01500
surrogate= 0.00540, entropy= 2.15869, loss= 0.00540
surrogate= 0.01046, entropy= 2.15931, loss= 0.01046
surrogate=-0.01794, entropy= 2.15792, loss=-0.01794
surrogate=-0.02850, entropy= 2.15824, loss=-0.02850
surrogate=-0.01322, entropy= 2.15839, loss=-0.01322
surrogate=-0.01676, entropy= 2.15814, loss=-0.01676
surrogate=-0.05405, entropy= 2.15790, loss=-0.05405
std_min= 0.45077, std_max= 0.54581, std_mean= 0.49828
val lr: [0.00019851434426229508], policy lr: [0.00023821721311475407]
Policy Loss: -0.054053, | Entropy Bonus: -0, | Value Loss: 22.55, | Advantage Loss: 7.6019
Time elapsed (s): 1.693246841430664
Agent stdevs: 0.49828258
--------------------------------------------------------------------------------

Step 201
++++++++ Policy training ++++++++++
Current mean reward: 1693.240184 | mean episode length: 527.666667
val_loss=407.16815
val_loss=36.81562
val_loss=1510.89697
val_loss=1346.04626
val_loss=62.04013
val_loss=141.57494
val_loss=32.40251
val_loss=73.85161
val_loss=55.48242
val_loss=44.59411
adv_loss= 2.81013
adv_loss= 2.11853
adv_loss= 3.04415
adv_loss= 4.21067
adv_loss= 6.91812
adv_loss= 2.06799
adv_loss=1413.49048
adv_loss= 3.81717
adv_loss=10.24209
adv_loss=1419.08508
surrogate= 0.00866, entropy= 2.15584, loss= 0.00866
surrogate=-0.00990, entropy= 2.15396, loss=-0.00990
surrogate=-0.00288, entropy= 2.15278, loss=-0.00288
surrogate=-0.00444, entropy= 2.15227, loss=-0.00444
surrogate=-0.01632, entropy= 2.15085, loss=-0.01632
surrogate= 0.01030, entropy= 2.14964, loss= 0.01030
surrogate=-0.00365, entropy= 2.14824, loss=-0.00365
surrogate=-0.00781, entropy= 2.14587, loss=-0.00781
surrogate= 0.00142, entropy= 2.14382, loss= 0.00142
surrogate=-0.01490, entropy= 2.14382, loss=-0.01490
std_min= 0.44721, std_max= 0.54770, std_mean= 0.49612
val lr: [0.00019825819672131148], policy lr: [0.00023790983606557375]
Policy Loss: -0.014897, | Entropy Bonus: -0, | Value Loss: 44.594, | Advantage Loss: 1419.1
Time elapsed (s): 1.68528151512146
Agent stdevs: 0.49611914
--------------------------------------------------------------------------------

Step 202
++++++++ Policy training ++++++++++
Current mean reward: 1461.374529 | mean episode length: 469.250000
val_loss=60.63146
val_loss=290.92029
val_loss=61.47211
val_loss=42.29281
val_loss=154.24327
val_loss=118.83918
val_loss=252.98602
val_loss=70.34807
val_loss=676.23328
val_loss=77.69102
adv_loss= 5.23191
adv_loss= 3.07986
adv_loss= 6.30347
adv_loss= 2.69010
adv_loss= 2.87860
adv_loss= 6.11719
adv_loss= 4.85882
adv_loss= 3.52464
adv_loss= 4.65220
adv_loss= 2.03450
surrogate= 0.00893, entropy= 2.14200, loss= 0.00893
surrogate= 0.00321, entropy= 2.14279, loss= 0.00321
surrogate=-0.02284, entropy= 2.14585, loss=-0.02284
surrogate= 0.00750, entropy= 2.14578, loss= 0.00750
surrogate=-0.01532, entropy= 2.14434, loss=-0.01532
surrogate=-0.02698, entropy= 2.14640, loss=-0.02698
surrogate=-0.00494, entropy= 2.14585, loss=-0.00494
surrogate=-0.00827, entropy= 2.14661, loss=-0.00827
surrogate=-0.00471, entropy= 2.14672, loss=-0.00471
surrogate=-0.01195, entropy= 2.14802, loss=-0.01195
std_min= 0.45098, std_max= 0.54633, std_mean= 0.49666
val lr: [0.00019800204918032788], policy lr: [0.00023760245901639342]
Policy Loss: -0.011955, | Entropy Bonus: -0, | Value Loss: 77.691, | Advantage Loss: 2.0345
Time elapsed (s): 1.6959223747253418
Agent stdevs: 0.4966568
--------------------------------------------------------------------------------

Step 203
++++++++ Policy training ++++++++++
Current mean reward: 2051.722431 | mean episode length: 619.000000
val_loss=797.78540
val_loss=37.22696
val_loss=1268.41101
val_loss=335.02612
val_loss=532.44489
val_loss=1094.51111
val_loss=72.37086
val_loss=54.04791
val_loss=428.45132
val_loss=49.32547
adv_loss= 2.08069
adv_loss= 3.24850
adv_loss= 3.48936
adv_loss= 3.13368
adv_loss= 3.83195
adv_loss= 1.85120
adv_loss= 2.57044
adv_loss= 5.89141
adv_loss= 1.76954
adv_loss= 3.27139
surrogate= 0.00127, entropy= 2.14898, loss= 0.00127
surrogate=-0.02541, entropy= 2.14799, loss=-0.02541
surrogate=-0.00194, entropy= 2.14848, loss=-0.00194
surrogate= 0.00837, entropy= 2.14953, loss= 0.00837
surrogate=-0.00136, entropy= 2.15107, loss=-0.00136
surrogate= 0.00291, entropy= 2.15095, loss= 0.00291
surrogate= 0.00658, entropy= 2.14966, loss= 0.00658
surrogate=-0.00167, entropy= 2.14930, loss=-0.00167
surrogate=-0.01992, entropy= 2.14897, loss=-0.01992
surrogate=-0.01404, entropy= 2.14880, loss=-0.01404
std_min= 0.45462, std_max= 0.54703, std_mean= 0.49671
val lr: [0.00019774590163934425], policy lr: [0.00023729508196721307]
Policy Loss: -0.014043, | Entropy Bonus: -0, | Value Loss: 49.325, | Advantage Loss: 3.2714
Time elapsed (s): 1.6784188747406006
Agent stdevs: 0.49670777
--------------------------------------------------------------------------------

Step 204
++++++++ Policy training ++++++++++
Current mean reward: 1928.955751 | mean episode length: 594.666667
val_loss=37.70051
val_loss=29.06434
val_loss=101.78640
val_loss=39.71431
val_loss=110.30953
val_loss=41.27531
val_loss=209.20172
val_loss=221.32199
val_loss=47.50475
val_loss=27.45307
adv_loss= 4.64605
adv_loss= 2.04823
adv_loss= 4.15832
adv_loss= 2.88051
adv_loss= 2.87335
adv_loss= 2.89754
adv_loss= 1.86311
adv_loss= 3.23127
adv_loss= 2.64332
adv_loss= 4.27846
surrogate=-0.00168, entropy= 2.15086, loss=-0.00168
surrogate=-0.00647, entropy= 2.15269, loss=-0.00647
surrogate=-0.02438, entropy= 2.15555, loss=-0.02438
surrogate=-0.00581, entropy= 2.15811, loss=-0.00581
surrogate=-0.02324, entropy= 2.16310, loss=-0.02324
surrogate=-0.00281, entropy= 2.16464, loss=-0.00281
surrogate=-0.01783, entropy= 2.16704, loss=-0.01783
surrogate=-0.01521, entropy= 2.16646, loss=-0.01521
surrogate=-0.00291, entropy= 2.16688, loss=-0.00291
surrogate=-0.01328, entropy= 2.16778, loss=-0.01328
std_min= 0.46030, std_max= 0.55435, std_mean= 0.49995
val lr: [0.00019748975409836068], policy lr: [0.00023698770491803277]
Policy Loss: -0.013281, | Entropy Bonus: -0, | Value Loss: 27.453, | Advantage Loss: 4.2785
Time elapsed (s): 1.7497782707214355
Agent stdevs: 0.49994704
--------------------------------------------------------------------------------

Step 205
++++++++ Policy training ++++++++++
Current mean reward: 1566.813087 | mean episode length: 471.750000
val_loss=1128.91895
val_loss=154.44728
val_loss=35.48306
val_loss=103.22643
val_loss=525.11224
val_loss=899.26221
val_loss=39.42292
val_loss=58.28344
val_loss=791.17291
val_loss=395.96539
adv_loss= 5.70209
adv_loss= 2.78155
adv_loss= 3.03810
adv_loss= 5.08263
adv_loss= 4.98351
adv_loss= 4.20265
adv_loss= 3.18425
adv_loss=1128.31116
adv_loss= 6.67229
adv_loss= 5.53115
surrogate=-0.00309, entropy= 2.16725, loss=-0.00309
surrogate=-0.01229, entropy= 2.16779, loss=-0.01229
surrogate= 0.01461, entropy= 2.16953, loss= 0.01461
surrogate= 0.00185, entropy= 2.17008, loss= 0.00185
surrogate=-0.01800, entropy= 2.17078, loss=-0.01800
surrogate=-0.00782, entropy= 2.17213, loss=-0.00782
surrogate=-0.02029, entropy= 2.17249, loss=-0.02029
surrogate=-0.03409, entropy= 2.17483, loss=-0.03409
surrogate=-0.01254, entropy= 2.17384, loss=-0.01254
surrogate=-0.01341, entropy= 2.17584, loss=-0.01341
std_min= 0.46554, std_max= 0.55200, std_mean= 0.50106
val lr: [0.00019723360655737705], policy lr: [0.00023668032786885242]
Policy Loss: -0.013414, | Entropy Bonus: -0, | Value Loss: 395.97, | Advantage Loss: 5.5312
Time elapsed (s): 1.693403959274292
Agent stdevs: 0.50105745
--------------------------------------------------------------------------------

Step 206
++++++++ Policy training ++++++++++
Current mean reward: 1110.012177 | mean episode length: 323.166667
val_loss=29.18771
val_loss=13.10971
val_loss=16.80501
val_loss=18.13570
val_loss=16.76414
val_loss=11.71535
val_loss=11.41073
val_loss=17.99199
val_loss= 9.69098
val_loss=17.09134
adv_loss= 4.33574
adv_loss= 7.44372
adv_loss= 7.74975
adv_loss=13.05272
adv_loss= 4.02031
adv_loss= 2.49366
adv_loss= 3.54229
adv_loss= 2.90568
adv_loss= 2.60688
adv_loss= 1.72013
surrogate= 0.04096, entropy= 2.17604, loss= 0.04096
surrogate=-0.01274, entropy= 2.17506, loss=-0.01274
surrogate=-0.01615, entropy= 2.17638, loss=-0.01615
surrogate=-0.01216, entropy= 2.17435, loss=-0.01216
surrogate=-0.00097, entropy= 2.17518, loss=-0.00097
surrogate= 0.00862, entropy= 2.17704, loss= 0.00862
surrogate=-0.02495, entropy= 2.17548, loss=-0.02495
surrogate=-0.00872, entropy= 2.17564, loss=-0.00872
surrogate=-0.02985, entropy= 2.17637, loss=-0.02985
surrogate= 0.02245, entropy= 2.17577, loss= 0.02245
std_min= 0.46468, std_max= 0.54854, std_mean= 0.50093
val lr: [0.00019697745901639345], policy lr: [0.0002363729508196721]
Policy Loss: 0.022453, | Entropy Bonus: -0, | Value Loss: 17.091, | Advantage Loss: 1.7201
Time elapsed (s): 1.7235431671142578
Agent stdevs: 0.50093263
--------------------------------------------------------------------------------

Step 207
++++++++ Policy training ++++++++++
Current mean reward: 1571.137809 | mean episode length: 478.000000
val_loss=30.05613
val_loss=65.86816
val_loss=54.01388
val_loss=879.50427
val_loss=1095.58667
val_loss=782.19183
val_loss=36.71538
val_loss=220.43205
val_loss=40.96285
val_loss=427.14230
adv_loss= 7.36221
adv_loss= 7.10048
adv_loss= 1.41374
adv_loss= 3.10464
adv_loss= 4.08449
adv_loss= 2.08493
adv_loss= 3.53145
adv_loss= 2.49488
adv_loss= 2.75588
adv_loss= 2.07933
surrogate= 0.02515, entropy= 2.17635, loss= 0.02515
surrogate= 0.00881, entropy= 2.17416, loss= 0.00881
surrogate=-0.01708, entropy= 2.17376, loss=-0.01708
surrogate=-0.02607, entropy= 2.17340, loss=-0.02607
surrogate=-0.02027, entropy= 2.17491, loss=-0.02027
surrogate= 0.00245, entropy= 2.17577, loss= 0.00245
surrogate=-0.00800, entropy= 2.17620, loss=-0.00800
surrogate=-0.03062, entropy= 2.17715, loss=-0.03062
surrogate=-0.01267, entropy= 2.17740, loss=-0.01267
surrogate=-0.01338, entropy= 2.18004, loss=-0.01338
std_min= 0.46046, std_max= 0.55085, std_mean= 0.50182
val lr: [0.00019672131147540983], policy lr: [0.00023606557377049177]
Policy Loss: -0.013382, | Entropy Bonus: -0, | Value Loss: 427.14, | Advantage Loss: 2.0793
Time elapsed (s): 1.7059242725372314
Agent stdevs: 0.50182205
--------------------------------------------------------------------------------

Step 208
++++++++ Policy training ++++++++++
Current mean reward: 1196.097727 | mean episode length: 351.800000
val_loss=23.01204
val_loss=89.47148
val_loss=74.49287
val_loss=109.02664
val_loss=15.83300
val_loss=22.45533
val_loss=64.62115
val_loss=52.10506
val_loss=31.77459
val_loss=40.63165
adv_loss= 5.78506
adv_loss= 3.13914
adv_loss= 3.47372
adv_loss= 4.53253
adv_loss=16.14318
adv_loss= 5.30374
adv_loss= 2.02525
adv_loss= 2.37449
adv_loss= 5.23535
adv_loss= 6.03159
surrogate=-0.01060, entropy= 2.17982, loss=-0.01060
surrogate= 0.00687, entropy= 2.18042, loss= 0.00687
surrogate= 0.00001, entropy= 2.18144, loss= 0.00001
surrogate=-0.01251, entropy= 2.18502, loss=-0.01251
surrogate=-0.04352, entropy= 2.18783, loss=-0.04352
surrogate= 0.00865, entropy= 2.18850, loss= 0.00865
surrogate=-0.01716, entropy= 2.19047, loss=-0.01716
surrogate=-0.04351, entropy= 2.19209, loss=-0.04351
surrogate=-0.00667, entropy= 2.19459, loss=-0.00667
surrogate=-0.02651, entropy= 2.19465, loss=-0.02651
std_min= 0.46532, std_max= 0.55498, std_mean= 0.50427
val lr: [0.00019646516393442622], policy lr: [0.00023575819672131144]
Policy Loss: -0.026512, | Entropy Bonus: -0, | Value Loss: 40.632, | Advantage Loss: 6.0316
Time elapsed (s): 1.740656852722168
Agent stdevs: 0.504272
--------------------------------------------------------------------------------

Step 209
++++++++ Policy training ++++++++++
Current mean reward: 2776.333065 | mean episode length: 847.500000
val_loss=199.77939
val_loss=1028.44128
val_loss=77.83162
val_loss=59.51838
val_loss=28.65385
val_loss=118.95790
val_loss=108.77057
val_loss=315.04932
val_loss=462.82401
val_loss=40.52375
adv_loss= 4.21182
adv_loss= 3.23913
adv_loss= 1.51333
adv_loss= 2.21088
adv_loss= 4.59928
adv_loss= 2.85872
adv_loss= 1.96151
adv_loss= 3.44356
adv_loss= 3.09433
adv_loss= 1.65360
surrogate=-0.00464, entropy= 2.19805, loss=-0.00464
surrogate=-0.01954, entropy= 2.20022, loss=-0.01954
surrogate=-0.02246, entropy= 2.20070, loss=-0.02246
surrogate=-0.03100, entropy= 2.20446, loss=-0.03100
surrogate= 0.01910, entropy= 2.20717, loss= 0.01910
surrogate=-0.01295, entropy= 2.20675, loss=-0.01295
surrogate= 0.03038, entropy= 2.20803, loss= 0.03038
surrogate= 0.00029, entropy= 2.21100, loss= 0.00029
surrogate=-0.02728, entropy= 2.21371, loss=-0.02728
surrogate=-0.02986, entropy= 2.21315, loss=-0.02986
std_min= 0.46478, std_max= 0.56445, std_mean= 0.50769
val lr: [0.00019620901639344262], policy lr: [0.00023545081967213112]
Policy Loss: -0.029861, | Entropy Bonus: -0, | Value Loss: 40.524, | Advantage Loss: 1.6536
Time elapsed (s): 1.7284352779388428
Agent stdevs: 0.50768715
--------------------------------------------------------------------------------

Step 210
++++++++ Policy training ++++++++++
Current mean reward: 1829.723198 | mean episode length: 553.000000
val_loss=49.58551
val_loss=41.58793
val_loss=64.35845
val_loss=58.95806
val_loss=73.86466
val_loss=22.75388
val_loss=36.41549
val_loss=32.57130
val_loss=45.63609
val_loss=34.58198
adv_loss= 6.34445
adv_loss= 3.81104
adv_loss= 4.14083
adv_loss= 2.28995
adv_loss= 1.84145
adv_loss= 3.51178
adv_loss= 3.51830
adv_loss= 4.42647
adv_loss= 6.35691
adv_loss= 3.32715
surrogate=-0.00657, entropy= 2.21223, loss=-0.00657
surrogate= 0.01294, entropy= 2.20984, loss= 0.01294
surrogate= 0.01510, entropy= 2.20858, loss= 0.01510
surrogate=-0.00798, entropy= 2.20658, loss=-0.00798
surrogate=-0.02464, entropy= 2.20377, loss=-0.02464
surrogate=-0.01590, entropy= 2.20041, loss=-0.01590
surrogate=-0.00782, entropy= 2.19875, loss=-0.00782
surrogate=-0.00567, entropy= 2.19624, loss=-0.00567
surrogate=-0.02127, entropy= 2.19405, loss=-0.02127
surrogate=-0.00803, entropy= 2.19307, loss=-0.00803
std_min= 0.45803, std_max= 0.56056, std_mean= 0.50436
val lr: [0.00019595286885245902], policy lr: [0.00023514344262295082]
Policy Loss: -0.0080316, | Entropy Bonus: -0, | Value Loss: 34.582, | Advantage Loss: 3.3271
Time elapsed (s): 1.7621753215789795
Agent stdevs: 0.50436085
--------------------------------------------------------------------------------

Step 211
++++++++ Policy training ++++++++++
Current mean reward: 1260.817649 | mean episode length: 377.000000
val_loss=93.65965
val_loss=101.74257
val_loss=52.08234
val_loss=1125.97388
val_loss=126.90723
val_loss=1141.42651
val_loss=135.05707
val_loss=244.60507
val_loss=302.96542
val_loss=729.69891
adv_loss= 5.91472
adv_loss= 6.67896
adv_loss=10.63999
adv_loss= 6.99001
adv_loss= 6.51936
adv_loss= 8.33389
adv_loss= 7.44777
adv_loss= 6.30556
adv_loss= 6.57579
adv_loss=1049.80347
surrogate=-0.01343, entropy= 2.18805, loss=-0.01343
surrogate= 0.01399, entropy= 2.18746, loss= 0.01399
surrogate= 0.00787, entropy= 2.18816, loss= 0.00787
surrogate= 0.00081, entropy= 2.18639, loss= 0.00081
surrogate=-0.01966, entropy= 2.18753, loss=-0.01966
surrogate=-0.00802, entropy= 2.18736, loss=-0.00802
surrogate=-0.02858, entropy= 2.18763, loss=-0.02858
surrogate= 0.00257, entropy= 2.18561, loss= 0.00257
surrogate=-0.00732, entropy= 2.18376, loss=-0.00732
surrogate=-0.03000, entropy= 2.18189, loss=-0.03000
std_min= 0.45409, std_max= 0.55547, std_mean= 0.50245
val lr: [0.00019569672131147542], policy lr: [0.0002348360655737705]
Policy Loss: -0.030002, | Entropy Bonus: -0, | Value Loss: 729.7, | Advantage Loss: 1049.8
Time elapsed (s): 1.6971731185913086
Agent stdevs: 0.50245446
--------------------------------------------------------------------------------

Step 212
++++++++ Policy training ++++++++++
Current mean reward: 972.266506 | mean episode length: 280.714286
val_loss=46.83598
val_loss=47.78344
val_loss=48.55954
val_loss=36.03263
val_loss=17.31672
val_loss=29.09536
val_loss=25.34598
val_loss=21.76006
val_loss=23.87953
val_loss=30.55340
adv_loss= 5.94768
adv_loss= 3.07715
adv_loss=17.30402
adv_loss= 1.61617
adv_loss=15.18624
adv_loss= 3.52625
adv_loss= 5.70185
adv_loss= 4.96711
adv_loss= 2.50086
adv_loss= 3.77150
surrogate= 0.00556, entropy= 2.18148, loss= 0.00556
surrogate=-0.00388, entropy= 2.18214, loss=-0.00388
surrogate= 0.03300, entropy= 2.18318, loss= 0.03300
surrogate=-0.01740, entropy= 2.18334, loss=-0.01740
surrogate=-0.02721, entropy= 2.18282, loss=-0.02721
surrogate=-0.02256, entropy= 2.18304, loss=-0.02256
surrogate=-0.00706, entropy= 2.18477, loss=-0.00706
surrogate=-0.02662, entropy= 2.18511, loss=-0.02662
surrogate=-0.02620, entropy= 2.18682, loss=-0.02620
surrogate=-0.01053, entropy= 2.18857, loss=-0.01053
std_min= 0.45766, std_max= 0.55378, std_mean= 0.50342
val lr: [0.0001954405737704918], policy lr: [0.0002345286885245901]
Policy Loss: -0.010527, | Entropy Bonus: -0, | Value Loss: 30.553, | Advantage Loss: 3.7715
Time elapsed (s): 1.6943955421447754
Agent stdevs: 0.5034179
--------------------------------------------------------------------------------

Step 213
++++++++ Policy training ++++++++++
Current mean reward: 1583.155766 | mean episode length: 500.250000
val_loss=1252.23132
val_loss=2662.72949
val_loss=370.52594
val_loss=90.03008
val_loss=197.14951
val_loss=1134.68311
val_loss=385.44440
val_loss=136.61163
val_loss=457.32147
val_loss=118.77037
adv_loss= 2.07275
adv_loss= 2.58164
adv_loss= 3.06308
adv_loss= 2.45124
adv_loss= 4.63405
adv_loss= 1.68576
adv_loss= 2.43553
adv_loss= 2.44830
adv_loss= 2.21297
adv_loss= 3.30720
surrogate= 0.03995, entropy= 2.18530, loss= 0.03995
surrogate= 0.00331, entropy= 2.18822, loss= 0.00331
surrogate=-0.02031, entropy= 2.19038, loss=-0.02031
surrogate=-0.02966, entropy= 2.19253, loss=-0.02966
surrogate= 0.00901, entropy= 2.19371, loss= 0.00901
surrogate=-0.01379, entropy= 2.19632, loss=-0.01379
surrogate=-0.02692, entropy= 2.19755, loss=-0.02692
surrogate=-0.03849, entropy= 2.19991, loss=-0.03849
surrogate=-0.02423, entropy= 2.19915, loss=-0.02423
surrogate= 0.00511, entropy= 2.20130, loss= 0.00511
std_min= 0.45844, std_max= 0.55815, std_mean= 0.50566
val lr: [0.00019518442622950822], policy lr: [0.00023422131147540984]
Policy Loss: 0.005109, | Entropy Bonus: -0, | Value Loss: 118.77, | Advantage Loss: 3.3072
Time elapsed (s): 1.7046160697937012
Agent stdevs: 0.5056623
--------------------------------------------------------------------------------

Step 214
++++++++ Policy training ++++++++++
Current mean reward: 2244.042206 | mean episode length: 688.500000
val_loss=157.09842
val_loss=194.86189
val_loss=241.05450
val_loss=124.25608
val_loss=98.23033
val_loss=1426.85327
val_loss=73.12647
val_loss=505.21909
val_loss=75.71893
val_loss=727.96313
adv_loss=25.11729
adv_loss= 7.33627
adv_loss= 6.99152
adv_loss=16.87781
adv_loss= 3.25144
adv_loss= 7.24520
adv_loss= 6.47601
adv_loss= 4.25853
adv_loss= 3.82448
adv_loss= 4.40397
surrogate=-0.00746, entropy= 2.20394, loss=-0.00746
surrogate=-0.01504, entropy= 2.20429, loss=-0.01504
surrogate=-0.00269, entropy= 2.20460, loss=-0.00269
surrogate= 0.00334, entropy= 2.20182, loss= 0.00334
surrogate= 0.01321, entropy= 2.20203, loss= 0.01321
surrogate=-0.00955, entropy= 2.20138, loss=-0.00955
surrogate=-0.03142, entropy= 2.20083, loss=-0.03142
surrogate=-0.03321, entropy= 2.20066, loss=-0.03321
surrogate=-0.01028, entropy= 2.19782, loss=-0.01028
surrogate=-0.01390, entropy= 2.19632, loss=-0.01390
std_min= 0.45783, std_max= 0.55304, std_mean= 0.50465
val lr: [0.0001949282786885246], policy lr: [0.0002339139344262295]
Policy Loss: -0.0139, | Entropy Bonus: -0, | Value Loss: 727.96, | Advantage Loss: 4.404
Time elapsed (s): 1.6947267055511475
Agent stdevs: 0.5046504
--------------------------------------------------------------------------------

Step 215
++++++++ Policy training ++++++++++
Current mean reward: 1077.561260 | mean episode length: 325.600000
val_loss=61.46875
val_loss=882.82013
val_loss=140.39403
val_loss=343.71985
val_loss=422.06177
val_loss=181.51498
val_loss=73.03547
val_loss=431.74564
val_loss=149.66154
val_loss=1187.38977
adv_loss= 5.45962
adv_loss=1315.75354
adv_loss= 9.89470
adv_loss= 3.11313
adv_loss= 4.52438
adv_loss= 2.37620
adv_loss= 2.63163
adv_loss= 4.72276
adv_loss=12.30376
adv_loss= 8.58871
surrogate= 0.02871, entropy= 2.19529, loss= 0.02871
surrogate= 0.04841, entropy= 2.19476, loss= 0.04841
surrogate=-0.00937, entropy= 2.19658, loss=-0.00937
surrogate=-0.00457, entropy= 2.19662, loss=-0.00457
surrogate=-0.02550, entropy= 2.19585, loss=-0.02550
surrogate= 0.00464, entropy= 2.19586, loss= 0.00464
surrogate=-0.02773, entropy= 2.19471, loss=-0.02773
surrogate= 0.00670, entropy= 2.19401, loss= 0.00670
surrogate= 0.00646, entropy= 2.19398, loss= 0.00646
surrogate=-0.01905, entropy= 2.19233, loss=-0.01905
std_min= 0.45305, std_max= 0.54885, std_mean= 0.50406
val lr: [0.00019467213114754097], policy lr: [0.00023360655737704916]
Policy Loss: -0.019048, | Entropy Bonus: -0, | Value Loss: 1187.4, | Advantage Loss: 8.5887
Time elapsed (s): 1.6942660808563232
Agent stdevs: 0.5040627
--------------------------------------------------------------------------------

Step 216
++++++++ Policy training ++++++++++
Current mean reward: 1963.245945 | mean episode length: 623.000000
val_loss=1227.95203
val_loss=61.49633
val_loss=373.71246
val_loss=551.99292
val_loss=473.54312
val_loss=589.92731
val_loss=215.66069
val_loss=1297.29297
val_loss=440.99722
val_loss=857.29724
adv_loss= 7.04026
adv_loss= 4.23715
adv_loss= 4.83263
adv_loss= 4.68552
adv_loss= 2.12277
adv_loss= 7.47361
adv_loss= 4.76932
adv_loss= 3.61958
adv_loss= 5.43030
adv_loss= 4.62704
surrogate= 0.00604, entropy= 2.19267, loss= 0.00604
surrogate=-0.02027, entropy= 2.19750, loss=-0.02027
surrogate=-0.00463, entropy= 2.19913, loss=-0.00463
surrogate=-0.00306, entropy= 2.20243, loss=-0.00306
surrogate=-0.03342, entropy= 2.20476, loss=-0.03342
surrogate=-0.02061, entropy= 2.20806, loss=-0.02061
surrogate=-0.00497, entropy= 2.21102, loss=-0.00497
surrogate=-0.00585, entropy= 2.21336, loss=-0.00585
surrogate=-0.02712, entropy= 2.21385, loss=-0.02712
surrogate=-0.03823, entropy= 2.21674, loss=-0.03823
std_min= 0.45509, std_max= 0.55195, std_mean= 0.50823
val lr: [0.00019441598360655737], policy lr: [0.00023329918032786884]
Policy Loss: -0.038232, | Entropy Bonus: -0, | Value Loss: 857.3, | Advantage Loss: 4.627
Time elapsed (s): 1.743234634399414
Agent stdevs: 0.5082302
--------------------------------------------------------------------------------

Step 217
++++++++ Policy training ++++++++++
Current mean reward: 1517.474493 | mean episode length: 444.750000
val_loss=38.46093
val_loss=33.60129
val_loss=40.11804
val_loss=43.35523
val_loss=37.26515
val_loss=62.37809
val_loss=40.23689
val_loss=35.11658
val_loss=33.35175
val_loss=12.33972
adv_loss= 2.81558
adv_loss= 6.81222
adv_loss= 2.72676
adv_loss=10.49458
adv_loss= 3.39177
adv_loss= 3.49637
adv_loss= 7.09617
adv_loss= 4.22631
adv_loss= 3.38293
adv_loss= 3.55051
surrogate=-0.01507, entropy= 2.21624, loss=-0.01507
surrogate=-0.00524, entropy= 2.21413, loss=-0.00524
surrogate=-0.00421, entropy= 2.21218, loss=-0.00421
surrogate= 0.00852, entropy= 2.20917, loss= 0.00852
surrogate= 0.00667, entropy= 2.21054, loss= 0.00667
surrogate=-0.03842, entropy= 2.20859, loss=-0.03842
surrogate=-0.03506, entropy= 2.20856, loss=-0.03506
surrogate=-0.01436, entropy= 2.20913, loss=-0.01436
surrogate=-0.03136, entropy= 2.20786, loss=-0.03136
surrogate=-0.00880, entropy= 2.20536, loss=-0.00880
std_min= 0.45402, std_max= 0.55359, std_mean= 0.50634
val lr: [0.00019415983606557377], policy lr: [0.0002329918032786885]
Policy Loss: -0.0087954, | Entropy Bonus: -0, | Value Loss: 12.34, | Advantage Loss: 3.5505
Time elapsed (s): 1.6957364082336426
Agent stdevs: 0.5063379
--------------------------------------------------------------------------------

Step 218
++++++++ Policy training ++++++++++
Current mean reward: 1253.873826 | mean episode length: 355.200000
val_loss=32.19567
val_loss=21.87923
val_loss=20.51757
val_loss=15.69023
val_loss=18.86479
val_loss=24.46069
val_loss=19.91057
val_loss=10.10700
val_loss=16.97452
val_loss=16.91837
adv_loss= 1.41745
adv_loss= 2.99596
adv_loss= 2.73058
adv_loss= 4.69060
adv_loss= 3.18170
adv_loss= 4.35288
adv_loss= 1.92286
adv_loss= 2.73972
adv_loss= 2.07294
adv_loss= 1.87545
surrogate= 0.02264, entropy= 2.20184, loss= 0.02264
surrogate=-0.00493, entropy= 2.19909, loss=-0.00493
surrogate=-0.03183, entropy= 2.19769, loss=-0.03183
surrogate=-0.01328, entropy= 2.19691, loss=-0.01328
surrogate=-0.02341, entropy= 2.19625, loss=-0.02341
surrogate=-0.00950, entropy= 2.19455, loss=-0.00950
surrogate=-0.01727, entropy= 2.19368, loss=-0.01727
surrogate=-0.00044, entropy= 2.19340, loss=-0.00044
surrogate= 0.00164, entropy= 2.19400, loss= 0.00164
surrogate=-0.03110, entropy= 2.19392, loss=-0.03110
std_min= 0.45127, std_max= 0.55311, std_mean= 0.50453
val lr: [0.00019390368852459017], policy lr: [0.0002326844262295082]
Policy Loss: -0.031097, | Entropy Bonus: -0, | Value Loss: 16.918, | Advantage Loss: 1.8754
Time elapsed (s): 1.684211254119873
Agent stdevs: 0.5045266
--------------------------------------------------------------------------------

Step 219
++++++++ Policy training ++++++++++
Current mean reward: 1193.078616 | mean episode length: 355.000000
val_loss=25.96309
val_loss=21.57290
val_loss=27.69329
val_loss=47.26108
val_loss=40.49337
val_loss=45.60239
val_loss=35.82972
val_loss=34.40897
val_loss=32.14754
val_loss=37.40538
adv_loss= 1.88340
adv_loss=19.21745
adv_loss=24.39411
adv_loss= 3.69970
adv_loss= 5.13193
adv_loss= 2.50581
adv_loss= 2.62207
adv_loss= 5.41088
adv_loss= 3.93559
adv_loss= 4.93328
surrogate= 0.00442, entropy= 2.19332, loss= 0.00442
surrogate= 0.01850, entropy= 2.18976, loss= 0.01850
surrogate=-0.00469, entropy= 2.18823, loss=-0.00469
surrogate=-0.00779, entropy= 2.18532, loss=-0.00779
surrogate=-0.03501, entropy= 2.18420, loss=-0.03501
surrogate=-0.00253, entropy= 2.18015, loss=-0.00253
surrogate=-0.02215, entropy= 2.17772, loss=-0.02215
surrogate=-0.00140, entropy= 2.17754, loss=-0.00140
surrogate=-0.02892, entropy= 2.17567, loss=-0.02892
surrogate= 0.00016, entropy= 2.17378, loss= 0.00016
std_min= 0.44813, std_max= 0.54912, std_mean= 0.50110
val lr: [0.00019364754098360657], policy lr: [0.00023237704918032786]
Policy Loss: 0.00015733, | Entropy Bonus: -0, | Value Loss: 37.405, | Advantage Loss: 4.9333
Time elapsed (s): 1.7052772045135498
Agent stdevs: 0.501099
--------------------------------------------------------------------------------

Step 220
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1401
++++++++ Policy training ++++++++++
Current mean reward: 1508.920126 | mean episode length: 453.750000
val_loss=389.79535
val_loss=60.43725
val_loss=60.53168
val_loss=66.40048
val_loss=94.74577
val_loss=51.89135
val_loss=50.00453
val_loss=42.65008
val_loss=393.66125
val_loss=217.60069
adv_loss= 1.57495
adv_loss= 3.41363
adv_loss= 1.88844
adv_loss= 3.01015
adv_loss= 2.06098
adv_loss= 2.33249
adv_loss= 2.28628
adv_loss= 4.40091
adv_loss= 7.35370
adv_loss= 3.00879
surrogate=-0.00533, entropy= 2.17080, loss=-0.00533
surrogate= 0.02116, entropy= 2.16861, loss= 0.02116
surrogate= 0.02506, entropy= 2.16673, loss= 0.02506
surrogate= 0.01694, entropy= 2.16304, loss= 0.01694
surrogate=-0.00756, entropy= 2.16045, loss=-0.00756
surrogate=-0.01475, entropy= 2.15861, loss=-0.01475
surrogate=-0.01351, entropy= 2.15526, loss=-0.01351
surrogate=-0.02321, entropy= 2.15332, loss=-0.02321
surrogate=-0.02813, entropy= 2.15178, loss=-0.02813
surrogate=-0.03438, entropy= 2.14959, loss=-0.03438
std_min= 0.44575, std_max= 0.55056, std_mean= 0.49723
val lr: [0.00019339139344262297], policy lr: [0.00023206967213114754]
Policy Loss: -0.034376, | Entropy Bonus: -0, | Value Loss: 217.6, | Advantage Loss: 3.0088
Time elapsed (s): 1.6900484561920166
Agent stdevs: 0.49722815
--------------------------------------------------------------------------------

Step 221
++++++++ Policy training ++++++++++
Current mean reward: 1347.325930 | mean episode length: 392.200000
val_loss=21.99693
val_loss=30.58159
val_loss=27.01727
val_loss=21.48103
val_loss=31.04934
val_loss=33.44071
val_loss=48.23003
val_loss=24.32124
val_loss=25.41156
val_loss=24.12825
adv_loss= 1.63869
adv_loss= 2.30720
adv_loss=15.38310
adv_loss= 1.25443
adv_loss= 1.33983
adv_loss= 2.04391
adv_loss= 2.53426
adv_loss= 1.81241
adv_loss= 1.45222
adv_loss= 1.44678
surrogate= 0.02079, entropy= 2.14844, loss= 0.02079
surrogate=-0.00415, entropy= 2.14236, loss=-0.00415
surrogate=-0.01691, entropy= 2.14077, loss=-0.01691
surrogate=-0.00759, entropy= 2.13796, loss=-0.00759
surrogate= 0.01828, entropy= 2.13501, loss= 0.01828
surrogate=-0.00656, entropy= 2.13187, loss=-0.00656
surrogate= 0.02571, entropy= 2.13059, loss= 0.02571
surrogate=-0.00732, entropy= 2.12882, loss=-0.00732
surrogate=-0.01013, entropy= 2.12673, loss=-0.01013
surrogate= 0.00162, entropy= 2.12393, loss= 0.00162
std_min= 0.44637, std_max= 0.54287, std_mean= 0.49273
val lr: [0.00019313524590163934], policy lr: [0.00023176229508196718]
Policy Loss: 0.0016222, | Entropy Bonus: -0, | Value Loss: 24.128, | Advantage Loss: 1.4468
Time elapsed (s): 1.7136642932891846
Agent stdevs: 0.4927318
--------------------------------------------------------------------------------

Step 222
++++++++ Policy training ++++++++++
Current mean reward: 2263.467648 | mean episode length: 664.666667
val_loss=644.16681
val_loss=242.07733
val_loss=62.82765
val_loss=147.19411
val_loss=81.62969
val_loss=631.79205
val_loss=106.38362
val_loss=101.82085
val_loss=124.03326
val_loss=33.94866
adv_loss= 4.90045
adv_loss= 8.32188
adv_loss= 4.78106
adv_loss= 4.68393
adv_loss= 3.64213
adv_loss= 2.53156
adv_loss= 5.55076
adv_loss= 5.34035
adv_loss= 1.88047
adv_loss= 4.14596
surrogate=-0.02161, entropy= 2.12398, loss=-0.02161
surrogate=-0.01808, entropy= 2.12519, loss=-0.01808
surrogate= 0.01165, entropy= 2.12732, loss= 0.01165
surrogate= 0.01673, entropy= 2.12899, loss= 0.01673
surrogate= 0.00902, entropy= 2.12956, loss= 0.00902
surrogate=-0.01470, entropy= 2.13165, loss=-0.01470
surrogate= 0.01032, entropy= 2.13340, loss= 0.01032
surrogate=-0.02653, entropy= 2.13486, loss=-0.02653
surrogate=-0.04811, entropy= 2.13476, loss=-0.04811
surrogate=-0.00745, entropy= 2.13403, loss=-0.00745
std_min= 0.44578, std_max= 0.54806, std_mean= 0.49458
val lr: [0.00019287909836065577], policy lr: [0.00023145491803278688]
Policy Loss: -0.0074506, | Entropy Bonus: -0, | Value Loss: 33.949, | Advantage Loss: 4.146
Time elapsed (s): 1.7369747161865234
Agent stdevs: 0.4945785
--------------------------------------------------------------------------------

Step 223
++++++++ Policy training ++++++++++
Current mean reward: 1131.160385 | mean episode length: 322.666667
val_loss=64.83785
val_loss=47.99148
val_loss=75.45369
val_loss=50.19966
val_loss=37.39999
val_loss=47.49442
val_loss=48.58638
val_loss=37.63329
val_loss=38.56779
val_loss=34.43817
adv_loss= 3.53458
adv_loss= 3.56375
adv_loss= 4.91994
adv_loss=12.64501
adv_loss= 5.69057
adv_loss= 6.61815
adv_loss= 1.73355
adv_loss= 5.29976
adv_loss=10.76264
adv_loss= 3.00103
surrogate= 0.00573, entropy= 2.13381, loss= 0.00573
surrogate= 0.01408, entropy= 2.13408, loss= 0.01408
surrogate= 0.02119, entropy= 2.13193, loss= 0.02119
surrogate= 0.02708, entropy= 2.13094, loss= 0.02708
surrogate= 0.00124, entropy= 2.13065, loss= 0.00124
surrogate=-0.02196, entropy= 2.12949, loss=-0.02196
surrogate=-0.01472, entropy= 2.13034, loss=-0.01472
surrogate=-0.02432, entropy= 2.13080, loss=-0.02432
surrogate=-0.03329, entropy= 2.13006, loss=-0.03329
surrogate=-0.00404, entropy= 2.12927, loss=-0.00404
std_min= 0.44346, std_max= 0.55254, std_mean= 0.49406
val lr: [0.00019262295081967211], policy lr: [0.00023114754098360653]
Policy Loss: -0.0040394, | Entropy Bonus: -0, | Value Loss: 34.438, | Advantage Loss: 3.001
Time elapsed (s): 1.6932644844055176
Agent stdevs: 0.49405983
--------------------------------------------------------------------------------

Step 224
++++++++ Policy training ++++++++++
Current mean reward: 1703.292497 | mean episode length: 496.750000
val_loss=425.08618
val_loss=87.63978
val_loss=51.47752
val_loss=93.13181
val_loss=66.13065
val_loss=233.61333
val_loss=43.03762
val_loss=74.58630
val_loss=19.72454
val_loss=34.40654
adv_loss= 8.12118
adv_loss= 2.35955
adv_loss= 5.22556
adv_loss=20.16978
adv_loss= 5.41224
adv_loss=24.40563
adv_loss= 3.18406
adv_loss= 4.50745
adv_loss= 5.36850
adv_loss= 4.19135
surrogate=-0.01370, entropy= 2.12923, loss=-0.01370
surrogate=-0.01595, entropy= 2.12839, loss=-0.01595
surrogate=-0.00178, entropy= 2.13003, loss=-0.00178
surrogate=-0.01417, entropy= 2.12998, loss=-0.01417
surrogate=-0.01853, entropy= 2.13169, loss=-0.01853
surrogate=-0.01898, entropy= 2.13242, loss=-0.01898
surrogate=-0.03093, entropy= 2.13382, loss=-0.03093
surrogate=-0.02324, entropy= 2.13358, loss=-0.02324
surrogate=-0.03002, entropy= 2.13368, loss=-0.03002
surrogate=-0.04798, entropy= 2.13316, loss=-0.04798
std_min= 0.44181, std_max= 0.55596, std_mean= 0.49490
val lr: [0.00019236680327868851], policy lr: [0.0002308401639344262]
Policy Loss: -0.04798, | Entropy Bonus: -0, | Value Loss: 34.407, | Advantage Loss: 4.1914
Time elapsed (s): 1.6856977939605713
Agent stdevs: 0.4948987
--------------------------------------------------------------------------------

Step 225
++++++++ Policy training ++++++++++
Current mean reward: 1906.288296 | mean episode length: 562.666667
val_loss=48.74331
val_loss=402.60651
val_loss=79.42246
val_loss=92.07214
val_loss=399.19049
val_loss=274.87628
val_loss=40.87818
val_loss=954.12622
val_loss=444.92136
val_loss=100.70897
adv_loss= 4.26498
adv_loss= 1.81878
adv_loss= 2.41040
adv_loss= 3.15326
adv_loss=1572.54333
adv_loss= 2.50100
adv_loss= 2.30772
adv_loss= 2.99361
adv_loss= 1.99786
adv_loss= 1.44964
surrogate= 0.00442, entropy= 2.13654, loss= 0.00442
surrogate= 0.01157, entropy= 2.13823, loss= 0.01157
surrogate=-0.02321, entropy= 2.13675, loss=-0.02321
surrogate=-0.02691, entropy= 2.13996, loss=-0.02691
surrogate= 0.00213, entropy= 2.14091, loss= 0.00213
surrogate=-0.00705, entropy= 2.14006, loss=-0.00705
surrogate=-0.02919, entropy= 2.13912, loss=-0.02919
surrogate=-0.01866, entropy= 2.14046, loss=-0.01866
surrogate=-0.00841, entropy= 2.14062, loss=-0.00841
surrogate=-0.00338, entropy= 2.13912, loss=-0.00338
std_min= 0.44400, std_max= 0.55892, std_mean= 0.49589
val lr: [0.00019211065573770491], policy lr: [0.00023053278688524588]
Policy Loss: -0.0033753, | Entropy Bonus: -0, | Value Loss: 100.71, | Advantage Loss: 1.4496
Time elapsed (s): 1.6853723526000977
Agent stdevs: 0.49588773
--------------------------------------------------------------------------------

Step 226
++++++++ Policy training ++++++++++
Current mean reward: 1072.975499 | mean episode length: 309.500000
val_loss=78.26584
val_loss=32.82654
val_loss=36.98485
val_loss=69.80167
val_loss=64.43768
val_loss=53.77853
val_loss=26.79492
val_loss=39.59233
val_loss=14.66398
val_loss=32.37090
adv_loss= 5.54380
adv_loss= 7.69693
adv_loss= 4.90712
adv_loss= 2.79795
adv_loss= 2.87081
adv_loss= 3.91764
adv_loss= 3.20322
adv_loss= 7.81270
adv_loss= 3.47657
adv_loss= 2.73100
surrogate=-0.01131, entropy= 2.13769, loss=-0.01131
surrogate=-0.02230, entropy= 2.13801, loss=-0.02230
surrogate= 0.00006, entropy= 2.13663, loss= 0.00006
surrogate=-0.00827, entropy= 2.13470, loss=-0.00827
surrogate=-0.01294, entropy= 2.13389, loss=-0.01294
surrogate=-0.01967, entropy= 2.13309, loss=-0.01967
surrogate= 0.00776, entropy= 2.13279, loss= 0.00776
surrogate=-0.03787, entropy= 2.13049, loss=-0.03787
surrogate=-0.03488, entropy= 2.12794, loss=-0.03488
surrogate=-0.03013, entropy= 2.12719, loss=-0.03013
std_min= 0.44142, std_max= 0.55641, std_mean= 0.49396
val lr: [0.00019185450819672131], policy lr: [0.00023022540983606556]
Policy Loss: -0.030126, | Entropy Bonus: -0, | Value Loss: 32.371, | Advantage Loss: 2.731
Time elapsed (s): 1.662485122680664
Agent stdevs: 0.49395582
--------------------------------------------------------------------------------

Step 227
++++++++ Policy training ++++++++++
Current mean reward: 1704.711717 | mean episode length: 506.000000
val_loss=34.46582
val_loss=15.86177
val_loss=20.69558
val_loss=14.96291
val_loss=19.96344
val_loss=13.61469
val_loss=35.97407
val_loss=12.46436
val_loss=15.13877
val_loss=16.66444
adv_loss= 8.58085
adv_loss= 1.46799
adv_loss= 3.10830
adv_loss= 1.51639
adv_loss= 2.39639
adv_loss= 2.31677
adv_loss= 2.11030
adv_loss=10.18684
adv_loss= 9.24849
adv_loss= 1.47785
surrogate=-0.01868, entropy= 2.12966, loss=-0.01868
surrogate= 0.01693, entropy= 2.13244, loss= 0.01693
surrogate= 0.01049, entropy= 2.13070, loss= 0.01049
surrogate=-0.01249, entropy= 2.13195, loss=-0.01249
surrogate= 0.04558, entropy= 2.13203, loss= 0.04558
surrogate=-0.02026, entropy= 2.13263, loss=-0.02026
surrogate=-0.02605, entropy= 2.13470, loss=-0.02605
surrogate=-0.04355, entropy= 2.13490, loss=-0.04355
surrogate=-0.01123, entropy= 2.13730, loss=-0.01123
surrogate=-0.02721, entropy= 2.13707, loss=-0.02721
std_min= 0.44217, std_max= 0.55298, std_mean= 0.49538
val lr: [0.00019159836065573771], policy lr: [0.00022991803278688523]
Policy Loss: -0.027214, | Entropy Bonus: -0, | Value Loss: 16.664, | Advantage Loss: 1.4779
Time elapsed (s): 1.6957602500915527
Agent stdevs: 0.49538445
--------------------------------------------------------------------------------

Step 228
++++++++ Policy training ++++++++++
Current mean reward: 1307.686781 | mean episode length: 388.400000
val_loss=1324.04407
val_loss=84.98383
val_loss=57.65913
val_loss=77.98151
val_loss=180.57784
val_loss=355.89648
val_loss=48.63602
val_loss=175.69957
val_loss=198.97549
val_loss=1005.35992
adv_loss=10.39757
adv_loss= 8.33971
adv_loss= 5.99640
adv_loss= 5.27009
adv_loss= 5.53694
adv_loss= 6.68275
adv_loss= 2.86336
adv_loss= 2.64034
adv_loss= 8.76015
adv_loss= 2.82711
surrogate= 0.02824, entropy= 2.13475, loss= 0.02824
surrogate= 0.00370, entropy= 2.13299, loss= 0.00370
surrogate=-0.01159, entropy= 2.13088, loss=-0.01159
surrogate=-0.00297, entropy= 2.12862, loss=-0.00297
surrogate=-0.02692, entropy= 2.12807, loss=-0.02692
surrogate=-0.00880, entropy= 2.12983, loss=-0.00880
surrogate= 0.00064, entropy= 2.12863, loss= 0.00064
surrogate= 0.00459, entropy= 2.12793, loss= 0.00459
surrogate=-0.00460, entropy= 2.12730, loss=-0.00460
surrogate=-0.00572, entropy= 2.12709, loss=-0.00572
std_min= 0.43853, std_max= 0.55077, std_mean= 0.49383
val lr: [0.00019134221311475411], policy lr: [0.0002296106557377049]
Policy Loss: -0.0057188, | Entropy Bonus: -0, | Value Loss: 1005.4, | Advantage Loss: 2.8271
Time elapsed (s): 1.7223334312438965
Agent stdevs: 0.49382845
--------------------------------------------------------------------------------

Step 229
++++++++ Policy training ++++++++++
Current mean reward: 1531.089248 | mean episode length: 456.750000
val_loss=76.31895
val_loss=135.57471
val_loss=217.34753
val_loss=112.72649
val_loss=142.33275
val_loss=53.42551
val_loss=152.20206
val_loss=156.47868
val_loss=55.85892
val_loss=48.70528
adv_loss= 4.40430
adv_loss= 4.94961
adv_loss= 9.24641
adv_loss= 8.64896
adv_loss= 2.92942
adv_loss= 5.99752
adv_loss= 2.63463
adv_loss=15.20720
adv_loss=14.40642
adv_loss= 3.51223
surrogate=-0.02067, entropy= 2.12631, loss=-0.02067
surrogate=-0.00651, entropy= 2.12349, loss=-0.00651
surrogate= 0.00840, entropy= 2.12151, loss= 0.00840
surrogate= 0.00044, entropy= 2.11928, loss= 0.00044
surrogate=-0.00513, entropy= 2.11748, loss=-0.00513
surrogate=-0.00516, entropy= 2.11650, loss=-0.00516
surrogate=-0.03733, entropy= 2.11292, loss=-0.03733
surrogate= 0.01567, entropy= 2.11158, loss= 0.01567
surrogate=-0.00337, entropy= 2.11082, loss=-0.00337
surrogate=-0.01907, entropy= 2.10944, loss=-0.01907
std_min= 0.43194, std_max= 0.54865, std_mean= 0.49115
val lr: [0.00019108606557377051], policy lr: [0.00022930327868852458]
Policy Loss: -0.019073, | Entropy Bonus: -0, | Value Loss: 48.705, | Advantage Loss: 3.5122
Time elapsed (s): 1.6808745861053467
Agent stdevs: 0.4911468
--------------------------------------------------------------------------------

Step 230
++++++++ Policy training ++++++++++
Current mean reward: 1122.758418 | mean episode length: 326.500000
val_loss=36.23125
val_loss=50.89888
val_loss=37.22566
val_loss=51.94704
val_loss=26.53597
val_loss=49.00515
val_loss=45.65149
val_loss=25.65534
val_loss=36.94304
val_loss=15.85783
adv_loss= 6.82910
adv_loss= 5.78139
adv_loss= 4.93176
adv_loss= 6.50177
adv_loss= 2.46818
adv_loss=17.08397
adv_loss= 4.72813
adv_loss= 8.33421
adv_loss= 4.32682
adv_loss= 3.95875
surrogate=-0.00602, entropy= 2.11037, loss=-0.00602
surrogate=-0.00355, entropy= 2.11206, loss=-0.00355
surrogate= 0.00215, entropy= 2.11210, loss= 0.00215
surrogate=-0.02599, entropy= 2.11356, loss=-0.02599
surrogate=-0.01035, entropy= 2.11118, loss=-0.01035
surrogate= 0.00092, entropy= 2.11318, loss= 0.00092
surrogate=-0.00333, entropy= 2.11314, loss=-0.00333
surrogate=-0.02600, entropy= 2.11334, loss=-0.02600
surrogate=-0.01163, entropy= 2.11215, loss=-0.01163
surrogate= 0.06227, entropy= 2.11152, loss= 0.06227
std_min= 0.42952, std_max= 0.54807, std_mean= 0.49159
val lr: [0.0001908299180327869], policy lr: [0.00022899590163934423]
Policy Loss: 0.062274, | Entropy Bonus: -0, | Value Loss: 15.858, | Advantage Loss: 3.9587
Time elapsed (s): 1.6863811016082764
Agent stdevs: 0.49158645
--------------------------------------------------------------------------------

Step 231
++++++++ Policy training ++++++++++
Current mean reward: 1038.459413 | mean episode length: 294.666667
val_loss=41.30582
val_loss=26.45498
val_loss=39.82244
val_loss=37.44714
val_loss=21.26843
val_loss=25.22380
val_loss=18.32640
val_loss=39.86636
val_loss=22.87656
val_loss=22.08458
adv_loss= 2.06261
adv_loss= 1.45690
adv_loss= 4.62340
adv_loss= 1.33104
adv_loss= 3.95964
adv_loss= 2.50372
adv_loss= 2.68558
adv_loss= 2.10510
adv_loss= 2.31818
adv_loss=10.81305
surrogate= 0.02582, entropy= 2.11866, loss= 0.02582
surrogate=-0.02636, entropy= 2.11859, loss=-0.02636
surrogate=-0.04052, entropy= 2.12023, loss=-0.04052
surrogate=-0.01005, entropy= 2.12107, loss=-0.01005
surrogate=-0.00080, entropy= 2.12148, loss=-0.00080
surrogate=-0.05281, entropy= 2.12468, loss=-0.05281
surrogate=-0.01783, entropy= 2.12445, loss=-0.01783
surrogate=-0.02061, entropy= 2.12604, loss=-0.02061
surrogate=-0.02849, entropy= 2.12793, loss=-0.02849
surrogate=-0.00805, entropy= 2.12799, loss=-0.00805
std_min= 0.43105, std_max= 0.55491, std_mean= 0.49448
val lr: [0.0001905737704918033], policy lr: [0.00022868852459016393]
Policy Loss: -0.0080465, | Entropy Bonus: -0, | Value Loss: 22.085, | Advantage Loss: 10.813
Time elapsed (s): 1.6936991214752197
Agent stdevs: 0.49447605
--------------------------------------------------------------------------------

Step 232
++++++++ Policy training ++++++++++
Current mean reward: 998.589631 | mean episode length: 286.250000
val_loss=46.11568
val_loss=30.42817
val_loss=29.12560
val_loss=44.78563
val_loss=18.75772
val_loss=32.80607
val_loss=36.61449
val_loss=15.48540
val_loss=25.20352
val_loss=18.24438
adv_loss= 1.50330
adv_loss= 3.69700
adv_loss= 6.81165
adv_loss= 2.49214
adv_loss= 2.18355
adv_loss= 2.22255
adv_loss= 3.77785
adv_loss= 4.46354
adv_loss= 4.17947
adv_loss= 2.14654
surrogate= 0.01916, entropy= 2.13153, loss= 0.01916
surrogate= 0.01693, entropy= 2.13347, loss= 0.01693
surrogate= 0.00547, entropy= 2.13356, loss= 0.00547
surrogate=-0.02453, entropy= 2.13406, loss=-0.02453
surrogate=-0.00957, entropy= 2.13651, loss=-0.00957
surrogate=-0.01466, entropy= 2.13788, loss=-0.01466
surrogate=-0.03895, entropy= 2.13963, loss=-0.03895
surrogate=-0.03079, entropy= 2.13889, loss=-0.03079
surrogate= 0.01608, entropy= 2.14207, loss= 0.01608
surrogate=-0.03504, entropy= 2.14309, loss=-0.03504
std_min= 0.43568, std_max= 0.55929, std_mean= 0.49689
val lr: [0.00019031762295081966], policy lr: [0.00022838114754098358]
Policy Loss: -0.035036, | Entropy Bonus: -0, | Value Loss: 18.244, | Advantage Loss: 2.1465
Time elapsed (s): 1.685025930404663
Agent stdevs: 0.49689385
--------------------------------------------------------------------------------

Step 233
++++++++ Policy training ++++++++++
Current mean reward: 987.272877 | mean episode length: 289.833333
val_loss=93.63144
val_loss=100.86371
val_loss=45.99154
val_loss=22.54066
val_loss=22.12147
val_loss=17.54989
val_loss=73.51599
val_loss=21.63895
val_loss=22.35783
val_loss=53.14154
adv_loss= 3.30809
adv_loss= 3.77213
adv_loss=13.49218
adv_loss= 6.10766
adv_loss= 5.67678
adv_loss= 5.30619
adv_loss= 3.29257
adv_loss= 7.16449
adv_loss= 4.04331
adv_loss= 6.64415
surrogate=-0.01681, entropy= 2.14317, loss=-0.01681
surrogate=-0.00071, entropy= 2.14489, loss=-0.00071
surrogate= 0.00540, entropy= 2.14659, loss= 0.00540
surrogate= 0.00005, entropy= 2.14802, loss= 0.00005
surrogate=-0.01239, entropy= 2.14903, loss=-0.01239
surrogate=-0.04098, entropy= 2.14959, loss=-0.04098
surrogate=-0.02687, entropy= 2.14911, loss=-0.02687
surrogate= 0.00163, entropy= 2.15181, loss= 0.00163
surrogate=-0.04818, entropy= 2.15306, loss=-0.04818
surrogate=-0.00989, entropy= 2.15367, loss=-0.00989
std_min= 0.43073, std_max= 0.56505, std_mean= 0.49910
val lr: [0.00019006147540983606], policy lr: [0.00022807377049180325]
Policy Loss: -0.009894, | Entropy Bonus: -0, | Value Loss: 53.142, | Advantage Loss: 6.6442
Time elapsed (s): 1.6994938850402832
Agent stdevs: 0.49910334
--------------------------------------------------------------------------------

Step 234
++++++++ Policy training ++++++++++
Current mean reward: 1344.719701 | mean episode length: 388.800000
val_loss=22.99710
val_loss=25.58079
val_loss=14.51750
val_loss=11.36744
val_loss=11.74315
val_loss=25.88782
val_loss=23.50744
val_loss=16.60350
val_loss=12.57825
val_loss=10.24944
adv_loss= 2.52245
adv_loss= 3.79903
adv_loss= 2.41280
adv_loss= 2.98747
adv_loss= 5.28898
adv_loss= 3.95159
adv_loss= 4.90520
adv_loss= 4.05421
adv_loss= 2.81129
adv_loss= 1.85784
surrogate=-0.00741, entropy= 2.15541, loss=-0.00741
surrogate= 0.00588, entropy= 2.15726, loss= 0.00588
surrogate= 0.01057, entropy= 2.15951, loss= 0.01057
surrogate=-0.03656, entropy= 2.16087, loss=-0.03656
surrogate= 0.00929, entropy= 2.16098, loss= 0.00929
surrogate=-0.01902, entropy= 2.16354, loss=-0.01902
surrogate=-0.04228, entropy= 2.16289, loss=-0.04228
surrogate=-0.02217, entropy= 2.16458, loss=-0.02217
surrogate=-0.03344, entropy= 2.16543, loss=-0.03344
surrogate=-0.03505, entropy= 2.16646, loss=-0.03505
std_min= 0.43462, std_max= 0.56300, std_mean= 0.50098
val lr: [0.00018980532786885246], policy lr: [0.00022776639344262292]
Policy Loss: -0.03505, | Entropy Bonus: -0, | Value Loss: 10.249, | Advantage Loss: 1.8578
Time elapsed (s): 1.7064132690429688
Agent stdevs: 0.5009789
--------------------------------------------------------------------------------

Step 235
++++++++ Policy training ++++++++++
Current mean reward: 1210.110478 | mean episode length: 351.000000
val_loss=19.56848
val_loss=16.07624
val_loss=20.50802
val_loss=14.81717
val_loss=13.84625
val_loss=18.75012
val_loss=22.40234
val_loss=11.59393
val_loss= 8.59675
val_loss=11.62042
adv_loss= 2.68165
adv_loss= 5.95753
adv_loss= 2.53190
adv_loss= 1.83350
adv_loss= 3.47740
adv_loss= 2.47892
adv_loss= 3.15442
adv_loss= 3.56251
adv_loss= 5.70841
adv_loss= 4.87284
surrogate= 0.00964, entropy= 2.16538, loss= 0.00964
surrogate= 0.00267, entropy= 2.16412, loss= 0.00267
surrogate=-0.00110, entropy= 2.16383, loss=-0.00110
surrogate=-0.01963, entropy= 2.16400, loss=-0.01963
surrogate= 0.02871, entropy= 2.16403, loss= 0.02871
surrogate=-0.02129, entropy= 2.16070, loss=-0.02129
surrogate=-0.01483, entropy= 2.15964, loss=-0.01483
surrogate= 0.00338, entropy= 2.15983, loss= 0.00338
surrogate= 0.02460, entropy= 2.15814, loss= 0.02460
surrogate=-0.02710, entropy= 2.15869, loss=-0.02710
std_min= 0.43557, std_max= 0.56336, std_mean= 0.49962
val lr: [0.00018954918032786886], policy lr: [0.0002274590163934426]
Policy Loss: -0.027098, | Entropy Bonus: -0, | Value Loss: 11.62, | Advantage Loss: 4.8728
Time elapsed (s): 1.6821627616882324
Agent stdevs: 0.49961874
--------------------------------------------------------------------------------

Step 236
++++++++ Policy training ++++++++++
Current mean reward: 1724.746872 | mean episode length: 504.500000
val_loss=991.73108
val_loss=336.04782
val_loss=140.30687
val_loss=632.43994
val_loss=251.46559
val_loss=35.84855
val_loss=1074.12097
val_loss=322.88052
val_loss=95.03602
val_loss=30.53418
adv_loss= 1.68537
adv_loss=11.27300
adv_loss= 1.44920
adv_loss= 2.81726
adv_loss= 3.37893
adv_loss=10.84213
adv_loss= 2.71332
adv_loss= 5.70069
adv_loss= 1.54484
adv_loss= 1.67163
surrogate=-0.00855, entropy= 2.15623, loss=-0.00855
surrogate=-0.00732, entropy= 2.15597, loss=-0.00732
surrogate=-0.00577, entropy= 2.15454, loss=-0.00577
surrogate=-0.01532, entropy= 2.15486, loss=-0.01532
surrogate=-0.00024, entropy= 2.15344, loss=-0.00024
surrogate=-0.00041, entropy= 2.15065, loss=-0.00041
surrogate=-0.01819, entropy= 2.15273, loss=-0.01819
surrogate= 0.01312, entropy= 2.15409, loss= 0.01312
surrogate=-0.01425, entropy= 2.15127, loss=-0.01425
surrogate=-0.03122, entropy= 2.15111, loss=-0.03122
std_min= 0.43661, std_max= 0.56513, std_mean= 0.49843
val lr: [0.00018929303278688526], policy lr: [0.00022715163934426227]
Policy Loss: -0.031225, | Entropy Bonus: -0, | Value Loss: 30.534, | Advantage Loss: 1.6716
Time elapsed (s): 1.67411470413208
Agent stdevs: 0.49842787
--------------------------------------------------------------------------------

Step 237
++++++++ Policy training ++++++++++
Current mean reward: 1166.017623 | mean episode length: 336.000000
val_loss=28.85429
val_loss=28.46679
val_loss=24.35786
val_loss=24.33704
val_loss=21.42858
val_loss=25.96965
val_loss=16.23096
val_loss=17.32127
val_loss=17.47802
val_loss=11.92542
adv_loss= 3.47511
adv_loss= 5.45075
adv_loss= 4.09714
adv_loss= 5.61004
adv_loss= 2.75961
adv_loss= 2.77646
adv_loss= 1.43823
adv_loss= 2.00322
adv_loss= 3.36582
adv_loss= 2.63576
surrogate= 0.01251, entropy= 2.15240, loss= 0.01251
surrogate=-0.00183, entropy= 2.15133, loss=-0.00183
surrogate=-0.00201, entropy= 2.14949, loss=-0.00201
surrogate=-0.03219, entropy= 2.14969, loss=-0.03219
surrogate=-0.01555, entropy= 2.15002, loss=-0.01555
surrogate= 0.00225, entropy= 2.14802, loss= 0.00225
surrogate= 0.00070, entropy= 2.14509, loss= 0.00070
surrogate=-0.03443, entropy= 2.14399, loss=-0.03443
surrogate=-0.02557, entropy= 2.14286, loss=-0.02557
surrogate=-0.03279, entropy= 2.14381, loss=-0.03279
std_min= 0.43630, std_max= 0.56495, std_mean= 0.49721
val lr: [0.00018903688524590166], policy lr: [0.00022684426229508195]
Policy Loss: -0.032787, | Entropy Bonus: -0, | Value Loss: 11.925, | Advantage Loss: 2.6358
Time elapsed (s): 1.6806178092956543
Agent stdevs: 0.4972115
--------------------------------------------------------------------------------

Step 238
++++++++ Policy training ++++++++++
Current mean reward: 1300.514018 | mean episode length: 378.400000
val_loss=32.13434
val_loss=74.65500
val_loss=54.35167
val_loss=46.58149
val_loss=37.39988
val_loss=38.52006
val_loss=34.26331
val_loss=43.37571
val_loss=21.70878
val_loss=63.90339
adv_loss= 1.22926
adv_loss= 3.59332
adv_loss= 4.46916
adv_loss= 6.43019
adv_loss= 2.71338
adv_loss= 1.60426
adv_loss= 2.00578
adv_loss= 1.81884
adv_loss= 3.89171
adv_loss= 2.37906
surrogate= 0.00280, entropy= 2.14299, loss= 0.00280
surrogate=-0.02573, entropy= 2.14037, loss=-0.02573
surrogate= 0.02275, entropy= 2.13825, loss= 0.02275
surrogate=-0.01851, entropy= 2.13650, loss=-0.01851
surrogate= 0.00805, entropy= 2.13503, loss= 0.00805
surrogate=-0.02738, entropy= 2.13518, loss=-0.02738
surrogate=-0.01394, entropy= 2.13498, loss=-0.01394
surrogate=-0.00411, entropy= 2.13315, loss=-0.00411
surrogate=-0.00691, entropy= 2.13191, loss=-0.00691
surrogate=-0.01234, entropy= 2.13219, loss=-0.01234
std_min= 0.43097, std_max= 0.56458, std_mean= 0.49554
val lr: [0.00018878073770491806], policy lr: [0.00022653688524590162]
Policy Loss: -0.012337, | Entropy Bonus: -0, | Value Loss: 63.903, | Advantage Loss: 2.3791
Time elapsed (s): 1.6762909889221191
Agent stdevs: 0.49554428
--------------------------------------------------------------------------------

Step 239
++++++++ Policy training ++++++++++
Current mean reward: 1601.285135 | mean episode length: 492.250000
val_loss=33.93782
val_loss=55.02848
val_loss=72.85418
val_loss=213.00882
val_loss=979.31250
val_loss=201.83952
val_loss=49.55553
val_loss=51.19296
val_loss=1400.89819
val_loss=62.52637
adv_loss= 4.21055
adv_loss= 4.55429
adv_loss= 2.44559
adv_loss= 1.63927
adv_loss=10.13727
adv_loss= 5.27350
adv_loss= 3.76836
adv_loss= 1.90667
adv_loss= 2.06929
adv_loss= 5.38633
surrogate=-0.01402, entropy= 2.13348, loss=-0.01402
surrogate= 0.04809, entropy= 2.13353, loss= 0.04809
surrogate=-0.00961, entropy= 2.13126, loss=-0.00961
surrogate= 0.02571, entropy= 2.13180, loss= 0.02571
surrogate=-0.00679, entropy= 2.13094, loss=-0.00679
surrogate=-0.00208, entropy= 2.12911, loss=-0.00208
surrogate=-0.02983, entropy= 2.13035, loss=-0.02983
surrogate=-0.01146, entropy= 2.12928, loss=-0.01146
surrogate=-0.02106, entropy= 2.12849, loss=-0.02106
surrogate= 0.00057, entropy= 2.12981, loss= 0.00057
std_min= 0.43101, std_max= 0.55467, std_mean= 0.49476
val lr: [0.0001885245901639344], policy lr: [0.00022622950819672127]
Policy Loss: 0.00057312, | Entropy Bonus: -0, | Value Loss: 62.526, | Advantage Loss: 5.3863
Time elapsed (s): 1.70953369140625
Agent stdevs: 0.49476254
--------------------------------------------------------------------------------

Step 240
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1400.5
++++++++ Policy training ++++++++++
Current mean reward: 1681.273679 | mean episode length: 504.500000
val_loss=1259.20667
val_loss=42.89997
val_loss=1346.69836
val_loss=164.71523
val_loss=34.63358
val_loss=52.29740
val_loss=111.57343
val_loss=1022.07751
val_loss=83.50139
val_loss=903.63208
adv_loss= 5.15434
adv_loss= 5.00642
adv_loss=1454.68298
adv_loss= 7.75972
adv_loss= 4.01490
adv_loss= 5.01870
adv_loss= 5.61119
adv_loss= 3.39404
adv_loss= 6.11039
adv_loss= 5.73626
surrogate=-0.00552, entropy= 2.12989, loss=-0.00552
surrogate=-0.01641, entropy= 2.13143, loss=-0.01641
surrogate= 0.01165, entropy= 2.13025, loss= 0.01165
surrogate=-0.00954, entropy= 2.13164, loss=-0.00954
surrogate=-0.00681, entropy= 2.13224, loss=-0.00681
surrogate=-0.01601, entropy= 2.13105, loss=-0.01601
surrogate=-0.01695, entropy= 2.13031, loss=-0.01695
surrogate=-0.01311, entropy= 2.12966, loss=-0.01311
surrogate=-0.02234, entropy= 2.12936, loss=-0.02234
surrogate=-0.00373, entropy= 2.13001, loss=-0.00373
std_min= 0.43135, std_max= 0.55552, std_mean= 0.49480
val lr: [0.0001882684426229508], policy lr: [0.00022592213114754094]
Policy Loss: -0.0037308, | Entropy Bonus: -0, | Value Loss: 903.63, | Advantage Loss: 5.7363
Time elapsed (s): 1.705146312713623
Agent stdevs: 0.494797
--------------------------------------------------------------------------------

Step 241
++++++++ Policy training ++++++++++
Current mean reward: 1941.408758 | mean episode length: 599.333333
val_loss=500.44699
val_loss=32.61415
val_loss=88.80183
val_loss=50.75394
val_loss=98.85194
val_loss=41.53067
val_loss=785.79370
val_loss=1021.82404
val_loss=75.10718
val_loss=350.22614
adv_loss= 3.71508
adv_loss= 2.30149
adv_loss= 4.43154
adv_loss= 2.92226
adv_loss= 4.79127
adv_loss= 3.64204
adv_loss= 3.74159
adv_loss= 2.76327
adv_loss= 4.05877
adv_loss=1014.85742
surrogate= 0.00446, entropy= 2.12913, loss= 0.00446
surrogate=-0.02666, entropy= 2.12959, loss=-0.02666
surrogate=-0.02005, entropy= 2.12915, loss=-0.02005
surrogate=-0.01569, entropy= 2.12766, loss=-0.01569
surrogate=-0.01409, entropy= 2.12710, loss=-0.01409
surrogate=-0.00910, entropy= 2.12659, loss=-0.00910
surrogate=-0.03994, entropy= 2.12547, loss=-0.03994
surrogate=-0.04205, entropy= 2.12288, loss=-0.04205
surrogate=-0.03162, entropy= 2.12143, loss=-0.03162
surrogate= 0.00462, entropy= 2.12527, loss= 0.00462
std_min= 0.42674, std_max= 0.55830, std_mean= 0.49435
val lr: [0.0001880122950819672], policy lr: [0.00022561475409836062]
Policy Loss: 0.0046207, | Entropy Bonus: -0, | Value Loss: 350.23, | Advantage Loss: 1014.9
Time elapsed (s): 1.7010610103607178
Agent stdevs: 0.49434745
--------------------------------------------------------------------------------

Step 242
++++++++ Policy training ++++++++++
Current mean reward: 1182.481829 | mean episode length: 353.750000
val_loss=98.10177
val_loss=88.48496
val_loss=187.83896
val_loss=188.00258
val_loss=61.90607
val_loss=109.74825
val_loss=40.24158
val_loss=52.88765
val_loss=47.09503
val_loss=114.27727
adv_loss= 4.26092
adv_loss= 7.33626
adv_loss= 6.14779
adv_loss= 7.79028
adv_loss=21.27798
adv_loss=17.89827
adv_loss=14.51202
adv_loss= 6.49826
adv_loss= 7.21301
adv_loss= 4.73265
surrogate=-0.01550, entropy= 2.12684, loss=-0.01550
surrogate=-0.01421, entropy= 2.12958, loss=-0.01421
surrogate=-0.00378, entropy= 2.13318, loss=-0.00378
surrogate= 0.01052, entropy= 2.13524, loss= 0.01052
surrogate=-0.01742, entropy= 2.13651, loss=-0.01742
surrogate=-0.01230, entropy= 2.13888, loss=-0.01230
surrogate=-0.02194, entropy= 2.13953, loss=-0.02194
surrogate=-0.01155, entropy= 2.14157, loss=-0.01155
surrogate=-0.02265, entropy= 2.14134, loss=-0.02265
surrogate=-0.03608, entropy= 2.14479, loss=-0.03608
std_min= 0.43159, std_max= 0.56007, std_mean= 0.49742
val lr: [0.0001877561475409836], policy lr: [0.0002253073770491803]
Policy Loss: -0.036079, | Entropy Bonus: -0, | Value Loss: 114.28, | Advantage Loss: 4.7327
Time elapsed (s): 1.6865198612213135
Agent stdevs: 0.49742272
--------------------------------------------------------------------------------

Step 243
++++++++ Policy training ++++++++++
Current mean reward: 879.395639 | mean episode length: 255.000000
val_loss=74.18302
val_loss=49.25126
val_loss=78.39036
val_loss=36.49167
val_loss=116.53179
val_loss=68.39297
val_loss=31.77701
val_loss=43.68975
val_loss=38.85321
val_loss=47.77204
adv_loss= 6.81410
adv_loss= 6.11474
adv_loss= 6.39336
adv_loss=10.20184
adv_loss=18.25360
adv_loss=13.93602
adv_loss=14.87289
adv_loss= 8.74754
adv_loss= 7.74506
adv_loss= 4.52808
surrogate=-0.02081, entropy= 2.14482, loss=-0.02081
surrogate= 0.02405, entropy= 2.14298, loss= 0.02405
surrogate=-0.01058, entropy= 2.14131, loss=-0.01058
surrogate= 0.00059, entropy= 2.13847, loss= 0.00059
surrogate=-0.01880, entropy= 2.13673, loss=-0.01880
surrogate=-0.00327, entropy= 2.13246, loss=-0.00327
surrogate=-0.00437, entropy= 2.13098, loss=-0.00437
surrogate=-0.04689, entropy= 2.12879, loss=-0.04689
surrogate=-0.00444, entropy= 2.12729, loss=-0.00444
surrogate=-0.01478, entropy= 2.12555, loss=-0.01478
std_min= 0.43152, std_max= 0.55799, std_mean= 0.49415
val lr: [0.0001875], policy lr: [0.000225]
Policy Loss: -0.014783, | Entropy Bonus: -0, | Value Loss: 47.772, | Advantage Loss: 4.5281
Time elapsed (s): 1.6778631210327148
Agent stdevs: 0.49414966
--------------------------------------------------------------------------------

Step 244
++++++++ Policy training ++++++++++
Current mean reward: 1535.093293 | mean episode length: 485.666667
val_loss=211.58859
val_loss=429.16214
val_loss=60.53189
val_loss=460.57806
val_loss=61.82952
val_loss=133.20094
val_loss=89.96876
val_loss=57.07414
val_loss=608.16217
val_loss=74.55261
adv_loss= 5.34825
adv_loss= 2.84742
adv_loss= 3.43817
adv_loss= 3.91036
adv_loss= 3.11464
adv_loss= 2.05010
adv_loss= 3.20312
adv_loss= 3.90303
adv_loss= 2.79057
adv_loss= 4.01843
surrogate=-0.01294, entropy= 2.12696, loss=-0.01294
surrogate= 0.00691, entropy= 2.12561, loss= 0.00691
surrogate=-0.00026, entropy= 2.12552, loss=-0.00026
surrogate=-0.02545, entropy= 2.12620, loss=-0.02545
surrogate=-0.02967, entropy= 2.12792, loss=-0.02967
surrogate=-0.02709, entropy= 2.12807, loss=-0.02709
surrogate=-0.04114, entropy= 2.12964, loss=-0.04114
surrogate= 0.00895, entropy= 2.13072, loss= 0.00895
surrogate=-0.00076, entropy= 2.13080, loss=-0.00076
surrogate= 0.01997, entropy= 2.13089, loss= 0.01997
std_min= 0.43633, std_max= 0.55527, std_mean= 0.49469
val lr: [0.0001872438524590164], policy lr: [0.00022469262295081967]
Policy Loss: 0.019971, | Entropy Bonus: -0, | Value Loss: 74.553, | Advantage Loss: 4.0184
Time elapsed (s): 1.6955077648162842
Agent stdevs: 0.49468875
--------------------------------------------------------------------------------

Step 245
++++++++ Policy training ++++++++++
Current mean reward: 1198.830768 | mean episode length: 353.000000
val_loss=87.82819
val_loss=38.86788
val_loss=116.57614
val_loss=53.35772
val_loss=425.71017
val_loss=553.87677
val_loss=109.47648
val_loss=33.02240
val_loss=430.04532
val_loss=41.29622
adv_loss= 2.63267
adv_loss= 3.20673
adv_loss= 5.11287
adv_loss= 4.09893
adv_loss= 6.36184
adv_loss= 1.67728
adv_loss= 4.41531
adv_loss= 7.35520
adv_loss=907.23602
adv_loss=907.73480
surrogate= 0.02365, entropy= 2.13001, loss= 0.02365
surrogate=-0.02804, entropy= 2.13288, loss=-0.02804
surrogate=-0.02629, entropy= 2.13397, loss=-0.02629
surrogate=-0.01293, entropy= 2.13509, loss=-0.01293
surrogate=-0.01474, entropy= 2.13816, loss=-0.01474
surrogate= 0.03412, entropy= 2.13882, loss= 0.03412
surrogate=-0.03618, entropy= 2.14013, loss=-0.03618
surrogate=-0.02827, entropy= 2.14226, loss=-0.02827
surrogate=-0.02891, entropy= 2.14219, loss=-0.02891
surrogate=-0.00930, entropy= 2.14116, loss=-0.00930
std_min= 0.43830, std_max= 0.55458, std_mean= 0.49629
val lr: [0.0001869877049180328], policy lr: [0.00022438524590163934]
Policy Loss: -0.0093049, | Entropy Bonus: -0, | Value Loss: 41.296, | Advantage Loss: 907.73
Time elapsed (s): 1.728525161743164
Agent stdevs: 0.49629351
--------------------------------------------------------------------------------

Step 246
++++++++ Policy training ++++++++++
Current mean reward: 984.050769 | mean episode length: 288.285714
val_loss=71.47559
val_loss=92.83846
val_loss=79.77271
val_loss=43.24817
val_loss=44.68885
val_loss=54.42723
val_loss=29.25593
val_loss=30.16973
val_loss=28.88982
val_loss=55.50191
adv_loss= 7.68549
adv_loss= 3.93225
adv_loss= 8.36375
adv_loss= 8.38285
adv_loss= 2.79933
adv_loss= 7.51905
adv_loss= 9.78385
adv_loss=11.22686
adv_loss= 7.19897
adv_loss= 3.45326
surrogate=-0.00006, entropy= 2.14366, loss=-0.00006
surrogate= 0.01350, entropy= 2.14776, loss= 0.01350
surrogate= 0.00245, entropy= 2.15042, loss= 0.00245
surrogate=-0.01698, entropy= 2.15398, loss=-0.01698
surrogate=-0.01172, entropy= 2.15403, loss=-0.01172
surrogate=-0.01730, entropy= 2.15717, loss=-0.01730
surrogate=-0.01197, entropy= 2.15960, loss=-0.01197
surrogate=-0.02650, entropy= 2.16046, loss=-0.02650
surrogate=-0.02027, entropy= 2.16206, loss=-0.02027
surrogate=-0.00753, entropy= 2.16356, loss=-0.00753
std_min= 0.43861, std_max= 0.55718, std_mean= 0.50010
val lr: [0.00018673155737704918], policy lr: [0.00022407786885245896]
Policy Loss: -0.0075321, | Entropy Bonus: -0, | Value Loss: 55.502, | Advantage Loss: 3.4533
Time elapsed (s): 1.7194750308990479
Agent stdevs: 0.5000953
--------------------------------------------------------------------------------

Step 247
++++++++ Policy training ++++++++++
Current mean reward: 2097.096088 | mean episode length: 661.500000
val_loss=265.53516
val_loss=128.51492
val_loss=1162.45068
val_loss=45.28358
val_loss=71.42135
val_loss=1184.83337
val_loss=554.82825
val_loss=306.53622
val_loss=134.42226
val_loss=150.78696
adv_loss= 5.01815
adv_loss= 6.81654
adv_loss= 8.96969
adv_loss= 7.30995
adv_loss= 6.76953
adv_loss= 7.09591
adv_loss= 3.44894
adv_loss= 4.38135
adv_loss=11.67473
adv_loss= 4.06288
surrogate= 0.02251, entropy= 2.16151, loss= 0.02251
surrogate= 0.01467, entropy= 2.16168, loss= 0.01467
surrogate= 0.02057, entropy= 2.16614, loss= 0.02057
surrogate= 0.01125, entropy= 2.16450, loss= 0.01125
surrogate=-0.04280, entropy= 2.16339, loss=-0.04280
surrogate=-0.04279, entropy= 2.16375, loss=-0.04279
surrogate=-0.03746, entropy= 2.16333, loss=-0.03746
surrogate=-0.02184, entropy= 2.16276, loss=-0.02184
surrogate=-0.04741, entropy= 2.16452, loss=-0.04741
surrogate=-0.02125, entropy= 2.16328, loss=-0.02125
std_min= 0.43616, std_max= 0.55908, std_mean= 0.50023
val lr: [0.0001864754098360656], policy lr: [0.0002237704918032787]
Policy Loss: -0.021252, | Entropy Bonus: -0, | Value Loss: 150.79, | Advantage Loss: 4.0629
Time elapsed (s): 1.7026188373565674
Agent stdevs: 0.50022554
--------------------------------------------------------------------------------

Step 248
++++++++ Policy training ++++++++++
Current mean reward: 1385.692499 | mean episode length: 395.800000
val_loss=27.02142
val_loss=18.57332
val_loss=34.09535
val_loss=17.79736
val_loss=11.10793
val_loss=33.06218
val_loss=18.15364
val_loss=19.78555
val_loss=22.25130
val_loss=14.53653
adv_loss= 3.93484
adv_loss= 4.74428
adv_loss= 2.94795
adv_loss= 5.76601
adv_loss= 3.44155
adv_loss= 1.99855
adv_loss= 1.59251
adv_loss= 2.23580
adv_loss= 3.13972
adv_loss= 7.69193
surrogate= 0.01277, entropy= 2.16096, loss= 0.01277
surrogate= 0.00433, entropy= 2.16156, loss= 0.00433
surrogate= 0.00342, entropy= 2.15854, loss= 0.00342
surrogate=-0.02167, entropy= 2.15818, loss=-0.02167
surrogate= 0.00802, entropy= 2.15752, loss= 0.00802
surrogate=-0.02087, entropy= 2.15636, loss=-0.02087
surrogate= 0.00090, entropy= 2.15494, loss= 0.00090
surrogate=-0.00248, entropy= 2.15345, loss=-0.00248
surrogate= 0.01922, entropy= 2.15456, loss= 0.01922
surrogate=-0.03662, entropy= 2.15397, loss=-0.03662
std_min= 0.43730, std_max= 0.55265, std_mean= 0.49840
val lr: [0.00018621926229508195], policy lr: [0.00022346311475409834]
Policy Loss: -0.036622, | Entropy Bonus: -0, | Value Loss: 14.537, | Advantage Loss: 7.6919
Time elapsed (s): 1.7285635471343994
Agent stdevs: 0.49839866
--------------------------------------------------------------------------------

Step 249
++++++++ Policy training ++++++++++
Current mean reward: 1347.693742 | mean episode length: 397.800000
val_loss=37.66726
val_loss=28.68982
val_loss=22.61640
val_loss=23.57921
val_loss=37.95822
val_loss=41.46159
val_loss=17.52486
val_loss=17.97674
val_loss=12.89039
val_loss=22.25105
adv_loss= 4.50791
adv_loss= 1.54133
adv_loss= 5.40931
adv_loss= 3.82215
adv_loss= 1.63621
adv_loss= 1.67535
adv_loss= 3.39867
adv_loss= 2.56480
adv_loss= 5.95854
adv_loss= 4.04175
surrogate= 0.00295, entropy= 2.15417, loss= 0.00295
surrogate= 0.00186, entropy= 2.15002, loss= 0.00186
surrogate=-0.02158, entropy= 2.14936, loss=-0.02158
surrogate=-0.00655, entropy= 2.14738, loss=-0.00655
surrogate=-0.03285, entropy= 2.14627, loss=-0.03285
surrogate= 0.02065, entropy= 2.14249, loss= 0.02065
surrogate= 0.01885, entropy= 2.14206, loss= 0.01885
surrogate=-0.01114, entropy= 2.14184, loss=-0.01114
surrogate=-0.02770, entropy= 2.13981, loss=-0.02770
surrogate=-0.03672, entropy= 2.13870, loss=-0.03672
std_min= 0.43538, std_max= 0.55091, std_mean= 0.49588
val lr: [0.00018596311475409838], policy lr: [0.00022315573770491804]
Policy Loss: -0.036719, | Entropy Bonus: -0, | Value Loss: 22.251, | Advantage Loss: 4.0417
Time elapsed (s): 1.6796581745147705
Agent stdevs: 0.49588266
--------------------------------------------------------------------------------

Step 250
++++++++ Policy training ++++++++++
Current mean reward: 1276.753715 | mean episode length: 371.000000
val_loss=22.13158
val_loss=16.85490
val_loss=11.01363
val_loss=11.04939
val_loss=15.25967
val_loss=10.22071
val_loss=16.48937
val_loss=14.94088
val_loss=24.41503
val_loss=15.08756
adv_loss= 2.16853
adv_loss= 7.42305
adv_loss= 2.75896
adv_loss= 1.15258
adv_loss= 2.14023
adv_loss= 4.74328
adv_loss= 4.08807
adv_loss= 2.90984
adv_loss= 1.49869
adv_loss= 1.74084
surrogate=-0.01390, entropy= 2.13893, loss=-0.01390
surrogate=-0.01054, entropy= 2.13950, loss=-0.01054
surrogate=-0.03121, entropy= 2.13986, loss=-0.03121
surrogate= 0.00741, entropy= 2.13879, loss= 0.00741
surrogate= 0.00629, entropy= 2.13859, loss= 0.00629
surrogate=-0.00949, entropy= 2.13997, loss=-0.00949
surrogate= 0.00021, entropy= 2.13952, loss= 0.00021
surrogate=-0.04066, entropy= 2.13986, loss=-0.04066
surrogate= 0.00336, entropy= 2.14060, loss= 0.00336
surrogate=-0.03167, entropy= 2.13990, loss=-0.03167
std_min= 0.43689, std_max= 0.54859, std_mean= 0.49595
val lr: [0.00018570696721311475], policy lr: [0.0002228483606557377]
Policy Loss: -0.031674, | Entropy Bonus: -0, | Value Loss: 15.088, | Advantage Loss: 1.7408
Time elapsed (s): 1.713338851928711
Agent stdevs: 0.49594823
--------------------------------------------------------------------------------

Step 251
++++++++ Policy training ++++++++++
Current mean reward: 1085.541306 | mean episode length: 310.166667
val_loss=22.59448
val_loss=54.92744
val_loss=15.87864
val_loss=14.30459
val_loss=13.14652
val_loss=17.45435
val_loss=14.03984
val_loss=14.60368
val_loss=16.16991
val_loss=18.45529
adv_loss= 2.72695
adv_loss= 3.48817
adv_loss= 2.28192
adv_loss= 2.09564
adv_loss= 3.44303
adv_loss= 4.08023
adv_loss= 1.27243
adv_loss= 2.39027
adv_loss= 2.23162
adv_loss= 5.12746
surrogate=-0.00962, entropy= 2.13806, loss=-0.00962
surrogate=-0.03606, entropy= 2.13411, loss=-0.03606
surrogate=-0.01607, entropy= 2.13259, loss=-0.01607
surrogate= 0.01031, entropy= 2.12998, loss= 0.01031
surrogate=-0.00558, entropy= 2.12931, loss=-0.00558
surrogate=-0.03068, entropy= 2.12772, loss=-0.03068
surrogate=-0.00509, entropy= 2.12653, loss=-0.00509
surrogate=-0.03399, entropy= 2.12488, loss=-0.03399
surrogate=-0.01272, entropy= 2.12290, loss=-0.01272
surrogate=-0.03511, entropy= 2.12324, loss=-0.03511
std_min= 0.42646, std_max= 0.54906, std_mean= 0.49374
val lr: [0.00018545081967213115], policy lr: [0.00022254098360655736]
Policy Loss: -0.035107, | Entropy Bonus: -0, | Value Loss: 18.455, | Advantage Loss: 5.1275
Time elapsed (s): 1.7287707328796387
Agent stdevs: 0.49374023
--------------------------------------------------------------------------------

Step 252
++++++++ Policy training ++++++++++
Current mean reward: 1167.846941 | mean episode length: 336.333333
val_loss=28.12223
val_loss=72.04202
val_loss=30.21600
val_loss=67.38086
val_loss=29.83413
val_loss=35.85326
val_loss=24.32286
val_loss=12.28370
val_loss=45.22256
val_loss=40.26241
adv_loss= 7.86912
adv_loss= 6.39612
adv_loss= 2.87082
adv_loss= 2.28165
adv_loss= 3.35432
adv_loss= 2.60147
adv_loss= 6.29375
adv_loss= 9.86204
adv_loss= 2.40746
adv_loss= 7.07834
surrogate= 0.00509, entropy= 2.12391, loss= 0.00509
surrogate=-0.00709, entropy= 2.12551, loss=-0.00709
surrogate=-0.00194, entropy= 2.12884, loss=-0.00194
surrogate= 0.00575, entropy= 2.13290, loss= 0.00575
surrogate=-0.03563, entropy= 2.13521, loss=-0.03563
surrogate=-0.02522, entropy= 2.13716, loss=-0.02522
surrogate= 0.02460, entropy= 2.13781, loss= 0.02460
surrogate= 0.00953, entropy= 2.13895, loss= 0.00953
surrogate=-0.01874, entropy= 2.14067, loss=-0.01874
surrogate= 0.01551, entropy= 2.14207, loss= 0.01551
std_min= 0.43343, std_max= 0.55274, std_mean= 0.49661
val lr: [0.00018519467213114755], policy lr: [0.00022223360655737704]
Policy Loss: 0.015511, | Entropy Bonus: -0, | Value Loss: 40.262, | Advantage Loss: 7.0783
Time elapsed (s): 1.6966662406921387
Agent stdevs: 0.4966086
--------------------------------------------------------------------------------

Step 253
++++++++ Policy training ++++++++++
Current mean reward: 873.522414 | mean episode length: 256.142857
val_loss=40.32992
val_loss=45.39021
val_loss=15.30675
val_loss=11.38878
val_loss=14.49947
val_loss= 9.98998
val_loss=12.69379
val_loss=16.80458
val_loss=28.68651
val_loss=18.87291
adv_loss= 6.20318
adv_loss= 5.46733
adv_loss= 3.71561
adv_loss= 5.48347
adv_loss= 2.26416
adv_loss= 2.31194
adv_loss= 2.68816
adv_loss= 2.62551
adv_loss= 1.64097
adv_loss= 4.44511
surrogate=-0.02126, entropy= 2.14176, loss=-0.02126
surrogate=-0.00447, entropy= 2.14179, loss=-0.00447
surrogate=-0.01156, entropy= 2.14219, loss=-0.01156
surrogate=-0.00035, entropy= 2.14457, loss=-0.00035
surrogate=-0.01566, entropy= 2.14540, loss=-0.01566
surrogate=-0.02417, entropy= 2.14554, loss=-0.02417
surrogate=-0.02993, entropy= 2.14458, loss=-0.02993
surrogate=-0.00474, entropy= 2.14517, loss=-0.00474
surrogate=-0.02568, entropy= 2.14424, loss=-0.02568
surrogate=-0.03074, entropy= 2.14556, loss=-0.03074
std_min= 0.43408, std_max= 0.55188, std_mean= 0.49714
val lr: [0.00018493852459016395], policy lr: [0.0002219262295081967]
Policy Loss: -0.03074, | Entropy Bonus: -0, | Value Loss: 18.873, | Advantage Loss: 4.4451
Time elapsed (s): 1.692533016204834
Agent stdevs: 0.4971422
--------------------------------------------------------------------------------

Step 254
++++++++ Policy training ++++++++++
Current mean reward: 1426.440928 | mean episode length: 436.500000
val_loss=109.33712
val_loss=82.35954
val_loss=86.53708
val_loss=134.06897
val_loss=205.20808
val_loss=50.15812
val_loss=80.29967
val_loss=141.06015
val_loss=152.68866
val_loss=132.06718
adv_loss= 6.92407
adv_loss=203.46964
adv_loss= 4.89213
adv_loss= 9.36903
adv_loss= 4.06498
adv_loss=11.70289
adv_loss= 9.61857
adv_loss= 4.51871
adv_loss= 4.75259
adv_loss= 9.14759
surrogate= 0.00098, entropy= 2.14556, loss= 0.00098
surrogate= 0.00682, entropy= 2.14284, loss= 0.00682
surrogate=-0.02280, entropy= 2.14314, loss=-0.02280
surrogate=-0.02121, entropy= 2.14304, loss=-0.02121
surrogate=-0.00236, entropy= 2.14253, loss=-0.00236
surrogate=-0.04068, entropy= 2.14130, loss=-0.04068
surrogate=-0.03740, entropy= 2.13948, loss=-0.03740
surrogate=-0.01379, entropy= 2.13878, loss=-0.01379
surrogate=-0.00398, entropy= 2.13987, loss=-0.00398
surrogate=-0.02683, entropy= 2.13875, loss=-0.02683
std_min= 0.43091, std_max= 0.54995, std_mean= 0.49611
val lr: [0.00018468237704918035], policy lr: [0.00022161885245901639]
Policy Loss: -0.026828, | Entropy Bonus: -0, | Value Loss: 132.07, | Advantage Loss: 9.1476
Time elapsed (s): 1.7121763229370117
Agent stdevs: 0.49611056
--------------------------------------------------------------------------------

Step 255
++++++++ Policy training ++++++++++
Current mean reward: 931.959626 | mean episode length: 269.500000
val_loss=95.68546
val_loss=30.24017
val_loss=39.18729
val_loss=37.05913
val_loss=41.78969
val_loss=27.26444
val_loss=27.20367
val_loss=31.56925
val_loss=46.17850
val_loss=49.56979
adv_loss= 4.55426
adv_loss= 5.02089
adv_loss=13.63958
adv_loss= 3.10289
adv_loss= 2.85303
adv_loss= 3.70076
adv_loss= 6.98717
adv_loss= 6.00289
adv_loss= 7.73804
adv_loss= 5.58558
surrogate= 0.01646, entropy= 2.13739, loss= 0.01646
surrogate=-0.03341, entropy= 2.13691, loss=-0.03341
surrogate= 0.00331, entropy= 2.13649, loss= 0.00331
surrogate= 0.00666, entropy= 2.13720, loss= 0.00666
surrogate=-0.02214, entropy= 2.13794, loss=-0.02214
surrogate=-0.03033, entropy= 2.13757, loss=-0.03033
surrogate= 0.01530, entropy= 2.13680, loss= 0.01530
surrogate=-0.02760, entropy= 2.13671, loss=-0.02760
surrogate=-0.02937, entropy= 2.13752, loss=-0.02937
surrogate=-0.03575, entropy= 2.13723, loss=-0.03575
std_min= 0.42654, std_max= 0.54968, std_mean= 0.49611
val lr: [0.00018442622950819672], policy lr: [0.00022131147540983603]
Policy Loss: -0.035754, | Entropy Bonus: -0, | Value Loss: 49.57, | Advantage Loss: 5.5856
Time elapsed (s): 1.7130684852600098
Agent stdevs: 0.49610877
--------------------------------------------------------------------------------

Step 256
++++++++ Policy training ++++++++++
Current mean reward: 1687.554132 | mean episode length: 502.750000
val_loss=796.99011
val_loss=28.78990
val_loss=146.31071
val_loss=33.69512
val_loss=153.84705
val_loss=27.77791
val_loss=37.35274
val_loss=331.67145
val_loss=448.36087
val_loss=23.67271
adv_loss= 6.31662
adv_loss= 5.62196
adv_loss= 4.27758
adv_loss= 2.20168
adv_loss= 3.71335
adv_loss= 4.18354
adv_loss= 2.45814
adv_loss= 5.60086
adv_loss= 5.36275
adv_loss= 5.29238
surrogate= 0.00063, entropy= 2.13601, loss= 0.00063
surrogate=-0.00606, entropy= 2.13709, loss=-0.00606
surrogate= 0.00271, entropy= 2.13889, loss= 0.00271
surrogate=-0.00140, entropy= 2.13772, loss=-0.00140
surrogate= 0.01661, entropy= 2.13832, loss= 0.01661
surrogate=-0.02189, entropy= 2.14136, loss=-0.02189
surrogate=-0.00426, entropy= 2.14184, loss=-0.00426
surrogate=-0.00616, entropy= 2.14243, loss=-0.00616
surrogate= 0.00398, entropy= 2.14322, loss= 0.00398
surrogate= 0.00551, entropy= 2.14463, loss= 0.00551
std_min= 0.42866, std_max= 0.54865, std_mean= 0.49723
val lr: [0.00018417008196721312], policy lr: [0.00022100409836065573]
Policy Loss: 0.0055125, | Entropy Bonus: -0, | Value Loss: 23.673, | Advantage Loss: 5.2924
Time elapsed (s): 1.6981005668640137
Agent stdevs: 0.49723434
--------------------------------------------------------------------------------

Step 257
++++++++ Policy training ++++++++++
Current mean reward: 1511.251947 | mean episode length: 451.000000
val_loss=115.11804
val_loss=1858.02356
val_loss=619.55957
val_loss=64.91706
val_loss=59.66048
val_loss=56.47412
val_loss=36.99730
val_loss=169.12593
val_loss=46.98647
val_loss=1574.77625
adv_loss= 2.52873
adv_loss= 3.57332
adv_loss= 3.15185
adv_loss= 3.99459
adv_loss= 2.62677
adv_loss= 1.41151
adv_loss= 3.66057
adv_loss= 7.92801
adv_loss= 4.22203
adv_loss= 5.45785
surrogate=-0.02071, entropy= 2.14288, loss=-0.02071
surrogate=-0.01604, entropy= 2.13889, loss=-0.01604
surrogate=-0.01842, entropy= 2.13650, loss=-0.01842
surrogate= 0.01253, entropy= 2.13418, loss= 0.01253
surrogate=-0.01371, entropy= 2.13404, loss=-0.01371
surrogate=-0.02815, entropy= 2.13233, loss=-0.02815
surrogate=-0.01174, entropy= 2.12954, loss=-0.01174
surrogate=-0.00408, entropy= 2.12888, loss=-0.00408
surrogate=-0.00223, entropy= 2.12844, loss=-0.00223
surrogate= 0.00263, entropy= 2.12797, loss= 0.00263
std_min= 0.42651, std_max= 0.54432, std_mean= 0.49442
val lr: [0.0001839139344262295], policy lr: [0.00022069672131147538]
Policy Loss: 0.0026297, | Entropy Bonus: -0, | Value Loss: 1574.8, | Advantage Loss: 5.4578
Time elapsed (s): 1.7301266193389893
Agent stdevs: 0.49442446
--------------------------------------------------------------------------------

Step 258
++++++++ Policy training ++++++++++
Current mean reward: 1027.572519 | mean episode length: 301.833333
val_loss=1092.48730
val_loss=72.93508
val_loss=368.77335
val_loss=225.26494
val_loss=401.26501
val_loss=201.93370
val_loss=122.00805
val_loss=171.12138
val_loss=179.21765
val_loss=86.81692
adv_loss= 4.66307
adv_loss=10.64120
adv_loss= 8.80311
adv_loss= 9.45136
adv_loss= 9.40323
adv_loss= 5.83577
adv_loss= 5.83752
adv_loss= 9.78261
adv_loss= 6.18743
adv_loss= 8.31811
surrogate=-0.00483, entropy= 2.12590, loss=-0.00483
surrogate=-0.03499, entropy= 2.12617, loss=-0.03499
surrogate=-0.02528, entropy= 2.12597, loss=-0.02528
surrogate= 0.00193, entropy= 2.12363, loss= 0.00193
surrogate=-0.01065, entropy= 2.12124, loss=-0.01065
surrogate=-0.00502, entropy= 2.12149, loss=-0.00502
surrogate=-0.01711, entropy= 2.12307, loss=-0.01711
surrogate=-0.05092, entropy= 2.12334, loss=-0.05092
surrogate=-0.03333, entropy= 2.12342, loss=-0.03333
surrogate= 0.01664, entropy= 2.12511, loss= 0.01664
std_min= 0.42798, std_max= 0.54557, std_mean= 0.49388
val lr: [0.00018365778688524592], policy lr: [0.00022038934426229508]
Policy Loss: 0.016641, | Entropy Bonus: -0, | Value Loss: 86.817, | Advantage Loss: 8.3181
Time elapsed (s): 1.70033597946167
Agent stdevs: 0.49388433
--------------------------------------------------------------------------------

Step 259
++++++++ Policy training ++++++++++
Current mean reward: 2468.460508 | mean episode length: 736.500000
val_loss=390.60236
val_loss=22.26344
val_loss=95.89411
val_loss=59.00904
val_loss=66.74063
val_loss=130.07776
val_loss=646.89026
val_loss=26.74497
val_loss=43.86812
val_loss=45.45044
adv_loss= 2.98807
adv_loss= 8.99186
adv_loss= 4.75187
adv_loss= 4.78514
adv_loss= 1.90514
adv_loss= 3.44347
adv_loss= 3.76391
adv_loss= 1.55837
adv_loss= 3.39033
adv_loss= 4.93070
surrogate= 0.01098, entropy= 2.12679, loss= 0.01098
surrogate=-0.00964, entropy= 2.12618, loss=-0.00964
surrogate=-0.02078, entropy= 2.12661, loss=-0.02078
surrogate=-0.01402, entropy= 2.12906, loss=-0.01402
surrogate=-0.03269, entropy= 2.12936, loss=-0.03269
surrogate=-0.02188, entropy= 2.12980, loss=-0.02188
surrogate=-0.02803, entropy= 2.13096, loss=-0.02803
surrogate= 0.00128, entropy= 2.13288, loss= 0.00128
surrogate=-0.02201, entropy= 2.13224, loss=-0.02201
surrogate=-0.04823, entropy= 2.13270, loss=-0.04823
std_min= 0.42562, std_max= 0.54534, std_mean= 0.49531
val lr: [0.0001834016393442623], policy lr: [0.00022008196721311473]
Policy Loss: -0.048233, | Entropy Bonus: -0, | Value Loss: 45.45, | Advantage Loss: 4.9307
Time elapsed (s): 1.7080621719360352
Agent stdevs: 0.49531296
--------------------------------------------------------------------------------

Step 260
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1525.4
++++++++ Policy training ++++++++++
Current mean reward: 952.070771 | mean episode length: 287.000000
val_loss=370.22427
val_loss=81.55572
val_loss=77.15551
val_loss=52.79314
val_loss=119.42371
val_loss=72.80357
val_loss=106.14805
val_loss=49.35402
val_loss=30.30243
val_loss=23.61268
adv_loss= 9.26028
adv_loss= 3.96772
adv_loss=11.63276
adv_loss= 5.06679
adv_loss= 8.03223
adv_loss= 5.62251
adv_loss= 6.01878
adv_loss= 4.68173
adv_loss= 5.58614
adv_loss=10.05762
surrogate=-0.00683, entropy= 2.13082, loss=-0.00683
surrogate=-0.00597, entropy= 2.12933, loss=-0.00597
surrogate=-0.00836, entropy= 2.13029, loss=-0.00836
surrogate=-0.02232, entropy= 2.12876, loss=-0.02232
surrogate=-0.02368, entropy= 2.12886, loss=-0.02368
surrogate=-0.01526, entropy= 2.12989, loss=-0.01526
surrogate=-0.02528, entropy= 2.13131, loss=-0.02528
surrogate=-0.02056, entropy= 2.13269, loss=-0.02056
surrogate=-0.03217, entropy= 2.13114, loss=-0.03217
surrogate=-0.03122, entropy= 2.13275, loss=-0.03122
std_min= 0.42983, std_max= 0.54824, std_mean= 0.49512
val lr: [0.0001831454918032787], policy lr: [0.0002197745901639344]
Policy Loss: -0.031216, | Entropy Bonus: -0, | Value Loss: 23.613, | Advantage Loss: 10.058
Time elapsed (s): 1.6903917789459229
Agent stdevs: 0.49512103
--------------------------------------------------------------------------------

Step 261
++++++++ Policy training ++++++++++
Current mean reward: 1574.484193 | mean episode length: 455.333333
val_loss=27.90027
val_loss=22.16598
val_loss=27.85563
val_loss=32.39810
val_loss=16.07960
val_loss=18.18631
val_loss=11.61001
val_loss=17.39206
val_loss= 7.74059
val_loss=15.38790
adv_loss= 3.17344
adv_loss= 3.19023
adv_loss= 4.24651
adv_loss= 3.20856
adv_loss= 3.87275
adv_loss= 7.15744
adv_loss= 6.58395
adv_loss= 3.44658
adv_loss= 3.05006
adv_loss= 2.39199
surrogate=-0.01305, entropy= 2.13675, loss=-0.01305
surrogate=-0.02442, entropy= 2.13625, loss=-0.02442
surrogate=-0.00024, entropy= 2.13485, loss=-0.00024
surrogate=-0.04905, entropy= 2.13865, loss=-0.04905
surrogate=-0.03617, entropy= 2.13811, loss=-0.03617
surrogate=-0.03594, entropy= 2.13807, loss=-0.03594
surrogate= 0.01329, entropy= 2.14022, loss= 0.01329
surrogate=-0.01556, entropy= 2.14050, loss=-0.01556
surrogate=-0.02463, entropy= 2.14106, loss=-0.02463
surrogate=-0.02467, entropy= 2.14285, loss=-0.02467
std_min= 0.43201, std_max= 0.55062, std_mean= 0.49679
val lr: [0.0001828893442622951], policy lr: [0.00021946721311475408]
Policy Loss: -0.024673, | Entropy Bonus: -0, | Value Loss: 15.388, | Advantage Loss: 2.392
Time elapsed (s): 1.6785192489624023
Agent stdevs: 0.49679062
--------------------------------------------------------------------------------

Step 262
++++++++ Policy training ++++++++++
Current mean reward: 1079.289685 | mean episode length: 312.666667
val_loss=26.55340
val_loss=30.84212
val_loss=25.02941
val_loss=26.41519
val_loss=22.23193
val_loss=15.29930
val_loss=15.69192
val_loss=20.45407
val_loss=20.33587
val_loss=23.58507
adv_loss= 3.86310
adv_loss=13.17443
adv_loss= 3.52924
adv_loss= 9.50722
adv_loss= 5.08826
adv_loss= 5.90737
adv_loss= 2.28406
adv_loss= 4.88452
adv_loss= 2.20020
adv_loss= 4.42895
surrogate=-0.00453, entropy= 2.14621, loss=-0.00453
surrogate= 0.01249, entropy= 2.14431, loss= 0.01249
surrogate= 0.00718, entropy= 2.14617, loss= 0.00718
surrogate=-0.00803, entropy= 2.14795, loss=-0.00803
surrogate=-0.01196, entropy= 2.14832, loss=-0.01196
surrogate=-0.00255, entropy= 2.14503, loss=-0.00255
surrogate= 0.00474, entropy= 2.14319, loss= 0.00474
surrogate=-0.02974, entropy= 2.14315, loss=-0.02974
surrogate=-0.04607, entropy= 2.14223, loss=-0.04607
surrogate=-0.01813, entropy= 2.14073, loss=-0.01813
std_min= 0.43097, std_max= 0.55240, std_mean= 0.49650
val lr: [0.0001826331967213115], policy lr: [0.00021915983606557375]
Policy Loss: -0.018129, | Entropy Bonus: -0, | Value Loss: 23.585, | Advantage Loss: 4.429
Time elapsed (s): 1.6892738342285156
Agent stdevs: 0.4965049
--------------------------------------------------------------------------------

Step 263
++++++++ Policy training ++++++++++
Current mean reward: 1446.644387 | mean episode length: 427.500000
val_loss=188.94795
val_loss=73.35876
val_loss=361.09488
val_loss=353.51004
val_loss=110.03568
val_loss=39.09023
val_loss=275.38245
val_loss=537.58075
val_loss=85.77563
val_loss=302.29608
adv_loss= 9.44044
adv_loss= 5.54706
adv_loss= 3.85584
adv_loss= 1.91018
adv_loss= 5.02257
adv_loss= 1.86756
adv_loss=24.10198
adv_loss=10.88528
adv_loss= 4.37700
adv_loss= 1.52573
surrogate=-0.01750, entropy= 2.14228, loss=-0.01750
surrogate=-0.01070, entropy= 2.14445, loss=-0.01070
surrogate= 0.00917, entropy= 2.14528, loss= 0.00917
surrogate=-0.03137, entropy= 2.14785, loss=-0.03137
surrogate=-0.04088, entropy= 2.14940, loss=-0.04088
surrogate=-0.01973, entropy= 2.14932, loss=-0.01973
surrogate=-0.00982, entropy= 2.15057, loss=-0.00982
surrogate= 0.03617, entropy= 2.15127, loss= 0.03617
surrogate=-0.03144, entropy= 2.15334, loss=-0.03144
surrogate=-0.02625, entropy= 2.15445, loss=-0.02625
std_min= 0.43468, std_max= 0.54958, std_mean= 0.49857
val lr: [0.0001823770491803279], policy lr: [0.00021885245901639343]
Policy Loss: -0.026253, | Entropy Bonus: -0, | Value Loss: 302.3, | Advantage Loss: 1.5257
Time elapsed (s): 1.728867530822754
Agent stdevs: 0.498569
--------------------------------------------------------------------------------

Step 264
++++++++ Policy training ++++++++++
Current mean reward: 1007.429852 | mean episode length: 293.600000
val_loss=77.48303
val_loss=30.72336
val_loss=19.04202
val_loss=22.37791
val_loss=25.75842
val_loss= 9.72049
val_loss=21.10353
val_loss=12.58050
val_loss=11.19687
val_loss=25.33515
adv_loss= 2.36506
adv_loss= 2.85452
adv_loss=11.55073
adv_loss=10.45490
adv_loss= 6.43601
adv_loss= 6.62159
adv_loss=15.40396
adv_loss= 1.76117
adv_loss= 3.12950
adv_loss= 4.95536
surrogate= 0.00136, entropy= 2.15455, loss= 0.00136
surrogate=-0.00303, entropy= 2.15209, loss=-0.00303
surrogate= 0.01411, entropy= 2.14951, loss= 0.01411
surrogate=-0.01266, entropy= 2.14736, loss=-0.01266
surrogate= 0.02018, entropy= 2.14652, loss= 0.02018
surrogate=-0.01936, entropy= 2.14627, loss=-0.01936
surrogate=-0.01838, entropy= 2.14608, loss=-0.01838
surrogate=-0.02688, entropy= 2.14388, loss=-0.02688
surrogate=-0.02835, entropy= 2.14162, loss=-0.02835
surrogate= 0.00596, entropy= 2.13977, loss= 0.00596
std_min= 0.42805, std_max= 0.54767, std_mean= 0.49641
val lr: [0.00018212090163934424], policy lr: [0.00021854508196721308]
Policy Loss: 0.0059643, | Entropy Bonus: -0, | Value Loss: 25.335, | Advantage Loss: 4.9554
Time elapsed (s): 1.7006778717041016
Agent stdevs: 0.49640772
--------------------------------------------------------------------------------

Step 265
++++++++ Policy training ++++++++++
Current mean reward: 1289.368515 | mean episode length: 377.000000
val_loss=21.44010
val_loss=29.62230
val_loss=38.45065
val_loss=40.44574
val_loss=16.09634
val_loss=30.92566
val_loss=31.52831
val_loss=38.78043
val_loss=36.85355
val_loss=14.46991
adv_loss= 4.68840
adv_loss= 5.54888
adv_loss= 7.86581
adv_loss= 4.02117
adv_loss= 2.97645
adv_loss= 8.40552
adv_loss= 3.77700
adv_loss= 3.68992
adv_loss= 2.26637
adv_loss= 3.82401
surrogate= 0.00565, entropy= 2.13699, loss= 0.00565
surrogate= 0.00659, entropy= 2.13420, loss= 0.00659
surrogate= 0.02844, entropy= 2.13393, loss= 0.02844
surrogate=-0.01319, entropy= 2.13163, loss=-0.01319
surrogate=-0.02483, entropy= 2.12922, loss=-0.02483
surrogate=-0.01825, entropy= 2.12666, loss=-0.01825
surrogate=-0.03902, entropy= 2.12563, loss=-0.03902
surrogate=-0.00732, entropy= 2.12428, loss=-0.00732
surrogate=-0.01533, entropy= 2.12087, loss=-0.01533
surrogate=-0.02978, entropy= 2.11845, loss=-0.02978
std_min= 0.42034, std_max= 0.55296, std_mean= 0.49344
val lr: [0.00018186475409836067], policy lr: [0.00021823770491803278]
Policy Loss: -0.02978, | Entropy Bonus: -0, | Value Loss: 14.47, | Advantage Loss: 3.824
Time elapsed (s): 1.698180913925171
Agent stdevs: 0.49343714
--------------------------------------------------------------------------------

Step 266
++++++++ Policy training ++++++++++
Current mean reward: 1512.584094 | mean episode length: 452.750000
val_loss=68.43557
val_loss=489.48346
val_loss=296.03296
val_loss=469.81744
val_loss=1181.38965
val_loss=139.41301
val_loss=144.61118
val_loss=274.68091
val_loss=175.35486
val_loss=70.92595
adv_loss=15.31383
adv_loss= 8.66254
adv_loss= 5.14120
adv_loss= 7.22662
adv_loss= 6.59765
adv_loss= 4.80208
adv_loss= 2.05887
adv_loss= 8.19974
adv_loss= 4.75529
adv_loss= 5.30441
surrogate= 0.02364, entropy= 2.11879, loss= 0.02364
surrogate=-0.00792, entropy= 2.12186, loss=-0.00792
surrogate=-0.01630, entropy= 2.12634, loss=-0.01630
surrogate=-0.01290, entropy= 2.12921, loss=-0.01290
surrogate=-0.02390, entropy= 2.13148, loss=-0.02390
surrogate=-0.02039, entropy= 2.13566, loss=-0.02039
surrogate= 0.00541, entropy= 2.13722, loss= 0.00541
surrogate=-0.04110, entropy= 2.13755, loss=-0.04110
surrogate=-0.03130, entropy= 2.13952, loss=-0.03130
surrogate=-0.03324, entropy= 2.13986, loss=-0.03324
std_min= 0.42367, std_max= 0.55800, std_mean= 0.49699
val lr: [0.00018160860655737704], policy lr: [0.00021793032786885242]
Policy Loss: -0.033237, | Entropy Bonus: -0, | Value Loss: 70.926, | Advantage Loss: 5.3044
Time elapsed (s): 1.702467918395996
Agent stdevs: 0.496987
--------------------------------------------------------------------------------

Step 267
++++++++ Policy training ++++++++++
Current mean reward: 1418.294539 | mean episode length: 416.250000
val_loss=43.91031
val_loss=36.47305
val_loss=34.42672
val_loss=27.50854
val_loss=43.66394
val_loss=17.81557
val_loss=23.82712
val_loss=20.52366
val_loss=18.85279
val_loss=14.50681
adv_loss= 4.85814
adv_loss= 5.56681
adv_loss= 4.19361
adv_loss= 5.49707
adv_loss= 5.50403
adv_loss= 2.14907
adv_loss= 4.31961
adv_loss= 4.24450
adv_loss= 7.59496
adv_loss=18.50872
surrogate= 0.00561, entropy= 2.14114, loss= 0.00561
surrogate= 0.00648, entropy= 2.14157, loss= 0.00648
surrogate=-0.04038, entropy= 2.14140, loss=-0.04038
surrogate= 0.02914, entropy= 2.14170, loss= 0.02914
surrogate=-0.02812, entropy= 2.14261, loss=-0.02812
surrogate=-0.01677, entropy= 2.14446, loss=-0.01677
surrogate= 0.00854, entropy= 2.14491, loss= 0.00854
surrogate=-0.01563, entropy= 2.14656, loss=-0.01563
surrogate=-0.03543, entropy= 2.14546, loss=-0.03543
surrogate=-0.01918, entropy= 2.14752, loss=-0.01918
std_min= 0.42436, std_max= 0.55639, std_mean= 0.49820
val lr: [0.00018135245901639344], policy lr: [0.0002176229508196721]
Policy Loss: -0.019181, | Entropy Bonus: -0, | Value Loss: 14.507, | Advantage Loss: 18.509
Time elapsed (s): 1.6857843399047852
Agent stdevs: 0.49819875
--------------------------------------------------------------------------------

Step 268
++++++++ Policy training ++++++++++
Current mean reward: 1017.208382 | mean episode length: 290.285714
val_loss=25.01040
val_loss=26.01100
val_loss=22.49018
val_loss=11.44452
val_loss=17.32320
val_loss=20.85247
val_loss=11.50891
val_loss=21.60490
val_loss=16.22503
val_loss=16.32417
adv_loss= 7.51724
adv_loss= 3.02647
adv_loss= 3.56996
adv_loss= 4.02998
adv_loss= 7.43177
adv_loss= 3.51055
adv_loss= 6.10598
adv_loss= 6.12022
adv_loss= 4.57257
adv_loss=11.94521
surrogate= 0.02459, entropy= 2.14953, loss= 0.02459
surrogate=-0.00774, entropy= 2.14831, loss=-0.00774
surrogate=-0.01003, entropy= 2.14870, loss=-0.01003
surrogate=-0.01568, entropy= 2.14870, loss=-0.01568
surrogate=-0.01272, entropy= 2.14721, loss=-0.01272
surrogate=-0.00158, entropy= 2.14609, loss=-0.00158
surrogate=-0.03496, entropy= 2.14631, loss=-0.03496
surrogate=-0.01278, entropy= 2.14543, loss=-0.01278
surrogate=-0.01663, entropy= 2.14432, loss=-0.01663
surrogate= 0.00119, entropy= 2.14272, loss= 0.00119
std_min= 0.42072, std_max= 0.55436, std_mean= 0.49758
val lr: [0.00018109631147540984], policy lr: [0.00021731557377049177]
Policy Loss: 0.001193, | Entropy Bonus: -0, | Value Loss: 16.324, | Advantage Loss: 11.945
Time elapsed (s): 1.7272577285766602
Agent stdevs: 0.49758112
--------------------------------------------------------------------------------

Step 269
++++++++ Policy training ++++++++++
Current mean reward: 1653.771173 | mean episode length: 499.250000
val_loss=1037.91284
val_loss=239.95782
val_loss=59.38794
val_loss=29.45628
val_loss=30.20601
val_loss=116.43145
val_loss=27.27076
val_loss=639.20844
val_loss=105.06530
val_loss=710.02087
adv_loss= 4.63180
adv_loss= 4.31658
adv_loss= 2.05022
adv_loss= 3.72227
adv_loss= 2.79832
adv_loss= 3.22909
adv_loss= 2.92565
adv_loss= 5.42307
adv_loss= 1.96206
adv_loss= 3.06303
surrogate=-0.02600, entropy= 2.14273, loss=-0.02600
surrogate=-0.00199, entropy= 2.14460, loss=-0.00199
surrogate= 0.00286, entropy= 2.14370, loss= 0.00286
surrogate=-0.01709, entropy= 2.14311, loss=-0.01709
surrogate=-0.00180, entropy= 2.14240, loss=-0.00180
surrogate=-0.01505, entropy= 2.14199, loss=-0.01505
surrogate= 0.02840, entropy= 2.14253, loss= 0.02840
surrogate= 0.01802, entropy= 2.14183, loss= 0.01802
surrogate=-0.02354, entropy= 2.14155, loss=-0.02354
surrogate=-0.00938, entropy= 2.14045, loss=-0.00938
std_min= 0.41549, std_max= 0.55980, std_mean= 0.49776
val lr: [0.00018084016393442624], policy lr: [0.00021700819672131145]
Policy Loss: -0.0093841, | Entropy Bonus: -0, | Value Loss: 710.02, | Advantage Loss: 3.063
Time elapsed (s): 1.7424440383911133
Agent stdevs: 0.4977616
--------------------------------------------------------------------------------

Step 270
++++++++ Policy training ++++++++++
Current mean reward: 2137.245317 | mean episode length: 645.000000
val_loss=322.37125
val_loss=865.03027
val_loss=50.23932
val_loss=430.82687
val_loss=410.49991
val_loss=41.59682
val_loss=1413.72388
val_loss=208.23474
val_loss=17.64247
val_loss=28.30787
adv_loss= 2.29067
adv_loss= 3.63637
adv_loss= 3.20620
adv_loss=12.10765
adv_loss= 3.39937
adv_loss= 2.57049
adv_loss= 2.04746
adv_loss= 2.28043
adv_loss= 2.11752
adv_loss= 3.98113
surrogate=-0.01685, entropy= 2.13843, loss=-0.01685
surrogate= 0.03513, entropy= 2.13649, loss= 0.03513
surrogate= 0.00987, entropy= 2.13450, loss= 0.00987
surrogate= 0.00318, entropy= 2.13449, loss= 0.00318
surrogate= 0.00486, entropy= 2.13395, loss= 0.00486
surrogate=-0.00628, entropy= 2.12980, loss=-0.00628
surrogate=-0.03957, entropy= 2.12815, loss=-0.03957
surrogate=-0.03332, entropy= 2.12875, loss=-0.03332
surrogate= 0.00282, entropy= 2.12751, loss= 0.00282
surrogate=-0.03321, entropy= 2.12646, loss=-0.03321
std_min= 0.41189, std_max= 0.55236, std_mean= 0.49545
val lr: [0.00018058401639344264], policy lr: [0.00021670081967213112]
Policy Loss: -0.033213, | Entropy Bonus: -0, | Value Loss: 28.308, | Advantage Loss: 3.9811
Time elapsed (s): 1.6875934600830078
Agent stdevs: 0.4954537
--------------------------------------------------------------------------------

Step 271
++++++++ Policy training ++++++++++
Current mean reward: 1331.472508 | mean episode length: 384.000000
val_loss=27.65590
val_loss=13.61022
val_loss=20.20812
val_loss=22.98272
val_loss=17.74004
val_loss=10.52009
val_loss=10.68641
val_loss=10.68124
val_loss= 8.70927
val_loss=10.45411
adv_loss= 7.70586
adv_loss= 4.72972
adv_loss= 5.29374
adv_loss= 3.17578
adv_loss= 4.63029
adv_loss= 7.78467
adv_loss= 3.55879
adv_loss= 5.10797
adv_loss= 2.67633
adv_loss= 2.19289
surrogate= 0.00836, entropy= 2.12573, loss= 0.00836
surrogate= 0.01179, entropy= 2.12389, loss= 0.01179
surrogate= 0.01239, entropy= 2.12208, loss= 0.01239
surrogate=-0.01583, entropy= 2.12227, loss=-0.01583
surrogate= 0.01589, entropy= 2.12146, loss= 0.01589
surrogate=-0.01887, entropy= 2.11965, loss=-0.01887
surrogate=-0.03218, entropy= 2.11922, loss=-0.03218
surrogate=-0.04978, entropy= 2.11937, loss=-0.04978
surrogate=-0.02758, entropy= 2.11685, loss=-0.02758
surrogate=-0.05308, entropy= 2.11656, loss=-0.05308
std_min= 0.40701, std_max= 0.55174, std_mean= 0.49418
val lr: [0.00018032786885245904], policy lr: [0.0002163934426229508]
Policy Loss: -0.053083, | Entropy Bonus: -0, | Value Loss: 10.454, | Advantage Loss: 2.1929
Time elapsed (s): 1.697068214416504
Agent stdevs: 0.49417594
--------------------------------------------------------------------------------

Step 272
++++++++ Policy training ++++++++++
Current mean reward: 1620.122043 | mean episode length: 473.750000
val_loss=141.74873
val_loss=1869.73535
val_loss=76.94624
val_loss=941.27667
val_loss=45.25557
val_loss=844.81702
val_loss=55.22854
val_loss=53.73767
val_loss=691.65125
val_loss=83.68830
adv_loss= 6.27892
adv_loss= 2.02087
adv_loss= 4.89750
adv_loss= 3.42038
adv_loss= 1.97167
adv_loss= 2.49111
adv_loss= 2.88269
adv_loss= 2.44530
adv_loss= 3.59316
adv_loss= 3.75821
surrogate= 0.00954, entropy= 2.11610, loss= 0.00954
surrogate=-0.00077, entropy= 2.11899, loss=-0.00077
surrogate=-0.00926, entropy= 2.11824, loss=-0.00926
surrogate=-0.00720, entropy= 2.11729, loss=-0.00720
surrogate=-0.02289, entropy= 2.11712, loss=-0.02289
surrogate=-0.00716, entropy= 2.11676, loss=-0.00716
surrogate=-0.03020, entropy= 2.11567, loss=-0.03020
surrogate= 0.01359, entropy= 2.11570, loss= 0.01359
surrogate=-0.00948, entropy= 2.11719, loss=-0.00948
surrogate=-0.03173, entropy= 2.11797, loss=-0.03173
std_min= 0.40291, std_max= 0.55558, std_mean= 0.49490
val lr: [0.0001800717213114754], policy lr: [0.00021608606557377047]
Policy Loss: -0.031725, | Entropy Bonus: -0, | Value Loss: 83.688, | Advantage Loss: 3.7582
Time elapsed (s): 1.70387601852417
Agent stdevs: 0.49489915
--------------------------------------------------------------------------------

Step 273
++++++++ Policy training ++++++++++
Current mean reward: 1846.053288 | mean episode length: 528.333333
val_loss=26.16152
val_loss=14.60459
val_loss= 8.66109
val_loss=13.84672
val_loss= 8.27549
val_loss=14.92461
val_loss=11.51921
val_loss=11.62717
val_loss=16.28420
val_loss= 8.32130
adv_loss= 1.77879
adv_loss= 1.88739
adv_loss= 2.15078
adv_loss= 2.89190
adv_loss= 1.58533
adv_loss= 1.95938
adv_loss= 2.90778
adv_loss= 0.95759
adv_loss= 1.03650
adv_loss= 4.29613
surrogate=-0.02381, entropy= 2.11594, loss=-0.02381
surrogate= 0.00132, entropy= 2.11422, loss= 0.00132
surrogate=-0.01800, entropy= 2.11022, loss=-0.01800
surrogate=-0.01220, entropy= 2.10711, loss=-0.01220
surrogate= 0.00386, entropy= 2.10605, loss= 0.00386
surrogate=-0.03325, entropy= 2.10238, loss=-0.03325
surrogate=-0.03462, entropy= 2.10083, loss=-0.03462
surrogate=-0.00613, entropy= 2.09927, loss=-0.00613
surrogate=-0.02926, entropy= 2.09549, loss=-0.02926
surrogate=-0.00813, entropy= 2.09526, loss=-0.00813
std_min= 0.40481, std_max= 0.54903, std_mean= 0.49065
val lr: [0.00017981557377049178], policy lr: [0.00021577868852459012]
Policy Loss: -0.0081266, | Entropy Bonus: -0, | Value Loss: 8.3213, | Advantage Loss: 4.2961
Time elapsed (s): 1.6818280220031738
Agent stdevs: 0.4906472
--------------------------------------------------------------------------------

Step 274
++++++++ Policy training ++++++++++
Current mean reward: 1105.103342 | mean episode length: 316.166667
val_loss=15.18225
val_loss=19.53156
val_loss=14.36325
val_loss=10.34740
val_loss=16.55496
val_loss=33.40829
val_loss=42.73258
val_loss=14.93313
val_loss=10.22567
val_loss=17.03974
adv_loss= 3.17423
adv_loss= 5.80159
adv_loss= 3.30708
adv_loss= 3.07590
adv_loss= 1.82738
adv_loss= 1.99773
adv_loss= 3.02332
adv_loss= 1.87499
adv_loss= 3.62001
adv_loss= 1.41081
surrogate= 0.04488, entropy= 2.09903, loss= 0.04488
surrogate=-0.01754, entropy= 2.10053, loss=-0.01754
surrogate=-0.02993, entropy= 2.10032, loss=-0.02993
surrogate= 0.01061, entropy= 2.10144, loss= 0.01061
surrogate= 0.02035, entropy= 2.10342, loss= 0.02035
surrogate=-0.02103, entropy= 2.10302, loss=-0.02103
surrogate=-0.02276, entropy= 2.10361, loss=-0.02276
surrogate=-0.00981, entropy= 2.10409, loss=-0.00981
surrogate=-0.03193, entropy= 2.10401, loss=-0.03193
surrogate=-0.01844, entropy= 2.10353, loss=-0.01844
std_min= 0.40516, std_max= 0.55232, std_mean= 0.49210
val lr: [0.0001795594262295082], policy lr: [0.00021547131147540985]
Policy Loss: -0.018437, | Entropy Bonus: -0, | Value Loss: 17.04, | Advantage Loss: 1.4108
Time elapsed (s): 1.723243236541748
Agent stdevs: 0.49210346
--------------------------------------------------------------------------------

Step 275
++++++++ Policy training ++++++++++
Current mean reward: 1708.525498 | mean episode length: 495.000000
val_loss=52.68637
val_loss=51.46819
val_loss=37.94943
val_loss=160.14003
val_loss=50.53917
val_loss=1991.08728
val_loss=267.12085
val_loss=29.60169
val_loss=318.81836
val_loss=32.92160
adv_loss= 2.73072
adv_loss= 4.20041
adv_loss= 2.81124
adv_loss=1529.92175
adv_loss= 2.00068
adv_loss= 3.94204
adv_loss= 6.49253
adv_loss= 1.72320
adv_loss= 1.93665
adv_loss=1511.18152
surrogate= 0.00661, entropy= 2.10689, loss= 0.00661
surrogate= 0.01493, entropy= 2.10861, loss= 0.01493
surrogate=-0.01611, entropy= 2.11259, loss=-0.01611
surrogate= 0.00943, entropy= 2.11440, loss= 0.00943
surrogate= 0.02036, entropy= 2.11613, loss= 0.02036
surrogate=-0.02709, entropy= 2.11748, loss=-0.02709
surrogate=-0.02755, entropy= 2.11886, loss=-0.02755
surrogate=-0.00228, entropy= 2.11876, loss=-0.00228
surrogate=-0.01688, entropy= 2.11904, loss=-0.01688
surrogate=-0.02743, entropy= 2.12130, loss=-0.02743
std_min= 0.40562, std_max= 0.56075, std_mean= 0.49534
val lr: [0.00017930327868852458], policy lr: [0.00021516393442622947]
Policy Loss: -0.027425, | Entropy Bonus: -0, | Value Loss: 32.922, | Advantage Loss: 1511.2
Time elapsed (s): 1.7079553604125977
Agent stdevs: 0.49533963
--------------------------------------------------------------------------------

Step 276
++++++++ Policy training ++++++++++
Current mean reward: 990.341413 | mean episode length: 285.142857
val_loss=22.00518
val_loss=17.09480
val_loss=21.44876
val_loss=20.25846
val_loss=17.03547
val_loss= 9.94882
val_loss=17.45797
val_loss=21.55677
val_loss=24.09749
val_loss=16.84851
adv_loss= 3.29252
adv_loss= 3.11759
adv_loss= 6.14984
adv_loss= 2.57910
adv_loss= 4.05620
adv_loss= 5.92888
adv_loss= 4.43710
adv_loss= 4.65137
adv_loss= 3.30106
adv_loss= 3.35549
surrogate= 0.00077, entropy= 2.12143, loss= 0.00077
surrogate= 0.01350, entropy= 2.11961, loss= 0.01350
surrogate= 0.00290, entropy= 2.11684, loss= 0.00290
surrogate= 0.01452, entropy= 2.11579, loss= 0.01452
surrogate= 0.02065, entropy= 2.11503, loss= 0.02065
surrogate=-0.01002, entropy= 2.11173, loss=-0.01002
surrogate= 0.01296, entropy= 2.11160, loss= 0.01296
surrogate=-0.02220, entropy= 2.11234, loss=-0.02220
surrogate=-0.01927, entropy= 2.11208, loss=-0.01927
surrogate=-0.02996, entropy= 2.10908, loss=-0.02996
std_min= 0.40328, std_max= 0.55997, std_mean= 0.49342
val lr: [0.00017904713114754098], policy lr: [0.00021485655737704914]
Policy Loss: -0.029963, | Entropy Bonus: -0, | Value Loss: 16.849, | Advantage Loss: 3.3555
Time elapsed (s): 1.6787457466125488
Agent stdevs: 0.49342382
--------------------------------------------------------------------------------

Step 277
++++++++ Policy training ++++++++++
Current mean reward: 1421.951197 | mean episode length: 402.600000
val_loss=37.02087
val_loss=17.66837
val_loss=18.83466
val_loss=20.26713
val_loss=13.61376
val_loss=10.90962
val_loss=11.31726
val_loss=10.20429
val_loss=10.45544
val_loss= 8.05249
adv_loss= 4.30689
adv_loss= 1.90094
adv_loss= 1.20216
adv_loss= 3.26558
adv_loss= 1.64175
adv_loss= 0.86313
adv_loss= 2.05909
adv_loss= 1.46444
adv_loss= 2.11815
adv_loss= 6.27590
surrogate= 0.00406, entropy= 2.10862, loss= 0.00406
surrogate= 0.02802, entropy= 2.10633, loss= 0.02802
surrogate=-0.02354, entropy= 2.10249, loss=-0.02354
surrogate=-0.01145, entropy= 2.09811, loss=-0.01145
surrogate=-0.00910, entropy= 2.09628, loss=-0.00910
surrogate=-0.00471, entropy= 2.09317, loss=-0.00471
surrogate=-0.01524, entropy= 2.09037, loss=-0.01524
surrogate=-0.01650, entropy= 2.08877, loss=-0.01650
surrogate= 0.00168, entropy= 2.08658, loss= 0.00168
surrogate=-0.02163, entropy= 2.08296, loss=-0.02163
std_min= 0.39734, std_max= 0.55297, std_mean= 0.48932
val lr: [0.00017879098360655738], policy lr: [0.00021454918032786884]
Policy Loss: -0.021635, | Entropy Bonus: -0, | Value Loss: 8.0525, | Advantage Loss: 6.2759
Time elapsed (s): 1.6896169185638428
Agent stdevs: 0.48932156
--------------------------------------------------------------------------------

Step 278
++++++++ Policy training ++++++++++
Current mean reward: 2028.041237 | mean episode length: 585.333333
val_loss=18.35280
val_loss=18.09579
val_loss=34.75499
val_loss=12.60406
val_loss=14.55077
val_loss=13.23230
val_loss=30.68289
val_loss= 7.53595
val_loss= 6.64578
val_loss=10.64322
adv_loss= 2.69174
adv_loss= 1.53950
adv_loss= 1.94554
adv_loss= 0.81044
adv_loss= 2.51176
adv_loss= 3.17429
adv_loss= 2.56085
adv_loss= 1.32770
adv_loss= 0.68080
adv_loss= 1.25738
surrogate= 0.00339, entropy= 2.08613, loss= 0.00339
surrogate=-0.02518, entropy= 2.08948, loss=-0.02518
surrogate=-0.01873, entropy= 2.09367, loss=-0.01873
surrogate=-0.03627, entropy= 2.09582, loss=-0.03627
surrogate=-0.00418, entropy= 2.09705, loss=-0.00418
surrogate= 0.00664, entropy= 2.10084, loss= 0.00664
surrogate=-0.02060, entropy= 2.10056, loss=-0.02060
surrogate=-0.01604, entropy= 2.10194, loss=-0.01604
surrogate=-0.00352, entropy= 2.10263, loss=-0.00352
surrogate=-0.03202, entropy= 2.10259, loss=-0.03202
std_min= 0.40401, std_max= 0.54861, std_mean= 0.49198
val lr: [0.00017853483606557378], policy lr: [0.00021424180327868852]
Policy Loss: -0.032015, | Entropy Bonus: -0, | Value Loss: 10.643, | Advantage Loss: 1.2574
Time elapsed (s): 1.7056350708007812
Agent stdevs: 0.49198064
--------------------------------------------------------------------------------

Step 279
++++++++ Policy training ++++++++++
Current mean reward: 1511.550824 | mean episode length: 434.500000
val_loss=16.27910
val_loss= 9.62751
val_loss=11.68635
val_loss=13.59717
val_loss=18.37028
val_loss= 8.55745
val_loss=14.09793
val_loss=16.43265
val_loss=11.87357
val_loss=12.65362
adv_loss= 0.99922
adv_loss= 1.82192
adv_loss= 1.67504
adv_loss= 0.91037
adv_loss= 1.63290
adv_loss= 5.07475
adv_loss= 2.18457
adv_loss= 3.16036
adv_loss= 2.16481
adv_loss= 3.46864
surrogate=-0.02781, entropy= 2.10330, loss=-0.02781
surrogate=-0.00353, entropy= 2.10382, loss=-0.00353
surrogate=-0.03866, entropy= 2.10393, loss=-0.03866
surrogate=-0.00577, entropy= 2.10514, loss=-0.00577
surrogate=-0.01338, entropy= 2.10513, loss=-0.01338
surrogate=-0.01815, entropy= 2.10555, loss=-0.01815
surrogate=-0.00476, entropy= 2.10540, loss=-0.00476
surrogate=-0.02539, entropy= 2.10589, loss=-0.02539
surrogate=-0.01683, entropy= 2.10604, loss=-0.01683
surrogate=-0.01515, entropy= 2.10647, loss=-0.01515
std_min= 0.40487, std_max= 0.54951, std_mean= 0.49258
val lr: [0.00017827868852459018], policy lr: [0.0002139344262295082]
Policy Loss: -0.01515, | Entropy Bonus: -0, | Value Loss: 12.654, | Advantage Loss: 3.4686
Time elapsed (s): 1.6856892108917236
Agent stdevs: 0.49257508
--------------------------------------------------------------------------------

Step 280
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1532.1
++++++++ Policy training ++++++++++
Current mean reward: 1443.908969 | mean episode length: 419.750000
val_loss=18.83409
val_loss=24.31278
val_loss=24.13880
val_loss=26.41409
val_loss=31.30390
val_loss=28.57339
val_loss=28.37219
val_loss=11.30087
val_loss=29.40225
val_loss=17.78808
adv_loss= 3.44923
adv_loss= 4.19016
adv_loss= 5.57328
adv_loss= 1.64869
adv_loss= 4.85643
adv_loss= 9.14756
adv_loss= 2.52843
adv_loss= 1.53873
adv_loss= 1.47462
adv_loss= 1.15013
surrogate= 0.00389, entropy= 2.10303, loss= 0.00389
surrogate=-0.00493, entropy= 2.10004, loss=-0.00493
surrogate=-0.02465, entropy= 2.09956, loss=-0.02465
surrogate=-0.01594, entropy= 2.09737, loss=-0.01594
surrogate= 0.00679, entropy= 2.09744, loss= 0.00679
surrogate= 0.00552, entropy= 2.09628, loss= 0.00552
surrogate=-0.03381, entropy= 2.09685, loss=-0.03381
surrogate=-0.02599, entropy= 2.09681, loss=-0.02599
surrogate=-0.02341, entropy= 2.09777, loss=-0.02341
surrogate=-0.04136, entropy= 2.09435, loss=-0.04136
std_min= 0.40236, std_max= 0.55300, std_mean= 0.49079
val lr: [0.00017802254098360656], policy lr: [0.00021362704918032787]
Policy Loss: -0.041362, | Entropy Bonus: -0, | Value Loss: 17.788, | Advantage Loss: 1.1501
Time elapsed (s): 1.7049024105072021
Agent stdevs: 0.49078548
--------------------------------------------------------------------------------

Step 281
++++++++ Policy training ++++++++++
Current mean reward: 1494.883415 | mean episode length: 433.500000
val_loss=19.19569
val_loss=15.76389
val_loss=28.44748
val_loss=37.25421
val_loss=10.33791
val_loss=19.08201
val_loss= 9.88040
val_loss=17.18755
val_loss=13.54278
val_loss= 9.34983
adv_loss= 9.34110
adv_loss= 2.80796
adv_loss= 2.87631
adv_loss= 7.12462
adv_loss= 4.51405
adv_loss= 1.08275
adv_loss= 0.48521
adv_loss= 3.44362
adv_loss= 1.15735
adv_loss= 5.96265
surrogate= 0.01965, entropy= 2.09453, loss= 0.01965
surrogate= 0.00933, entropy= 2.09627, loss= 0.00933
surrogate=-0.02555, entropy= 2.09965, loss=-0.02555
surrogate=-0.00010, entropy= 2.09988, loss=-0.00010
surrogate=-0.00446, entropy= 2.10032, loss=-0.00446
surrogate= 0.02148, entropy= 2.10253, loss= 0.02148
surrogate=-0.01105, entropy= 2.10164, loss=-0.01105
surrogate=-0.02851, entropy= 2.10392, loss=-0.02851
surrogate=-0.03837, entropy= 2.10261, loss=-0.03837
surrogate=-0.01575, entropy= 2.10452, loss=-0.01575
std_min= 0.40692, std_max= 0.55516, std_mean= 0.49220
val lr: [0.00017776639344262296], policy lr: [0.00021331967213114754]
Policy Loss: -0.015754, | Entropy Bonus: -0, | Value Loss: 9.3498, | Advantage Loss: 5.9626
Time elapsed (s): 1.708024263381958
Agent stdevs: 0.49219576
--------------------------------------------------------------------------------

Step 282
++++++++ Policy training ++++++++++
Current mean reward: 1388.557691 | mean episode length: 401.750000
val_loss=17.43018
val_loss=21.91112
val_loss=27.46726
val_loss=19.06269
val_loss=10.17255
val_loss=13.72677
val_loss=11.42216
val_loss=17.50426
val_loss= 7.57792
val_loss=19.03287
adv_loss= 2.29021
adv_loss= 2.47387
adv_loss= 7.58355
adv_loss= 1.29958
adv_loss= 0.82839
adv_loss= 3.89633
adv_loss= 1.77785
adv_loss= 2.24850
adv_loss= 5.12925
adv_loss= 3.89541
surrogate= 0.01761, entropy= 2.10760, loss= 0.01761
surrogate=-0.00360, entropy= 2.10913, loss=-0.00360
surrogate=-0.02111, entropy= 2.10870, loss=-0.02111
surrogate=-0.02128, entropy= 2.10787, loss=-0.02128
surrogate=-0.00808, entropy= 2.10804, loss=-0.00808
surrogate=-0.02524, entropy= 2.10738, loss=-0.02524
surrogate=-0.02757, entropy= 2.10706, loss=-0.02757
surrogate=-0.00091, entropy= 2.10695, loss=-0.00091
surrogate=-0.02175, entropy= 2.10816, loss=-0.02175
surrogate=-0.04923, entropy= 2.10961, loss=-0.04923
std_min= 0.41054, std_max= 0.55485, std_mean= 0.49273
val lr: [0.00017751024590163933], policy lr: [0.0002130122950819672]
Policy Loss: -0.049227, | Entropy Bonus: -0, | Value Loss: 19.033, | Advantage Loss: 3.8954
Time elapsed (s): 1.662151575088501
Agent stdevs: 0.49272966
--------------------------------------------------------------------------------

Step 283
++++++++ Policy training ++++++++++
Current mean reward: 1214.869789 | mean episode length: 348.400000
val_loss=13.36731
val_loss=19.18263
val_loss=11.43276
val_loss=11.04370
val_loss=10.94091
val_loss= 9.81252
val_loss=14.32914
val_loss=12.77876
val_loss=15.91458
val_loss=10.19934
adv_loss= 2.77640
adv_loss= 1.74186
adv_loss= 1.49944
adv_loss= 1.31055
adv_loss= 2.46941
adv_loss= 1.89329
adv_loss= 2.48824
adv_loss= 1.03077
adv_loss= 0.90777
adv_loss= 1.27172
surrogate=-0.00467, entropy= 2.10937, loss=-0.00467
surrogate= 0.03203, entropy= 2.10700, loss= 0.03203
surrogate= 0.02647, entropy= 2.10716, loss= 0.02647
surrogate= 0.01006, entropy= 2.10476, loss= 0.01006
surrogate=-0.00109, entropy= 2.10167, loss=-0.00109
surrogate=-0.03243, entropy= 2.09942, loss=-0.03243
surrogate=-0.00834, entropy= 2.09735, loss=-0.00834
surrogate=-0.03384, entropy= 2.09502, loss=-0.03384
surrogate=-0.01261, entropy= 2.09381, loss=-0.01261
surrogate=-0.00361, entropy= 2.09240, loss=-0.00361
std_min= 0.40528, std_max= 0.55332, std_mean= 0.49020
val lr: [0.00017725409836065576], policy lr: [0.0002127049180327869]
Policy Loss: -0.0036115, | Entropy Bonus: -0, | Value Loss: 10.199, | Advantage Loss: 1.2717
Time elapsed (s): 1.6878788471221924
Agent stdevs: 0.49020424
--------------------------------------------------------------------------------

Step 284
++++++++ Policy training ++++++++++
Current mean reward: 2479.219276 | mean episode length: 730.500000
val_loss=1065.40149
val_loss=102.54024
val_loss=38.30141
val_loss=2466.74536
val_loss=87.66682
val_loss=39.63246
val_loss=1685.94360
val_loss=122.09985
val_loss=40.06978
val_loss=63.13149
adv_loss= 5.44482
adv_loss= 1.90955
adv_loss= 1.45653
adv_loss= 1.56515
adv_loss= 2.19933
adv_loss= 1.91606
adv_loss= 4.39875
adv_loss= 1.28614
adv_loss= 3.89199
adv_loss= 1.15217
surrogate=-0.00495, entropy= 2.09494, loss=-0.00495
surrogate=-0.00133, entropy= 2.09833, loss=-0.00133
surrogate=-0.01161, entropy= 2.10046, loss=-0.01161
surrogate=-0.02084, entropy= 2.10147, loss=-0.02084
surrogate= 0.02369, entropy= 2.10311, loss= 0.02369
surrogate=-0.02030, entropy= 2.10720, loss=-0.02030
surrogate=-0.00672, entropy= 2.11034, loss=-0.00672
surrogate= 0.00116, entropy= 2.11352, loss= 0.00116
surrogate=-0.01704, entropy= 2.11771, loss=-0.01704
surrogate=-0.02662, entropy= 2.11720, loss=-0.02662
std_min= 0.40987, std_max= 0.56080, std_mean= 0.49425
val lr: [0.00017699795081967213], policy lr: [0.00021239754098360654]
Policy Loss: -0.026622, | Entropy Bonus: -0, | Value Loss: 63.131, | Advantage Loss: 1.1522
Time elapsed (s): 1.6588623523712158
Agent stdevs: 0.4942478
--------------------------------------------------------------------------------

Step 285
++++++++ Policy training ++++++++++
Current mean reward: 2133.882727 | mean episode length: 610.500000
val_loss=13.17453
val_loss=17.35042
val_loss=14.77641
val_loss=20.29492
val_loss=12.58136
val_loss= 8.95376
val_loss=10.71008
val_loss=15.46924
val_loss=10.85964
val_loss= 9.29645
adv_loss= 1.35620
adv_loss= 1.70623
adv_loss= 1.08815
adv_loss= 1.12550
adv_loss= 1.32102
adv_loss= 0.98515
adv_loss= 1.01216
adv_loss= 1.10593
adv_loss= 1.60851
adv_loss= 2.14389
surrogate=-0.00078, entropy= 2.11889, loss=-0.00078
surrogate=-0.01847, entropy= 2.12110, loss=-0.01847
surrogate= 0.00866, entropy= 2.12287, loss= 0.00866
surrogate=-0.01808, entropy= 2.12652, loss=-0.01808
surrogate= 0.02454, entropy= 2.12673, loss= 0.02454
surrogate=-0.01864, entropy= 2.13080, loss=-0.01864
surrogate=-0.00687, entropy= 2.13252, loss=-0.00687
surrogate= 0.00482, entropy= 2.13334, loss= 0.00482
surrogate= 0.04041, entropy= 2.13407, loss= 0.04041
surrogate=-0.02305, entropy= 2.13566, loss=-0.02305
std_min= 0.41331, std_max= 0.56046, std_mean= 0.49713
val lr: [0.00017674180327868853], policy lr: [0.0002120901639344262]
Policy Loss: -0.023048, | Entropy Bonus: -0, | Value Loss: 9.2965, | Advantage Loss: 2.1439
Time elapsed (s): 1.6525487899780273
Agent stdevs: 0.49713126
--------------------------------------------------------------------------------

Step 286
++++++++ Policy training ++++++++++
Current mean reward: 1243.124055 | mean episode length: 354.800000
val_loss=22.78728
val_loss=25.87757
val_loss=14.44469
val_loss=16.64972
val_loss=15.06434
val_loss=14.28457
val_loss=16.03825
val_loss=14.05503
val_loss= 7.90489
val_loss=18.24869
adv_loss= 1.40619
adv_loss= 2.65712
adv_loss= 1.76611
adv_loss= 1.32166
adv_loss= 2.54739
adv_loss= 3.94023
adv_loss= 2.62385
adv_loss= 4.10827
adv_loss= 3.91207
adv_loss= 1.57176
surrogate=-0.02323, entropy= 2.13388, loss=-0.02323
surrogate= 0.01512, entropy= 2.12950, loss= 0.01512
surrogate=-0.01110, entropy= 2.12687, loss=-0.01110
surrogate= 0.00668, entropy= 2.12610, loss= 0.00668
surrogate=-0.04200, entropy= 2.12213, loss=-0.04200
surrogate=-0.00396, entropy= 2.12200, loss=-0.00396
surrogate=-0.00721, entropy= 2.11957, loss=-0.00721
surrogate=-0.02864, entropy= 2.11919, loss=-0.02864
surrogate=-0.02076, entropy= 2.11790, loss=-0.02076
surrogate=-0.01989, entropy= 2.11592, loss=-0.01989
std_min= 0.41124, std_max= 0.55205, std_mean= 0.49369
val lr: [0.00017648565573770493], policy lr: [0.00021178278688524589]
Policy Loss: -0.019887, | Entropy Bonus: -0, | Value Loss: 18.249, | Advantage Loss: 1.5718
Time elapsed (s): 1.6757144927978516
Agent stdevs: 0.4936882
--------------------------------------------------------------------------------

Step 287
++++++++ Policy training ++++++++++
Current mean reward: 1648.254924 | mean episode length: 468.500000
val_loss=13.76203
val_loss=12.33556
val_loss= 9.21042
val_loss=10.92082
val_loss=10.04510
val_loss= 5.61381
val_loss= 7.56722
val_loss=12.30890
val_loss=14.71302
val_loss= 8.32484
adv_loss= 1.60298
adv_loss= 2.87095
adv_loss= 1.21847
adv_loss= 0.90880
adv_loss= 1.30384
adv_loss= 1.61681
adv_loss= 2.00278
adv_loss= 2.42306
adv_loss= 5.04986
adv_loss= 2.31293
surrogate= 0.00695, entropy= 2.11612, loss= 0.00695
surrogate= 0.01228, entropy= 2.11549, loss= 0.01228
surrogate= 0.02130, entropy= 2.11506, loss= 0.02130
surrogate= 0.01544, entropy= 2.11348, loss= 0.01544
surrogate=-0.00560, entropy= 2.11216, loss=-0.00560
surrogate=-0.00811, entropy= 2.11149, loss=-0.00811
surrogate=-0.03373, entropy= 2.11072, loss=-0.03373
surrogate=-0.02619, entropy= 2.10785, loss=-0.02619
surrogate=-0.04362, entropy= 2.10684, loss=-0.04362
surrogate=-0.01856, entropy= 2.10418, loss=-0.01856
std_min= 0.41144, std_max= 0.54914, std_mean= 0.49160
val lr: [0.00017622950819672133], policy lr: [0.00021147540983606556]
Policy Loss: -0.018556, | Entropy Bonus: -0, | Value Loss: 8.3248, | Advantage Loss: 2.3129
Time elapsed (s): 1.6732230186462402
Agent stdevs: 0.49159822
--------------------------------------------------------------------------------

Step 288
++++++++ Policy training ++++++++++
Current mean reward: 1368.759431 | mean episode length: 389.500000
val_loss=15.38287
val_loss=11.97087
val_loss=13.51457
val_loss=10.06207
val_loss=16.38609
val_loss=19.10257
val_loss= 8.63010
val_loss=16.79170
val_loss= 7.02003
val_loss= 7.26728
adv_loss= 1.44102
adv_loss= 2.87190
adv_loss= 0.85343
adv_loss= 1.60235
adv_loss= 1.19022
adv_loss= 1.49389
adv_loss= 1.24193
adv_loss= 1.79138
adv_loss= 0.52353
adv_loss= 1.29118
surrogate= 0.02148, entropy= 2.10596, loss= 0.02148
surrogate=-0.00204, entropy= 2.10664, loss=-0.00204
surrogate=-0.01981, entropy= 2.10814, loss=-0.01981
surrogate=-0.00029, entropy= 2.10869, loss=-0.00029
surrogate=-0.01949, entropy= 2.10825, loss=-0.01949
surrogate=-0.03074, entropy= 2.10821, loss=-0.03074
surrogate=-0.00344, entropy= 2.10851, loss=-0.00344
surrogate=-0.02755, entropy= 2.10722, loss=-0.02755
surrogate=-0.02599, entropy= 2.10648, loss=-0.02599
surrogate=-0.05174, entropy= 2.10530, loss=-0.05174
std_min= 0.41276, std_max= 0.54934, std_mean= 0.49168
val lr: [0.0001759733606557377], policy lr: [0.00021116803278688524]
Policy Loss: -0.051739, | Entropy Bonus: -0, | Value Loss: 7.2673, | Advantage Loss: 1.2912
Time elapsed (s): 1.6518971920013428
Agent stdevs: 0.49167934
--------------------------------------------------------------------------------

Step 289
++++++++ Policy training ++++++++++
Current mean reward: 1717.755468 | mean episode length: 501.500000
val_loss=59.22638
val_loss=21.70523
val_loss=39.70424
val_loss=27.08439
val_loss=10.32645
val_loss=18.38049
val_loss=19.04762
val_loss=11.67981
val_loss=26.25879
val_loss=14.38973
adv_loss= 2.52623
adv_loss= 2.11203
adv_loss= 3.67548
adv_loss= 1.03732
adv_loss= 3.06286
adv_loss= 1.62014
adv_loss= 3.94773
adv_loss= 3.79049
adv_loss= 2.38820
adv_loss= 2.80817
surrogate= 0.01506, entropy= 2.10632, loss= 0.01506
surrogate=-0.00284, entropy= 2.10845, loss=-0.00284
surrogate= 0.00731, entropy= 2.10831, loss= 0.00731
surrogate= 0.00516, entropy= 2.10938, loss= 0.00516
surrogate=-0.01699, entropy= 2.11296, loss=-0.01699
surrogate=-0.04204, entropy= 2.11585, loss=-0.04204
surrogate=-0.01839, entropy= 2.11875, loss=-0.01839
surrogate=-0.01209, entropy= 2.11956, loss=-0.01209
surrogate= 0.00070, entropy= 2.12056, loss= 0.00070
surrogate=-0.00831, entropy= 2.12325, loss=-0.00831
std_min= 0.41095, std_max= 0.55509, std_mean= 0.49506
val lr: [0.00017571721311475407], policy lr: [0.00021086065573770488]
Policy Loss: -0.0083112, | Entropy Bonus: -0, | Value Loss: 14.39, | Advantage Loss: 2.8082
Time elapsed (s): 1.6479544639587402
Agent stdevs: 0.49505606
--------------------------------------------------------------------------------

Step 290
++++++++ Policy training ++++++++++
Current mean reward: 2016.037520 | mean episode length: 588.000000
val_loss=214.27356
val_loss=1346.74976
val_loss=59.31293
val_loss=1310.95874
val_loss=55.48368
val_loss=122.77484
val_loss=48.42777
val_loss=82.34180
val_loss=131.14705
val_loss=309.75140
adv_loss= 1.23820
adv_loss= 1.66145
adv_loss= 1.76794
adv_loss= 6.68833
adv_loss= 1.55644
adv_loss=1393.16052
adv_loss= 3.23663
adv_loss= 1.48568
adv_loss= 1.48299
adv_loss= 4.00418
surrogate=-0.01077, entropy= 2.12756, loss=-0.01077
surrogate=-0.02800, entropy= 2.13041, loss=-0.02800
surrogate=-0.03000, entropy= 2.13330, loss=-0.03000
surrogate=-0.02632, entropy= 2.13595, loss=-0.02632
surrogate= 0.03122, entropy= 2.13951, loss= 0.03122
surrogate=-0.00793, entropy= 2.14124, loss=-0.00793
surrogate=-0.00130, entropy= 2.14273, loss=-0.00130
surrogate=-0.02154, entropy= 2.14420, loss=-0.02154
surrogate= 0.00920, entropy= 2.14594, loss= 0.00920
surrogate=-0.00550, entropy= 2.14789, loss=-0.00550
std_min= 0.41459, std_max= 0.56141, std_mean= 0.49914
val lr: [0.0001754610655737705], policy lr: [0.00021055327868852458]
Policy Loss: -0.0054959, | Entropy Bonus: -0, | Value Loss: 309.75, | Advantage Loss: 4.0042
Time elapsed (s): 1.6415128707885742
Agent stdevs: 0.499144
--------------------------------------------------------------------------------

Step 291
++++++++ Policy training ++++++++++
Current mean reward: 2122.259390 | mean episode length: 646.000000
val_loss=278.76288
val_loss=36.12282
val_loss=39.13197
val_loss=474.61014
val_loss=40.88313
val_loss=28.36385
val_loss=756.40668
val_loss=213.51378
val_loss=396.85281
val_loss=58.42972
adv_loss= 5.04369
adv_loss= 4.64198
adv_loss= 2.62896
adv_loss= 2.54483
adv_loss= 2.57090
adv_loss= 1.42464
adv_loss= 6.60263
adv_loss= 1.25553
adv_loss= 2.59154
adv_loss= 2.09515
surrogate= 0.02528, entropy= 2.14732, loss= 0.02528
surrogate=-0.01055, entropy= 2.14341, loss=-0.01055
surrogate=-0.01010, entropy= 2.14027, loss=-0.01010
surrogate= 0.02711, entropy= 2.13938, loss= 0.02711
surrogate=-0.01572, entropy= 2.13926, loss=-0.01572
surrogate=-0.02246, entropy= 2.13723, loss=-0.02246
surrogate=-0.00421, entropy= 2.13735, loss=-0.00421
surrogate= 0.01905, entropy= 2.13829, loss= 0.01905
surrogate= 0.00963, entropy= 2.13717, loss= 0.00963
surrogate= 0.00015, entropy= 2.13408, loss= 0.00015
std_min= 0.41175, std_max= 0.55970, std_mean= 0.49696
val lr: [0.00017520491803278687], policy lr: [0.00021024590163934423]
Policy Loss: 0.00014596, | Entropy Bonus: -0, | Value Loss: 58.43, | Advantage Loss: 2.0952
Time elapsed (s): 1.642326831817627
Agent stdevs: 0.49695572
--------------------------------------------------------------------------------

Step 292
++++++++ Policy training ++++++++++
Current mean reward: 2243.376846 | mean episode length: 639.000000
val_loss=17.00201
val_loss=15.32757
val_loss=24.04100
val_loss=14.42698
val_loss= 6.94405
val_loss=15.51119
val_loss=12.06413
val_loss=10.27341
val_loss=16.52182
val_loss= 9.74427
adv_loss= 2.54866
adv_loss= 3.03141
adv_loss= 1.96762
adv_loss= 2.39600
adv_loss= 1.74789
adv_loss= 1.81900
adv_loss= 1.48800
adv_loss= 1.78443
adv_loss= 2.92986
adv_loss= 4.14858
surrogate=-0.01635, entropy= 2.13423, loss=-0.01635
surrogate= 0.00250, entropy= 2.13253, loss= 0.00250
surrogate= 0.00734, entropy= 2.13005, loss= 0.00734
surrogate=-0.00382, entropy= 2.12778, loss=-0.00382
surrogate=-0.03978, entropy= 2.12778, loss=-0.03978
surrogate=-0.01688, entropy= 2.12652, loss=-0.01688
surrogate=-0.01029, entropy= 2.12592, loss=-0.01029
surrogate= 0.00419, entropy= 2.12561, loss= 0.00419
surrogate=-0.01578, entropy= 2.12351, loss=-0.01578
surrogate=-0.01429, entropy= 2.12369, loss=-0.01429
std_min= 0.41095, std_max= 0.55507, std_mean= 0.49511
val lr: [0.0001749487704918033], policy lr: [0.00020993852459016393]
Policy Loss: -0.014288, | Entropy Bonus: -0, | Value Loss: 9.7443, | Advantage Loss: 4.1486
Time elapsed (s): 1.6876184940338135
Agent stdevs: 0.49511096
--------------------------------------------------------------------------------

Step 293
++++++++ Policy training ++++++++++
Current mean reward: 1424.163140 | mean episode length: 405.600000
val_loss=18.59905
val_loss=22.84699
val_loss=10.56701
val_loss=15.23399
val_loss=11.45266
val_loss=12.87405
val_loss=10.96367
val_loss=11.70987
val_loss= 6.79669
val_loss= 9.61041
adv_loss= 5.03155
adv_loss= 2.41903
adv_loss= 3.33673
adv_loss= 3.45886
adv_loss= 4.74404
adv_loss= 2.86113
adv_loss= 4.66584
adv_loss= 2.05576
adv_loss= 4.81038
adv_loss= 2.16939
surrogate=-0.00016, entropy= 2.12062, loss=-0.00016
surrogate= 0.01333, entropy= 2.11923, loss= 0.01333
surrogate= 0.00803, entropy= 2.11552, loss= 0.00803
surrogate= 0.00419, entropy= 2.11400, loss= 0.00419
surrogate=-0.02739, entropy= 2.11117, loss=-0.02739
surrogate=-0.01302, entropy= 2.11266, loss=-0.01302
surrogate=-0.03715, entropy= 2.11094, loss=-0.03715
surrogate=-0.03584, entropy= 2.11220, loss=-0.03584
surrogate=-0.03574, entropy= 2.11028, loss=-0.03574
surrogate=-0.03577, entropy= 2.11057, loss=-0.03577
std_min= 0.41155, std_max= 0.54997, std_mean= 0.49271
val lr: [0.00017469262295081967], policy lr: [0.00020963114754098358]
Policy Loss: -0.035775, | Entropy Bonus: -0, | Value Loss: 9.6104, | Advantage Loss: 2.1694
Time elapsed (s): 1.6754670143127441
Agent stdevs: 0.4927098
--------------------------------------------------------------------------------

Step 294
++++++++ Policy training ++++++++++
Current mean reward: 1147.936728 | mean episode length: 329.500000
val_loss=23.47375
val_loss=14.76670
val_loss=27.79333
val_loss=21.86936
val_loss=17.73153
val_loss=21.01796
val_loss=12.57505
val_loss=17.48910
val_loss=12.47286
val_loss=11.20079
adv_loss= 3.49103
adv_loss= 2.12580
adv_loss= 3.01089
adv_loss= 3.32574
adv_loss= 3.30245
adv_loss= 4.72618
adv_loss= 4.00321
adv_loss= 2.47423
adv_loss= 1.83535
adv_loss= 5.97567
surrogate= 0.00639, entropy= 2.10933, loss= 0.00639
surrogate=-0.01759, entropy= 2.10794, loss=-0.01759
surrogate=-0.00139, entropy= 2.10463, loss=-0.00139
surrogate=-0.00019, entropy= 2.10204, loss=-0.00019
surrogate= 0.01831, entropy= 2.10022, loss= 0.01831
surrogate=-0.01481, entropy= 2.09919, loss=-0.01481
surrogate=-0.00597, entropy= 2.09807, loss=-0.00597
surrogate=-0.00923, entropy= 2.09814, loss=-0.00923
surrogate=-0.01625, entropy= 2.09513, loss=-0.01625
surrogate=-0.01920, entropy= 2.09547, loss=-0.01920
std_min= 0.40653, std_max= 0.55162, std_mean= 0.49056
val lr: [0.00017443647540983607], policy lr: [0.00020932377049180325]
Policy Loss: -0.019199, | Entropy Bonus: -0, | Value Loss: 11.201, | Advantage Loss: 5.9757
Time elapsed (s): 1.6619937419891357
Agent stdevs: 0.49055624
--------------------------------------------------------------------------------

Step 295
++++++++ Policy training ++++++++++
Current mean reward: 1602.243764 | mean episode length: 455.250000
val_loss=16.04726
val_loss=10.82272
val_loss= 8.48940
val_loss=13.57094
val_loss=10.75721
val_loss= 8.87549
val_loss= 8.43298
val_loss=10.09458
val_loss=13.00613
val_loss= 7.10236
adv_loss= 1.23247
adv_loss= 4.39859
adv_loss= 1.53451
adv_loss= 3.27577
adv_loss= 1.82699
adv_loss= 5.33844
adv_loss= 1.49999
adv_loss= 2.60399
adv_loss= 1.67498
adv_loss= 1.68542
surrogate= 0.01832, entropy= 2.09333, loss= 0.01832
surrogate=-0.00146, entropy= 2.09026, loss=-0.00146
surrogate=-0.00719, entropy= 2.08782, loss=-0.00719
surrogate= 0.01752, entropy= 2.08567, loss= 0.01752
surrogate=-0.01791, entropy= 2.08313, loss=-0.01791
surrogate=-0.01480, entropy= 2.08255, loss=-0.01480
surrogate=-0.01171, entropy= 2.08046, loss=-0.01171
surrogate=-0.01574, entropy= 2.07798, loss=-0.01574
surrogate=-0.04777, entropy= 2.07626, loss=-0.04777
surrogate=-0.01528, entropy= 2.07593, loss=-0.01528
std_min= 0.40058, std_max= 0.55340, std_mean= 0.48781
val lr: [0.00017418032786885247], policy lr: [0.00020901639344262293]
Policy Loss: -0.015284, | Entropy Bonus: -0, | Value Loss: 7.1024, | Advantage Loss: 1.6854
Time elapsed (s): 1.661933183670044
Agent stdevs: 0.48781052
--------------------------------------------------------------------------------

Step 296
++++++++ Policy training ++++++++++
Current mean reward: 1602.954555 | mean episode length: 453.500000
val_loss=46.27877
val_loss=17.96881
val_loss=18.31479
val_loss=18.74753
val_loss=18.57745
val_loss=24.30909
val_loss=20.29510
val_loss=28.16138
val_loss=17.60162
val_loss=26.78633
adv_loss= 0.87732
adv_loss= 5.46748
adv_loss= 9.06862
adv_loss= 7.54829
adv_loss= 2.88667
adv_loss= 2.19595
adv_loss=10.26533
adv_loss= 1.53106
adv_loss=10.51661
adv_loss= 1.53387
surrogate= 0.00345, entropy= 2.07520, loss= 0.00345
surrogate= 0.02420, entropy= 2.07303, loss= 0.02420
surrogate= 0.02040, entropy= 2.07349, loss= 0.02040
surrogate=-0.00655, entropy= 2.07096, loss=-0.00655
surrogate=-0.01064, entropy= 2.07161, loss=-0.01064
surrogate=-0.00273, entropy= 2.06990, loss=-0.00273
surrogate=-0.04168, entropy= 2.06960, loss=-0.04168
surrogate=-0.01283, entropy= 2.06672, loss=-0.01283
surrogate= 0.00203, entropy= 2.06654, loss= 0.00203
surrogate=-0.01327, entropy= 2.06783, loss=-0.01327
std_min= 0.40104, std_max= 0.56056, std_mean= 0.48666
val lr: [0.00017392418032786885], policy lr: [0.0002087090163934426]
Policy Loss: -0.013275, | Entropy Bonus: -0, | Value Loss: 26.786, | Advantage Loss: 1.5339
Time elapsed (s): 1.6401994228363037
Agent stdevs: 0.4866561
--------------------------------------------------------------------------------

Step 297
++++++++ Policy training ++++++++++
Current mean reward: 1106.365815 | mean episode length: 312.500000
val_loss=10.14553
val_loss=14.00779
val_loss=14.11749
val_loss=12.64415
val_loss=13.00681
val_loss=10.50782
val_loss=11.58323
val_loss=12.44625
val_loss=14.91420
val_loss= 5.38546
adv_loss= 3.09524
adv_loss= 0.83760
adv_loss= 2.52530
adv_loss= 1.86710
adv_loss= 4.66319
adv_loss= 2.49925
adv_loss= 4.76218
adv_loss= 3.60715
adv_loss= 1.30962
adv_loss= 1.46999
surrogate= 0.00614, entropy= 2.07314, loss= 0.00614
surrogate= 0.00546, entropy= 2.07482, loss= 0.00546
surrogate=-0.01385, entropy= 2.07463, loss=-0.01385
surrogate= 0.00113, entropy= 2.07532, loss= 0.00113
surrogate= 0.00655, entropy= 2.07556, loss= 0.00655
surrogate=-0.03624, entropy= 2.07416, loss=-0.03624
surrogate=-0.04175, entropy= 2.07452, loss=-0.04175
surrogate=-0.04559, entropy= 2.07416, loss=-0.04559
surrogate=-0.01062, entropy= 2.07459, loss=-0.01062
surrogate=-0.03440, entropy= 2.07450, loss=-0.03440
std_min= 0.40393, std_max= 0.55571, std_mean= 0.48735
val lr: [0.00017366803278688525], policy lr: [0.00020840163934426228]
Policy Loss: -0.0344, | Entropy Bonus: -0, | Value Loss: 5.3855, | Advantage Loss: 1.47
Time elapsed (s): 1.6358649730682373
Agent stdevs: 0.48734808
--------------------------------------------------------------------------------

Step 298
++++++++ Policy training ++++++++++
Current mean reward: 1158.708175 | mean episode length: 340.333333
val_loss=218.93739
val_loss=680.83417
val_loss=50.19246
val_loss=155.72131
val_loss=80.73796
val_loss=164.08734
val_loss=660.84125
val_loss=59.84779
val_loss=614.34332
val_loss=48.03989
adv_loss=13.86453
adv_loss=11.70123
adv_loss=14.40010
adv_loss=12.05474
adv_loss=13.84146
adv_loss=15.07884
adv_loss=11.77016
adv_loss= 5.00191
adv_loss=15.59471
adv_loss= 9.61046
surrogate= 0.01630, entropy= 2.07431, loss= 0.01630
surrogate=-0.02186, entropy= 2.07368, loss=-0.02186
surrogate=-0.02260, entropy= 2.07404, loss=-0.02260
surrogate= 0.00178, entropy= 2.07459, loss= 0.00178
surrogate=-0.00859, entropy= 2.07369, loss=-0.00859
surrogate=-0.02703, entropy= 2.07369, loss=-0.02703
surrogate=-0.01002, entropy= 2.07421, loss=-0.01002
surrogate=-0.02423, entropy= 2.07496, loss=-0.02423
surrogate=-0.01577, entropy= 2.07512, loss=-0.01577
surrogate=-0.02714, entropy= 2.07677, loss=-0.02714
std_min= 0.40278, std_max= 0.55624, std_mean= 0.48786
val lr: [0.00017341188524590162], policy lr: [0.00020809426229508193]
Policy Loss: -0.027144, | Entropy Bonus: -0, | Value Loss: 48.04, | Advantage Loss: 9.6105
Time elapsed (s): 1.6921324729919434
Agent stdevs: 0.48785993
--------------------------------------------------------------------------------

Step 299
++++++++ Policy training ++++++++++
Current mean reward: 1497.605414 | mean episode length: 420.750000
val_loss=44.65485
val_loss=35.58606
val_loss=16.18837
val_loss=18.43389
val_loss=12.21012
val_loss=24.43563
val_loss=12.55957
val_loss=17.07653
val_loss=11.67212
val_loss= 5.99493
adv_loss= 2.75542
adv_loss= 2.40900
adv_loss= 4.90007
adv_loss= 4.10399
adv_loss= 2.54698
adv_loss= 5.29914
adv_loss= 3.31241
adv_loss= 3.39694
adv_loss= 3.07693
adv_loss= 2.30907
surrogate= 0.00681, entropy= 2.07755, loss= 0.00681
surrogate= 0.00851, entropy= 2.07552, loss= 0.00851
surrogate= 0.00343, entropy= 2.07347, loss= 0.00343
surrogate=-0.00718, entropy= 2.07283, loss=-0.00718
surrogate=-0.02613, entropy= 2.07097, loss=-0.02613
surrogate=-0.01695, entropy= 2.06835, loss=-0.01695
surrogate=-0.00626, entropy= 2.06860, loss=-0.00626
surrogate= 0.01224, entropy= 2.06549, loss= 0.01224
surrogate=-0.05459, entropy= 2.06281, loss=-0.05459
surrogate=-0.00172, entropy= 2.06198, loss=-0.00172
std_min= 0.40277, std_max= 0.54910, std_mean= 0.48515
val lr: [0.00017315573770491804], policy lr: [0.00020778688524590163]
Policy Loss: -0.001715, | Entropy Bonus: -0, | Value Loss: 5.9949, | Advantage Loss: 2.3091
Time elapsed (s): 1.678440809249878
Agent stdevs: 0.48514828
--------------------------------------------------------------------------------

Step 300
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1393.6
++++++++ Policy training ++++++++++
Current mean reward: 1452.478586 | mean episode length: 412.666667
val_loss=20.10075
val_loss=29.15604
val_loss=12.68758
val_loss=11.78227
val_loss=14.56393
val_loss= 7.29462
val_loss= 8.38680
val_loss=57.95773
val_loss=16.61634
val_loss=14.03934
adv_loss= 2.94075
adv_loss= 7.48167
adv_loss=14.41696
adv_loss= 1.22593
adv_loss= 2.03023
adv_loss= 1.66267
adv_loss= 2.82775
adv_loss= 1.59926
adv_loss= 0.93831
adv_loss=12.89724
surrogate= 0.00029, entropy= 2.06144, loss= 0.00029
surrogate= 0.00210, entropy= 2.05874, loss= 0.00210
surrogate=-0.03620, entropy= 2.05755, loss=-0.03620
surrogate=-0.00461, entropy= 2.05784, loss=-0.00461
surrogate= 0.00321, entropy= 2.05877, loss= 0.00321
surrogate= 0.01064, entropy= 2.05939, loss= 0.01064
surrogate=-0.01919, entropy= 2.06143, loss=-0.01919
surrogate=-0.01282, entropy= 2.06151, loss=-0.01282
surrogate=-0.03762, entropy= 2.06370, loss=-0.03762
surrogate=-0.01909, entropy= 2.06354, loss=-0.01909
std_min= 0.40560, std_max= 0.54981, std_mean= 0.48519
val lr: [0.00017289959016393442], policy lr: [0.00020747950819672127]
Policy Loss: -0.019085, | Entropy Bonus: -0, | Value Loss: 14.039, | Advantage Loss: 12.897
Time elapsed (s): 1.6462328433990479
Agent stdevs: 0.48519447
--------------------------------------------------------------------------------

Step 301
++++++++ Policy training ++++++++++
Current mean reward: 2060.759125 | mean episode length: 600.666667
val_loss=40.79789
val_loss=22.64039
val_loss=312.93057
val_loss=64.68294
val_loss=187.28049
val_loss=343.15030
val_loss=29.68826
val_loss=151.57990
val_loss=67.11539
val_loss=675.67072
adv_loss= 4.58956
adv_loss= 2.21496
adv_loss= 4.49685
adv_loss= 3.42025
adv_loss=1664.68372
adv_loss= 1.55347
adv_loss= 1.82421
adv_loss= 2.16177
adv_loss= 1.03295
adv_loss=1660.98364
surrogate=-0.00205, entropy= 2.06189, loss=-0.00205
surrogate= 0.01257, entropy= 2.06335, loss= 0.01257
surrogate=-0.03848, entropy= 2.06459, loss=-0.03848
surrogate= 0.00193, entropy= 2.06605, loss= 0.00193
surrogate= 0.01339, entropy= 2.06633, loss= 0.01339
surrogate=-0.01454, entropy= 2.06793, loss=-0.01454
surrogate=-0.00920, entropy= 2.06638, loss=-0.00920
surrogate=-0.03157, entropy= 2.06778, loss=-0.03157
surrogate=-0.01249, entropy= 2.06650, loss=-0.01249
surrogate=-0.02048, entropy= 2.06616, loss=-0.02048
std_min= 0.40934, std_max= 0.55043, std_mean= 0.48539
val lr: [0.00017264344262295084], policy lr: [0.00020717213114754098]
Policy Loss: -0.020483, | Entropy Bonus: -0, | Value Loss: 675.67, | Advantage Loss: 1661
Time elapsed (s): 1.6726646423339844
Agent stdevs: 0.4853944
--------------------------------------------------------------------------------

Step 302
++++++++ Policy training ++++++++++
Current mean reward: 1187.904807 | mean episode length: 338.666667
val_loss=22.29405
val_loss=14.56638
val_loss= 6.25672
val_loss=11.60349
val_loss=10.33440
val_loss=15.18471
val_loss=16.52290
val_loss=13.41224
val_loss=15.55851
val_loss= 9.31827
adv_loss= 1.57774
adv_loss= 3.94584
adv_loss= 1.75002
adv_loss= 4.16817
adv_loss= 2.48489
adv_loss= 6.76977
adv_loss= 2.47630
adv_loss= 2.58881
adv_loss= 3.25605
adv_loss= 2.85140
surrogate= 0.02209, entropy= 2.06607, loss= 0.02209
surrogate= 0.00407, entropy= 2.06739, loss= 0.00407
surrogate=-0.00354, entropy= 2.06935, loss=-0.00354
surrogate=-0.01729, entropy= 2.07129, loss=-0.01729
surrogate=-0.01170, entropy= 2.07257, loss=-0.01170
surrogate= 0.00932, entropy= 2.07333, loss= 0.00932
surrogate=-0.02803, entropy= 2.07533, loss=-0.02803
surrogate=-0.01587, entropy= 2.07505, loss=-0.01587
surrogate=-0.03936, entropy= 2.07759, loss=-0.03936
surrogate=-0.02121, entropy= 2.07801, loss=-0.02121
std_min= 0.41330, std_max= 0.55122, std_mean= 0.48710
val lr: [0.00017238729508196722], policy lr: [0.00020686475409836062]
Policy Loss: -0.021211, | Entropy Bonus: -0, | Value Loss: 9.3183, | Advantage Loss: 2.8514
Time elapsed (s): 1.6351885795593262
Agent stdevs: 0.48710153
--------------------------------------------------------------------------------

Step 303
++++++++ Policy training ++++++++++
Current mean reward: 2412.240433 | mean episode length: 686.000000
val_loss=10.16415
val_loss= 8.81157
val_loss=11.38405
val_loss=10.11590
val_loss=12.51105
val_loss= 7.04978
val_loss= 8.89715
val_loss= 7.56000
val_loss= 8.42525
val_loss= 9.14970
adv_loss= 1.53084
adv_loss= 2.79526
adv_loss= 1.38919
adv_loss= 1.85267
adv_loss= 1.07705
adv_loss= 2.24981
adv_loss= 2.65641
adv_loss= 1.36538
adv_loss= 1.36356
adv_loss= 2.03088
surrogate=-0.00267, entropy= 2.07925, loss=-0.00267
surrogate= 0.01228, entropy= 2.08017, loss= 0.01228
surrogate= 0.01073, entropy= 2.08145, loss= 0.01073
surrogate=-0.00677, entropy= 2.08118, loss=-0.00677
surrogate= 0.01802, entropy= 2.08285, loss= 0.01802
surrogate=-0.02497, entropy= 2.08523, loss=-0.02497
surrogate=-0.01223, entropy= 2.08376, loss=-0.01223
surrogate=-0.03108, entropy= 2.08533, loss=-0.03108
surrogate=-0.01781, entropy= 2.08709, loss=-0.01781
surrogate=-0.00649, entropy= 2.08686, loss=-0.00649
std_min= 0.41403, std_max= 0.55931, std_mean= 0.48881
val lr: [0.00017213114754098362], policy lr: [0.0002065573770491803]
Policy Loss: -0.0064892, | Entropy Bonus: -0, | Value Loss: 9.1497, | Advantage Loss: 2.0309
Time elapsed (s): 1.634552240371704
Agent stdevs: 0.48880827
--------------------------------------------------------------------------------

Step 304
++++++++ Policy training ++++++++++
Current mean reward: 1763.344655 | mean episode length: 502.500000
val_loss=15.38761
val_loss=17.55782
val_loss=14.92694
val_loss= 9.90118
val_loss=21.13067
val_loss=15.40362
val_loss= 9.29562
val_loss=11.41351
val_loss= 7.18885
val_loss=10.55083
adv_loss= 3.09282
adv_loss= 1.78520
adv_loss= 1.92638
adv_loss= 1.53229
adv_loss= 0.66952
adv_loss= 3.88211
adv_loss= 2.56988
adv_loss= 4.87389
adv_loss= 1.34071
adv_loss= 4.74606
surrogate=-0.02233, entropy= 2.08836, loss=-0.02233
surrogate= 0.00955, entropy= 2.08735, loss= 0.00955
surrogate= 0.00128, entropy= 2.08886, loss= 0.00128
surrogate= 0.00274, entropy= 2.08980, loss= 0.00274
surrogate=-0.02472, entropy= 2.08947, loss=-0.02472
surrogate=-0.01283, entropy= 2.09011, loss=-0.01283
surrogate=-0.03642, entropy= 2.08919, loss=-0.03642
surrogate= 0.00140, entropy= 2.08828, loss= 0.00140
surrogate=-0.02292, entropy= 2.08815, loss=-0.02292
surrogate=-0.02982, entropy= 2.08836, loss=-0.02982
std_min= 0.41656, std_max= 0.55709, std_mean= 0.48880
val lr: [0.000171875], policy lr: [0.00020624999999999997]
Policy Loss: -0.029824, | Entropy Bonus: -0, | Value Loss: 10.551, | Advantage Loss: 4.7461
Time elapsed (s): 1.6654508113861084
Agent stdevs: 0.48879585
--------------------------------------------------------------------------------

Step 305
++++++++ Policy training ++++++++++
Current mean reward: 1109.995856 | mean episode length: 313.333333
val_loss= 9.21595
val_loss=10.59093
val_loss=13.60001
val_loss= 7.10249
val_loss=11.65199
val_loss= 9.43594
val_loss=10.83833
val_loss=14.57528
val_loss=14.68987
val_loss= 7.57942
adv_loss= 1.59418
adv_loss= 4.14179
adv_loss= 1.54272
adv_loss= 1.98050
adv_loss= 3.39217
adv_loss= 3.73714
adv_loss= 1.95761
adv_loss= 2.97035
adv_loss= 3.33894
adv_loss= 2.24261
surrogate= 0.00251, entropy= 2.08705, loss= 0.00251
surrogate= 0.01762, entropy= 2.08517, loss= 0.01762
surrogate=-0.01452, entropy= 2.08432, loss=-0.01452
surrogate=-0.02603, entropy= 2.08563, loss=-0.02603
surrogate=-0.01046, entropy= 2.08397, loss=-0.01046
surrogate= 0.01076, entropy= 2.08191, loss= 0.01076
surrogate=-0.01273, entropy= 2.08059, loss=-0.01273
surrogate=-0.00525, entropy= 2.07963, loss=-0.00525
surrogate=-0.00575, entropy= 2.07952, loss=-0.00575
surrogate= 0.01162, entropy= 2.07807, loss= 0.01162
std_min= 0.41218, std_max= 0.56019, std_mean= 0.48753
val lr: [0.0001716188524590164], policy lr: [0.00020594262295081965]
Policy Loss: 0.011619, | Entropy Bonus: -0, | Value Loss: 7.5794, | Advantage Loss: 2.2426
Time elapsed (s): 1.692892074584961
Agent stdevs: 0.4875258
--------------------------------------------------------------------------------

Step 306
++++++++ Policy training ++++++++++
Current mean reward: 1906.628785 | mean episode length: 546.666667
val_loss=16.07225
val_loss=13.13147
val_loss=13.01098
val_loss=18.36500
val_loss=21.77080
val_loss=15.82557
val_loss= 9.61142
val_loss=14.02265
val_loss=13.06215
val_loss=11.47455
adv_loss= 5.01301
adv_loss= 2.27643
adv_loss= 4.92387
adv_loss= 0.62011
adv_loss= 4.23317
adv_loss= 1.34765
adv_loss= 1.52521
adv_loss= 1.08083
adv_loss= 1.29364
adv_loss= 1.69793
surrogate= 0.02481, entropy= 2.07749, loss= 0.02481
surrogate=-0.02345, entropy= 2.07597, loss=-0.02345
surrogate=-0.01999, entropy= 2.07225, loss=-0.01999
surrogate= 0.00734, entropy= 2.07136, loss= 0.00734
surrogate=-0.01525, entropy= 2.07057, loss=-0.01525
surrogate=-0.01552, entropy= 2.06806, loss=-0.01552
surrogate=-0.02924, entropy= 2.06686, loss=-0.02924
surrogate=-0.02934, entropy= 2.06679, loss=-0.02934
surrogate=-0.03476, entropy= 2.06578, loss=-0.03476
surrogate=-0.03499, entropy= 2.06504, loss=-0.03499
std_min= 0.40870, std_max= 0.55780, std_mean= 0.48551
val lr: [0.0001713627049180328], policy lr: [0.00020563524590163932]
Policy Loss: -0.034994, | Entropy Bonus: -0, | Value Loss: 11.475, | Advantage Loss: 1.6979
Time elapsed (s): 1.6797456741333008
Agent stdevs: 0.48551187
--------------------------------------------------------------------------------

Step 307
++++++++ Policy training ++++++++++
Current mean reward: 1802.969257 | mean episode length: 532.000000
val_loss=20.20836
val_loss=1649.24792
val_loss=94.76488
val_loss=1214.96729
val_loss=313.44177
val_loss=331.00415
val_loss=1433.05762
val_loss=1385.86963
val_loss=894.12823
val_loss=23.88595
adv_loss= 2.22735
adv_loss= 2.16243
adv_loss= 1.83740
adv_loss= 2.64436
adv_loss= 2.12070
adv_loss= 1.47336
adv_loss= 1.70817
adv_loss= 2.05040
adv_loss= 1.67800
adv_loss= 1.87586
surrogate=-0.01164, entropy= 2.06543, loss=-0.01164
surrogate= 0.00497, entropy= 2.06680, loss= 0.00497
surrogate= 0.00350, entropy= 2.06774, loss= 0.00350
surrogate=-0.00837, entropy= 2.06949, loss=-0.00837
surrogate=-0.01207, entropy= 2.06930, loss=-0.01207
surrogate=-0.00205, entropy= 2.07155, loss=-0.00205
surrogate= 0.01005, entropy= 2.07355, loss= 0.01005
surrogate=-0.01156, entropy= 2.07542, loss=-0.01156
surrogate= 0.02731, entropy= 2.07694, loss= 0.02731
surrogate=-0.02188, entropy= 2.07730, loss=-0.02188
std_min= 0.40624, std_max= 0.55851, std_mean= 0.48774
val lr: [0.00017110655737704916], policy lr: [0.00020532786885245897]
Policy Loss: -0.021882, | Entropy Bonus: -0, | Value Loss: 23.886, | Advantage Loss: 1.8759
Time elapsed (s): 1.6604645252227783
Agent stdevs: 0.48773924
--------------------------------------------------------------------------------

Step 308
++++++++ Policy training ++++++++++
Current mean reward: 1735.786292 | mean episode length: 503.250000
val_loss=133.29179
val_loss=48.35914
val_loss=541.91467
val_loss=61.75266
val_loss=79.19489
val_loss=66.09077
val_loss=95.68324
val_loss=558.13837
val_loss=196.95105
val_loss=32.10127
adv_loss= 5.37199
adv_loss= 2.36192
adv_loss= 4.82809
adv_loss= 1.59189
adv_loss= 1.00837
adv_loss= 1.65195
adv_loss= 1.58382
adv_loss= 3.59966
adv_loss= 3.81032
adv_loss= 1.18913
surrogate= 0.00173, entropy= 2.07795, loss= 0.00173
surrogate= 0.03097, entropy= 2.07756, loss= 0.03097
surrogate=-0.00770, entropy= 2.07826, loss=-0.00770
surrogate=-0.02323, entropy= 2.07787, loss=-0.02323
surrogate= 0.01346, entropy= 2.07842, loss= 0.01346
surrogate=-0.00412, entropy= 2.07903, loss=-0.00412
surrogate=-0.01150, entropy= 2.07828, loss=-0.01150
surrogate= 0.00475, entropy= 2.07958, loss= 0.00475
surrogate=-0.00221, entropy= 2.08051, loss=-0.00221
surrogate=-0.02002, entropy= 2.08005, loss=-0.02002
std_min= 0.40821, std_max= 0.55958, std_mean= 0.48808
val lr: [0.0001708504098360656], policy lr: [0.0002050204918032787]
Policy Loss: -0.020023, | Entropy Bonus: -0, | Value Loss: 32.101, | Advantage Loss: 1.1891
Time elapsed (s): 1.6413002014160156
Agent stdevs: 0.48807505
--------------------------------------------------------------------------------

Step 309
++++++++ Policy training ++++++++++
Current mean reward: 1652.706300 | mean episode length: 470.000000
val_loss=15.20063
val_loss=52.74462
val_loss=17.89769
val_loss=17.40991
val_loss=18.65772
val_loss=19.28148
val_loss=24.86498
val_loss=39.69845
val_loss=10.67451
val_loss=10.29805
adv_loss= 1.21649
adv_loss= 3.81668
adv_loss= 2.19834
adv_loss= 2.33081
adv_loss= 6.11433
adv_loss= 6.70721
adv_loss= 2.55252
adv_loss= 3.41172
adv_loss= 0.86957
adv_loss= 1.54499
surrogate= 0.02666, entropy= 2.08146, loss= 0.02666
surrogate= 0.06670, entropy= 2.08138, loss= 0.06670
surrogate= 0.00563, entropy= 2.08163, loss= 0.00563
surrogate=-0.01098, entropy= 2.08223, loss=-0.01098
surrogate=-0.01112, entropy= 2.08228, loss=-0.01112
surrogate=-0.01834, entropy= 2.08278, loss=-0.01834
surrogate= 0.00475, entropy= 2.08242, loss= 0.00475
surrogate= 0.00543, entropy= 2.07879, loss= 0.00543
surrogate= 0.00566, entropy= 2.08027, loss= 0.00566
surrogate=-0.01719, entropy= 2.07958, loss=-0.01719
std_min= 0.40516, std_max= 0.56183, std_mean= 0.48831
val lr: [0.00017059426229508196], policy lr: [0.00020471311475409832]
Policy Loss: -0.017188, | Entropy Bonus: -0, | Value Loss: 10.298, | Advantage Loss: 1.545
Time elapsed (s): 1.6416864395141602
Agent stdevs: 0.48831245
--------------------------------------------------------------------------------

Step 310
++++++++ Policy training ++++++++++
Current mean reward: 1336.491007 | mean episode length: 379.600000
val_loss=44.54903
val_loss=16.47163
val_loss=14.12194
val_loss=20.76290
val_loss=21.15783
val_loss= 6.70236
val_loss=17.37762
val_loss=14.05059
val_loss=11.61259
val_loss= 8.33584
adv_loss= 1.37365
adv_loss= 3.77065
adv_loss= 5.32176
adv_loss= 1.70835
adv_loss= 3.68132
adv_loss= 1.08484
adv_loss= 2.35051
adv_loss= 3.58925
adv_loss= 2.11981
adv_loss= 1.84416
surrogate=-0.01348, entropy= 2.07740, loss=-0.01348
surrogate= 0.01771, entropy= 2.07537, loss= 0.01771
surrogate=-0.02348, entropy= 2.07415, loss=-0.02348
surrogate=-0.02010, entropy= 2.07387, loss=-0.02010
surrogate= 0.00386, entropy= 2.07215, loss= 0.00386
surrogate=-0.02777, entropy= 2.07129, loss=-0.02777
surrogate= 0.01676, entropy= 2.06804, loss= 0.01676
surrogate= 0.00621, entropy= 2.06761, loss= 0.00621
surrogate=-0.02843, entropy= 2.06653, loss=-0.02843
surrogate=-0.01582, entropy= 2.06439, loss=-0.01582
std_min= 0.40286, std_max= 0.55501, std_mean= 0.48571
val lr: [0.0001703381147540984], policy lr: [0.00020440573770491805]
Policy Loss: -0.015815, | Entropy Bonus: -0, | Value Loss: 8.3358, | Advantage Loss: 1.8442
Time elapsed (s): 1.6738262176513672
Agent stdevs: 0.48571292
--------------------------------------------------------------------------------

Step 311
++++++++ Policy training ++++++++++
Current mean reward: 3274.796295 | mean episode length: 1000.000000
val_loss=72.28229
val_loss=338.73010
val_loss=1547.26782
val_loss=213.36113
val_loss=1487.67383
val_loss=125.73377
val_loss=253.74095
val_loss=884.79187
val_loss=1191.09509
val_loss=1713.60986
adv_loss= 3.21130
adv_loss= 4.66125
adv_loss= 4.89002
adv_loss= 4.31363
adv_loss= 4.27797
adv_loss= 4.73091
adv_loss= 5.80868
adv_loss= 2.88977
adv_loss= 1.45771
adv_loss=1585.71021
surrogate=-0.00162, entropy= 2.06461, loss=-0.00162
surrogate= 0.00663, entropy= 2.06187, loss= 0.00663
surrogate= 0.00568, entropy= 2.06269, loss= 0.00568
surrogate=-0.00662, entropy= 2.06169, loss=-0.00662
surrogate=-0.01782, entropy= 2.06128, loss=-0.01782
surrogate= 0.02313, entropy= 2.06178, loss= 0.02313
surrogate=-0.02097, entropy= 2.06153, loss=-0.02097
surrogate=-0.02869, entropy= 2.06206, loss=-0.02869
surrogate=-0.01805, entropy= 2.06097, loss=-0.01805
surrogate=-0.00593, entropy= 2.05816, loss=-0.00593
std_min= 0.40280, std_max= 0.55433, std_mean= 0.48467
val lr: [0.00017008196721311476], policy lr: [0.0002040983606557377]
Policy Loss: -0.005933, | Entropy Bonus: -0, | Value Loss: 1713.6, | Advantage Loss: 1585.7
Time elapsed (s): 1.678985595703125
Agent stdevs: 0.48467264
--------------------------------------------------------------------------------

Step 312
++++++++ Policy training ++++++++++
Current mean reward: 1762.490950 | mean episode length: 491.750000
val_loss=15.06866
val_loss=16.58181
val_loss=13.95229
val_loss= 6.79198
val_loss= 6.74885
val_loss= 7.48827
val_loss=15.55699
val_loss= 9.64381
val_loss=11.92834
val_loss=15.60456
adv_loss= 2.26240
adv_loss= 1.31019
adv_loss= 8.45549
adv_loss= 2.58217
adv_loss= 4.14537
adv_loss= 2.13841
adv_loss= 3.09240
adv_loss= 1.65358
adv_loss= 2.34909
adv_loss= 4.50087
surrogate=-0.02565, entropy= 2.05635, loss=-0.02565
surrogate=-0.02297, entropy= 2.05481, loss=-0.02297
surrogate= 0.00530, entropy= 2.05220, loss= 0.00530
surrogate=-0.01079, entropy= 2.05048, loss=-0.01079
surrogate= 0.00032, entropy= 2.05153, loss= 0.00032
surrogate=-0.01209, entropy= 2.04915, loss=-0.01209
surrogate=-0.02275, entropy= 2.04794, loss=-0.02275
surrogate=-0.03083, entropy= 2.04784, loss=-0.03083
surrogate=-0.03126, entropy= 2.04571, loss=-0.03126
surrogate=-0.01750, entropy= 2.04618, loss=-0.01750
std_min= 0.40136, std_max= 0.55345, std_mean= 0.48276
val lr: [0.00016982581967213116], policy lr: [0.00020379098360655737]
Policy Loss: -0.0175, | Entropy Bonus: -0, | Value Loss: 15.605, | Advantage Loss: 4.5009
Time elapsed (s): 1.6488869190216064
Agent stdevs: 0.48276472
--------------------------------------------------------------------------------

Step 313
++++++++ Policy training ++++++++++
Current mean reward: 1772.065810 | mean episode length: 496.250000
val_loss=15.74297
val_loss=20.84144
val_loss=13.32875
val_loss= 7.20693
val_loss= 9.96190
val_loss=13.40869
val_loss=23.30225
val_loss=15.51769
val_loss=13.63765
val_loss= 7.42590
adv_loss= 2.37289
adv_loss= 2.84750
adv_loss= 2.77342
adv_loss= 1.43415
adv_loss= 2.39701
adv_loss= 0.91206
adv_loss= 2.51656
adv_loss= 2.74426
adv_loss= 3.47838
adv_loss= 1.98296
surrogate= 0.01431, entropy= 2.04126, loss= 0.01431
surrogate=-0.00302, entropy= 2.03753, loss=-0.00302
surrogate=-0.01152, entropy= 2.03192, loss=-0.01152
surrogate=-0.01554, entropy= 2.02880, loss=-0.01554
surrogate=-0.00396, entropy= 2.02417, loss=-0.00396
surrogate=-0.00888, entropy= 2.01851, loss=-0.00888
surrogate=-0.00887, entropy= 2.01405, loss=-0.00887
surrogate=-0.02914, entropy= 2.01198, loss=-0.02914
surrogate=-0.02019, entropy= 2.00884, loss=-0.02019
surrogate=-0.02278, entropy= 2.00422, loss=-0.02278
std_min= 0.39756, std_max= 0.54243, std_mean= 0.47580
val lr: [0.00016956967213114753], policy lr: [0.00020348360655737704]
Policy Loss: -0.022776, | Entropy Bonus: -0, | Value Loss: 7.4259, | Advantage Loss: 1.983
Time elapsed (s): 1.643080711364746
Agent stdevs: 0.47580394
--------------------------------------------------------------------------------

Step 314
++++++++ Policy training ++++++++++
Current mean reward: 3220.042476 | mean episode length: 967.500000
val_loss=150.10553
val_loss=484.60162
val_loss=127.24132
val_loss=131.51892
val_loss=204.95552
val_loss=702.68652
val_loss=420.95325
val_loss=1934.33618
val_loss=253.88286
val_loss=40.14080
adv_loss= 5.63426
adv_loss= 3.68653
adv_loss= 1.16679
adv_loss= 1.32289
adv_loss= 0.88634
adv_loss= 1.32949
adv_loss= 2.11699
adv_loss= 1.30381
adv_loss= 1.39580
adv_loss= 2.20804
surrogate= 0.01912, entropy= 2.00229, loss= 0.01912
surrogate= 0.02386, entropy= 2.00202, loss= 0.02386
surrogate=-0.00727, entropy= 2.00174, loss=-0.00727
surrogate=-0.01500, entropy= 2.00316, loss=-0.01500
surrogate=-0.00018, entropy= 2.00263, loss=-0.00018
surrogate= 0.01159, entropy= 2.00248, loss= 0.01159
surrogate=-0.01744, entropy= 2.00167, loss=-0.01744
surrogate=-0.01952, entropy= 2.00028, loss=-0.01952
surrogate=-0.00671, entropy= 1.99911, loss=-0.00671
surrogate=-0.01946, entropy= 1.99988, loss=-0.01946
std_min= 0.39515, std_max= 0.54546, std_mean= 0.47541
val lr: [0.00016931352459016393], policy lr: [0.00020317622950819672]
Policy Loss: -0.019456, | Entropy Bonus: -0, | Value Loss: 40.141, | Advantage Loss: 2.208
Time elapsed (s): 1.635183334350586
Agent stdevs: 0.47541094
--------------------------------------------------------------------------------

Step 315
++++++++ Policy training ++++++++++
Current mean reward: 2584.256842 | mean episode length: 775.500000
val_loss=24.41600
val_loss=19.56625
val_loss=34.72305
val_loss=44.37085
val_loss=31.84026
val_loss=23.57051
val_loss=19.37976
val_loss=15.53078
val_loss=25.87070
val_loss=17.57548
adv_loss= 2.85796
adv_loss= 4.88346
adv_loss= 4.00462
adv_loss= 3.24298
adv_loss= 3.58491
adv_loss= 2.06140
adv_loss= 3.91314
adv_loss= 5.80906
adv_loss= 1.88621
adv_loss= 1.59491
surrogate=-0.00567, entropy= 2.00204, loss=-0.00567
surrogate= 0.01393, entropy= 2.00268, loss= 0.01393
surrogate= 0.01203, entropy= 2.00321, loss= 0.01203
surrogate=-0.02059, entropy= 2.00333, loss=-0.02059
surrogate=-0.01264, entropy= 2.00237, loss=-0.01264
surrogate=-0.00386, entropy= 2.00351, loss=-0.00386
surrogate=-0.04540, entropy= 2.00335, loss=-0.04540
surrogate=-0.03012, entropy= 2.00454, loss=-0.03012
surrogate=-0.03319, entropy= 2.00428, loss=-0.03319
surrogate=-0.02572, entropy= 2.00535, loss=-0.02572
std_min= 0.39232, std_max= 0.54958, std_mean= 0.47666
val lr: [0.00016905737704918033], policy lr: [0.0002028688524590164]
Policy Loss: -0.025721, | Entropy Bonus: -0, | Value Loss: 17.575, | Advantage Loss: 1.5949
Time elapsed (s): 1.6309034824371338
Agent stdevs: 0.4766556
--------------------------------------------------------------------------------

Step 316
++++++++ Policy training ++++++++++
Current mean reward: 1330.972943 | mean episode length: 393.500000
val_loss=1202.05127
val_loss=158.78267
val_loss=91.42894
val_loss=345.79788
val_loss=45.26738
val_loss=511.48203
val_loss=23.29279
val_loss=79.03122
val_loss=1384.46729
val_loss=451.68936
adv_loss=1210.23096
adv_loss= 9.62726
adv_loss= 6.44929
adv_loss=1198.89636
adv_loss=10.00183
adv_loss=1189.13049
adv_loss= 7.51112
adv_loss= 3.79392
adv_loss= 8.52941
adv_loss= 3.26431
surrogate=-0.00260, entropy= 2.00294, loss=-0.00260
surrogate= 0.04899, entropy= 2.00123, loss= 0.04899
surrogate=-0.02144, entropy= 1.99925, loss=-0.02144
surrogate=-0.00321, entropy= 1.99804, loss=-0.00321
surrogate=-0.02240, entropy= 1.99713, loss=-0.02240
surrogate= 0.01620, entropy= 1.99623, loss= 0.01620
surrogate=-0.03513, entropy= 1.99488, loss=-0.03513
surrogate=-0.02217, entropy= 1.99282, loss=-0.02217
surrogate=-0.01921, entropy= 1.99330, loss=-0.01921
surrogate=-0.02627, entropy= 1.99204, loss=-0.02627
std_min= 0.39098, std_max= 0.54494, std_mean= 0.47446
val lr: [0.0001688012295081967], policy lr: [0.00020256147540983604]
Policy Loss: -0.026273, | Entropy Bonus: -0, | Value Loss: 451.69, | Advantage Loss: 3.2643
Time elapsed (s): 1.6498160362243652
Agent stdevs: 0.4744563
--------------------------------------------------------------------------------

Step 317
++++++++ Policy training ++++++++++
Current mean reward: 3306.197477 | mean episode length: 1000.000000
val_loss=2149.20093
val_loss=113.26583
val_loss=835.81989
val_loss=1399.07190
val_loss=973.37518
val_loss=286.22998
val_loss=123.98270
val_loss=102.43150
val_loss=220.82449
val_loss=1222.25464
adv_loss= 4.34548
adv_loss= 2.43357
adv_loss= 3.36894
adv_loss= 1.60855
adv_loss= 2.32595
adv_loss= 1.00670
adv_loss= 1.87641
adv_loss= 2.47880
adv_loss= 1.27216
adv_loss= 1.45759
surrogate=-0.00521, entropy= 1.98581, loss=-0.00521
surrogate=-0.00827, entropy= 1.98172, loss=-0.00827
surrogate= 0.00812, entropy= 1.97960, loss= 0.00812
surrogate= 0.05167, entropy= 1.98066, loss= 0.05167
surrogate=-0.01780, entropy= 1.98011, loss=-0.01780
surrogate= 0.01832, entropy= 1.97913, loss= 0.01832
surrogate= 0.01110, entropy= 1.98113, loss= 0.01110
surrogate= 0.03868, entropy= 1.97985, loss= 0.03868
surrogate=-0.02541, entropy= 1.97878, loss=-0.02541
surrogate= 0.00594, entropy= 1.97987, loss= 0.00594
std_min= 0.38848, std_max= 0.54139, std_mean= 0.47257
val lr: [0.00016854508196721313], policy lr: [0.00020225409836065574]
Policy Loss: 0.0059392, | Entropy Bonus: -0, | Value Loss: 1222.3, | Advantage Loss: 1.4576
Time elapsed (s): 1.6637229919433594
Agent stdevs: 0.4725722
--------------------------------------------------------------------------------

Step 318
++++++++ Policy training ++++++++++
Current mean reward: 2279.046543 | mean episode length: 662.000000
val_loss=16.46856
val_loss=22.43277
val_loss=22.42378
val_loss=21.78603
val_loss=34.07379
val_loss=14.81233
val_loss=19.72195
val_loss=18.98234
val_loss=17.99490
val_loss=19.29037
adv_loss= 2.57962
adv_loss= 3.82868
adv_loss= 2.26601
adv_loss= 1.63295
adv_loss= 3.93811
adv_loss= 1.93640
adv_loss= 9.57622
adv_loss= 4.84115
adv_loss= 2.46387
adv_loss= 2.17211
surrogate= 0.02449, entropy= 1.98035, loss= 0.02449
surrogate=-0.01340, entropy= 1.98166, loss=-0.01340
surrogate= 0.00709, entropy= 1.98311, loss= 0.00709
surrogate=-0.01734, entropy= 1.98450, loss=-0.01734
surrogate=-0.02504, entropy= 1.98506, loss=-0.02504
surrogate=-0.00320, entropy= 1.98633, loss=-0.00320
surrogate=-0.01727, entropy= 1.98551, loss=-0.01727
surrogate=-0.02746, entropy= 1.98623, loss=-0.02746
surrogate=-0.02320, entropy= 1.98519, loss=-0.02320
surrogate= 0.05599, entropy= 1.98526, loss= 0.05599
std_min= 0.38721, std_max= 0.54676, std_mean= 0.47371
val lr: [0.0001682889344262295], policy lr: [0.0002019467213114754]
Policy Loss: 0.05599, | Entropy Bonus: -0, | Value Loss: 19.29, | Advantage Loss: 2.1721
Time elapsed (s): 1.639411449432373
Agent stdevs: 0.47370934
--------------------------------------------------------------------------------

Step 319
++++++++ Policy training ++++++++++
Current mean reward: 2019.341519 | mean episode length: 592.500000
val_loss=79.32465
val_loss=58.56050
val_loss=56.77129
val_loss=54.37434
val_loss=15.69510
val_loss=137.35925
val_loss=15.22421
val_loss=42.13984
val_loss=32.11977
val_loss=19.54125
adv_loss= 2.19040
adv_loss= 5.47452
adv_loss= 1.86166
adv_loss= 2.52183
adv_loss= 1.25338
adv_loss= 3.44228
adv_loss=12.29155
adv_loss= 2.76888
adv_loss=10.70470
adv_loss= 1.89963
surrogate=-0.00525, entropy= 1.98551, loss=-0.00525
surrogate=-0.03974, entropy= 1.98667, loss=-0.03974
surrogate=-0.02635, entropy= 1.98915, loss=-0.02635
surrogate= 0.01881, entropy= 1.98930, loss= 0.01881
surrogate=-0.04729, entropy= 1.98901, loss=-0.04729
surrogate=-0.01526, entropy= 1.98909, loss=-0.01526
surrogate=-0.03208, entropy= 1.99046, loss=-0.03208
surrogate=-0.01985, entropy= 1.99190, loss=-0.01985
surrogate=-0.02216, entropy= 1.99213, loss=-0.02216
surrogate=-0.02572, entropy= 1.99283, loss=-0.02572
std_min= 0.38762, std_max= 0.54762, std_mean= 0.47496
val lr: [0.00016803278688524593], policy lr: [0.0002016393442622951]
Policy Loss: -0.025716, | Entropy Bonus: -0, | Value Loss: 19.541, | Advantage Loss: 1.8996
Time elapsed (s): 1.6546516418457031
Agent stdevs: 0.47495624
--------------------------------------------------------------------------------

Step 320
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2304
++++++++ Policy training ++++++++++
Current mean reward: 1300.375447 | mean episode length: 372.750000
val_loss=18.24814
val_loss=20.37616
val_loss=46.68262
val_loss=63.11082
val_loss=30.33022
val_loss=18.27376
val_loss=12.50522
val_loss=44.31844
val_loss=27.64867
val_loss=23.06923
adv_loss= 8.57734
adv_loss= 2.10986
adv_loss= 1.63650
adv_loss= 1.80004
adv_loss= 3.22391
adv_loss= 2.17780
adv_loss= 4.14907
adv_loss= 2.61172
adv_loss= 4.69576
adv_loss= 2.15392
surrogate=-0.01539, entropy= 1.99392, loss=-0.01539
surrogate= 0.00347, entropy= 1.99266, loss= 0.00347
surrogate=-0.01074, entropy= 1.99355, loss=-0.01074
surrogate=-0.00407, entropy= 1.99185, loss=-0.00407
surrogate=-0.01432, entropy= 1.99242, loss=-0.01432
surrogate=-0.02759, entropy= 1.99177, loss=-0.02759
surrogate= 0.01034, entropy= 1.99298, loss= 0.01034
surrogate=-0.01649, entropy= 1.99240, loss=-0.01649
surrogate= 0.00769, entropy= 1.99152, loss= 0.00769
surrogate=-0.01653, entropy= 1.99046, loss=-0.01653
std_min= 0.38298, std_max= 0.55071, std_mean= 0.47510
val lr: [0.0001677766393442623], policy lr: [0.00020133196721311474]
Policy Loss: -0.016534, | Entropy Bonus: -0, | Value Loss: 23.069, | Advantage Loss: 2.1539
Time elapsed (s): 1.662438154220581
Agent stdevs: 0.47510195
--------------------------------------------------------------------------------

Step 321
++++++++ Policy training ++++++++++
Current mean reward: 2188.825262 | mean episode length: 630.666667
val_loss=17.27972
val_loss=15.96625
val_loss=11.88414
val_loss=30.33856
val_loss=11.27809
val_loss= 9.14211
val_loss= 4.93873
val_loss=11.76800
val_loss=13.15188
val_loss=11.37937
adv_loss= 2.13445
adv_loss= 0.87127
adv_loss= 3.17154
adv_loss= 1.58794
adv_loss= 2.34791
adv_loss= 1.48056
adv_loss= 4.99924
adv_loss= 2.13242
adv_loss= 1.43299
adv_loss= 2.56862
surrogate= 0.00577, entropy= 1.98937, loss= 0.00577
surrogate= 0.04265, entropy= 1.98866, loss= 0.04265
surrogate=-0.03052, entropy= 1.98589, loss=-0.03052
surrogate=-0.00981, entropy= 1.98610, loss=-0.00981
surrogate=-0.03374, entropy= 1.98504, loss=-0.03374
surrogate=-0.03188, entropy= 1.98370, loss=-0.03188
surrogate= 0.01738, entropy= 1.98405, loss= 0.01738
surrogate=-0.02170, entropy= 1.98244, loss=-0.02170
surrogate=-0.03408, entropy= 1.98124, loss=-0.03408
surrogate=-0.02718, entropy= 1.98136, loss=-0.02718
std_min= 0.38090, std_max= 0.54525, std_mean= 0.47362
val lr: [0.00016752049180327868], policy lr: [0.0002010245901639344]
Policy Loss: -0.027179, | Entropy Bonus: -0, | Value Loss: 11.379, | Advantage Loss: 2.5686
Time elapsed (s): 1.62479829788208
Agent stdevs: 0.4736224
--------------------------------------------------------------------------------

Step 322
++++++++ Policy training ++++++++++
Current mean reward: 1617.420715 | mean episode length: 463.750000
val_loss=17.78192
val_loss=11.77364
val_loss=16.28368
val_loss=13.77396
val_loss= 8.86625
val_loss=10.50770
val_loss=13.55764
val_loss= 5.97569
val_loss= 9.78902
val_loss= 7.52465
adv_loss=10.00443
adv_loss= 2.09101
adv_loss= 2.40970
adv_loss= 1.71962
adv_loss= 3.00349
adv_loss= 8.82599
adv_loss= 1.37545
adv_loss= 3.13379
adv_loss= 2.32037
adv_loss= 1.08365
surrogate=-0.00190, entropy= 1.97957, loss=-0.00190
surrogate= 0.00191, entropy= 1.97995, loss= 0.00191
surrogate= 0.03888, entropy= 1.97927, loss= 0.03888
surrogate=-0.02650, entropy= 1.97653, loss=-0.02650
surrogate=-0.03965, entropy= 1.97630, loss=-0.03965
surrogate=-0.01716, entropy= 1.97571, loss=-0.01716
surrogate=-0.02427, entropy= 1.97472, loss=-0.02427
surrogate= 0.02353, entropy= 1.97309, loss= 0.02353
surrogate=-0.06140, entropy= 1.97383, loss=-0.06140
surrogate=-0.00865, entropy= 1.97110, loss=-0.00865
std_min= 0.38272, std_max= 0.54714, std_mean= 0.47186
val lr: [0.00016726434426229508], policy lr: [0.00020071721311475408]
Policy Loss: -0.0086522, | Entropy Bonus: -0, | Value Loss: 7.5247, | Advantage Loss: 1.0836
Time elapsed (s): 1.6708996295928955
Agent stdevs: 0.47186065
--------------------------------------------------------------------------------

Step 323
++++++++ Policy training ++++++++++
Current mean reward: 1605.988552 | mean episode length: 465.500000
val_loss=18.97009
val_loss=23.42337
val_loss=17.10734
val_loss=25.61635
val_loss=21.65323
val_loss=15.64700
val_loss=17.10174
val_loss=13.48871
val_loss=10.78966
val_loss=17.88304
adv_loss= 3.59099
adv_loss= 1.70409
adv_loss= 0.68238
adv_loss= 1.92452
adv_loss= 1.37072
adv_loss=14.15400
adv_loss= 1.33505
adv_loss=16.14080
adv_loss= 1.75462
adv_loss= 9.28600
surrogate=-0.01952, entropy= 1.97245, loss=-0.01952
surrogate=-0.01800, entropy= 1.97131, loss=-0.01800
surrogate=-0.02546, entropy= 1.97288, loss=-0.02546
surrogate=-0.00605, entropy= 1.97335, loss=-0.00605
surrogate=-0.02878, entropy= 1.97454, loss=-0.02878
surrogate=-0.03297, entropy= 1.97424, loss=-0.03297
surrogate=-0.01693, entropy= 1.97481, loss=-0.01693
surrogate=-0.02737, entropy= 1.97342, loss=-0.02737
surrogate=-0.01346, entropy= 1.97439, loss=-0.01346
surrogate=-0.01085, entropy= 1.97491, loss=-0.01085
std_min= 0.38537, std_max= 0.54344, std_mean= 0.47208
val lr: [0.00016700819672131148], policy lr: [0.00020040983606557376]
Policy Loss: -0.010851, | Entropy Bonus: -0, | Value Loss: 17.883, | Advantage Loss: 9.286
Time elapsed (s): 1.6772315502166748
Agent stdevs: 0.47207645
--------------------------------------------------------------------------------

Step 324
++++++++ Policy training ++++++++++
Current mean reward: 2243.771181 | mean episode length: 626.000000
val_loss= 9.57879
val_loss= 4.88354
val_loss= 8.83922
val_loss= 6.87395
val_loss=10.98384
val_loss= 4.20261
val_loss= 6.88475
val_loss= 7.14367
val_loss= 4.42706
val_loss= 3.77633
adv_loss= 0.93580
adv_loss= 1.10724
adv_loss= 1.23359
adv_loss= 3.56583
adv_loss= 0.82854
adv_loss= 0.87055
adv_loss= 1.21194
adv_loss= 0.33057
adv_loss= 0.91046
adv_loss= 0.52001
surrogate= 0.00466, entropy= 1.97370, loss= 0.00466
surrogate= 0.02342, entropy= 1.97448, loss= 0.02342
surrogate=-0.01156, entropy= 1.97504, loss=-0.01156
surrogate= 0.00496, entropy= 1.97605, loss= 0.00496
surrogate=-0.02398, entropy= 1.97236, loss=-0.02398
surrogate=-0.01226, entropy= 1.97212, loss=-0.01226
surrogate=-0.03532, entropy= 1.97351, loss=-0.03532
surrogate=-0.00606, entropy= 1.97072, loss=-0.00606
surrogate=-0.02320, entropy= 1.96931, loss=-0.02320
surrogate=-0.01788, entropy= 1.97084, loss=-0.01788
std_min= 0.38543, std_max= 0.54361, std_mean= 0.47143
val lr: [0.00016675204918032788], policy lr: [0.00020010245901639343]
Policy Loss: -0.017885, | Entropy Bonus: -0, | Value Loss: 3.7763, | Advantage Loss: 0.52001
Time elapsed (s): 1.6570672988891602
Agent stdevs: 0.4714333
--------------------------------------------------------------------------------

Step 325
++++++++ Policy training ++++++++++
Current mean reward: 1720.730876 | mean episode length: 506.500000
val_loss=91.57465
val_loss=302.53571
val_loss=100.94211
val_loss=749.10028
val_loss=256.03824
val_loss=156.03720
val_loss=58.60762
val_loss=288.52115
val_loss=186.73727
val_loss=701.90320
adv_loss= 0.93432
adv_loss= 5.13830
adv_loss= 5.80675
adv_loss= 3.59309
adv_loss= 4.30336
adv_loss= 2.87193
adv_loss= 2.32387
adv_loss= 8.31694
adv_loss= 2.95473
adv_loss= 4.62676
surrogate=-0.00652, entropy= 1.97361, loss=-0.00652
surrogate=-0.00917, entropy= 1.97471, loss=-0.00917
surrogate=-0.03484, entropy= 1.97528, loss=-0.03484
surrogate= 0.02157, entropy= 1.97538, loss= 0.02157
surrogate=-0.02650, entropy= 1.97841, loss=-0.02650
surrogate=-0.02227, entropy= 1.97742, loss=-0.02227
surrogate=-0.02992, entropy= 1.97866, loss=-0.02992
surrogate=-0.01589, entropy= 1.97918, loss=-0.01589
surrogate= 0.00044, entropy= 1.98153, loss= 0.00044
surrogate=-0.02531, entropy= 1.98315, loss=-0.02531
std_min= 0.38855, std_max= 0.54260, std_mean= 0.47313
val lr: [0.00016649590163934425], policy lr: [0.00019979508196721308]
Policy Loss: -0.025312, | Entropy Bonus: -0, | Value Loss: 701.9, | Advantage Loss: 4.6268
Time elapsed (s): 1.6586241722106934
Agent stdevs: 0.4731321
--------------------------------------------------------------------------------

Step 326
++++++++ Policy training ++++++++++
Current mean reward: 2695.144851 | mean episode length: 773.500000
val_loss=11.23405
val_loss= 6.34647
val_loss= 5.16656
val_loss= 5.41003
val_loss= 4.53318
val_loss= 6.32496
val_loss= 6.39460
val_loss= 3.67759
val_loss= 7.82689
val_loss= 5.20711
adv_loss= 1.38873
adv_loss= 2.00435
adv_loss= 1.56887
adv_loss= 1.93576
adv_loss= 1.04222
adv_loss= 1.55889
adv_loss= 3.33198
adv_loss= 1.16008
adv_loss= 1.58395
adv_loss= 1.82997
surrogate= 0.00713, entropy= 1.98488, loss= 0.00713
surrogate=-0.02548, entropy= 1.98535, loss=-0.02548
surrogate= 0.01230, entropy= 1.98337, loss= 0.01230
surrogate=-0.01967, entropy= 1.98139, loss=-0.01967
surrogate=-0.05038, entropy= 1.97987, loss=-0.05038
surrogate=-0.00761, entropy= 1.98055, loss=-0.00761
surrogate=-0.02563, entropy= 1.97855, loss=-0.02563
surrogate=-0.01682, entropy= 1.97824, loss=-0.01682
surrogate=-0.03943, entropy= 1.97712, loss=-0.03943
surrogate=-0.03841, entropy= 1.97743, loss=-0.03841
std_min= 0.38751, std_max= 0.53822, std_mean= 0.47215
val lr: [0.00016623975409836068], policy lr: [0.00019948770491803278]
Policy Loss: -0.038406, | Entropy Bonus: -0, | Value Loss: 5.2071, | Advantage Loss: 1.83
Time elapsed (s): 1.637568712234497
Agent stdevs: 0.4721531
--------------------------------------------------------------------------------

Step 327
++++++++ Policy training ++++++++++
Current mean reward: 2461.994799 | mean episode length: 733.000000
val_loss=538.22974
val_loss=32.51292
val_loss=53.20808
val_loss=73.78150
val_loss=62.90027
val_loss=190.93839
val_loss=173.86852
val_loss=46.73024
val_loss=299.34900
val_loss=250.54901
adv_loss=14.65547
adv_loss= 8.38632
adv_loss= 8.14028
adv_loss= 7.83403
adv_loss=16.57769
adv_loss= 2.83128
adv_loss= 9.17472
adv_loss=1315.69153
adv_loss= 4.52246
adv_loss= 3.02116
surrogate= 0.00494, entropy= 1.97849, loss= 0.00494
surrogate= 0.01015, entropy= 1.97978, loss= 0.01015
surrogate= 0.01083, entropy= 1.97951, loss= 0.01083
surrogate= 0.00833, entropy= 1.97848, loss= 0.00833
surrogate= 0.01697, entropy= 1.97919, loss= 0.01697
surrogate=-0.01238, entropy= 1.98009, loss=-0.01238
surrogate=-0.02672, entropy= 1.98225, loss=-0.02672
surrogate= 0.01182, entropy= 1.98289, loss= 0.01182
surrogate=-0.01218, entropy= 1.98357, loss=-0.01218
surrogate=-0.00768, entropy= 1.98382, loss=-0.00768
std_min= 0.39056, std_max= 0.53860, std_mean= 0.47293
val lr: [0.00016598360655737705], policy lr: [0.00019918032786885243]
Policy Loss: -0.0076831, | Entropy Bonus: -0, | Value Loss: 250.55, | Advantage Loss: 3.0212
Time elapsed (s): 1.6555423736572266
Agent stdevs: 0.47293338
--------------------------------------------------------------------------------

Step 328
++++++++ Policy training ++++++++++
Current mean reward: 1614.015233 | mean episode length: 462.750000
val_loss=65.47090
val_loss=23.59146
val_loss=38.73358
val_loss=29.52896
val_loss=18.82488
val_loss=50.94242
val_loss=32.48188
val_loss=18.20351
val_loss=37.06734
val_loss=35.21498
adv_loss= 1.17473
adv_loss= 2.64328
adv_loss= 2.66283
adv_loss= 1.93816
adv_loss=21.34870
adv_loss= 2.72893
adv_loss= 5.28937
adv_loss= 1.79749
adv_loss= 2.86138
adv_loss= 1.97616
surrogate= 0.02540, entropy= 1.98199, loss= 0.02540
surrogate=-0.02561, entropy= 1.98182, loss=-0.02561
surrogate= 0.00790, entropy= 1.98169, loss= 0.00790
surrogate=-0.01900, entropy= 1.98037, loss=-0.01900
surrogate=-0.04556, entropy= 1.98136, loss=-0.04556
surrogate=-0.00729, entropy= 1.98197, loss=-0.00729
surrogate=-0.03271, entropy= 1.98156, loss=-0.03271
surrogate=-0.02491, entropy= 1.98323, loss=-0.02491
surrogate=-0.04436, entropy= 1.98310, loss=-0.04436
surrogate=-0.01913, entropy= 1.98233, loss=-0.01913
std_min= 0.39092, std_max= 0.53470, std_mean= 0.47254
val lr: [0.00016572745901639345], policy lr: [0.0001988729508196721]
Policy Loss: -0.019131, | Entropy Bonus: -0, | Value Loss: 35.215, | Advantage Loss: 1.9762
Time elapsed (s): 1.6554300785064697
Agent stdevs: 0.47253713
--------------------------------------------------------------------------------

Step 329
++++++++ Policy training ++++++++++
Current mean reward: 2026.150642 | mean episode length: 595.666667
val_loss=39.82868
val_loss=391.23291
val_loss=33.16247
val_loss=23.25383
val_loss=111.98586
val_loss=313.81299
val_loss=38.34863
val_loss=47.70396
val_loss=211.16556
val_loss=75.44429
adv_loss= 3.07771
adv_loss= 2.10717
adv_loss= 3.05966
adv_loss= 3.36534
adv_loss= 3.03799
adv_loss= 2.36587
adv_loss= 2.36795
adv_loss= 3.65097
adv_loss= 2.48700
adv_loss= 3.30765
surrogate= 0.02910, entropy= 1.97925, loss= 0.02910
surrogate=-0.02435, entropy= 1.97893, loss=-0.02435
surrogate=-0.00892, entropy= 1.97551, loss=-0.00892
surrogate= 0.02512, entropy= 1.97461, loss= 0.02512
surrogate=-0.01547, entropy= 1.97454, loss=-0.01547
surrogate=-0.01889, entropy= 1.97359, loss=-0.01889
surrogate=-0.03046, entropy= 1.97322, loss=-0.03046
surrogate=-0.02362, entropy= 1.97337, loss=-0.02362
surrogate=-0.02869, entropy= 1.97055, loss=-0.02869
surrogate=-0.00046, entropy= 1.96944, loss=-0.00046
std_min= 0.38520, std_max= 0.53828, std_mean= 0.47106
val lr: [0.00016547131147540982], policy lr: [0.00019856557377049178]
Policy Loss: -0.00046106, | Entropy Bonus: -0, | Value Loss: 75.444, | Advantage Loss: 3.3077
Time elapsed (s): 1.685011863708496
Agent stdevs: 0.47105536
--------------------------------------------------------------------------------

Step 330
++++++++ Policy training ++++++++++
Current mean reward: 1098.142638 | mean episode length: 315.833333
val_loss=34.78146
val_loss=21.41774
val_loss=28.98735
val_loss=20.44894
val_loss=26.77329
val_loss=28.43368
val_loss=41.65208
val_loss= 9.63323
val_loss=10.27309
val_loss=16.30680
adv_loss= 3.83341
adv_loss= 2.57768
adv_loss= 1.78342
adv_loss= 3.05405
adv_loss= 9.03043
adv_loss= 2.68117
adv_loss= 2.13542
adv_loss= 1.59679
adv_loss= 2.08786
adv_loss= 2.38554
surrogate= 0.07390, entropy= 1.96892, loss= 0.07390
surrogate= 0.01136, entropy= 1.96619, loss= 0.01136
surrogate= 0.02261, entropy= 1.96675, loss= 0.02261
surrogate= 0.00878, entropy= 1.96484, loss= 0.00878
surrogate=-0.00621, entropy= 1.96434, loss=-0.00621
surrogate=-0.01415, entropy= 1.96347, loss=-0.01415
surrogate=-0.00104, entropy= 1.96137, loss=-0.00104
surrogate=-0.03665, entropy= 1.95883, loss=-0.03665
surrogate=-0.03595, entropy= 1.95722, loss=-0.03595
surrogate=-0.03648, entropy= 1.95525, loss=-0.03648
std_min= 0.38660, std_max= 0.53489, std_mean= 0.46850
val lr: [0.00016521516393442622], policy lr: [0.00019825819672131145]
Policy Loss: -0.036478, | Entropy Bonus: -0, | Value Loss: 16.307, | Advantage Loss: 2.3855
Time elapsed (s): 1.639359951019287
Agent stdevs: 0.46849987
--------------------------------------------------------------------------------

Step 331
++++++++ Policy training ++++++++++
Current mean reward: 1684.017691 | mean episode length: 492.250000
val_loss=50.59302
val_loss=412.14938
val_loss=46.36550
val_loss=109.92419
val_loss=333.38596
val_loss=122.99782
val_loss=60.55813
val_loss=1256.75562
val_loss=197.64281
val_loss=118.52753
adv_loss= 4.94440
adv_loss= 3.23179
adv_loss= 3.21404
adv_loss= 2.88104
adv_loss=10.39473
adv_loss= 2.77836
adv_loss=14.58765
adv_loss= 3.21608
adv_loss= 4.85541
adv_loss= 2.87544
surrogate= 0.01757, entropy= 1.95422, loss= 0.01757
surrogate=-0.03355, entropy= 1.95255, loss=-0.03355
surrogate=-0.01365, entropy= 1.95273, loss=-0.01365
surrogate=-0.00894, entropy= 1.95279, loss=-0.00894
surrogate=-0.00236, entropy= 1.95283, loss=-0.00236
surrogate=-0.00219, entropy= 1.95138, loss=-0.00219
surrogate=-0.00763, entropy= 1.94990, loss=-0.00763
surrogate=-0.00749, entropy= 1.94863, loss=-0.00749
surrogate=-0.00954, entropy= 1.94990, loss=-0.00954
surrogate=-0.04141, entropy= 1.95055, loss=-0.04141
std_min= 0.38608, std_max= 0.53336, std_mean= 0.46776
val lr: [0.00016495901639344262], policy lr: [0.00019795081967213113]
Policy Loss: -0.041411, | Entropy Bonus: -0, | Value Loss: 118.53, | Advantage Loss: 2.8754
Time elapsed (s): 1.653726577758789
Agent stdevs: 0.46776083
--------------------------------------------------------------------------------

Step 332
++++++++ Policy training ++++++++++
Current mean reward: 2214.689086 | mean episode length: 627.000000
val_loss=44.20019
val_loss=18.16914
val_loss=18.12195
val_loss=28.30203
val_loss=19.06961
val_loss=28.47827
val_loss=32.76493
val_loss=30.84201
val_loss=49.54837
val_loss=20.70840
adv_loss= 9.07183
adv_loss= 2.24955
adv_loss= 2.74416
adv_loss= 3.72026
adv_loss= 1.93040
adv_loss= 2.11623
adv_loss= 4.00896
adv_loss= 1.19061
adv_loss= 4.59992
adv_loss= 2.65402
surrogate=-0.01029, entropy= 1.94974, loss=-0.01029
surrogate=-0.00204, entropy= 1.94802, loss=-0.00204
surrogate=-0.01645, entropy= 1.94600, loss=-0.01645
surrogate= 0.00559, entropy= 1.94450, loss= 0.00559
surrogate=-0.01547, entropy= 1.94371, loss=-0.01547
surrogate=-0.03936, entropy= 1.94123, loss=-0.03936
surrogate=-0.00783, entropy= 1.94038, loss=-0.00783
surrogate=-0.02218, entropy= 1.93967, loss=-0.02218
surrogate=-0.02969, entropy= 1.93983, loss=-0.02969
surrogate=-0.01948, entropy= 1.93797, loss=-0.01948
std_min= 0.38472, std_max= 0.53035, std_mean= 0.46576
val lr: [0.00016470286885245902], policy lr: [0.0001976434426229508]
Policy Loss: -0.019483, | Entropy Bonus: -0, | Value Loss: 20.708, | Advantage Loss: 2.654
Time elapsed (s): 1.6325011253356934
Agent stdevs: 0.4657649
--------------------------------------------------------------------------------

Step 333
++++++++ Policy training ++++++++++
Current mean reward: 1427.356604 | mean episode length: 408.800000
val_loss=71.17436
val_loss=26.57166
val_loss=50.98992
val_loss=27.69931
val_loss=15.74390
val_loss=61.73356
val_loss=22.42074
val_loss=13.70848
val_loss=18.13006
val_loss=28.19785
adv_loss=24.81863
adv_loss= 2.76400
adv_loss= 4.27929
adv_loss=10.45779
adv_loss= 1.11705
adv_loss= 5.67920
adv_loss= 2.58609
adv_loss= 2.71850
adv_loss=21.93364
adv_loss= 3.54430
surrogate= 0.04527, entropy= 1.94077, loss= 0.04527
surrogate=-0.02784, entropy= 1.94002, loss=-0.02784
surrogate=-0.00537, entropy= 1.94181, loss=-0.00537
surrogate=-0.01853, entropy= 1.94315, loss=-0.01853
surrogate=-0.03367, entropy= 1.94474, loss=-0.03367
surrogate=-0.02696, entropy= 1.94474, loss=-0.02696
surrogate=-0.01820, entropy= 1.94623, loss=-0.01820
surrogate=-0.00621, entropy= 1.94922, loss=-0.00621
surrogate=-0.00455, entropy= 1.95119, loss=-0.00455
surrogate=-0.00627, entropy= 1.95146, loss=-0.00627
std_min= 0.38728, std_max= 0.53300, std_mean= 0.46780
val lr: [0.00016444672131147542], policy lr: [0.00019733606557377048]
Policy Loss: -0.0062682, | Entropy Bonus: -0, | Value Loss: 28.198, | Advantage Loss: 3.5443
Time elapsed (s): 1.633782148361206
Agent stdevs: 0.4677999
--------------------------------------------------------------------------------

Step 334
++++++++ Policy training ++++++++++
Current mean reward: 1414.596595 | mean episode length: 400.600000
val_loss=41.67064
val_loss=37.76582
val_loss=18.50564
val_loss=56.87242
val_loss=50.97361
val_loss=15.04102
val_loss=13.59690
val_loss=21.77816
val_loss=26.77377
val_loss=39.31682
adv_loss= 4.60929
adv_loss=19.23763
adv_loss= 2.08354
adv_loss= 1.63121
adv_loss= 4.24110
adv_loss= 1.20070
adv_loss= 1.09596
adv_loss= 5.13833
adv_loss= 1.60334
adv_loss= 4.32024
surrogate=-0.02355, entropy= 1.94792, loss=-0.02355
surrogate= 0.00151, entropy= 1.94313, loss= 0.00151
surrogate=-0.01364, entropy= 1.93970, loss=-0.01364
surrogate=-0.01235, entropy= 1.93722, loss=-0.01235
surrogate=-0.01017, entropy= 1.93550, loss=-0.01017
surrogate=-0.02945, entropy= 1.93248, loss=-0.02945
surrogate=-0.01041, entropy= 1.92934, loss=-0.01041
surrogate=-0.02023, entropy= 1.92726, loss=-0.02023
surrogate=-0.03591, entropy= 1.92500, loss=-0.03591
surrogate=-0.03502, entropy= 1.92404, loss=-0.03502
std_min= 0.38410, std_max= 0.52262, std_mean= 0.46335
val lr: [0.0001641905737704918], policy lr: [0.00019702868852459012]
Policy Loss: -0.035021, | Entropy Bonus: -0, | Value Loss: 39.317, | Advantage Loss: 4.3202
Time elapsed (s): 1.6455001831054688
Agent stdevs: 0.46334696
--------------------------------------------------------------------------------

Step 335
++++++++ Policy training ++++++++++
Current mean reward: 2075.606358 | mean episode length: 621.333333
val_loss=52.82957
val_loss=47.80289
val_loss=46.05254
val_loss=88.34174
val_loss=79.12654
val_loss=1311.53271
val_loss=129.50366
val_loss=75.46051
val_loss=449.08081
val_loss=423.25925
adv_loss= 2.78977
adv_loss= 1.89302
adv_loss= 3.07123
adv_loss= 3.53377
adv_loss= 2.31050
adv_loss= 2.45381
adv_loss= 5.43247
adv_loss= 2.02129
adv_loss= 3.17414
adv_loss= 4.64083
surrogate= 0.00105, entropy= 1.92422, loss= 0.00105
surrogate= 0.01752, entropy= 1.92635, loss= 0.01752
surrogate=-0.01647, entropy= 1.92844, loss=-0.01647
surrogate=-0.00538, entropy= 1.93163, loss=-0.00538
surrogate=-0.00786, entropy= 1.93391, loss=-0.00786
surrogate=-0.01660, entropy= 1.93631, loss=-0.01660
surrogate=-0.02393, entropy= 1.93717, loss=-0.02393
surrogate=-0.04002, entropy= 1.93956, loss=-0.04002
surrogate=-0.00677, entropy= 1.94116, loss=-0.00677
surrogate=-0.02668, entropy= 1.94383, loss=-0.02668
std_min= 0.38956, std_max= 0.52186, std_mean= 0.46608
val lr: [0.00016393442622950822], policy lr: [0.00019672131147540983]
Policy Loss: -0.026684, | Entropy Bonus: -0, | Value Loss: 423.26, | Advantage Loss: 4.6408
Time elapsed (s): 1.6836297512054443
Agent stdevs: 0.46607503
--------------------------------------------------------------------------------

Step 336
++++++++ Policy training ++++++++++
Current mean reward: 1132.275279 | mean episode length: 323.800000
val_loss=203.05394
val_loss=33.21185
val_loss=386.39984
val_loss=105.85266
val_loss=226.29202
val_loss=100.10746
val_loss=97.30585
val_loss=39.88318
val_loss=50.71778
val_loss=105.06033
adv_loss= 9.12792
adv_loss= 5.61662
adv_loss= 5.44242
adv_loss=12.62569
adv_loss= 4.02687
adv_loss= 1.53937
adv_loss= 4.18811
adv_loss= 9.48214
adv_loss=10.13450
adv_loss=356.21408
surrogate= 0.01459, entropy= 1.94426, loss= 0.01459
surrogate= 0.01122, entropy= 1.94360, loss= 0.01122
surrogate=-0.00329, entropy= 1.94455, loss=-0.00329
surrogate= 0.00130, entropy= 1.94428, loss= 0.00130
surrogate=-0.01515, entropy= 1.94175, loss=-0.01515
surrogate= 0.01191, entropy= 1.94171, loss= 0.01191
surrogate=-0.01131, entropy= 1.94207, loss=-0.01131
surrogate=-0.03431, entropy= 1.93999, loss=-0.03431
surrogate=-0.01526, entropy= 1.93999, loss=-0.01526
surrogate=-0.00869, entropy= 1.93866, loss=-0.00869
std_min= 0.38925, std_max= 0.52088, std_mean= 0.46525
val lr: [0.0001636782786885246], policy lr: [0.00019641393442622947]
Policy Loss: -0.0086948, | Entropy Bonus: -0, | Value Loss: 105.06, | Advantage Loss: 356.21
Time elapsed (s): 1.6481900215148926
Agent stdevs: 0.4652488
--------------------------------------------------------------------------------

Step 337
++++++++ Policy training ++++++++++
Current mean reward: 1354.521059 | mean episode length: 384.250000
val_loss=51.80188
val_loss=84.23334
val_loss=40.06154
val_loss=27.36114
val_loss=39.57912
val_loss=33.67150
val_loss=31.34606
val_loss=15.92982
val_loss=27.78156
val_loss=19.09357
adv_loss= 4.58695
adv_loss= 1.97402
adv_loss= 2.59626
adv_loss= 3.69595
adv_loss= 2.85583
adv_loss= 3.09280
adv_loss= 7.55081
adv_loss= 4.84971
adv_loss= 3.11237
adv_loss= 2.04278
surrogate= 0.01187, entropy= 1.93826, loss= 0.01187
surrogate= 0.02401, entropy= 1.93810, loss= 0.02401
surrogate=-0.01541, entropy= 1.93863, loss=-0.01541
surrogate= 0.00107, entropy= 1.93869, loss= 0.00107
surrogate= 0.00699, entropy= 1.93883, loss= 0.00699
surrogate=-0.01284, entropy= 1.93868, loss=-0.01284
surrogate=-0.01565, entropy= 1.93808, loss=-0.01565
surrogate=-0.03676, entropy= 1.93683, loss=-0.03676
surrogate=-0.01108, entropy= 1.93723, loss=-0.01108
surrogate=-0.02160, entropy= 1.93757, loss=-0.02160
std_min= 0.38922, std_max= 0.51848, std_mean= 0.46501
val lr: [0.00016342213114754097], policy lr: [0.00019610655737704915]
Policy Loss: -0.021604, | Entropy Bonus: -0, | Value Loss: 19.094, | Advantage Loss: 2.0428
Time elapsed (s): 1.6523644924163818
Agent stdevs: 0.46500722
--------------------------------------------------------------------------------

Step 338
++++++++ Policy training ++++++++++
Current mean reward: 1406.986421 | mean episode length: 409.500000
val_loss=54.80761
val_loss=22.51381
val_loss=28.29072
val_loss=20.17018
val_loss=34.37890
val_loss=18.60123
val_loss=21.80029
val_loss=10.65963
val_loss=13.25004
val_loss= 9.71824
adv_loss= 7.58907
adv_loss= 2.21978
adv_loss= 1.76488
adv_loss= 0.93200
adv_loss= 3.05543
adv_loss= 8.21024
adv_loss= 1.23199
adv_loss= 2.79475
adv_loss= 8.47521
adv_loss= 4.12299
surrogate= 0.00647, entropy= 1.93823, loss= 0.00647
surrogate=-0.01938, entropy= 1.93920, loss=-0.01938
surrogate=-0.01087, entropy= 1.93618, loss=-0.01087
surrogate=-0.00465, entropy= 1.93776, loss=-0.00465
surrogate=-0.00560, entropy= 1.93682, loss=-0.00560
surrogate=-0.02053, entropy= 1.93798, loss=-0.02053
surrogate=-0.02157, entropy= 1.93844, loss=-0.02157
surrogate= 0.00356, entropy= 1.93896, loss= 0.00356
surrogate=-0.00679, entropy= 1.93731, loss=-0.00679
surrogate= 0.00666, entropy= 1.93646, loss= 0.00666
std_min= 0.38969, std_max= 0.51815, std_mean= 0.46478
val lr: [0.00016316598360655737], policy lr: [0.00019579918032786882]
Policy Loss: 0.0066641, | Entropy Bonus: -0, | Value Loss: 9.7182, | Advantage Loss: 4.123
Time elapsed (s): 1.6456584930419922
Agent stdevs: 0.46478412
--------------------------------------------------------------------------------

Step 339
++++++++ Policy training ++++++++++
Current mean reward: 2509.046964 | mean episode length: 709.500000
val_loss=39.78748
val_loss=14.88563
val_loss=68.09595
val_loss=13.11885
val_loss= 9.39572
val_loss= 8.30757
val_loss= 7.15428
val_loss= 6.49445
val_loss= 7.14061
val_loss= 6.20635
adv_loss= 0.82406
adv_loss= 0.91010
adv_loss= 0.90042
adv_loss= 1.60212
adv_loss= 0.96033
adv_loss= 1.92737
adv_loss= 1.66700
adv_loss= 0.89392
adv_loss= 0.69603
adv_loss= 1.75864
surrogate= 0.00143, entropy= 1.93427, loss= 0.00143
surrogate=-0.00824, entropy= 1.93368, loss=-0.00824
surrogate=-0.00629, entropy= 1.93298, loss=-0.00629
surrogate= 0.01755, entropy= 1.93159, loss= 0.01755
surrogate=-0.04148, entropy= 1.93191, loss=-0.04148
surrogate=-0.00573, entropy= 1.93085, loss=-0.00573
surrogate=-0.00890, entropy= 1.92898, loss=-0.00890
surrogate=-0.02207, entropy= 1.92808, loss=-0.02207
surrogate=-0.02768, entropy= 1.92643, loss=-0.02768
surrogate=-0.01553, entropy= 1.92502, loss=-0.01553
std_min= 0.38727, std_max= 0.51891, std_mean= 0.46316
val lr: [0.00016290983606557377], policy lr: [0.0001954918032786885]
Policy Loss: -0.015534, | Entropy Bonus: -0, | Value Loss: 6.2064, | Advantage Loss: 1.7586
Time elapsed (s): 1.643434762954712
Agent stdevs: 0.46315762
--------------------------------------------------------------------------------

Step 340
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1695.7
++++++++ Policy training ++++++++++
Current mean reward: 1290.231566 | mean episode length: 365.400000
val_loss=47.73332
val_loss=21.23659
val_loss=63.15672
val_loss=18.86183
val_loss=25.16201
val_loss=77.82848
val_loss=22.17408
val_loss=15.33301
val_loss=35.00684
val_loss=33.31115
adv_loss= 1.17977
adv_loss= 1.23729
adv_loss= 2.35603
adv_loss= 6.84172
adv_loss= 4.25525
adv_loss= 6.15572
adv_loss= 1.24310
adv_loss= 2.14245
adv_loss= 1.75936
adv_loss= 1.52870
surrogate= 0.01178, entropy= 1.92358, loss= 0.01178
surrogate= 0.03377, entropy= 1.92042, loss= 0.03377
surrogate=-0.00272, entropy= 1.91809, loss=-0.00272
surrogate=-0.03512, entropy= 1.91704, loss=-0.03512
surrogate=-0.02303, entropy= 1.91572, loss=-0.02303
surrogate= 0.00462, entropy= 1.91512, loss= 0.00462
surrogate=-0.02630, entropy= 1.91299, loss=-0.02630
surrogate=-0.04069, entropy= 1.91259, loss=-0.04069
surrogate= 0.02035, entropy= 1.91128, loss= 0.02035
surrogate=-0.02532, entropy= 1.90949, loss=-0.02532
std_min= 0.38419, std_max= 0.52285, std_mean= 0.46102
val lr: [0.00016265368852459017], policy lr: [0.0001951844262295082]
Policy Loss: -0.025322, | Entropy Bonus: -0, | Value Loss: 33.311, | Advantage Loss: 1.5287
Time elapsed (s): 1.6405858993530273
Agent stdevs: 0.46102497
--------------------------------------------------------------------------------

Step 341
++++++++ Policy training ++++++++++
Current mean reward: 1403.290941 | mean episode length: 398.600000
val_loss=121.49799
val_loss=105.42433
val_loss=150.24353
val_loss=31.32126
val_loss=36.52924
val_loss=119.82259
val_loss=107.79198
val_loss=25.07986
val_loss=20.50329
val_loss=25.74656
adv_loss= 2.61374
adv_loss= 4.19426
adv_loss= 2.59088
adv_loss= 2.37011
adv_loss= 2.31746
adv_loss= 4.02954
adv_loss= 7.48438
adv_loss= 1.62928
adv_loss= 2.22716
adv_loss= 5.69081
surrogate= 0.00741, entropy= 1.90858, loss= 0.00741
surrogate=-0.01165, entropy= 1.90793, loss=-0.01165
surrogate=-0.01934, entropy= 1.90839, loss=-0.01934
surrogate=-0.00587, entropy= 1.90705, loss=-0.00587
surrogate=-0.01641, entropy= 1.90660, loss=-0.01641
surrogate=-0.01700, entropy= 1.90445, loss=-0.01700
surrogate=-0.01649, entropy= 1.90395, loss=-0.01649
surrogate=-0.02188, entropy= 1.90364, loss=-0.02188
surrogate= 0.00319, entropy= 1.90223, loss= 0.00319
surrogate=-0.03801, entropy= 1.90163, loss=-0.03801
std_min= 0.37568, std_max= 0.52823, std_mean= 0.46067
val lr: [0.00016239754098360657], policy lr: [0.00019487704918032787]
Policy Loss: -0.038008, | Entropy Bonus: -0, | Value Loss: 25.747, | Advantage Loss: 5.6908
Time elapsed (s): 1.6947782039642334
Agent stdevs: 0.46067217
--------------------------------------------------------------------------------

Step 342
++++++++ Policy training ++++++++++
Current mean reward: 1332.898410 | mean episode length: 394.400000
val_loss=70.78485
val_loss=64.34749
val_loss=103.37900
val_loss=84.72132
val_loss=104.14661
val_loss=68.34361
val_loss=63.54163
val_loss=125.07323
val_loss=77.08893
val_loss=22.48044
adv_loss= 4.47439
adv_loss= 4.59700
adv_loss= 2.56615
adv_loss= 2.29491
adv_loss= 3.30275
adv_loss= 3.87050
adv_loss= 3.83512
adv_loss= 5.59047
adv_loss= 3.87925
adv_loss= 2.85017
surrogate= 0.01230, entropy= 1.90209, loss= 0.01230
surrogate= 0.01331, entropy= 1.90048, loss= 0.01331
surrogate=-0.00512, entropy= 1.89801, loss=-0.00512
surrogate=-0.00946, entropy= 1.89968, loss=-0.00946
surrogate= 0.02864, entropy= 1.89856, loss= 0.02864
surrogate=-0.03384, entropy= 1.89854, loss=-0.03384
surrogate= 0.03612, entropy= 1.89829, loss= 0.03612
surrogate=-0.02401, entropy= 1.89701, loss=-0.02401
surrogate= 0.00093, entropy= 1.89783, loss= 0.00093
surrogate=-0.03870, entropy= 1.89689, loss=-0.03870
std_min= 0.37579, std_max= 0.52766, std_mean= 0.45990
val lr: [0.00016214139344262297], policy lr: [0.00019456967213114755]
Policy Loss: -0.038705, | Entropy Bonus: -0, | Value Loss: 22.48, | Advantage Loss: 2.8502
Time elapsed (s): 1.635512351989746
Agent stdevs: 0.45989987
--------------------------------------------------------------------------------

Step 343
++++++++ Policy training ++++++++++
Current mean reward: 2946.777929 | mean episode length: 869.500000
val_loss=1175.25989
val_loss=36.98061
val_loss=20.42995
val_loss=67.73180
val_loss=123.68830
val_loss=54.41098
val_loss=279.37503
val_loss=242.56439
val_loss=1304.97009
val_loss=504.81790
adv_loss= 1.70928
adv_loss= 3.10520
adv_loss=1499.29517
adv_loss= 3.53287
adv_loss= 3.19162
adv_loss= 4.36651
adv_loss= 8.94592
adv_loss= 3.26065
adv_loss= 2.50167
adv_loss= 3.82747
surrogate=-0.01707, entropy= 1.89942, loss=-0.01707
surrogate= 0.00760, entropy= 1.89917, loss= 0.00760
surrogate= 0.02242, entropy= 1.89996, loss= 0.02242
surrogate= 0.00388, entropy= 1.90170, loss= 0.00388
surrogate= 0.00386, entropy= 1.90425, loss= 0.00386
surrogate= 0.00629, entropy= 1.90328, loss= 0.00629
surrogate= 0.00776, entropy= 1.90399, loss= 0.00776
surrogate=-0.01260, entropy= 1.90314, loss=-0.01260
surrogate= 0.04238, entropy= 1.90358, loss= 0.04238
surrogate=-0.02659, entropy= 1.90464, loss=-0.02659
std_min= 0.37496, std_max= 0.52775, std_mean= 0.46121
val lr: [0.00016188524590163934], policy lr: [0.00019426229508196717]
Policy Loss: -0.026585, | Entropy Bonus: -0, | Value Loss: 504.82, | Advantage Loss: 3.8275
Time elapsed (s): 1.6454753875732422
Agent stdevs: 0.4612085
--------------------------------------------------------------------------------

Step 344
++++++++ Policy training ++++++++++
Current mean reward: 1173.818179 | mean episode length: 334.000000
val_loss=37.57267
val_loss=24.81686
val_loss=99.63622
val_loss=30.04928
val_loss=69.57267
val_loss=29.13093
val_loss=24.49076
val_loss=18.52775
val_loss=16.75638
val_loss=25.60848
adv_loss= 6.84862
adv_loss=19.10712
adv_loss= 3.26377
adv_loss=17.62694
adv_loss= 1.89037
adv_loss= 8.58932
adv_loss= 3.98831
adv_loss=12.94147
adv_loss= 0.97555
adv_loss= 2.75132
surrogate=-0.02436, entropy= 1.90476, loss=-0.02436
surrogate=-0.01098, entropy= 1.90489, loss=-0.01098
surrogate=-0.02015, entropy= 1.90500, loss=-0.02015
surrogate= 0.00418, entropy= 1.90398, loss= 0.00418
surrogate=-0.04586, entropy= 1.90247, loss=-0.04586
surrogate=-0.01283, entropy= 1.90288, loss=-0.01283
surrogate=-0.03845, entropy= 1.90268, loss=-0.03845
surrogate=-0.01133, entropy= 1.90426, loss=-0.01133
surrogate=-0.04163, entropy= 1.90448, loss=-0.04163
surrogate=-0.01612, entropy= 1.90359, loss=-0.01612
std_min= 0.37419, std_max= 0.52547, std_mean= 0.46104
val lr: [0.00016162909836065577], policy lr: [0.0001939549180327869]
Policy Loss: -0.016124, | Entropy Bonus: -0, | Value Loss: 25.608, | Advantage Loss: 2.7513
Time elapsed (s): 1.6306769847869873
Agent stdevs: 0.46103987
--------------------------------------------------------------------------------

Step 345
++++++++ Policy training ++++++++++
Current mean reward: 1749.316633 | mean episode length: 485.750000
val_loss=17.42562
val_loss=44.42195
val_loss=35.78379
val_loss=28.33836
val_loss=17.74948
val_loss=16.22019
val_loss=13.88732
val_loss= 8.64880
val_loss=27.55725
val_loss=29.40639
adv_loss= 1.95359
adv_loss= 3.56901
adv_loss= 0.93652
adv_loss= 1.01123
adv_loss= 2.77293
adv_loss= 1.22487
adv_loss= 2.40068
adv_loss= 1.37808
adv_loss= 2.10778
adv_loss= 2.50309
surrogate= 0.01105, entropy= 1.89941, loss= 0.01105
surrogate=-0.01836, entropy= 1.89510, loss=-0.01836
surrogate=-0.01800, entropy= 1.89157, loss=-0.01800
surrogate= 0.01386, entropy= 1.88755, loss= 0.01386
surrogate=-0.01381, entropy= 1.88524, loss=-0.01381
surrogate=-0.03470, entropy= 1.88115, loss=-0.03470
surrogate=-0.03103, entropy= 1.87890, loss=-0.03103
surrogate=-0.01282, entropy= 1.87472, loss=-0.01282
surrogate=-0.04140, entropy= 1.87248, loss=-0.04140
surrogate=-0.04217, entropy= 1.87069, loss=-0.04217
std_min= 0.37100, std_max= 0.51903, std_mean= 0.45590
val lr: [0.00016137295081967211], policy lr: [0.00019364754098360654]
Policy Loss: -0.042169, | Entropy Bonus: -0, | Value Loss: 29.406, | Advantage Loss: 2.5031
Time elapsed (s): 1.633575201034546
Agent stdevs: 0.455902
--------------------------------------------------------------------------------

Step 346
++++++++ Policy training ++++++++++
Current mean reward: 1942.735982 | mean episode length: 538.666667
val_loss=17.28766
val_loss=10.62847
val_loss=13.78081
val_loss=11.10648
val_loss=10.94745
val_loss=11.23674
val_loss= 9.28275
val_loss= 9.16141
val_loss= 8.90396
val_loss= 8.58875
adv_loss= 7.47833
adv_loss= 1.38213
adv_loss= 3.54024
adv_loss= 1.67438
adv_loss= 1.89859
adv_loss= 0.83447
adv_loss= 1.00242
adv_loss= 0.85952
adv_loss= 1.71903
adv_loss= 0.91876
surrogate= 0.00279, entropy= 1.87064, loss= 0.00279
surrogate= 0.00925, entropy= 1.86924, loss= 0.00925
surrogate=-0.00220, entropy= 1.86904, loss=-0.00220
surrogate= 0.01563, entropy= 1.86849, loss= 0.01563
surrogate= 0.01055, entropy= 1.86822, loss= 0.01055
surrogate=-0.02370, entropy= 1.86882, loss=-0.02370
surrogate= 0.00271, entropy= 1.86701, loss= 0.00271
surrogate= 0.02050, entropy= 1.86559, loss= 0.02050
surrogate=-0.02479, entropy= 1.86408, loss=-0.02479
surrogate=-0.01439, entropy= 1.86326, loss=-0.01439
std_min= 0.36708, std_max= 0.51769, std_mean= 0.45508
val lr: [0.00016111680327868851], policy lr: [0.00019334016393442622]
Policy Loss: -0.014389, | Entropy Bonus: -0, | Value Loss: 8.5888, | Advantage Loss: 0.91876
Time elapsed (s): 1.6500904560089111
Agent stdevs: 0.45507574
--------------------------------------------------------------------------------

Step 347
++++++++ Policy training ++++++++++
Current mean reward: 3473.801839 | mean episode length: 1000.000000
val_loss=281.88867
val_loss=301.53925
val_loss=281.70578
val_loss=37.70512
val_loss=1527.31445
val_loss=2329.81299
val_loss=63.45042
val_loss=703.87366
val_loss=184.30945
val_loss=1208.92041
adv_loss= 5.03356
adv_loss= 8.23740
adv_loss= 3.84000
adv_loss= 4.56264
adv_loss= 4.52704
adv_loss= 5.30960
adv_loss= 8.50913
adv_loss= 5.62736
adv_loss= 5.71849
adv_loss= 5.16591
surrogate=-0.00437, entropy= 1.86309, loss=-0.00437
surrogate=-0.02074, entropy= 1.86503, loss=-0.02074
surrogate=-0.03948, entropy= 1.86800, loss=-0.03948
surrogate=-0.01346, entropy= 1.86746, loss=-0.01346
surrogate=-0.02089, entropy= 1.87005, loss=-0.02089
surrogate=-0.01412, entropy= 1.87224, loss=-0.01412
surrogate=-0.00695, entropy= 1.87224, loss=-0.00695
surrogate=-0.02705, entropy= 1.87387, loss=-0.02705
surrogate=-0.02222, entropy= 1.87416, loss=-0.02222
surrogate=-0.00396, entropy= 1.87291, loss=-0.00396
std_min= 0.36623, std_max= 0.52010, std_mean= 0.45679
val lr: [0.00016086065573770491], policy lr: [0.0001930327868852459]
Policy Loss: -0.0039555, | Entropy Bonus: -0, | Value Loss: 1208.9, | Advantage Loss: 5.1659
Time elapsed (s): 1.707390308380127
Agent stdevs: 0.45678687
--------------------------------------------------------------------------------

Step 348
++++++++ Policy training ++++++++++
Current mean reward: 1423.038670 | mean episode length: 392.400000
val_loss=40.63600
val_loss=18.32490
val_loss=22.97563
val_loss=20.51956
val_loss=19.12398
val_loss=20.39043
val_loss=11.23575
val_loss=18.90780
val_loss= 8.91741
val_loss=19.10227
adv_loss=17.41502
adv_loss= 3.61642
adv_loss= 3.32278
adv_loss= 2.90527
adv_loss=19.66106
adv_loss= 4.40352
adv_loss=13.87461
adv_loss=14.59594
adv_loss= 3.26909
adv_loss= 5.36445
surrogate= 0.02251, entropy= 1.87405, loss= 0.02251
surrogate=-0.02477, entropy= 1.87312, loss=-0.02477
surrogate= 0.01960, entropy= 1.87413, loss= 0.01960
surrogate=-0.01177, entropy= 1.87605, loss=-0.01177
surrogate= 0.02368, entropy= 1.87648, loss= 0.02368
surrogate= 0.01364, entropy= 1.87773, loss= 0.01364
surrogate= 0.01914, entropy= 1.87837, loss= 0.01914
surrogate=-0.01079, entropy= 1.87875, loss=-0.01079
surrogate=-0.03528, entropy= 1.88087, loss=-0.03528
surrogate=-0.01698, entropy= 1.88001, loss=-0.01698
std_min= 0.36944, std_max= 0.52109, std_mean= 0.45760
val lr: [0.00016060450819672131], policy lr: [0.00019272540983606557]
Policy Loss: -0.01698, | Entropy Bonus: -0, | Value Loss: 19.102, | Advantage Loss: 5.3644
Time elapsed (s): 1.6420998573303223
Agent stdevs: 0.45759603
--------------------------------------------------------------------------------

Step 349
++++++++ Policy training ++++++++++
Current mean reward: 1551.771306 | mean episode length: 431.750000
val_loss=25.86608
val_loss=17.17221
val_loss=16.38200
val_loss=27.52962
val_loss=17.17675
val_loss=22.24372
val_loss=15.38405
val_loss=19.69124
val_loss=17.63467
val_loss=27.13900
adv_loss= 2.51475
adv_loss= 2.97062
adv_loss= 1.64143
adv_loss= 1.66611
adv_loss= 2.75987
adv_loss= 2.72577
adv_loss= 8.55643
adv_loss= 7.32336
adv_loss= 2.37535
adv_loss= 2.65126
surrogate= 0.01877, entropy= 1.87915, loss= 0.01877
surrogate= 0.00998, entropy= 1.88113, loss= 0.00998
surrogate=-0.00114, entropy= 1.88130, loss=-0.00114
surrogate= 0.03379, entropy= 1.88122, loss= 0.03379
surrogate= 0.02346, entropy= 1.88254, loss= 0.02346
surrogate=-0.03177, entropy= 1.88188, loss=-0.03177
surrogate=-0.02052, entropy= 1.88158, loss=-0.02052
surrogate= 0.01139, entropy= 1.88108, loss= 0.01139
surrogate= 0.01795, entropy= 1.88077, loss= 0.01795
surrogate=-0.02054, entropy= 1.88205, loss=-0.02054
std_min= 0.36632, std_max= 0.52510, std_mean= 0.45835
val lr: [0.00016034836065573771], policy lr: [0.00019241803278688524]
Policy Loss: -0.020542, | Entropy Bonus: -0, | Value Loss: 27.139, | Advantage Loss: 2.6513
Time elapsed (s): 1.6603782176971436
Agent stdevs: 0.4583502
--------------------------------------------------------------------------------

Step 350
++++++++ Policy training ++++++++++
Current mean reward: 1794.633986 | mean episode length: 502.500000
val_loss=24.89077
val_loss=14.45563
val_loss=11.32940
val_loss= 9.79047
val_loss=17.49350
val_loss= 8.49604
val_loss=12.90649
val_loss=10.11437
val_loss=17.43225
val_loss=13.79438
adv_loss= 1.95375
adv_loss= 2.99707
adv_loss= 7.35180
adv_loss= 1.28134
adv_loss= 2.43503
adv_loss= 1.82680
adv_loss= 2.52209
adv_loss= 4.12274
adv_loss= 2.61016
adv_loss= 3.08637
surrogate= 0.00821, entropy= 1.87962, loss= 0.00821
surrogate= 0.02475, entropy= 1.87715, loss= 0.02475
surrogate= 0.03330, entropy= 1.87497, loss= 0.03330
surrogate= 0.00903, entropy= 1.87195, loss= 0.00903
surrogate=-0.00665, entropy= 1.86944, loss=-0.00665
surrogate= 0.00352, entropy= 1.86804, loss= 0.00352
surrogate=-0.03852, entropy= 1.86644, loss=-0.03852
surrogate=-0.02686, entropy= 1.86523, loss=-0.02686
surrogate=-0.00914, entropy= 1.86387, loss=-0.00914
surrogate=-0.02774, entropy= 1.86334, loss=-0.02774
std_min= 0.36215, std_max= 0.52204, std_mean= 0.45571
val lr: [0.00016009221311475409], policy lr: [0.0001921106557377049]
Policy Loss: -0.027743, | Entropy Bonus: -0, | Value Loss: 13.794, | Advantage Loss: 3.0864
Time elapsed (s): 1.6761739253997803
Agent stdevs: 0.45571494
--------------------------------------------------------------------------------

Step 351
++++++++ Policy training ++++++++++
Current mean reward: 1935.568900 | mean episode length: 535.666667
val_loss=14.76686
val_loss=16.76136
val_loss=15.80439
val_loss= 9.49796
val_loss=13.24250
val_loss= 7.41137
val_loss= 8.74778
val_loss= 7.45654
val_loss= 6.38704
val_loss= 8.99129
adv_loss= 1.89841
adv_loss= 2.64479
adv_loss= 1.67428
adv_loss= 1.38317
adv_loss= 2.09695
adv_loss= 0.69250
adv_loss= 1.04374
adv_loss= 3.78103
adv_loss= 1.50483
adv_loss= 2.56209
surrogate=-0.03349, entropy= 1.86476, loss=-0.03349
surrogate=-0.02939, entropy= 1.86656, loss=-0.02939
surrogate=-0.00078, entropy= 1.86791, loss=-0.00078
surrogate= 0.00520, entropy= 1.86999, loss= 0.00520
surrogate=-0.02831, entropy= 1.87194, loss=-0.02831
surrogate=-0.03819, entropy= 1.87247, loss=-0.03819
surrogate=-0.02732, entropy= 1.87309, loss=-0.02732
surrogate=-0.01881, entropy= 1.87413, loss=-0.01881
surrogate=-0.03593, entropy= 1.87560, loss=-0.03593
surrogate=-0.01012, entropy= 1.87590, loss=-0.01012
std_min= 0.36127, std_max= 0.52985, std_mean= 0.45807
val lr: [0.0001598360655737705], policy lr: [0.0001918032786885246]
Policy Loss: -0.010122, | Entropy Bonus: -0, | Value Loss: 8.9913, | Advantage Loss: 2.5621
Time elapsed (s): 1.679785966873169
Agent stdevs: 0.45806906
--------------------------------------------------------------------------------

Step 352
++++++++ Policy training ++++++++++
Current mean reward: 1784.242105 | mean episode length: 491.250000
val_loss=16.59448
val_loss=11.98571
val_loss=37.76905
val_loss= 8.76283
val_loss=13.78745
val_loss=16.87503
val_loss=13.44707
val_loss= 6.04110
val_loss=11.66290
val_loss=10.42262
adv_loss= 3.99888
adv_loss= 6.12023
adv_loss= 3.19828
adv_loss= 1.06911
adv_loss= 2.42614
adv_loss= 2.55910
adv_loss= 0.93443
adv_loss= 3.65668
adv_loss= 1.46497
adv_loss= 1.43030
surrogate= 0.02748, entropy= 1.87340, loss= 0.02748
surrogate=-0.01506, entropy= 1.86920, loss=-0.01506
surrogate=-0.00589, entropy= 1.86486, loss=-0.00589
surrogate=-0.02155, entropy= 1.86226, loss=-0.02155
surrogate=-0.03714, entropy= 1.85752, loss=-0.03714
surrogate= 0.00845, entropy= 1.85371, loss= 0.00845
surrogate=-0.01732, entropy= 1.85055, loss=-0.01732
surrogate=-0.03308, entropy= 1.84708, loss=-0.03308
surrogate=-0.00737, entropy= 1.84447, loss=-0.00737
surrogate=-0.02642, entropy= 1.84193, loss=-0.02642
std_min= 0.35743, std_max= 0.52392, std_mean= 0.45286
val lr: [0.00015957991803278689], policy lr: [0.00019149590163934424]
Policy Loss: -0.026417, | Entropy Bonus: -0, | Value Loss: 10.423, | Advantage Loss: 1.4303
Time elapsed (s): 1.7014453411102295
Agent stdevs: 0.45286474
--------------------------------------------------------------------------------

Step 353
++++++++ Policy training ++++++++++
Current mean reward: 3002.940448 | mean episode length: 856.500000
val_loss=469.04135
val_loss=57.59124
val_loss=134.52736
val_loss=123.87943
val_loss=458.10849
val_loss=254.21510
val_loss=40.49960
val_loss=40.66162
val_loss=124.78947
val_loss=233.28568
adv_loss= 2.40058
adv_loss= 2.49003
adv_loss= 3.33226
adv_loss= 2.45562
adv_loss= 4.46579
adv_loss= 1.42669
adv_loss= 1.88340
adv_loss= 1.52198
adv_loss= 1.10554
adv_loss= 2.99638
surrogate=-0.00500, entropy= 1.83753, loss=-0.00500
surrogate=-0.02551, entropy= 1.83563, loss=-0.02551
surrogate=-0.01053, entropy= 1.83594, loss=-0.01053
surrogate= 0.01024, entropy= 1.83617, loss= 0.01024
surrogate=-0.02469, entropy= 1.83471, loss=-0.02469
surrogate=-0.02937, entropy= 1.83582, loss=-0.02937
surrogate= 0.01504, entropy= 1.83618, loss= 0.01504
surrogate=-0.02669, entropy= 1.83553, loss=-0.02669
surrogate=-0.02535, entropy= 1.83640, loss=-0.02535
surrogate=-0.03919, entropy= 1.83822, loss=-0.03919
std_min= 0.35693, std_max= 0.52499, std_mean= 0.45238
val lr: [0.00015932377049180329], policy lr: [0.00019118852459016394]
Policy Loss: -0.039195, | Entropy Bonus: -0, | Value Loss: 233.29, | Advantage Loss: 2.9964
Time elapsed (s): 1.762235403060913
Agent stdevs: 0.45238146
--------------------------------------------------------------------------------

Step 354
++++++++ Policy training ++++++++++
Current mean reward: 2145.470122 | mean episode length: 611.666667
val_loss=20.56573
val_loss=16.09869
val_loss= 9.39818
val_loss=14.95336
val_loss=15.15845
val_loss=12.39984
val_loss=10.52812
val_loss=10.63352
val_loss=13.74250
val_loss= 7.99858
adv_loss= 1.07498
adv_loss= 1.24754
adv_loss= 5.56546
adv_loss= 1.10623
adv_loss= 0.96567
adv_loss= 1.02895
adv_loss= 1.19997
adv_loss= 0.66759
adv_loss= 1.18470
adv_loss= 2.81506
surrogate=-0.02597, entropy= 1.83614, loss=-0.02597
surrogate= 0.04077, entropy= 1.83284, loss= 0.04077
surrogate=-0.01474, entropy= 1.83001, loss=-0.01474
surrogate=-0.00256, entropy= 1.82772, loss=-0.00256
surrogate= 0.03512, entropy= 1.82579, loss= 0.03512
surrogate= 0.01457, entropy= 1.82250, loss= 0.01457
surrogate=-0.02714, entropy= 1.82071, loss=-0.02714
surrogate=-0.01450, entropy= 1.81840, loss=-0.01450
surrogate=-0.00271, entropy= 1.81584, loss=-0.00271
surrogate=-0.02886, entropy= 1.81394, loss=-0.02886
std_min= 0.35404, std_max= 0.51979, std_mean= 0.44871
val lr: [0.00015906762295081966], policy lr: [0.00019088114754098359]
Policy Loss: -0.028856, | Entropy Bonus: -0, | Value Loss: 7.9986, | Advantage Loss: 2.8151
Time elapsed (s): 1.6884167194366455
Agent stdevs: 0.44871208
--------------------------------------------------------------------------------

Step 355
++++++++ Policy training ++++++++++
Current mean reward: 2181.543922 | mean episode length: 626.000000
val_loss=132.08669
val_loss=488.83862
val_loss=46.07915
val_loss=79.84177
val_loss=1506.45959
val_loss=78.80125
val_loss=178.72548
val_loss=75.74114
val_loss=63.70396
val_loss=66.76173
adv_loss= 3.34389
adv_loss= 2.71223
adv_loss= 2.67381
adv_loss= 2.62179
adv_loss= 3.33561
adv_loss= 2.06991
adv_loss= 3.17160
adv_loss= 2.77618
adv_loss= 2.69485
adv_loss= 2.23024
surrogate= 0.04388, entropy= 1.81393, loss= 0.04388
surrogate=-0.00520, entropy= 1.81220, loss=-0.00520
surrogate= 0.00825, entropy= 1.81231, loss= 0.00825
surrogate=-0.01359, entropy= 1.81220, loss=-0.01359
surrogate=-0.00026, entropy= 1.80947, loss=-0.00026
surrogate=-0.00518, entropy= 1.80920, loss=-0.00518
surrogate=-0.01067, entropy= 1.80826, loss=-0.01067
surrogate=-0.00705, entropy= 1.80724, loss=-0.00705
surrogate=-0.00484, entropy= 1.80704, loss=-0.00484
surrogate=-0.01833, entropy= 1.80751, loss=-0.01833
std_min= 0.35435, std_max= 0.51943, std_mean= 0.44763
val lr: [0.00015881147540983606], policy lr: [0.00019057377049180326]
Policy Loss: -0.018333, | Entropy Bonus: -0, | Value Loss: 66.762, | Advantage Loss: 2.2302
Time elapsed (s): 1.7189526557922363
Agent stdevs: 0.44763228
--------------------------------------------------------------------------------

Step 356
++++++++ Policy training ++++++++++
Current mean reward: 3079.105864 | mean episode length: 869.000000
val_loss=28.14082
val_loss=19.16401
val_loss=17.63964
val_loss=30.81825
val_loss=10.05129
val_loss=29.82185
val_loss=32.20180
val_loss=25.56290
val_loss=18.58623
val_loss=12.93192
adv_loss= 2.36154
adv_loss= 0.93191
adv_loss= 1.27775
adv_loss= 1.73034
adv_loss= 1.69739
adv_loss= 3.30349
adv_loss= 0.98009
adv_loss= 0.72594
adv_loss= 1.26739
adv_loss= 2.07857
surrogate= 0.00133, entropy= 1.80520, loss= 0.00133
surrogate=-0.01206, entropy= 1.80350, loss=-0.01206
surrogate=-0.00812, entropy= 1.80036, loss=-0.00812
surrogate= 0.03410, entropy= 1.79840, loss= 0.03410
surrogate=-0.00829, entropy= 1.79624, loss=-0.00829
surrogate=-0.00967, entropy= 1.79378, loss=-0.00967
surrogate=-0.02642, entropy= 1.79141, loss=-0.02642
surrogate=-0.03998, entropy= 1.78975, loss=-0.03998
surrogate=-0.00344, entropy= 1.78588, loss=-0.00344
surrogate= 0.00376, entropy= 1.78380, loss= 0.00376
std_min= 0.35067, std_max= 0.51331, std_mean= 0.44416
val lr: [0.00015855532786885246], policy lr: [0.00019026639344262293]
Policy Loss: 0.0037598, | Entropy Bonus: -0, | Value Loss: 12.932, | Advantage Loss: 2.0786
Time elapsed (s): 1.6796789169311523
Agent stdevs: 0.44415835
--------------------------------------------------------------------------------

Step 357
++++++++ Policy training ++++++++++
Current mean reward: 2186.882469 | mean episode length: 612.333333
val_loss=12.80453
val_loss=16.28887
val_loss=14.34693
val_loss= 6.51140
val_loss=18.27241
val_loss=21.86757
val_loss=15.16850
val_loss= 6.85406
val_loss=14.14448
val_loss=14.57343
adv_loss= 1.65919
adv_loss= 1.16735
adv_loss= 2.70989
adv_loss= 0.94294
adv_loss= 1.80684
adv_loss= 1.19263
adv_loss= 1.67323
adv_loss= 3.99313
adv_loss= 1.40161
adv_loss= 3.00582
surrogate= 0.01134, entropy= 1.78403, loss= 0.01134
surrogate=-0.00481, entropy= 1.78576, loss=-0.00481
surrogate= 0.00123, entropy= 1.78581, loss= 0.00123
surrogate=-0.01216, entropy= 1.78602, loss=-0.01216
surrogate=-0.01068, entropy= 1.78388, loss=-0.01068
surrogate=-0.01503, entropy= 1.78524, loss=-0.01503
surrogate=-0.03039, entropy= 1.78286, loss=-0.03039
surrogate=-0.01981, entropy= 1.78197, loss=-0.01981
surrogate=-0.01749, entropy= 1.78237, loss=-0.01749
surrogate=-0.04137, entropy= 1.78112, loss=-0.04137
std_min= 0.35188, std_max= 0.51263, std_mean= 0.44358
val lr: [0.00015829918032786886], policy lr: [0.0001899590163934426]
Policy Loss: -0.041373, | Entropy Bonus: -0, | Value Loss: 14.573, | Advantage Loss: 3.0058
Time elapsed (s): 1.6873500347137451
Agent stdevs: 0.4435793
--------------------------------------------------------------------------------

Step 358
++++++++ Policy training ++++++++++
Current mean reward: 3367.674922 | mean episode length: 1000.000000
val_loss=1036.09497
val_loss=315.42160
val_loss=1480.34534
val_loss=245.18152
val_loss=496.97778
val_loss=206.29602
val_loss=148.64752
val_loss=1674.06372
val_loss=44.43340
val_loss=985.83820
adv_loss= 5.32111
adv_loss= 1.91006
adv_loss= 4.15719
adv_loss= 2.76168
adv_loss= 2.17207
adv_loss= 2.96110
adv_loss= 1.99346
adv_loss= 3.03817
adv_loss= 1.83652
adv_loss= 1.91038
surrogate=-0.00997, entropy= 1.78052, loss=-0.00997
surrogate=-0.01353, entropy= 1.78085, loss=-0.01353
surrogate=-0.02081, entropy= 1.78032, loss=-0.02081
surrogate=-0.04007, entropy= 1.78015, loss=-0.04007
surrogate=-0.02347, entropy= 1.77936, loss=-0.02347
surrogate=-0.01562, entropy= 1.77856, loss=-0.01562
surrogate=-0.02146, entropy= 1.77801, loss=-0.02146
surrogate= 0.02952, entropy= 1.77866, loss= 0.02952
surrogate=-0.02733, entropy= 1.77711, loss=-0.02733
surrogate=-0.01537, entropy= 1.77604, loss=-0.01537
std_min= 0.35117, std_max= 0.51229, std_mean= 0.44286
val lr: [0.00015804303278688526], policy lr: [0.00018965163934426228]
Policy Loss: -0.015373, | Entropy Bonus: -0, | Value Loss: 985.84, | Advantage Loss: 1.9104
Time elapsed (s): 1.7106146812438965
Agent stdevs: 0.4428648
--------------------------------------------------------------------------------

Step 359
++++++++ Policy training ++++++++++
Current mean reward: 2778.592439 | mean episode length: 779.500000
val_loss=26.81191
val_loss=18.34919
val_loss=33.60716
val_loss=10.62121
val_loss=18.08823
val_loss=43.27219
val_loss=19.11217
val_loss=27.57614
val_loss=20.46539
val_loss= 7.98958
adv_loss= 1.03881
adv_loss= 6.40839
adv_loss= 2.46176
adv_loss= 5.69571
adv_loss= 1.71856
adv_loss= 2.55348
adv_loss= 1.38965
adv_loss= 3.11764
adv_loss= 2.10686
adv_loss= 2.56602
surrogate=-0.01343, entropy= 1.77565, loss=-0.01343
surrogate=-0.01769, entropy= 1.77761, loss=-0.01769
surrogate=-0.00608, entropy= 1.77650, loss=-0.00608
surrogate=-0.02092, entropy= 1.77693, loss=-0.02092
surrogate=-0.01395, entropy= 1.77833, loss=-0.01395
surrogate=-0.03525, entropy= 1.77843, loss=-0.03525
surrogate=-0.00792, entropy= 1.77905, loss=-0.00792
surrogate=-0.00024, entropy= 1.78181, loss=-0.00024
surrogate=-0.02011, entropy= 1.78096, loss=-0.02011
surrogate=-0.03032, entropy= 1.78200, loss=-0.03032
std_min= 0.35159, std_max= 0.51633, std_mean= 0.44388
val lr: [0.00015778688524590163], policy lr: [0.00018934426229508193]
Policy Loss: -0.030323, | Entropy Bonus: -0, | Value Loss: 7.9896, | Advantage Loss: 2.566
Time elapsed (s): 1.7446300983428955
Agent stdevs: 0.443876
--------------------------------------------------------------------------------

Step 360
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2718.8
++++++++ Policy training ++++++++++
Current mean reward: 1666.549044 | mean episode length: 462.500000
val_loss=21.49857
val_loss=18.78452
val_loss=14.89046
val_loss=21.88479
val_loss=17.19628
val_loss=17.85473
val_loss=22.34761
val_loss=15.77069
val_loss=31.42126
val_loss=11.90049
adv_loss=14.00941
adv_loss= 7.12876
adv_loss= 5.24007
adv_loss= 9.62683
adv_loss= 6.99568
adv_loss= 2.55761
adv_loss= 1.69032
adv_loss= 6.05408
adv_loss= 6.50559
adv_loss= 2.76228
surrogate=-0.00115, entropy= 1.78022, loss=-0.00115
surrogate= 0.00707, entropy= 1.77683, loss= 0.00707
surrogate= 0.00264, entropy= 1.77352, loss= 0.00264
surrogate=-0.01909, entropy= 1.77020, loss=-0.01909
surrogate= 0.00732, entropy= 1.76684, loss= 0.00732
surrogate=-0.01753, entropy= 1.76453, loss=-0.01753
surrogate=-0.03924, entropy= 1.76128, loss=-0.03924
surrogate=-0.02460, entropy= 1.75928, loss=-0.02460
surrogate=-0.00344, entropy= 1.75696, loss=-0.00344
surrogate= 0.00186, entropy= 1.75366, loss= 0.00186
std_min= 0.34904, std_max= 0.51185, std_mean= 0.43962
val lr: [0.00015753073770491806], policy lr: [0.00018903688524590163]
Policy Loss: 0.0018621, | Entropy Bonus: -0, | Value Loss: 11.9, | Advantage Loss: 2.7623
Time elapsed (s): 1.6822950839996338
Agent stdevs: 0.43961802
--------------------------------------------------------------------------------

Step 361
++++++++ Policy training ++++++++++
Current mean reward: 1835.864211 | mean episode length: 510.666667
val_loss=20.35805
val_loss=31.94171
val_loss=17.23608
val_loss=25.30112
val_loss=23.49720
val_loss=12.82847
val_loss=11.32129
val_loss=25.14083
val_loss=13.21500
val_loss=14.45890
adv_loss= 1.59186
adv_loss= 4.14783
adv_loss= 3.80951
adv_loss= 3.69701
adv_loss= 3.84922
adv_loss= 3.18919
adv_loss= 4.55693
adv_loss= 1.61793
adv_loss= 7.70398
adv_loss= 2.71218
surrogate= 0.01720, entropy= 1.75276, loss= 0.01720
surrogate= 0.01743, entropy= 1.75207, loss= 0.01743
surrogate=-0.01742, entropy= 1.75203, loss=-0.01742
surrogate=-0.03119, entropy= 1.75206, loss=-0.03119
surrogate=-0.04112, entropy= 1.75141, loss=-0.04112
surrogate=-0.03159, entropy= 1.74987, loss=-0.03159
surrogate=-0.01090, entropy= 1.74937, loss=-0.01090
surrogate=-0.02709, entropy= 1.74977, loss=-0.02709
surrogate=-0.03198, entropy= 1.74916, loss=-0.03198
surrogate=-0.01667, entropy= 1.74757, loss=-0.01667
std_min= 0.34926, std_max= 0.50232, std_mean= 0.43839
val lr: [0.0001572745901639344], policy lr: [0.00018872950819672128]
Policy Loss: -0.016673, | Entropy Bonus: -0, | Value Loss: 14.459, | Advantage Loss: 2.7122
Time elapsed (s): 1.6971509456634521
Agent stdevs: 0.438386
--------------------------------------------------------------------------------

Step 362
++++++++ Policy training ++++++++++
Current mean reward: 2805.159252 | mean episode length: 798.000000
val_loss=36.37572
val_loss=161.14571
val_loss=243.43042
val_loss=74.46738
val_loss=511.47717
val_loss=129.88522
val_loss=186.79460
val_loss=24.71115
val_loss=59.68666
val_loss=26.94641
adv_loss= 1.60044
adv_loss= 4.16302
adv_loss= 2.79412
adv_loss= 3.65265
adv_loss=13.27788
adv_loss= 4.43708
adv_loss= 2.11731
adv_loss= 2.72291
adv_loss= 4.77328
adv_loss=15.76588
surrogate= 0.05007, entropy= 1.74786, loss= 0.05007
surrogate=-0.00549, entropy= 1.74665, loss=-0.00549
surrogate=-0.02050, entropy= 1.74643, loss=-0.02050
surrogate=-0.02817, entropy= 1.74515, loss=-0.02817
surrogate=-0.01819, entropy= 1.74614, loss=-0.01819
surrogate=-0.01389, entropy= 1.74561, loss=-0.01389
surrogate=-0.01691, entropy= 1.74399, loss=-0.01691
surrogate=-0.02401, entropy= 1.74295, loss=-0.02401
surrogate=-0.00654, entropy= 1.74220, loss=-0.00654
surrogate=-0.02493, entropy= 1.74152, loss=-0.02493
std_min= 0.34879, std_max= 0.49972, std_mean= 0.43744
val lr: [0.00015701844262295083], policy lr: [0.00018842213114754098]
Policy Loss: -0.024932, | Entropy Bonus: -0, | Value Loss: 26.946, | Advantage Loss: 15.766
Time elapsed (s): 1.7076780796051025
Agent stdevs: 0.43744305
--------------------------------------------------------------------------------

Step 363
++++++++ Policy training ++++++++++
Current mean reward: 2782.150368 | mean episode length: 816.500000
val_loss=446.35901
val_loss=1387.52307
val_loss=441.32983
val_loss=132.39780
val_loss=1395.65686
val_loss=963.07172
val_loss=35.64404
val_loss=659.17352
val_loss=43.64608
val_loss=26.09065
adv_loss= 2.08184
adv_loss=1693.95581
adv_loss= 2.72789
adv_loss= 5.75961
adv_loss= 2.16428
adv_loss= 3.71927
adv_loss= 5.16034
adv_loss= 2.33682
adv_loss= 2.35591
adv_loss= 2.50236
surrogate=-0.00969, entropy= 1.74228, loss=-0.00969
surrogate= 0.01936, entropy= 1.74369, loss= 0.01936
surrogate=-0.01914, entropy= 1.74492, loss=-0.01914
surrogate=-0.02942, entropy= 1.74665, loss=-0.02942
surrogate= 0.00083, entropy= 1.74813, loss= 0.00083
surrogate=-0.02476, entropy= 1.74866, loss=-0.02476
surrogate=-0.01275, entropy= 1.74861, loss=-0.01275
surrogate=-0.02104, entropy= 1.74810, loss=-0.02104
surrogate=-0.00190, entropy= 1.74804, loss=-0.00190
surrogate=-0.02660, entropy= 1.74930, loss=-0.02660
std_min= 0.35106, std_max= 0.49823, std_mean= 0.43837
val lr: [0.0001567622950819672], policy lr: [0.00018811475409836063]
Policy Loss: -0.026601, | Entropy Bonus: -0, | Value Loss: 26.091, | Advantage Loss: 2.5024
Time elapsed (s): 1.6926333904266357
Agent stdevs: 0.43836665
--------------------------------------------------------------------------------

Step 364
++++++++ Policy training ++++++++++
Current mean reward: 1553.521849 | mean episode length: 434.666667
val_loss=21.28985
val_loss=34.23030
val_loss=15.43134
val_loss=12.64871
val_loss=11.58805
val_loss=12.46594
val_loss= 8.73164
val_loss=12.22346
val_loss=13.45124
val_loss=12.15573
adv_loss= 3.60386
adv_loss= 6.78611
adv_loss= 4.39585
adv_loss= 3.46295
adv_loss= 1.20054
adv_loss= 1.74087
adv_loss= 1.51400
adv_loss= 2.69762
adv_loss= 2.49795
adv_loss= 2.81744
surrogate= 0.01308, entropy= 1.74777, loss= 0.01308
surrogate=-0.03201, entropy= 1.74615, loss=-0.03201
surrogate=-0.01892, entropy= 1.74456, loss=-0.01892
surrogate=-0.02198, entropy= 1.74436, loss=-0.02198
surrogate= 0.01283, entropy= 1.74130, loss= 0.01283
surrogate= 0.01112, entropy= 1.74209, loss= 0.01112
surrogate=-0.01352, entropy= 1.73947, loss=-0.01352
surrogate=-0.02463, entropy= 1.73934, loss=-0.02463
surrogate=-0.00985, entropy= 1.73842, loss=-0.00985
surrogate= 0.00965, entropy= 1.73780, loss= 0.00965
std_min= 0.35249, std_max= 0.49577, std_mean= 0.43637
val lr: [0.0001565061475409836], policy lr: [0.0001878073770491803]
Policy Loss: 0.0096474, | Entropy Bonus: -0, | Value Loss: 12.156, | Advantage Loss: 2.8174
Time elapsed (s): 1.74930739402771
Agent stdevs: 0.43637475
--------------------------------------------------------------------------------

Step 365
++++++++ Policy training ++++++++++
Current mean reward: 1940.139775 | mean episode length: 549.666667
val_loss=44.12193
val_loss=28.04825
val_loss=41.03873
val_loss=28.81179
val_loss=22.45422
val_loss=28.03010
val_loss=31.72656
val_loss=22.53663
val_loss=21.06277
val_loss=21.13415
adv_loss= 4.43149
adv_loss= 9.54921
adv_loss= 2.14354
adv_loss= 4.99730
adv_loss= 2.93695
adv_loss= 2.18834
adv_loss= 2.12744
adv_loss= 2.01224
adv_loss= 3.06275
adv_loss=12.70427
surrogate= 0.00406, entropy= 1.73468, loss= 0.00406
surrogate= 0.00721, entropy= 1.73297, loss= 0.00721
surrogate=-0.00343, entropy= 1.72825, loss=-0.00343
surrogate= 0.02525, entropy= 1.72614, loss= 0.02525
surrogate=-0.01859, entropy= 1.72345, loss=-0.01859
surrogate=-0.02199, entropy= 1.71857, loss=-0.02199
surrogate= 0.01064, entropy= 1.71663, loss= 0.01064
surrogate=-0.04221, entropy= 1.71401, loss=-0.04221
surrogate=-0.00218, entropy= 1.71085, loss=-0.00218
surrogate=-0.01889, entropy= 1.70889, loss=-0.01889
std_min= 0.34957, std_max= 0.48964, std_mean= 0.43209
val lr: [0.00015625], policy lr: [0.00018749999999999998]
Policy Loss: -0.018894, | Entropy Bonus: -0, | Value Loss: 21.134, | Advantage Loss: 12.704
Time elapsed (s): 1.7376387119293213
Agent stdevs: 0.4320902
--------------------------------------------------------------------------------

Step 366
++++++++ Policy training ++++++++++
Current mean reward: 3033.420788 | mean episode length: 855.000000
val_loss=26.43038
val_loss=22.51749
val_loss=15.01270
val_loss=10.87643
val_loss=25.50568
val_loss=12.00044
val_loss=15.96118
val_loss=12.08001
val_loss= 6.98077
val_loss=11.63236
adv_loss= 3.62075
adv_loss= 2.37335
adv_loss= 2.99890
adv_loss= 1.49992
adv_loss= 0.69709
adv_loss= 2.36040
adv_loss= 1.68816
adv_loss= 4.05449
adv_loss= 1.50120
adv_loss= 1.38939
surrogate=-0.02802, entropy= 1.70780, loss=-0.02802
surrogate=-0.01351, entropy= 1.70600, loss=-0.01351
surrogate=-0.01125, entropy= 1.70482, loss=-0.01125
surrogate= 0.00651, entropy= 1.70338, loss= 0.00651
surrogate=-0.00697, entropy= 1.70175, loss=-0.00697
surrogate=-0.01254, entropy= 1.70158, loss=-0.01254
surrogate= 0.00176, entropy= 1.69966, loss= 0.00176
surrogate=-0.03418, entropy= 1.69864, loss=-0.03418
surrogate=-0.00733, entropy= 1.69801, loss=-0.00733
surrogate= 0.00451, entropy= 1.69549, loss= 0.00451
std_min= 0.34653, std_max= 0.49157, std_mean= 0.43042
val lr: [0.0001559938524590164], policy lr: [0.00018719262295081965]
Policy Loss: 0.0045121, | Entropy Bonus: -0, | Value Loss: 11.632, | Advantage Loss: 1.3894
Time elapsed (s): 1.6923751831054688
Agent stdevs: 0.43042338
--------------------------------------------------------------------------------

Step 367
++++++++ Policy training ++++++++++
Current mean reward: 2346.149243 | mean episode length: 647.333333
val_loss=21.64092
val_loss=31.14244
val_loss= 7.93958
val_loss=17.67986
val_loss=10.60922
val_loss= 6.63390
val_loss=12.85086
val_loss=16.00843
val_loss=17.06911
val_loss=11.59106
adv_loss= 0.66884
adv_loss= 2.45786
adv_loss= 1.41092
adv_loss= 1.54811
adv_loss=10.21923
adv_loss= 0.88924
adv_loss= 2.03851
adv_loss= 1.09229
adv_loss= 0.89954
adv_loss= 2.23462
surrogate=-0.01069, entropy= 1.69293, loss=-0.01069
surrogate=-0.00358, entropy= 1.69169, loss=-0.00358
surrogate=-0.03008, entropy= 1.68871, loss=-0.03008
surrogate=-0.02583, entropy= 1.68543, loss=-0.02583
surrogate=-0.02066, entropy= 1.68329, loss=-0.02066
surrogate=-0.00670, entropy= 1.68003, loss=-0.00670
surrogate=-0.00216, entropy= 1.67692, loss=-0.00216
surrogate=-0.01559, entropy= 1.67513, loss=-0.01559
surrogate=-0.04653, entropy= 1.67365, loss=-0.04653
surrogate=-0.01409, entropy= 1.67103, loss=-0.01409
std_min= 0.34233, std_max= 0.49181, std_mean= 0.42719
val lr: [0.0001557377049180328], policy lr: [0.00018688524590163933]
Policy Loss: -0.014087, | Entropy Bonus: -0, | Value Loss: 11.591, | Advantage Loss: 2.2346
Time elapsed (s): 1.709801435470581
Agent stdevs: 0.42719463
--------------------------------------------------------------------------------

Step 368
++++++++ Policy training ++++++++++
Current mean reward: 3269.242643 | mean episode length: 940.000000
val_loss=1702.90515
val_loss=399.64594
val_loss=37.79047
val_loss=127.99509
val_loss=767.59985
val_loss=27.59509
val_loss=444.08176
val_loss=20.18850
val_loss=44.77451
val_loss=61.23750
adv_loss= 2.42505
adv_loss= 1.18430
adv_loss= 1.31025
adv_loss= 1.35957
adv_loss= 1.32617
adv_loss= 1.74136
adv_loss= 1.17151
adv_loss= 1.52203
adv_loss= 1.43990
adv_loss= 1.22482
surrogate= 0.00395, entropy= 1.67077, loss= 0.00395
surrogate=-0.02775, entropy= 1.67035, loss=-0.02775
surrogate=-0.02281, entropy= 1.67056, loss=-0.02281
surrogate=-0.01509, entropy= 1.67130, loss=-0.01509
surrogate= 0.01162, entropy= 1.67116, loss= 0.01162
surrogate= 0.04145, entropy= 1.66961, loss= 0.04145
surrogate=-0.01772, entropy= 1.66840, loss=-0.01772
surrogate=-0.00429, entropy= 1.66900, loss=-0.00429
surrogate=-0.02808, entropy= 1.66967, loss=-0.02808
surrogate=-0.01517, entropy= 1.66964, loss=-0.01517
std_min= 0.34441, std_max= 0.48828, std_mean= 0.42669
val lr: [0.00015548155737704918], policy lr: [0.00018657786885245897]
Policy Loss: -0.015174, | Entropy Bonus: -0, | Value Loss: 61.238, | Advantage Loss: 1.2248
Time elapsed (s): 1.6703698635101318
Agent stdevs: 0.4266852
--------------------------------------------------------------------------------

Step 369
++++++++ Policy training ++++++++++
Current mean reward: 1194.153314 | mean episode length: 326.000000
val_loss=33.67740
val_loss=22.94963
val_loss=17.59299
val_loss=17.15004
val_loss=12.84626
val_loss=16.00674
val_loss=20.40474
val_loss=11.98456
val_loss=16.65271
val_loss=12.29456
adv_loss= 3.38585
adv_loss= 1.45022
adv_loss= 0.72867
adv_loss= 2.35441
adv_loss= 0.97580
adv_loss= 1.68363
adv_loss= 2.46518
adv_loss= 2.80987
adv_loss= 4.22408
adv_loss= 1.76146
surrogate= 0.00341, entropy= 1.67283, loss= 0.00341
surrogate= 0.01196, entropy= 1.67657, loss= 0.01196
surrogate=-0.00496, entropy= 1.67896, loss=-0.00496
surrogate= 0.00685, entropy= 1.68066, loss= 0.00685
surrogate= 0.00070, entropy= 1.68251, loss= 0.00070
surrogate= 0.00379, entropy= 1.68390, loss= 0.00379
surrogate=-0.02643, entropy= 1.68451, loss=-0.02643
surrogate=-0.01436, entropy= 1.68580, loss=-0.01436
surrogate=-0.02330, entropy= 1.68822, loss=-0.02330
surrogate=-0.01911, entropy= 1.68895, loss=-0.01911
std_min= 0.34686, std_max= 0.48997, std_mean= 0.42939
val lr: [0.0001552254098360656], policy lr: [0.00018627049180327867]
Policy Loss: -0.019112, | Entropy Bonus: -0, | Value Loss: 12.295, | Advantage Loss: 1.7615
Time elapsed (s): 1.6687090396881104
Agent stdevs: 0.4293859
--------------------------------------------------------------------------------

Step 370
++++++++ Policy training ++++++++++
Current mean reward: 1821.430935 | mean episode length: 505.750000
val_loss=20.14529
val_loss=23.46889
val_loss=23.63937
val_loss=11.99970
val_loss= 9.00401
val_loss=28.48059
val_loss= 7.94049
val_loss=13.29689
val_loss=10.86636
val_loss= 8.21783
adv_loss= 2.94725
adv_loss= 2.25970
adv_loss= 2.38415
adv_loss= 1.53824
adv_loss= 5.75257
adv_loss= 1.90421
adv_loss= 0.86984
adv_loss= 2.32781
adv_loss= 1.28993
adv_loss= 1.27646
surrogate=-0.02404, entropy= 1.68647, loss=-0.02404
surrogate=-0.02561, entropy= 1.68251, loss=-0.02561
surrogate=-0.02783, entropy= 1.67874, loss=-0.02783
surrogate=-0.00296, entropy= 1.67583, loss=-0.00296
surrogate=-0.00088, entropy= 1.67313, loss=-0.00088
surrogate=-0.00470, entropy= 1.67308, loss=-0.00470
surrogate=-0.04541, entropy= 1.67149, loss=-0.04541
surrogate=-0.04479, entropy= 1.66892, loss=-0.04479
surrogate=-0.02298, entropy= 1.66592, loss=-0.02298
surrogate=-0.03193, entropy= 1.66366, loss=-0.03193
std_min= 0.34713, std_max= 0.48449, std_mean= 0.42541
val lr: [0.00015496926229508195], policy lr: [0.00018596311475409832]
Policy Loss: -0.031934, | Entropy Bonus: -0, | Value Loss: 8.2178, | Advantage Loss: 1.2765
Time elapsed (s): 1.6917750835418701
Agent stdevs: 0.42540932
--------------------------------------------------------------------------------

Step 371
++++++++ Policy training ++++++++++
Current mean reward: 2114.689236 | mean episode length: 588.333333
val_loss=29.02621
val_loss= 6.77275
val_loss=14.96150
val_loss= 8.35308
val_loss= 8.65854
val_loss=15.82950
val_loss= 8.14470
val_loss= 8.26482
val_loss=13.46348
val_loss= 4.44264
adv_loss= 1.66435
adv_loss= 1.28879
adv_loss= 1.32823
adv_loss= 1.91116
adv_loss= 0.91416
adv_loss= 2.69631
adv_loss= 1.43262
adv_loss= 1.70765
adv_loss= 0.97924
adv_loss= 1.54651
surrogate= 0.03918, entropy= 1.66592, loss= 0.03918
surrogate= 0.01576, entropy= 1.66679, loss= 0.01576
surrogate=-0.00358, entropy= 1.67022, loss=-0.00358
surrogate=-0.01501, entropy= 1.67262, loss=-0.01501
surrogate=-0.01180, entropy= 1.67238, loss=-0.01180
surrogate= 0.00365, entropy= 1.67309, loss= 0.00365
surrogate=-0.02117, entropy= 1.67552, loss=-0.02117
surrogate=-0.00579, entropy= 1.67578, loss=-0.00579
surrogate=-0.00492, entropy= 1.67615, loss=-0.00492
surrogate=-0.00558, entropy= 1.67640, loss=-0.00558
std_min= 0.34940, std_max= 0.48553, std_mean= 0.42714
val lr: [0.00015471311475409838], policy lr: [0.00018565573770491805]
Policy Loss: -0.0055788, | Entropy Bonus: -0, | Value Loss: 4.4426, | Advantage Loss: 1.5465
Time elapsed (s): 1.6894044876098633
Agent stdevs: 0.42713714
--------------------------------------------------------------------------------

Step 372
++++++++ Policy training ++++++++++
Current mean reward: 3321.169070 | mean episode length: 950.000000
val_loss=1122.75891
val_loss=93.36818
val_loss=32.43340
val_loss=573.43427
val_loss=24.60562
val_loss=366.07660
val_loss=115.08189
val_loss=1385.63757
val_loss=63.19210
val_loss=104.42499
adv_loss= 2.78665
adv_loss= 1.90530
adv_loss= 5.61633
adv_loss= 1.92652
adv_loss= 2.16207
adv_loss= 4.27367
adv_loss= 1.06357
adv_loss= 3.19971
adv_loss= 1.95960
adv_loss= 1.36760
surrogate= 0.02440, entropy= 1.67631, loss= 0.02440
surrogate= 0.01372, entropy= 1.67491, loss= 0.01372
surrogate=-0.02649, entropy= 1.67580, loss=-0.02649
surrogate=-0.00169, entropy= 1.67655, loss=-0.00169
surrogate=-0.02278, entropy= 1.67704, loss=-0.02278
surrogate=-0.02779, entropy= 1.67764, loss=-0.02779
surrogate=-0.02860, entropy= 1.67861, loss=-0.02860
surrogate=-0.00250, entropy= 1.67951, loss=-0.00250
surrogate= 0.00057, entropy= 1.67818, loss= 0.00057
surrogate=-0.02057, entropy= 1.67505, loss=-0.02057
std_min= 0.35188, std_max= 0.48355, std_mean= 0.42666
val lr: [0.00015445696721311475], policy lr: [0.00018534836065573767]
Policy Loss: -0.020568, | Entropy Bonus: -0, | Value Loss: 104.42, | Advantage Loss: 1.3676
Time elapsed (s): 1.6510608196258545
Agent stdevs: 0.42665923
--------------------------------------------------------------------------------

Step 373
++++++++ Policy training ++++++++++
Current mean reward: 2052.591311 | mean episode length: 601.000000
val_loss=898.23004
val_loss=1144.61609
val_loss=1174.12195
val_loss=598.63763
val_loss=178.62592
val_loss=1370.77234
val_loss=64.41094
val_loss=358.65140
val_loss=86.85495
val_loss=1225.59912
adv_loss= 5.90013
adv_loss= 4.50444
adv_loss= 5.68667
adv_loss= 2.80171
adv_loss= 7.93635
adv_loss= 4.03045
adv_loss= 4.70478
adv_loss= 3.30015
adv_loss= 3.42742
adv_loss= 2.76368
surrogate=-0.00948, entropy= 1.67258, loss=-0.00948
surrogate=-0.00935, entropy= 1.67163, loss=-0.00935
surrogate= 0.02403, entropy= 1.67015, loss= 0.02403
surrogate=-0.00394, entropy= 1.66975, loss=-0.00394
surrogate=-0.00129, entropy= 1.66993, loss=-0.00129
surrogate=-0.01393, entropy= 1.66813, loss=-0.01393
surrogate=-0.02554, entropy= 1.66793, loss=-0.02554
surrogate=-0.01838, entropy= 1.66922, loss=-0.01838
surrogate=-0.00498, entropy= 1.66961, loss=-0.00498
surrogate=-0.01926, entropy= 1.66850, loss=-0.01926
std_min= 0.34993, std_max= 0.48464, std_mean= 0.42588
val lr: [0.00015420081967213115], policy lr: [0.00018504098360655735]
Policy Loss: -0.019263, | Entropy Bonus: -0, | Value Loss: 1225.6, | Advantage Loss: 2.7637
Time elapsed (s): 1.6833469867706299
Agent stdevs: 0.4258772
--------------------------------------------------------------------------------

Step 374
++++++++ Policy training ++++++++++
Current mean reward: 2180.223897 | mean episode length: 627.666667
val_loss=88.58091
val_loss=108.01815
val_loss=806.31439
val_loss=104.52057
val_loss=544.55511
val_loss=155.09161
val_loss=63.65197
val_loss=401.08893
val_loss=29.42535
val_loss=76.47512
adv_loss= 8.08421
adv_loss= 2.64745
adv_loss= 4.85790
adv_loss= 2.39107
adv_loss= 2.95703
adv_loss= 2.88623
adv_loss= 4.42577
adv_loss=1372.37122
adv_loss= 1.39038
adv_loss= 2.35268
surrogate= 0.01709, entropy= 1.66728, loss= 0.01709
surrogate=-0.01749, entropy= 1.66551, loss=-0.01749
surrogate=-0.02284, entropy= 1.66468, loss=-0.02284
surrogate=-0.01336, entropy= 1.66535, loss=-0.01336
surrogate=-0.00333, entropy= 1.66607, loss=-0.00333
surrogate=-0.02311, entropy= 1.66512, loss=-0.02311
surrogate=-0.00486, entropy= 1.66641, loss=-0.00486
surrogate=-0.03150, entropy= 1.66770, loss=-0.03150
surrogate=-0.02458, entropy= 1.66829, loss=-0.02458
surrogate=-0.02671, entropy= 1.66878, loss=-0.02671
std_min= 0.34840, std_max= 0.48508, std_mean= 0.42608
val lr: [0.00015394467213114755], policy lr: [0.00018473360655737705]
Policy Loss: -0.026709, | Entropy Bonus: -0, | Value Loss: 76.475, | Advantage Loss: 2.3527
Time elapsed (s): 1.6523475646972656
Agent stdevs: 0.42608
--------------------------------------------------------------------------------

Step 375
++++++++ Policy training ++++++++++
Current mean reward: 2940.251658 | mean episode length: 846.000000
val_loss=42.43844
val_loss=1679.65955
val_loss=85.79858
val_loss=2037.82825
val_loss=654.19489
val_loss=786.21594
val_loss=1423.13855
val_loss=627.35620
val_loss=169.10757
val_loss=247.29668
adv_loss= 8.64642
adv_loss= 5.82822
adv_loss=1340.25452
adv_loss= 5.14818
adv_loss= 7.90235
adv_loss= 8.38955
adv_loss= 6.09420
adv_loss= 3.21277
adv_loss= 6.52435
adv_loss= 7.06485
surrogate= 0.01234, entropy= 1.66629, loss= 0.01234
surrogate=-0.03059, entropy= 1.66516, loss=-0.03059
surrogate=-0.00570, entropy= 1.66469, loss=-0.00570
surrogate=-0.00276, entropy= 1.66195, loss=-0.00276
surrogate=-0.02502, entropy= 1.66145, loss=-0.02502
surrogate=-0.00592, entropy= 1.66315, loss=-0.00592
surrogate=-0.00294, entropy= 1.66358, loss=-0.00294
surrogate= 0.00542, entropy= 1.66171, loss= 0.00542
surrogate=-0.02544, entropy= 1.66082, loss=-0.02544
surrogate=-0.02418, entropy= 1.66024, loss=-0.02418
std_min= 0.34727, std_max= 0.48392, std_mean= 0.42489
val lr: [0.00015368852459016395], policy lr: [0.00018442622950819672]
Policy Loss: -0.024181, | Entropy Bonus: -0, | Value Loss: 247.3, | Advantage Loss: 7.0648
Time elapsed (s): 1.6821658611297607
Agent stdevs: 0.42488658
--------------------------------------------------------------------------------

Step 376
++++++++ Policy training ++++++++++
Current mean reward: 3342.551614 | mean episode length: 930.000000
val_loss=96.58004
val_loss=15.77832
val_loss=36.04869
val_loss=20.67226
val_loss=21.44784
val_loss=20.15935
val_loss=26.63480
val_loss=11.29968
val_loss=13.27143
val_loss=21.97759
adv_loss= 3.80827
adv_loss= 3.56339
adv_loss= 3.05397
adv_loss= 2.68404
adv_loss= 1.87693
adv_loss= 6.28725
adv_loss= 3.31753
adv_loss= 2.41487
adv_loss= 5.47735
adv_loss= 1.54951
surrogate= 0.00672, entropy= 1.65900, loss= 0.00672
surrogate= 0.00280, entropy= 1.65821, loss= 0.00280
surrogate=-0.01726, entropy= 1.65794, loss=-0.01726
surrogate= 0.02340, entropy= 1.65673, loss= 0.02340
surrogate= 0.03406, entropy= 1.65515, loss= 0.03406
surrogate= 0.00169, entropy= 1.65503, loss= 0.00169
surrogate=-0.04019, entropy= 1.65542, loss=-0.04019
surrogate=-0.01920, entropy= 1.65474, loss=-0.01920
surrogate=-0.03489, entropy= 1.65312, loss=-0.03489
surrogate=-0.01571, entropy= 1.65364, loss=-0.01571
std_min= 0.34804, std_max= 0.48295, std_mean= 0.42382
val lr: [0.00015343237704918035], policy lr: [0.0001841188524590164]
Policy Loss: -0.015712, | Entropy Bonus: -0, | Value Loss: 21.978, | Advantage Loss: 1.5495
Time elapsed (s): 1.7109556198120117
Agent stdevs: 0.42381692
--------------------------------------------------------------------------------

Step 377
++++++++ Policy training ++++++++++
Current mean reward: 2991.913042 | mean episode length: 884.500000
val_loss=1051.10449
val_loss=571.67456
val_loss=708.05676
val_loss=57.41426
val_loss=316.27234
val_loss=763.22626
val_loss=141.11292
val_loss=1032.03784
val_loss=1930.90967
val_loss=348.38086
adv_loss= 4.25819
adv_loss= 4.29802
adv_loss= 3.42982
adv_loss= 7.55316
adv_loss= 3.65635
adv_loss= 4.22711
adv_loss= 3.34526
adv_loss= 5.27304
adv_loss= 5.12849
adv_loss= 4.68768
surrogate=-0.00094, entropy= 1.65094, loss=-0.00094
surrogate= 0.00097, entropy= 1.64934, loss= 0.00097
surrogate= 0.01746, entropy= 1.64704, loss= 0.01746
surrogate=-0.02656, entropy= 1.64679, loss=-0.02656
surrogate=-0.00620, entropy= 1.64595, loss=-0.00620
surrogate= 0.02733, entropy= 1.64358, loss= 0.02733
surrogate=-0.00938, entropy= 1.64314, loss=-0.00938
surrogate=-0.01335, entropy= 1.64174, loss=-0.01335
surrogate=-0.02833, entropy= 1.63980, loss=-0.02833
surrogate=-0.01303, entropy= 1.63588, loss=-0.01303
std_min= 0.34286, std_max= 0.48547, std_mean= 0.42178
val lr: [0.00015317622950819672], policy lr: [0.00018381147540983602]
Policy Loss: -0.013027, | Entropy Bonus: -0, | Value Loss: 348.38, | Advantage Loss: 4.6877
Time elapsed (s): 1.689319133758545
Agent stdevs: 0.42177543
--------------------------------------------------------------------------------

Step 378
++++++++ Policy training ++++++++++
Current mean reward: 3377.605775 | mean episode length: 1000.000000
val_loss=392.21786
val_loss=878.84998
val_loss=757.54211
val_loss=1596.29456
val_loss=101.73329
val_loss=666.67401
val_loss=722.08154
val_loss=81.40896
val_loss=121.41027
val_loss=434.99045
adv_loss= 9.50420
adv_loss= 8.41859
adv_loss= 3.44570
adv_loss= 5.04620
adv_loss= 6.34742
adv_loss= 7.27298
adv_loss= 5.56044
adv_loss= 6.28069
adv_loss= 6.46174
adv_loss=1307.28918
surrogate=-0.00209, entropy= 1.63598, loss=-0.00209
surrogate= 0.01238, entropy= 1.63695, loss= 0.01238
surrogate=-0.00639, entropy= 1.63668, loss=-0.00639
surrogate=-0.00077, entropy= 1.63801, loss=-0.00077
surrogate=-0.02694, entropy= 1.63548, loss=-0.02694
surrogate=-0.04015, entropy= 1.63951, loss=-0.04015
surrogate= 0.01423, entropy= 1.63946, loss= 0.01423
surrogate=-0.03192, entropy= 1.64040, loss=-0.03192
surrogate= 0.00513, entropy= 1.64034, loss= 0.00513
surrogate= 0.00235, entropy= 1.64180, loss= 0.00235
std_min= 0.34449, std_max= 0.48381, std_mean= 0.42243
val lr: [0.00015292008196721312], policy lr: [0.00018350409836065574]
Policy Loss: 0.0023454, | Entropy Bonus: -0, | Value Loss: 434.99, | Advantage Loss: 1307.3
Time elapsed (s): 1.6517884731292725
Agent stdevs: 0.42243496
--------------------------------------------------------------------------------

Step 379
++++++++ Policy training ++++++++++
Current mean reward: 3170.001749 | mean episode length: 921.500000
val_loss=1109.07178
val_loss=507.73453
val_loss=398.88293
val_loss=329.94095
val_loss=84.94413
val_loss=70.37048
val_loss=191.88283
val_loss=335.04929
val_loss=219.28619
val_loss=96.68717
adv_loss= 9.92778
adv_loss= 8.36636
adv_loss= 9.96126
adv_loss= 5.28104
adv_loss= 4.40477
adv_loss= 3.70540
adv_loss= 3.67620
adv_loss= 6.06716
adv_loss= 7.09647
adv_loss= 3.54950
surrogate=-0.01539, entropy= 1.64160, loss=-0.01539
surrogate=-0.01281, entropy= 1.63917, loss=-0.01281
surrogate=-0.01450, entropy= 1.63742, loss=-0.01450
surrogate=-0.03146, entropy= 1.63468, loss=-0.03146
surrogate=-0.00969, entropy= 1.63356, loss=-0.00969
surrogate=-0.01364, entropy= 1.63288, loss=-0.01364
surrogate=-0.00548, entropy= 1.63199, loss=-0.00548
surrogate=-0.01098, entropy= 1.63139, loss=-0.01098
surrogate=-0.03697, entropy= 1.63184, loss=-0.03697
surrogate=-0.00638, entropy= 1.63140, loss=-0.00638
std_min= 0.34211, std_max= 0.48019, std_mean= 0.42103
val lr: [0.0001526639344262295], policy lr: [0.0001831967213114754]
Policy Loss: -0.006381, | Entropy Bonus: -0, | Value Loss: 96.687, | Advantage Loss: 3.5495
Time elapsed (s): 1.6961183547973633
Agent stdevs: 0.4210296
--------------------------------------------------------------------------------

Step 380
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 3164.5
++++++++ Policy training ++++++++++
Current mean reward: 2351.586162 | mean episode length: 670.666667
val_loss=111.94005
val_loss=259.30338
val_loss=1197.81787
val_loss=144.97403
val_loss=713.97363
val_loss=69.63345
val_loss=518.75647
val_loss=127.83445
val_loss=2421.84619
val_loss=74.86882
adv_loss= 7.44079
adv_loss= 3.72563
adv_loss= 3.79902
adv_loss=1485.67725
adv_loss= 5.48204
adv_loss= 2.91246
adv_loss= 6.43139
adv_loss= 4.00010
adv_loss= 4.33649
adv_loss= 4.93286
surrogate=-0.01406, entropy= 1.63176, loss=-0.01406
surrogate=-0.01023, entropy= 1.63148, loss=-0.01023
surrogate= 0.00158, entropy= 1.63115, loss= 0.00158
surrogate=-0.00165, entropy= 1.63199, loss=-0.00165
surrogate=-0.00521, entropy= 1.63416, loss=-0.00521
surrogate=-0.02244, entropy= 1.63392, loss=-0.02244
surrogate=-0.03443, entropy= 1.63406, loss=-0.03443
surrogate=-0.01332, entropy= 1.63509, loss=-0.01332
surrogate=-0.01739, entropy= 1.63568, loss=-0.01739
surrogate=-0.01972, entropy= 1.63766, loss=-0.01972
std_min= 0.34037, std_max= 0.48567, std_mean= 0.42227
val lr: [0.00015240778688524592], policy lr: [0.0001828893442622951]
Policy Loss: -0.019717, | Entropy Bonus: -0, | Value Loss: 74.869, | Advantage Loss: 4.9329
Time elapsed (s): 1.6645152568817139
Agent stdevs: 0.42226908
--------------------------------------------------------------------------------

Step 381
++++++++ Policy training ++++++++++
Current mean reward: 3482.244159 | mean episode length: 982.000000
val_loss=68.89417
val_loss=42.76484
val_loss=57.46873
val_loss=71.31811
val_loss=39.04872
val_loss=45.31345
val_loss=38.10287
val_loss=872.45264
val_loss=45.59253
val_loss=50.88064
adv_loss= 1.38145
adv_loss=1777.51819
adv_loss= 3.07645
adv_loss= 1.76725
adv_loss= 4.09208
adv_loss= 2.58507
adv_loss= 2.32268
adv_loss= 1.64397
adv_loss= 4.70526
adv_loss= 3.30558
surrogate= 0.02984, entropy= 1.64004, loss= 0.02984
surrogate= 0.03126, entropy= 1.64362, loss= 0.03126
surrogate= 0.00334, entropy= 1.64616, loss= 0.00334
surrogate=-0.02131, entropy= 1.64922, loss=-0.02131
surrogate= 0.00946, entropy= 1.64871, loss= 0.00946
surrogate=-0.03763, entropy= 1.64973, loss=-0.03763
surrogate= 0.00492, entropy= 1.64991, loss= 0.00492
surrogate=-0.01993, entropy= 1.65216, loss=-0.01993
surrogate= 0.01135, entropy= 1.65406, loss= 0.01135
surrogate=-0.03603, entropy= 1.65411, loss=-0.03603
std_min= 0.34399, std_max= 0.48620, std_mean= 0.42436
val lr: [0.0001521516393442623], policy lr: [0.00018258196721311474]
Policy Loss: -0.036031, | Entropy Bonus: -0, | Value Loss: 50.881, | Advantage Loss: 3.3056
Time elapsed (s): 1.6515746116638184
Agent stdevs: 0.42435876
--------------------------------------------------------------------------------

Step 382
++++++++ Policy training ++++++++++
Current mean reward: 2284.818821 | mean episode length: 655.500000
val_loss=188.75586
val_loss=110.93561
val_loss=217.92444
val_loss=129.53322
val_loss=697.14801
val_loss=68.85986
val_loss=146.36745
val_loss=185.67494
val_loss=48.89465
val_loss=1148.14429
adv_loss= 2.27392
adv_loss= 2.91587
adv_loss= 1.92583
adv_loss= 2.23925
adv_loss= 1.40121
adv_loss= 4.06669
adv_loss= 3.17790
adv_loss= 5.27909
adv_loss= 3.11367
adv_loss= 2.58410
surrogate=-0.01514, entropy= 1.65069, loss=-0.01514
surrogate=-0.01283, entropy= 1.64663, loss=-0.01283
surrogate=-0.00646, entropy= 1.64302, loss=-0.00646
surrogate= 0.01143, entropy= 1.64009, loss= 0.01143
surrogate=-0.00283, entropy= 1.63919, loss=-0.00283
surrogate= 0.00001, entropy= 1.63840, loss= 0.00001
surrogate=-0.01925, entropy= 1.63669, loss=-0.01925
surrogate=-0.00667, entropy= 1.63310, loss=-0.00667
surrogate=-0.03221, entropy= 1.63199, loss=-0.03221
surrogate=-0.01776, entropy= 1.63034, loss=-0.01776
std_min= 0.34043, std_max= 0.48109, std_mean= 0.42105
val lr: [0.0001518954918032787], policy lr: [0.00018227459016393442]
Policy Loss: -0.017756, | Entropy Bonus: -0, | Value Loss: 1148.1, | Advantage Loss: 2.5841
Time elapsed (s): 1.7101924419403076
Agent stdevs: 0.42105362
--------------------------------------------------------------------------------

Step 383
++++++++ Policy training ++++++++++
Current mean reward: 3173.532523 | mean episode length: 889.500000
val_loss=52.58109
val_loss=45.60181
val_loss=41.38779
val_loss=41.47731
val_loss=34.19739
val_loss=42.08360
val_loss=35.33608
val_loss=21.92092
val_loss=30.79220
val_loss=35.25021
adv_loss= 1.81422
adv_loss= 4.12294
adv_loss= 2.40965
adv_loss= 7.53195
adv_loss= 3.73815
adv_loss= 3.09536
adv_loss= 3.44168
adv_loss=16.71328
adv_loss= 6.24808
adv_loss= 5.88528
surrogate= 0.00899, entropy= 1.62737, loss= 0.00899
surrogate=-0.03090, entropy= 1.62679, loss=-0.03090
surrogate=-0.01607, entropy= 1.62629, loss=-0.01607
surrogate= 0.02567, entropy= 1.62447, loss= 0.02567
surrogate= 0.00569, entropy= 1.62210, loss= 0.00569
surrogate=-0.01173, entropy= 1.62109, loss=-0.01173
surrogate=-0.01608, entropy= 1.61739, loss=-0.01608
surrogate=-0.04687, entropy= 1.61635, loss=-0.04687
surrogate=-0.01544, entropy= 1.61568, loss=-0.01544
surrogate=-0.00581, entropy= 1.61459, loss=-0.00581
std_min= 0.33838, std_max= 0.48060, std_mean= 0.41894
val lr: [0.0001516393442622951], policy lr: [0.0001819672131147541]
Policy Loss: -0.0058103, | Entropy Bonus: -0, | Value Loss: 35.25, | Advantage Loss: 5.8853
Time elapsed (s): 1.6815133094787598
Agent stdevs: 0.41894102
--------------------------------------------------------------------------------

Step 384
++++++++ Policy training ++++++++++
Current mean reward: 1411.072036 | mean episode length: 404.666667
val_loss=71.16663
val_loss=213.72185
val_loss=55.22523
val_loss=28.91997
val_loss=120.61378
val_loss=17.06911
val_loss=100.00498
val_loss=124.95011
val_loss=59.61174
val_loss=19.35539
adv_loss= 5.00896
adv_loss= 6.88325
adv_loss= 1.83645
adv_loss= 2.14509
adv_loss= 1.86390
adv_loss= 6.80033
adv_loss= 8.45133
adv_loss= 1.29671
adv_loss= 2.47095
adv_loss= 2.85475
surrogate= 0.01788, entropy= 1.61418, loss= 0.01788
surrogate= 0.01567, entropy= 1.61257, loss= 0.01567
surrogate=-0.01242, entropy= 1.61284, loss=-0.01242
surrogate= 0.05547, entropy= 1.61079, loss= 0.05547
surrogate= 0.00327, entropy= 1.60939, loss= 0.00327
surrogate= 0.02305, entropy= 1.60763, loss= 0.02305
surrogate=-0.00376, entropy= 1.60751, loss=-0.00376
surrogate=-0.01269, entropy= 1.60713, loss=-0.01269
surrogate= 0.00399, entropy= 1.60773, loss= 0.00399
surrogate=-0.01330, entropy= 1.60618, loss=-0.01330
std_min= 0.33966, std_max= 0.47199, std_mean= 0.41736
val lr: [0.0001513831967213115], policy lr: [0.00018165983606557376]
Policy Loss: -0.013298, | Entropy Bonus: -0, | Value Loss: 19.355, | Advantage Loss: 2.8547
Time elapsed (s): 1.67368483543396
Agent stdevs: 0.4173626
--------------------------------------------------------------------------------

Step 385
++++++++ Policy training ++++++++++
Current mean reward: 1990.731249 | mean episode length: 559.333333
val_loss=52.12949
val_loss=146.95923
val_loss=95.17758
val_loss=254.55127
val_loss=39.78552
val_loss=63.60009
val_loss=282.34103
val_loss=54.88129
val_loss=24.27599
val_loss=680.37988
adv_loss= 4.15456
adv_loss= 4.31744
adv_loss= 7.32631
adv_loss= 5.60646
adv_loss= 5.46741
adv_loss= 4.65390
adv_loss= 6.53164
adv_loss= 4.71193
adv_loss= 4.37530
adv_loss= 3.90007
surrogate= 0.00794, entropy= 1.60703, loss= 0.00794
surrogate=-0.02807, entropy= 1.60799, loss=-0.02807
surrogate=-0.00005, entropy= 1.60925, loss=-0.00005
surrogate=-0.00943, entropy= 1.60989, loss=-0.00943
surrogate=-0.03641, entropy= 1.60953, loss=-0.03641
surrogate=-0.01362, entropy= 1.61183, loss=-0.01362
surrogate=-0.02975, entropy= 1.61009, loss=-0.02975
surrogate=-0.02958, entropy= 1.60937, loss=-0.02958
surrogate=-0.03172, entropy= 1.60870, loss=-0.03172
surrogate=-0.02644, entropy= 1.60997, loss=-0.02644
std_min= 0.33981, std_max= 0.47715, std_mean= 0.41803
val lr: [0.0001511270491803279], policy lr: [0.00018135245901639344]
Policy Loss: -0.026437, | Entropy Bonus: -0, | Value Loss: 680.38, | Advantage Loss: 3.9001
Time elapsed (s): 1.6874175071716309
Agent stdevs: 0.41802904
--------------------------------------------------------------------------------

Step 386
++++++++ Policy training ++++++++++
Current mean reward: 1466.204380 | mean episode length: 412.000000
val_loss=82.95847
val_loss=63.29864
val_loss=50.25748
val_loss=41.56604
val_loss=105.81816
val_loss=30.52947
val_loss=30.56045
val_loss=111.94315
val_loss=50.43169
val_loss=47.26549
adv_loss= 5.96799
adv_loss= 3.67260
adv_loss= 3.18841
adv_loss= 4.22497
adv_loss= 7.31472
adv_loss= 5.27035
adv_loss= 5.61821
adv_loss= 3.35528
adv_loss= 4.09836
adv_loss= 5.23881
surrogate=-0.02424, entropy= 1.61014, loss=-0.02424
surrogate=-0.00097, entropy= 1.60817, loss=-0.00097
surrogate=-0.01441, entropy= 1.60849, loss=-0.01441
surrogate=-0.00866, entropy= 1.60689, loss=-0.00866
surrogate=-0.01911, entropy= 1.60654, loss=-0.01911
surrogate=-0.00164, entropy= 1.60575, loss=-0.00164
surrogate=-0.00692, entropy= 1.60479, loss=-0.00692
surrogate=-0.01250, entropy= 1.60404, loss=-0.01250
surrogate=-0.02096, entropy= 1.60390, loss=-0.02096
surrogate=-0.02471, entropy= 1.60124, loss=-0.02471
std_min= 0.33945, std_max= 0.47559, std_mean= 0.41676
val lr: [0.00015087090163934424], policy lr: [0.00018104508196721309]
Policy Loss: -0.02471, | Entropy Bonus: -0, | Value Loss: 47.265, | Advantage Loss: 5.2388
Time elapsed (s): 1.654984951019287
Agent stdevs: 0.41675642
--------------------------------------------------------------------------------

Step 387
++++++++ Policy training ++++++++++
Current mean reward: 2250.726521 | mean episode length: 632.000000
val_loss=69.26831
val_loss=707.94574
val_loss=518.09711
val_loss=145.43913
val_loss=67.13827
val_loss=48.76781
val_loss=53.13867
val_loss=59.67694
val_loss=75.34544
val_loss=270.39023
adv_loss= 2.62175
adv_loss=10.29502
adv_loss= 9.46700
adv_loss= 2.58388
adv_loss= 2.09267
adv_loss= 2.29716
adv_loss= 9.13723
adv_loss=14.60287
adv_loss= 8.20649
adv_loss= 2.50925
surrogate=-0.01856, entropy= 1.60275, loss=-0.01856
surrogate=-0.00636, entropy= 1.60427, loss=-0.00636
surrogate=-0.00612, entropy= 1.60361, loss=-0.00612
surrogate=-0.03466, entropy= 1.60603, loss=-0.03466
surrogate=-0.02311, entropy= 1.60678, loss=-0.02311
surrogate= 0.00010, entropy= 1.60715, loss= 0.00010
surrogate=-0.00346, entropy= 1.60658, loss=-0.00346
surrogate= 0.00487, entropy= 1.60895, loss= 0.00487
surrogate=-0.01670, entropy= 1.60886, loss=-0.01670
surrogate= 0.00709, entropy= 1.60955, loss= 0.00709
std_min= 0.34213, std_max= 0.47186, std_mean= 0.41762
val lr: [0.00015061475409836067], policy lr: [0.0001807377049180328]
Policy Loss: 0.0070911, | Entropy Bonus: -0, | Value Loss: 270.39, | Advantage Loss: 2.5092
Time elapsed (s): 1.6580302715301514
Agent stdevs: 0.41761765
--------------------------------------------------------------------------------

Step 388
++++++++ Policy training ++++++++++
Current mean reward: 1887.130377 | mean episode length: 538.666667
val_loss=28.15775
val_loss=918.18176
val_loss=83.56019
val_loss=246.75647
val_loss=31.96254
val_loss=100.62988
val_loss=444.34650
val_loss=411.27316
val_loss=147.49016
val_loss=254.90244
adv_loss= 3.82653
adv_loss= 2.74338
adv_loss= 2.98477
adv_loss= 2.22431
adv_loss= 3.85836
adv_loss= 2.01014
adv_loss=13.45694
adv_loss= 3.48467
adv_loss= 3.34334
adv_loss= 5.01861
surrogate= 0.00051, entropy= 1.60932, loss= 0.00051
surrogate=-0.02246, entropy= 1.61043, loss=-0.02246
surrogate=-0.02506, entropy= 1.61154, loss=-0.02506
surrogate=-0.00868, entropy= 1.61258, loss=-0.00868
surrogate= 0.02790, entropy= 1.61263, loss= 0.02790
surrogate= 0.00972, entropy= 1.61335, loss= 0.00972
surrogate=-0.02541, entropy= 1.61439, loss=-0.02541
surrogate= 0.01451, entropy= 1.61389, loss= 0.01451
surrogate=-0.02270, entropy= 1.61439, loss=-0.02270
surrogate=-0.02699, entropy= 1.61481, loss=-0.02699
std_min= 0.34269, std_max= 0.47309, std_mean= 0.41836
val lr: [0.00015035860655737704], policy lr: [0.00018043032786885243]
Policy Loss: -0.026992, | Entropy Bonus: -0, | Value Loss: 254.9, | Advantage Loss: 5.0186
Time elapsed (s): 1.7101469039916992
Agent stdevs: 0.4183559
--------------------------------------------------------------------------------

Step 389
++++++++ Policy training ++++++++++
Current mean reward: 2231.458110 | mean episode length: 643.000000
val_loss=42.03482
val_loss=1875.16699
val_loss=175.31984
val_loss=855.19800
val_loss=295.47363
val_loss=2216.68286
val_loss=2116.35327
val_loss=102.99934
val_loss=1646.55981
val_loss=1320.29565
adv_loss= 8.03683
adv_loss= 4.92016
adv_loss= 6.78024
adv_loss= 2.23869
adv_loss= 2.14202
adv_loss= 5.35994
adv_loss= 2.38667
adv_loss= 1.27241
adv_loss= 3.31719
adv_loss= 3.40256
surrogate= 0.02733, entropy= 1.61569, loss= 0.02733
surrogate= 0.02738, entropy= 1.61871, loss= 0.02738
surrogate=-0.02623, entropy= 1.62097, loss=-0.02623
surrogate=-0.00869, entropy= 1.62159, loss=-0.00869
surrogate=-0.01988, entropy= 1.62453, loss=-0.01988
surrogate=-0.01606, entropy= 1.62637, loss=-0.01606
surrogate= 0.01613, entropy= 1.62816, loss= 0.01613
surrogate=-0.01717, entropy= 1.63020, loss=-0.01717
surrogate=-0.00749, entropy= 1.63376, loss=-0.00749
surrogate=-0.02261, entropy= 1.63614, loss=-0.02261
std_min= 0.34503, std_max= 0.47789, std_mean= 0.42139
val lr: [0.00015010245901639344], policy lr: [0.0001801229508196721]
Policy Loss: -0.022614, | Entropy Bonus: -0, | Value Loss: 1320.3, | Advantage Loss: 3.4026
Time elapsed (s): 1.6866824626922607
Agent stdevs: 0.42138633
--------------------------------------------------------------------------------

Step 390
++++++++ Policy training ++++++++++
Current mean reward: 2407.527239 | mean episode length: 674.000000
val_loss=41.59284
val_loss=29.38090
val_loss=21.94053
val_loss=23.44903
val_loss=27.98535
val_loss=21.35306
val_loss=13.25712
val_loss=15.48412
val_loss=23.23803
val_loss= 9.41885
adv_loss= 3.80307
adv_loss= 1.92589
adv_loss= 4.68983
adv_loss= 2.70067
adv_loss= 1.28347
adv_loss= 6.18660
adv_loss= 2.78566
adv_loss= 4.84398
adv_loss= 1.23796
adv_loss= 2.30020
surrogate=-0.01972, entropy= 1.63559, loss=-0.01972
surrogate=-0.00085, entropy= 1.63381, loss=-0.00085
surrogate= 0.01556, entropy= 1.63441, loss= 0.01556
surrogate= 0.00321, entropy= 1.63559, loss= 0.00321
surrogate=-0.02552, entropy= 1.63583, loss=-0.02552
surrogate=-0.02929, entropy= 1.63547, loss=-0.02929
surrogate=-0.03216, entropy= 1.63710, loss=-0.03216
surrogate=-0.02546, entropy= 1.63566, loss=-0.02546
surrogate=-0.06594, entropy= 1.63711, loss=-0.06594
surrogate=-0.01487, entropy= 1.63670, loss=-0.01487
std_min= 0.34463, std_max= 0.47589, std_mean= 0.42146
val lr: [0.00014984631147540984], policy lr: [0.00017981557377049178]
Policy Loss: -0.014865, | Entropy Bonus: -0, | Value Loss: 9.4189, | Advantage Loss: 2.3002
Time elapsed (s): 1.6684353351593018
Agent stdevs: 0.42146227
--------------------------------------------------------------------------------

Step 391
++++++++ Policy training ++++++++++
Current mean reward: 1923.303014 | mean episode length: 532.000000
val_loss=27.92035
val_loss=28.71955
val_loss=15.93191
val_loss=19.59735
val_loss=15.08666
val_loss=14.76334
val_loss= 6.38096
val_loss= 4.85283
val_loss= 6.96035
val_loss= 9.61564
adv_loss= 1.37683
adv_loss= 2.43931
adv_loss= 2.00416
adv_loss= 1.97825
adv_loss= 1.69859
adv_loss= 2.36646
adv_loss= 2.47763
adv_loss= 1.93361
adv_loss= 2.34801
adv_loss= 2.15920
surrogate= 0.01037, entropy= 1.63787, loss= 0.01037
surrogate= 0.01128, entropy= 1.63733, loss= 0.01128
surrogate= 0.00052, entropy= 1.63506, loss= 0.00052
surrogate=-0.00405, entropy= 1.63465, loss=-0.00405
surrogate=-0.00784, entropy= 1.63089, loss=-0.00784
surrogate=-0.03094, entropy= 1.62966, loss=-0.03094
surrogate=-0.02788, entropy= 1.62904, loss=-0.02788
surrogate=-0.04541, entropy= 1.62820, loss=-0.04541
surrogate= 0.00102, entropy= 1.62739, loss= 0.00102
surrogate=-0.01907, entropy= 1.62572, loss=-0.01907
std_min= 0.34308, std_max= 0.47644, std_mean= 0.41999
val lr: [0.00014959016393442624], policy lr: [0.00017950819672131146]
Policy Loss: -0.019073, | Entropy Bonus: -0, | Value Loss: 9.6156, | Advantage Loss: 2.1592
Time elapsed (s): 1.6755468845367432
Agent stdevs: 0.4199914
--------------------------------------------------------------------------------

Step 392
++++++++ Policy training ++++++++++
Current mean reward: 2166.908816 | mean episode length: 636.500000
val_loss=172.94325
val_loss=1606.79028
val_loss=43.91307
val_loss=272.00717
val_loss=42.36073
val_loss=34.19418
val_loss=591.07440
val_loss=128.99973
val_loss=73.47810
val_loss=109.13275
adv_loss= 2.81517
adv_loss= 2.41904
adv_loss= 2.71825
adv_loss= 9.52490
adv_loss= 1.92295
adv_loss= 4.73115
adv_loss= 9.23823
adv_loss= 3.60789
adv_loss= 3.23603
adv_loss= 2.64553
surrogate= 0.01697, entropy= 1.62388, loss= 0.01697
surrogate=-0.00538, entropy= 1.62094, loss=-0.00538
surrogate=-0.01206, entropy= 1.62005, loss=-0.01206
surrogate=-0.00176, entropy= 1.61787, loss=-0.00176
surrogate= 0.00102, entropy= 1.61276, loss= 0.00102
surrogate=-0.02140, entropy= 1.61051, loss=-0.02140
surrogate=-0.01251, entropy= 1.60880, loss=-0.01251
surrogate=-0.01326, entropy= 1.60539, loss=-0.01326
surrogate= 0.05048, entropy= 1.60562, loss= 0.05048
surrogate=-0.00980, entropy= 1.60247, loss=-0.00980
std_min= 0.33794, std_max= 0.47408, std_mean= 0.41700
val lr: [0.00014933401639344264], policy lr: [0.00017920081967213113]
Policy Loss: -0.0097987, | Entropy Bonus: -0, | Value Loss: 109.13, | Advantage Loss: 2.6455
Time elapsed (s): 1.6737661361694336
Agent stdevs: 0.4170011
--------------------------------------------------------------------------------

Step 393
++++++++ Policy training ++++++++++
Current mean reward: 3430.082763 | mean episode length: 1000.000000
val_loss=153.09135
val_loss=51.53294
val_loss=902.47675
val_loss=203.62122
val_loss=468.24634
val_loss=208.04082
val_loss=34.43306
val_loss=1607.79272
val_loss=324.67538
val_loss=312.84521
adv_loss= 2.39261
adv_loss= 3.84818
adv_loss= 5.20374
adv_loss=12.00850
adv_loss= 7.39797
adv_loss= 8.38421
adv_loss= 7.83177
adv_loss= 3.27087
adv_loss= 3.21364
adv_loss= 2.82504
surrogate= 0.01566, entropy= 1.60148, loss= 0.01566
surrogate=-0.02524, entropy= 1.60161, loss=-0.02524
surrogate=-0.01625, entropy= 1.60329, loss=-0.01625
surrogate=-0.00771, entropy= 1.60249, loss=-0.00771
surrogate= 0.01908, entropy= 1.60419, loss= 0.01908
surrogate=-0.02056, entropy= 1.60601, loss=-0.02056
surrogate=-0.00892, entropy= 1.60662, loss=-0.00892
surrogate=-0.01812, entropy= 1.60676, loss=-0.01812
surrogate=-0.03050, entropy= 1.60890, loss=-0.03050
surrogate=-0.01381, entropy= 1.60899, loss=-0.01381
std_min= 0.33930, std_max= 0.47613, std_mean= 0.41791
val lr: [0.00014907786885245904], policy lr: [0.0001788934426229508]
Policy Loss: -0.013807, | Entropy Bonus: -0, | Value Loss: 312.85, | Advantage Loss: 2.825
Time elapsed (s): 1.6666014194488525
Agent stdevs: 0.41790712
--------------------------------------------------------------------------------

Step 394
++++++++ Policy training ++++++++++
Current mean reward: 3396.459214 | mean episode length: 1000.000000
val_loss=2356.61255
val_loss=235.98814
val_loss=1124.90515
val_loss=96.77676
val_loss=289.83972
val_loss=67.17898
val_loss=200.44807
val_loss=2565.65283
val_loss=38.72216
val_loss=113.76344
adv_loss= 5.12947
adv_loss= 2.92985
adv_loss= 3.95688
adv_loss= 1.82858
adv_loss= 2.46951
adv_loss= 4.46246
adv_loss= 3.39443
adv_loss= 1.90828
adv_loss= 3.08849
adv_loss= 4.32263
surrogate= 0.03556, entropy= 1.60708, loss= 0.03556
surrogate= 0.00398, entropy= 1.60456, loss= 0.00398
surrogate=-0.00541, entropy= 1.60223, loss=-0.00541
surrogate=-0.02830, entropy= 1.60075, loss=-0.02830
surrogate= 0.02363, entropy= 1.60101, loss= 0.02363
surrogate=-0.03525, entropy= 1.59990, loss=-0.03525
surrogate=-0.04272, entropy= 1.59784, loss=-0.04272
surrogate=-0.00142, entropy= 1.59733, loss=-0.00142
surrogate= 0.03729, entropy= 1.59679, loss= 0.03729
surrogate=-0.01311, entropy= 1.59408, loss=-0.01311
std_min= 0.33826, std_max= 0.47057, std_mean= 0.41569
val lr: [0.0001488217213114754], policy lr: [0.00017858606557377048]
Policy Loss: -0.013113, | Entropy Bonus: -0, | Value Loss: 113.76, | Advantage Loss: 4.3226
Time elapsed (s): 1.6905944347381592
Agent stdevs: 0.41568694
--------------------------------------------------------------------------------

Step 395
++++++++ Policy training ++++++++++
Current mean reward: 3482.180837 | mean episode length: 1000.000000
val_loss=2358.48608
val_loss=176.62546
val_loss=615.84174
val_loss=966.09845
val_loss=179.17937
val_loss=190.98871
val_loss=120.03346
val_loss=142.29550
val_loss=166.72369
val_loss=232.63551
adv_loss=19.57788
adv_loss= 9.30367
adv_loss= 4.82440
adv_loss=12.69740
adv_loss=12.89328
adv_loss= 5.79172
adv_loss= 9.70323
adv_loss=982.91986
adv_loss= 9.24303
adv_loss=976.07568
surrogate= 0.01706, entropy= 1.59537, loss= 0.01706
surrogate= 0.01153, entropy= 1.59930, loss= 0.01153
surrogate= 0.01365, entropy= 1.60090, loss= 0.01365
surrogate=-0.02014, entropy= 1.60197, loss=-0.02014
surrogate=-0.00145, entropy= 1.60408, loss=-0.00145
surrogate= 0.02013, entropy= 1.60669, loss= 0.02013
surrogate=-0.03310, entropy= 1.60893, loss=-0.03310
surrogate= 0.00601, entropy= 1.61063, loss= 0.00601
surrogate=-0.00968, entropy= 1.61215, loss=-0.00968
surrogate=-0.02517, entropy= 1.61295, loss=-0.02517
std_min= 0.34010, std_max= 0.47481, std_mean= 0.41836
val lr: [0.00014856557377049178], policy lr: [0.00017827868852459013]
Policy Loss: -0.025168, | Entropy Bonus: -0, | Value Loss: 232.64, | Advantage Loss: 976.08
Time elapsed (s): 1.6700387001037598
Agent stdevs: 0.41836396
--------------------------------------------------------------------------------

Step 396
++++++++ Policy training ++++++++++
Current mean reward: 1742.457893 | mean episode length: 487.333333
val_loss=82.38139
val_loss=58.70743
val_loss=142.57086
val_loss=68.34323
val_loss=57.37293
val_loss=36.22058
val_loss=20.43165
val_loss=32.31945
val_loss=31.33437
val_loss=31.26525
adv_loss= 8.63618
adv_loss= 9.95644
adv_loss= 6.43630
adv_loss= 3.93606
adv_loss= 5.38788
adv_loss= 5.05342
adv_loss= 4.33402
adv_loss= 5.53392
adv_loss= 2.63489
adv_loss= 5.33928
surrogate= 0.03701, entropy= 1.61084, loss= 0.03701
surrogate= 0.00260, entropy= 1.61143, loss= 0.00260
surrogate=-0.01616, entropy= 1.61025, loss=-0.01616
surrogate= 0.00864, entropy= 1.60963, loss= 0.00864
surrogate= 0.00276, entropy= 1.60853, loss= 0.00276
surrogate=-0.04189, entropy= 1.60812, loss=-0.04189
surrogate=-0.01567, entropy= 1.60994, loss=-0.01567
surrogate=-0.00586, entropy= 1.60843, loss=-0.00586
surrogate=-0.02878, entropy= 1.60882, loss=-0.02878
surrogate=-0.01345, entropy= 1.60790, loss=-0.01345
std_min= 0.34173, std_max= 0.47169, std_mean= 0.41741
val lr: [0.0001483094262295082], policy lr: [0.00017797131147540983]
Policy Loss: -0.01345, | Entropy Bonus: -0, | Value Loss: 31.265, | Advantage Loss: 5.3393
Time elapsed (s): 1.6594865322113037
Agent stdevs: 0.41741386
--------------------------------------------------------------------------------

Step 397
++++++++ Policy training ++++++++++
Current mean reward: 2313.521550 | mean episode length: 661.333333
val_loss=46.69121
val_loss=760.42584
val_loss=89.36388
val_loss=1148.63794
val_loss=53.34863
val_loss=39.97424
val_loss=73.54514
val_loss=565.09045
val_loss=300.93106
val_loss=830.25751
adv_loss= 2.39708
adv_loss= 4.39166
adv_loss= 3.60523
adv_loss= 5.59560
adv_loss= 2.54642
adv_loss= 3.26891
adv_loss=1637.12280
adv_loss= 4.28742
adv_loss= 3.74896
adv_loss= 2.61138
surrogate= 0.01813, entropy= 1.60905, loss= 0.01813
surrogate=-0.02249, entropy= 1.60826, loss=-0.02249
surrogate=-0.00937, entropy= 1.60848, loss=-0.00937
surrogate=-0.01064, entropy= 1.60936, loss=-0.01064
surrogate= 0.00022, entropy= 1.61058, loss= 0.00022
surrogate=-0.01184, entropy= 1.61034, loss=-0.01184
surrogate=-0.02315, entropy= 1.61098, loss=-0.02315
surrogate=-0.01026, entropy= 1.60903, loss=-0.01026
surrogate=-0.01487, entropy= 1.60775, loss=-0.01487
surrogate=-0.00442, entropy= 1.60681, loss=-0.00442
std_min= 0.34320, std_max= 0.46808, std_mean= 0.41703
val lr: [0.00014805327868852458], policy lr: [0.00017766393442622948]
Policy Loss: -0.0044214, | Entropy Bonus: -0, | Value Loss: 830.26, | Advantage Loss: 2.6114
Time elapsed (s): 1.6736700534820557
Agent stdevs: 0.41702947
--------------------------------------------------------------------------------

Step 398
++++++++ Policy training ++++++++++
Current mean reward: 3420.844851 | mean episode length: 1000.000000
val_loss=560.14966
val_loss=986.54120
val_loss=1510.27710
val_loss=165.98254
val_loss=487.25565
val_loss=258.52106
val_loss=1466.75232
val_loss=851.34192
val_loss=68.79588
val_loss=91.73502
adv_loss= 8.94837
adv_loss= 5.58487
adv_loss= 3.18388
adv_loss= 5.00884
adv_loss=15.86493
adv_loss= 6.23467
adv_loss= 5.34527
adv_loss= 7.49508
adv_loss= 3.73069
adv_loss= 2.95287
surrogate= 0.02858, entropy= 1.60809, loss= 0.02858
surrogate=-0.02247, entropy= 1.60976, loss=-0.02247
surrogate=-0.04307, entropy= 1.61049, loss=-0.04307
surrogate= 0.00146, entropy= 1.61099, loss= 0.00146
surrogate=-0.01107, entropy= 1.61161, loss=-0.01107
surrogate=-0.01579, entropy= 1.61302, loss=-0.01579
surrogate=-0.00662, entropy= 1.61513, loss=-0.00662
surrogate=-0.04439, entropy= 1.61801, loss=-0.04439
surrogate= 0.00170, entropy= 1.61795, loss= 0.00170
surrogate=-0.00333, entropy= 1.61743, loss=-0.00333
std_min= 0.34628, std_max= 0.46823, std_mean= 0.41830
val lr: [0.00014779713114754098], policy lr: [0.00017735655737704915]
Policy Loss: -0.0033306, | Entropy Bonus: -0, | Value Loss: 91.735, | Advantage Loss: 2.9529
Time elapsed (s): 1.6496596336364746
Agent stdevs: 0.4183042
--------------------------------------------------------------------------------

Step 399
++++++++ Policy training ++++++++++
Current mean reward: 2149.891192 | mean episode length: 623.333333
val_loss=68.21087
val_loss=134.32332
val_loss=1009.55408
val_loss=970.32281
val_loss=927.55939
val_loss=494.90298
val_loss=52.81042
val_loss=1216.31116
val_loss=32.60000
val_loss=1591.47131
adv_loss= 3.89394
adv_loss= 3.86697
adv_loss= 6.06014
adv_loss=1687.44543
adv_loss= 8.52894
adv_loss=266.34036
adv_loss=1669.25513
adv_loss=10.29690
adv_loss= 4.33373
adv_loss= 7.22704
surrogate= 0.02464, entropy= 1.61728, loss= 0.02464
surrogate=-0.02194, entropy= 1.61539, loss=-0.02194
surrogate= 0.01442, entropy= 1.61245, loss= 0.01442
surrogate= 0.00277, entropy= 1.61262, loss= 0.00277
surrogate= 0.00864, entropy= 1.61215, loss= 0.00864
surrogate=-0.01293, entropy= 1.61012, loss=-0.01293
surrogate=-0.00991, entropy= 1.60923, loss=-0.00991
surrogate=-0.03516, entropy= 1.60822, loss=-0.03516
surrogate=-0.01878, entropy= 1.60738, loss=-0.01878
surrogate=-0.03048, entropy= 1.60635, loss=-0.03048
std_min= 0.34387, std_max= 0.46830, std_mean= 0.41690
val lr: [0.00014754098360655738], policy lr: [0.00017704918032786883]
Policy Loss: -0.030481, | Entropy Bonus: -0, | Value Loss: 1591.5, | Advantage Loss: 7.227
Time elapsed (s): 1.6659152507781982
Agent stdevs: 0.41689548
--------------------------------------------------------------------------------

Step 400
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2621.8
++++++++ Policy training ++++++++++
Current mean reward: 2167.064508 | mean episode length: 622.000000
val_loss=82.29305
val_loss=28.70151
val_loss=310.08521
val_loss=58.13885
val_loss=52.14695
val_loss=601.72430
val_loss=501.12738
val_loss=36.60501
val_loss=31.74274
val_loss=510.77783
adv_loss= 3.94064
adv_loss= 3.05136
adv_loss= 4.49611
adv_loss=663.03625
adv_loss= 4.36551
adv_loss= 4.06372
adv_loss= 9.59913
adv_loss= 6.55423
adv_loss=640.99762
adv_loss=22.27557
surrogate=-0.00891, entropy= 1.60661, loss=-0.00891
surrogate= 0.04081, entropy= 1.60565, loss= 0.04081
surrogate=-0.03270, entropy= 1.60497, loss=-0.03270
surrogate=-0.00708, entropy= 1.60336, loss=-0.00708
surrogate=-0.03218, entropy= 1.60392, loss=-0.03218
surrogate=-0.03184, entropy= 1.60164, loss=-0.03184
surrogate=-0.04294, entropy= 1.60107, loss=-0.04294
surrogate=-0.02328, entropy= 1.59923, loss=-0.02328
surrogate=-0.03227, entropy= 1.59819, loss=-0.03227
surrogate=-0.03378, entropy= 1.59826, loss=-0.03378
std_min= 0.34222, std_max= 0.46813, std_mean= 0.41587
val lr: [0.00014728483606557378], policy lr: [0.0001767418032786885]
Policy Loss: -0.03378, | Entropy Bonus: -0, | Value Loss: 510.78, | Advantage Loss: 22.276
Time elapsed (s): 1.7223641872406006
Agent stdevs: 0.4158708
--------------------------------------------------------------------------------

Step 401
++++++++ Policy training ++++++++++
Current mean reward: 2947.812200 | mean episode length: 836.500000
val_loss=20.80181
val_loss=35.09135
val_loss=64.02240
val_loss=1118.15356
val_loss=56.92144
val_loss=22.39548
val_loss=2123.86938
val_loss=67.23096
val_loss=87.65356
val_loss=38.37469
adv_loss= 5.90629
adv_loss= 8.67497
adv_loss= 7.16355
adv_loss= 3.67181
adv_loss=19.39437
adv_loss= 3.83302
adv_loss= 3.55498
adv_loss= 5.10295
adv_loss= 4.91366
adv_loss= 3.97166
surrogate=-0.01063, entropy= 1.59971, loss=-0.01063
surrogate=-0.03937, entropy= 1.59735, loss=-0.03937
surrogate=-0.02006, entropy= 1.59585, loss=-0.02006
surrogate= 0.00450, entropy= 1.59408, loss= 0.00450
surrogate=-0.01331, entropy= 1.59499, loss=-0.01331
surrogate=-0.00098, entropy= 1.59602, loss=-0.00098
surrogate=-0.01642, entropy= 1.59572, loss=-0.01642
surrogate=-0.03057, entropy= 1.59565, loss=-0.03057
surrogate=-0.03236, entropy= 1.59444, loss=-0.03236
surrogate=-0.03044, entropy= 1.59318, loss=-0.03044
std_min= 0.34228, std_max= 0.46700, std_mean= 0.41509
val lr: [0.00014702868852459018], policy lr: [0.00017643442622950818]
Policy Loss: -0.030444, | Entropy Bonus: -0, | Value Loss: 38.375, | Advantage Loss: 3.9717
Time elapsed (s): 1.6816456317901611
Agent stdevs: 0.4150922
--------------------------------------------------------------------------------

Step 402
++++++++ Policy training ++++++++++
Current mean reward: 1186.837337 | mean episode length: 329.000000
val_loss=73.37077
val_loss=58.00415
val_loss=26.48483
val_loss=52.02998
val_loss=23.79223
val_loss=36.17662
val_loss=29.55097
val_loss=46.23226
val_loss=13.98255
val_loss=20.39943
adv_loss= 1.64158
adv_loss= 4.24348
adv_loss=10.00585
adv_loss= 6.01620
adv_loss= 4.28757
adv_loss= 3.36862
adv_loss= 3.41671
adv_loss=27.19327
adv_loss= 4.28844
adv_loss=13.10815
surrogate= 0.04454, entropy= 1.58808, loss= 0.04454
surrogate=-0.00043, entropy= 1.58394, loss=-0.00043
surrogate=-0.00609, entropy= 1.58106, loss=-0.00609
surrogate=-0.05397, entropy= 1.57752, loss=-0.05397
surrogate=-0.03219, entropy= 1.57507, loss=-0.03219
surrogate=-0.01566, entropy= 1.57230, loss=-0.01566
surrogate=-0.02819, entropy= 1.56995, loss=-0.02819
surrogate=-0.06069, entropy= 1.56641, loss=-0.06069
surrogate=-0.02857, entropy= 1.56398, loss=-0.02857
surrogate=-0.00591, entropy= 1.56115, loss=-0.00591
std_min= 0.33934, std_max= 0.46321, std_mean= 0.41065
val lr: [0.00014677254098360655], policy lr: [0.00017612704918032785]
Policy Loss: -0.0059051, | Entropy Bonus: -0, | Value Loss: 20.399, | Advantage Loss: 13.108
Time elapsed (s): 1.66536283493042
Agent stdevs: 0.41065142
--------------------------------------------------------------------------------

Step 403
++++++++ Policy training ++++++++++
Current mean reward: 2547.331531 | mean episode length: 730.000000
val_loss=144.44551
val_loss=1333.21655
val_loss=613.73706
val_loss=730.83582
val_loss=494.30167
val_loss=428.65237
val_loss=63.52341
val_loss=1229.39917
val_loss=524.97998
val_loss=49.55912
adv_loss= 4.96483
adv_loss=17.63706
adv_loss= 9.03967
adv_loss= 8.25540
adv_loss=10.06597
adv_loss=11.51158
adv_loss= 8.93764
adv_loss=13.15315
adv_loss= 3.84160
adv_loss= 7.81962
surrogate= 0.05198, entropy= 1.55937, loss= 0.05198
surrogate= 0.00393, entropy= 1.55637, loss= 0.00393
surrogate=-0.00655, entropy= 1.55190, loss=-0.00655
surrogate=-0.02101, entropy= 1.55042, loss=-0.02101
surrogate=-0.01000, entropy= 1.55006, loss=-0.01000
surrogate= 0.00500, entropy= 1.54887, loss= 0.00500
surrogate=-0.01116, entropy= 1.54601, loss=-0.01116
surrogate=-0.02728, entropy= 1.54471, loss=-0.02728
surrogate=-0.00697, entropy= 1.54435, loss=-0.00697
surrogate=-0.01803, entropy= 1.54423, loss=-0.01803
std_min= 0.33543, std_max= 0.46398, std_mean= 0.40859
val lr: [0.00014651639344262295], policy lr: [0.00017581967213114755]
Policy Loss: -0.01803, | Entropy Bonus: -0, | Value Loss: 49.559, | Advantage Loss: 7.8196
Time elapsed (s): 1.6741735935211182
Agent stdevs: 0.40858665
--------------------------------------------------------------------------------

Step 404
++++++++ Policy training ++++++++++
Current mean reward: 1399.189655 | mean episode length: 389.500000
val_loss=67.41986
val_loss=28.47639
val_loss=31.15344
val_loss=28.90828
val_loss=63.41284
val_loss=28.92662
val_loss=13.60631
val_loss=31.27799
val_loss=62.93021
val_loss=31.20866
adv_loss= 3.60098
adv_loss= 2.97189
adv_loss= 9.43345
adv_loss= 8.67122
adv_loss=81.87377
adv_loss= 3.01966
adv_loss= 7.73251
adv_loss= 3.12145
adv_loss= 5.31156
adv_loss= 6.25104
surrogate= 0.00840, entropy= 1.54088, loss= 0.00840
surrogate=-0.02118, entropy= 1.53955, loss=-0.02118
surrogate= 0.00829, entropy= 1.53892, loss= 0.00829
surrogate=-0.01106, entropy= 1.53619, loss=-0.01106
surrogate= 0.03057, entropy= 1.53751, loss= 0.03057
surrogate= 0.01125, entropy= 1.53734, loss= 0.01125
surrogate=-0.01864, entropy= 1.53749, loss=-0.01864
surrogate=-0.01928, entropy= 1.53719, loss=-0.01928
surrogate=-0.03162, entropy= 1.53723, loss=-0.03162
surrogate=-0.02355, entropy= 1.53715, loss=-0.02355
std_min= 0.33190, std_max= 0.46741, std_mean= 0.40802
val lr: [0.00014626024590163933], policy lr: [0.00017551229508196717]
Policy Loss: -0.023545, | Entropy Bonus: -0, | Value Loss: 31.209, | Advantage Loss: 6.251
Time elapsed (s): 1.6647443771362305
Agent stdevs: 0.40801767
--------------------------------------------------------------------------------

Step 405
++++++++ Policy training ++++++++++
Current mean reward: 2466.650040 | mean episode length: 704.500000
val_loss=699.49139
val_loss=188.13138
val_loss=675.69580
val_loss=24.00929
val_loss=1493.91113
val_loss=494.91281
val_loss=837.00494
val_loss=650.37897
val_loss=26.10147
val_loss=12.58350
adv_loss= 2.55140
adv_loss= 2.43365
adv_loss= 1.40979
adv_loss= 2.50126
adv_loss= 3.96615
adv_loss= 1.50627
adv_loss= 3.62977
adv_loss= 1.86849
adv_loss= 1.36119
adv_loss= 1.75133
surrogate= 0.00532, entropy= 1.53150, loss= 0.00532
surrogate= 0.00597, entropy= 1.52712, loss= 0.00597
surrogate= 0.00810, entropy= 1.52453, loss= 0.00810
surrogate=-0.00796, entropy= 1.52176, loss=-0.00796
surrogate=-0.02114, entropy= 1.51900, loss=-0.02114
surrogate= 0.00486, entropy= 1.51640, loss= 0.00486
surrogate=-0.00923, entropy= 1.51487, loss=-0.00923
surrogate=-0.02168, entropy= 1.51096, loss=-0.02168
surrogate=-0.02533, entropy= 1.50886, loss=-0.02533
surrogate=-0.00840, entropy= 1.50535, loss=-0.00840
std_min= 0.32835, std_max= 0.46008, std_mean= 0.40365
val lr: [0.00014600409836065575], policy lr: [0.0001752049180327869]
Policy Loss: -0.0084021, | Entropy Bonus: -0, | Value Loss: 12.583, | Advantage Loss: 1.7513
Time elapsed (s): 1.6563177108764648
Agent stdevs: 0.40364733
--------------------------------------------------------------------------------

Step 406
++++++++ Policy training ++++++++++
Current mean reward: 1380.487752 | mean episode length: 385.600000
val_loss=29.91443
val_loss=39.21296
val_loss=32.03722
val_loss=19.19826
val_loss=16.70671
val_loss=30.82650
val_loss=21.77493
val_loss=16.31867
val_loss=22.58350
val_loss= 7.87828
adv_loss= 4.46560
adv_loss= 2.42112
adv_loss= 4.87241
adv_loss= 3.84850
adv_loss= 2.10862
adv_loss= 3.68672
adv_loss= 3.65449
adv_loss= 3.96737
adv_loss= 8.55457
adv_loss= 8.28446
surrogate= 0.01011, entropy= 1.50278, loss= 0.01011
surrogate= 0.01497, entropy= 1.50283, loss= 0.01497
surrogate=-0.00705, entropy= 1.50264, loss=-0.00705
surrogate=-0.01933, entropy= 1.50029, loss=-0.01933
surrogate=-0.02843, entropy= 1.50113, loss=-0.02843
surrogate=-0.01139, entropy= 1.50061, loss=-0.01139
surrogate= 0.00403, entropy= 1.50067, loss= 0.00403
surrogate=-0.01174, entropy= 1.50026, loss=-0.01174
surrogate=-0.00768, entropy= 1.49985, loss=-0.00768
surrogate=-0.02499, entropy= 1.49942, loss=-0.02499
std_min= 0.33020, std_max= 0.46148, std_mean= 0.40271
val lr: [0.00014574795081967213], policy lr: [0.00017489754098360652]
Policy Loss: -0.02499, | Entropy Bonus: -0, | Value Loss: 7.8783, | Advantage Loss: 8.2845
Time elapsed (s): 1.664902687072754
Agent stdevs: 0.40270615
--------------------------------------------------------------------------------

Step 407
++++++++ Policy training ++++++++++
Current mean reward: 3474.674182 | mean episode length: 1000.000000
val_loss=61.82174
val_loss=1149.06531
val_loss=254.11266
val_loss=1368.82434
val_loss=116.75238
val_loss=1254.64429
val_loss=50.08697
val_loss=585.79840
val_loss=64.78133
val_loss=978.55090
adv_loss= 2.35845
adv_loss= 1.85391
adv_loss= 3.45331
adv_loss=607.98541
adv_loss= 5.99277
adv_loss= 2.73335
adv_loss= 1.73139
adv_loss= 2.23976
adv_loss= 5.13093
adv_loss= 4.45848
surrogate=-0.01796, entropy= 1.49684, loss=-0.01796
surrogate=-0.03065, entropy= 1.49626, loss=-0.03065
surrogate= 0.00971, entropy= 1.49697, loss= 0.00971
surrogate= 0.01929, entropy= 1.49523, loss= 0.01929
surrogate=-0.01996, entropy= 1.49533, loss=-0.01996
surrogate=-0.00897, entropy= 1.49559, loss=-0.00897
surrogate=-0.00498, entropy= 1.49371, loss=-0.00498
surrogate=-0.03177, entropy= 1.49484, loss=-0.03177
surrogate=-0.01265, entropy= 1.49515, loss=-0.01265
surrogate=-0.03823, entropy= 1.49374, loss=-0.03823
std_min= 0.32903, std_max= 0.46386, std_mean= 0.40210
val lr: [0.00014549180327868853], policy lr: [0.0001745901639344262]
Policy Loss: -0.038229, | Entropy Bonus: -0, | Value Loss: 978.55, | Advantage Loss: 4.4585
Time elapsed (s): 1.6652486324310303
Agent stdevs: 0.40210035
--------------------------------------------------------------------------------

Step 408
++++++++ Policy training ++++++++++
Current mean reward: 1743.517235 | mean episode length: 491.250000
val_loss=99.61647
val_loss=51.92509
val_loss=67.22099
val_loss=36.22756
val_loss=42.62140
val_loss=60.35173
val_loss=42.52396
val_loss=26.69614
val_loss=29.73989
val_loss=30.38721
adv_loss= 7.90639
adv_loss= 8.42825
adv_loss= 3.22636
adv_loss= 3.46304
adv_loss= 4.55467
adv_loss= 5.19107
adv_loss= 5.29905
adv_loss= 6.13705
adv_loss= 2.10262
adv_loss= 2.57545
surrogate=-0.00337, entropy= 1.49429, loss=-0.00337
surrogate=-0.01379, entropy= 1.49480, loss=-0.01379
surrogate=-0.01410, entropy= 1.49581, loss=-0.01410
surrogate=-0.02350, entropy= 1.49683, loss=-0.02350
surrogate=-0.01501, entropy= 1.49708, loss=-0.01501
surrogate= 0.01650, entropy= 1.49638, loss= 0.01650
surrogate= 0.01078, entropy= 1.49586, loss= 0.01078
surrogate=-0.01148, entropy= 1.49863, loss=-0.01148
surrogate=-0.00981, entropy= 1.49737, loss=-0.00981
surrogate=-0.01865, entropy= 1.49707, loss=-0.01865
std_min= 0.32967, std_max= 0.46213, std_mean= 0.40245
val lr: [0.00014523565573770493], policy lr: [0.0001742827868852459]
Policy Loss: -0.018654, | Entropy Bonus: -0, | Value Loss: 30.387, | Advantage Loss: 2.5755
Time elapsed (s): 1.6535780429840088
Agent stdevs: 0.40245333
--------------------------------------------------------------------------------

Step 409
++++++++ Policy training ++++++++++
Current mean reward: 1403.905645 | mean episode length: 400.800000
val_loss=886.00616
val_loss=356.25058
val_loss=1273.18933
val_loss=822.81702
val_loss=416.85025
val_loss=52.34039
val_loss=92.05963
val_loss=435.20105
val_loss=281.00912
val_loss=280.73651
adv_loss=11.68701
adv_loss=667.51123
adv_loss=13.89946
adv_loss=16.71700
adv_loss=32.49126
adv_loss=661.59070
adv_loss=34.56008
adv_loss= 5.97399
adv_loss=21.40453
adv_loss=16.95867
surrogate=-0.00450, entropy= 1.49703, loss=-0.00450
surrogate=-0.01190, entropy= 1.49604, loss=-0.01190
surrogate=-0.02269, entropy= 1.49531, loss=-0.02269
surrogate=-0.02765, entropy= 1.49474, loss=-0.02765
surrogate=-0.00440, entropy= 1.49343, loss=-0.00440
surrogate=-0.02685, entropy= 1.49140, loss=-0.02685
surrogate=-0.02967, entropy= 1.49075, loss=-0.02967
surrogate= 0.00415, entropy= 1.48958, loss= 0.00415
surrogate=-0.03003, entropy= 1.48621, loss=-0.03003
surrogate=-0.04174, entropy= 1.48545, loss=-0.04174
std_min= 0.32845, std_max= 0.45715, std_mean= 0.40078
val lr: [0.00014497950819672133], policy lr: [0.00017397540983606557]
Policy Loss: -0.04174, | Entropy Bonus: -0, | Value Loss: 280.74, | Advantage Loss: 16.959
Time elapsed (s): 1.651623249053955
Agent stdevs: 0.40077612
--------------------------------------------------------------------------------

Step 410
++++++++ Policy training ++++++++++
Current mean reward: 1995.195512 | mean episode length: 582.666667
val_loss=72.98781
val_loss=113.64851
val_loss=1844.81140
val_loss=521.98944
val_loss=285.69568
val_loss=87.87710
val_loss=1349.83044
val_loss=112.60204
val_loss=26.84171
val_loss=259.53146
adv_loss= 3.44473
adv_loss= 1.98361
adv_loss= 3.45815
adv_loss= 9.56001
adv_loss= 2.47577
adv_loss=21.47079
adv_loss= 6.17761
adv_loss= 5.27691
adv_loss= 2.70621
adv_loss= 3.12021
surrogate= 0.03127, entropy= 1.48455, loss= 0.03127
surrogate= 0.01526, entropy= 1.48620, loss= 0.01526
surrogate= 0.00358, entropy= 1.48678, loss= 0.00358
surrogate= 0.03047, entropy= 1.48488, loss= 0.03047
surrogate=-0.01422, entropy= 1.48213, loss=-0.01422
surrogate=-0.03044, entropy= 1.48186, loss=-0.03044
surrogate=-0.01979, entropy= 1.48176, loss=-0.01979
surrogate= 0.01010, entropy= 1.48142, loss= 0.01010
surrogate= 0.01167, entropy= 1.48056, loss= 0.01167
surrogate=-0.02390, entropy= 1.47912, loss=-0.02390
std_min= 0.32648, std_max= 0.45979, std_mean= 0.40017
val lr: [0.0001447233606557377], policy lr: [0.00017366803278688525]
Policy Loss: -0.023897, | Entropy Bonus: -0, | Value Loss: 259.53, | Advantage Loss: 3.1202
Time elapsed (s): 1.6338260173797607
Agent stdevs: 0.40017232
--------------------------------------------------------------------------------

Step 411
++++++++ Policy training ++++++++++
Current mean reward: 3248.784939 | mean episode length: 926.000000
val_loss=616.08496
val_loss=83.13797
val_loss=176.65022
val_loss=70.51687
val_loss=1251.83594
val_loss=364.25375
val_loss=139.37631
val_loss=789.67371
val_loss=36.13758
val_loss=317.64331
adv_loss= 8.08112
adv_loss=1679.55640
adv_loss= 2.73379
adv_loss= 2.28472
adv_loss= 4.38142
adv_loss= 2.26168
adv_loss= 4.48671
adv_loss= 4.17348
adv_loss= 5.40988
adv_loss=10.13614
surrogate=-0.00534, entropy= 1.47808, loss=-0.00534
surrogate=-0.02425, entropy= 1.47728, loss=-0.02425
surrogate=-0.01084, entropy= 1.47741, loss=-0.01084
surrogate=-0.01449, entropy= 1.47737, loss=-0.01449
surrogate=-0.02948, entropy= 1.47823, loss=-0.02948
surrogate=-0.02495, entropy= 1.47909, loss=-0.02495
surrogate=-0.04033, entropy= 1.47872, loss=-0.04033
surrogate=-0.02035, entropy= 1.47837, loss=-0.02035
surrogate=-0.01345, entropy= 1.47913, loss=-0.01345
surrogate=-0.03002, entropy= 1.47892, loss=-0.03002
std_min= 0.32722, std_max= 0.45515, std_mean= 0.39993
val lr: [0.00014446721311475407], policy lr: [0.00017336065573770487]
Policy Loss: -0.030024, | Entropy Bonus: -0, | Value Loss: 317.64, | Advantage Loss: 10.136
Time elapsed (s): 1.6390020847320557
Agent stdevs: 0.39993158
--------------------------------------------------------------------------------

Step 412
++++++++ Policy training ++++++++++
Current mean reward: 1608.551485 | mean episode length: 444.000000
val_loss=18.10505
val_loss=12.54741
val_loss=24.54279
val_loss=12.39523
val_loss=18.59953
val_loss=19.20785
val_loss= 8.10708
val_loss=14.37937
val_loss=14.60177
val_loss= 9.78674
adv_loss= 3.55356
adv_loss= 3.02834
adv_loss= 3.22666
adv_loss= 2.06476
adv_loss= 2.09777
adv_loss= 2.49627
adv_loss= 2.15151
adv_loss= 2.29666
adv_loss= 2.22527
adv_loss= 3.23519
surrogate= 0.01512, entropy= 1.47867, loss= 0.01512
surrogate=-0.01184, entropy= 1.47501, loss=-0.01184
surrogate= 0.01025, entropy= 1.47446, loss= 0.01025
surrogate=-0.00472, entropy= 1.47476, loss=-0.00472
surrogate=-0.03425, entropy= 1.47415, loss=-0.03425
surrogate=-0.00213, entropy= 1.47252, loss=-0.00213
surrogate=-0.01021, entropy= 1.47205, loss=-0.01021
surrogate=-0.01304, entropy= 1.46973, loss=-0.01304
surrogate=-0.02716, entropy= 1.46983, loss=-0.02716
surrogate= 0.01459, entropy= 1.47047, loss= 0.01459
std_min= 0.32577, std_max= 0.45400, std_mean= 0.39885
val lr: [0.0001442110655737705], policy lr: [0.0001730532786885246]
Policy Loss: 0.014585, | Entropy Bonus: -0, | Value Loss: 9.7867, | Advantage Loss: 3.2352
Time elapsed (s): 1.6873252391815186
Agent stdevs: 0.39885274
--------------------------------------------------------------------------------

Step 413
++++++++ Policy training ++++++++++
Current mean reward: 1794.006805 | mean episode length: 504.250000
val_loss=24.15461
val_loss=17.28752
val_loss= 5.39788
val_loss=15.01439
val_loss=12.77240
val_loss= 9.56278
val_loss=12.18738
val_loss= 7.64020
val_loss= 8.32669
val_loss=14.01325
adv_loss= 4.16008
adv_loss= 2.97375
adv_loss= 3.08644
adv_loss= 3.12524
adv_loss= 4.95716
adv_loss= 2.08147
adv_loss= 3.55201
adv_loss= 2.96277
adv_loss= 2.75489
adv_loss= 5.99872
surrogate= 0.00890, entropy= 1.47166, loss= 0.00890
surrogate= 0.02187, entropy= 1.47411, loss= 0.02187
surrogate=-0.02433, entropy= 1.47516, loss=-0.02433
surrogate=-0.01731, entropy= 1.47727, loss=-0.01731
surrogate=-0.00786, entropy= 1.47969, loss=-0.00786
surrogate=-0.03031, entropy= 1.48112, loss=-0.03031
surrogate=-0.02052, entropy= 1.48230, loss=-0.02052
surrogate= 0.00500, entropy= 1.48450, loss= 0.00500
surrogate=-0.01061, entropy= 1.48547, loss=-0.01061
surrogate=-0.05206, entropy= 1.48585, loss=-0.05206
std_min= 0.32749, std_max= 0.45842, std_mean= 0.40097
val lr: [0.00014395491803278687], policy lr: [0.00017274590163934424]
Policy Loss: -0.052057, | Entropy Bonus: -0, | Value Loss: 14.013, | Advantage Loss: 5.9987
Time elapsed (s): 1.66902494430542
Agent stdevs: 0.40097228
--------------------------------------------------------------------------------

Step 414
++++++++ Policy training ++++++++++
Current mean reward: 1600.905645 | mean episode length: 441.250000
val_loss=17.35071
val_loss=12.75291
val_loss=18.78758
val_loss=16.85198
val_loss=13.35009
val_loss=10.91861
val_loss=14.56965
val_loss=12.16844
val_loss=13.12239
val_loss= 8.94300
adv_loss= 2.22306
adv_loss= 1.51611
adv_loss= 1.85821
adv_loss= 2.88482
adv_loss= 6.01941
adv_loss= 1.72567
adv_loss= 4.21645
adv_loss= 2.12221
adv_loss= 1.74689
adv_loss= 2.87071
surrogate= 0.00034, entropy= 1.48509, loss= 0.00034
surrogate= 0.00954, entropy= 1.48234, loss= 0.00954
surrogate=-0.01458, entropy= 1.48042, loss=-0.01458
surrogate=-0.00145, entropy= 1.47770, loss=-0.00145
surrogate=-0.00201, entropy= 1.47554, loss=-0.00201
surrogate=-0.03703, entropy= 1.47376, loss=-0.03703
surrogate= 0.02632, entropy= 1.47192, loss= 0.02632
surrogate=-0.00007, entropy= 1.47110, loss=-0.00007
surrogate=-0.01723, entropy= 1.46814, loss=-0.01723
surrogate= 0.01349, entropy= 1.46575, loss= 0.01349
std_min= 0.32464, std_max= 0.45564, std_mean= 0.39834
val lr: [0.0001436987704918033], policy lr: [0.00017243852459016394]
Policy Loss: 0.013494, | Entropy Bonus: -0, | Value Loss: 8.943, | Advantage Loss: 2.8707
Time elapsed (s): 1.660341739654541
Agent stdevs: 0.3983421
--------------------------------------------------------------------------------

Step 415
++++++++ Policy training ++++++++++
Current mean reward: 2367.245953 | mean episode length: 662.666667
val_loss= 8.30047
val_loss= 8.69474
val_loss= 6.31055
val_loss= 6.93026
val_loss= 5.38954
val_loss= 4.77152
val_loss= 8.75107
val_loss= 7.13777
val_loss= 3.68938
val_loss=10.36988
adv_loss= 2.33482
adv_loss= 1.29475
adv_loss= 3.55049
adv_loss= 5.15571
adv_loss= 1.23924
adv_loss= 1.34616
adv_loss= 3.22169
adv_loss= 6.73530
adv_loss= 2.90963
adv_loss= 2.50292
surrogate=-0.00722, entropy= 1.46407, loss=-0.00722
surrogate= 0.00893, entropy= 1.46085, loss= 0.00893
surrogate= 0.01851, entropy= 1.46002, loss= 0.01851
surrogate=-0.02935, entropy= 1.45735, loss=-0.02935
surrogate=-0.01182, entropy= 1.45481, loss=-0.01182
surrogate=-0.02021, entropy= 1.45448, loss=-0.02021
surrogate=-0.01468, entropy= 1.45387, loss=-0.01468
surrogate=-0.00595, entropy= 1.45101, loss=-0.00595
surrogate= 0.01072, entropy= 1.44820, loss= 0.01072
surrogate=-0.00262, entropy= 1.44731, loss=-0.00262
std_min= 0.32358, std_max= 0.45324, std_mean= 0.39583
val lr: [0.00014344262295081967], policy lr: [0.0001721311475409836]
Policy Loss: -0.0026155, | Entropy Bonus: -0, | Value Loss: 10.37, | Advantage Loss: 2.5029
Time elapsed (s): 1.6798794269561768
Agent stdevs: 0.39583334
--------------------------------------------------------------------------------

Step 416
++++++++ Policy training ++++++++++
Current mean reward: 2454.870999 | mean episode length: 706.000000
val_loss=775.73370
val_loss=12.83576
val_loss=156.66554
val_loss=198.89871
val_loss=368.71805
val_loss=53.69453
val_loss=45.92429
val_loss=37.42767
val_loss=39.67136
val_loss=125.88443
adv_loss= 1.79337
adv_loss= 3.84191
adv_loss= 1.39959
adv_loss= 3.14849
adv_loss= 2.74903
adv_loss= 1.31832
adv_loss= 1.64834
adv_loss= 2.65346
adv_loss= 2.82289
adv_loss= 2.17868
surrogate= 0.00605, entropy= 1.44832, loss= 0.00605
surrogate= 0.01074, entropy= 1.45118, loss= 0.01074
surrogate=-0.00363, entropy= 1.45241, loss=-0.00363
surrogate=-0.00854, entropy= 1.45532, loss=-0.00854
surrogate=-0.01821, entropy= 1.45910, loss=-0.01821
surrogate=-0.02417, entropy= 1.46016, loss=-0.02417
surrogate=-0.00580, entropy= 1.46140, loss=-0.00580
surrogate=-0.02215, entropy= 1.46291, loss=-0.02215
surrogate=-0.01642, entropy= 1.46378, loss=-0.01642
surrogate=-0.04741, entropy= 1.46430, loss=-0.04741
std_min= 0.32809, std_max= 0.45178, std_mean= 0.39773
val lr: [0.00014318647540983607], policy lr: [0.00017182377049180326]
Policy Loss: -0.047409, | Entropy Bonus: -0, | Value Loss: 125.88, | Advantage Loss: 2.1787
Time elapsed (s): 1.6492063999176025
Agent stdevs: 0.39773378
--------------------------------------------------------------------------------

Step 417
++++++++ Policy training ++++++++++
Current mean reward: 1811.752825 | mean episode length: 510.500000
val_loss=17.38913
val_loss=18.30876
val_loss=10.00355
val_loss=17.58858
val_loss=12.65472
val_loss=19.43040
val_loss=14.96393
val_loss=10.77869
val_loss=15.89323
val_loss= 8.06149
adv_loss= 4.94525
adv_loss= 2.74041
adv_loss= 2.11409
adv_loss= 6.25502
adv_loss= 2.55070
adv_loss= 1.25222
adv_loss= 2.11979
adv_loss=65.65434
adv_loss= 2.17538
adv_loss= 9.39575
surrogate=-0.01491, entropy= 1.46262, loss=-0.01491
surrogate=-0.01952, entropy= 1.46302, loss=-0.01952
surrogate= 0.00862, entropy= 1.46214, loss= 0.00862
surrogate=-0.01736, entropy= 1.46237, loss=-0.01736
surrogate=-0.02390, entropy= 1.46274, loss=-0.02390
surrogate=-0.01756, entropy= 1.46189, loss=-0.01756
surrogate=-0.01210, entropy= 1.46129, loss=-0.01210
surrogate=-0.02881, entropy= 1.46014, loss=-0.02881
surrogate=-0.00475, entropy= 1.45836, loss=-0.00475
surrogate=-0.04160, entropy= 1.45838, loss=-0.04160
std_min= 0.32814, std_max= 0.45039, std_mean= 0.39689
val lr: [0.00014293032786885247], policy lr: [0.00017151639344262294]
Policy Loss: -0.041596, | Entropy Bonus: -0, | Value Loss: 8.0615, | Advantage Loss: 9.3958
Time elapsed (s): 1.6555078029632568
Agent stdevs: 0.39688814
--------------------------------------------------------------------------------

Step 418
++++++++ Policy training ++++++++++
Current mean reward: 1334.653165 | mean episode length: 374.800000
val_loss=22.11135
val_loss=22.30508
val_loss=13.96343
val_loss=16.43947
val_loss=15.52567
val_loss=19.30297
val_loss=23.89002
val_loss=13.02676
val_loss=12.67019
val_loss=10.95647
adv_loss=15.93523
adv_loss= 4.63331
adv_loss= 2.36961
adv_loss= 5.78907
adv_loss= 3.71858
adv_loss= 4.27306
adv_loss= 1.52709
adv_loss= 2.37693
adv_loss= 9.38353
adv_loss= 4.49143
surrogate=-0.00208, entropy= 1.45562, loss=-0.00208
surrogate=-0.00479, entropy= 1.45398, loss=-0.00479
surrogate= 0.00289, entropy= 1.45106, loss= 0.00289
surrogate=-0.02445, entropy= 1.44882, loss=-0.02445
surrogate=-0.01881, entropy= 1.44634, loss=-0.01881
surrogate=-0.02664, entropy= 1.44542, loss=-0.02664
surrogate=-0.01479, entropy= 1.44265, loss=-0.01479
surrogate= 0.00621, entropy= 1.44107, loss= 0.00621
surrogate=-0.04059, entropy= 1.43816, loss=-0.04059
surrogate= 0.00159, entropy= 1.43663, loss= 0.00159
std_min= 0.32724, std_max= 0.44875, std_mean= 0.39393
val lr: [0.00014267418032786884], policy lr: [0.0001712090163934426]
Policy Loss: 0.0015916, | Entropy Bonus: -0, | Value Loss: 10.956, | Advantage Loss: 4.4914
Time elapsed (s): 1.6764793395996094
Agent stdevs: 0.39392507
--------------------------------------------------------------------------------

Step 419
++++++++ Policy training ++++++++++
Current mean reward: 2749.187371 | mean episode length: 792.500000
val_loss=77.86283
val_loss=41.70724
val_loss=87.93838
val_loss=53.29433
val_loss=43.92080
val_loss=1011.99860
val_loss=40.93962
val_loss=25.25493
val_loss=929.05176
val_loss=31.95779
adv_loss= 3.10104
adv_loss=1130.52722
adv_loss=26.14759
adv_loss=1118.96094
adv_loss= 7.39066
adv_loss= 7.75294
adv_loss=1115.47852
adv_loss= 4.28708
adv_loss= 5.72842
adv_loss= 5.80590
surrogate=-0.00241, entropy= 1.43329, loss=-0.00241
surrogate= 0.02417, entropy= 1.43192, loss= 0.02417
surrogate=-0.01993, entropy= 1.43204, loss=-0.01993
surrogate= 0.02588, entropy= 1.42962, loss= 0.02588
surrogate=-0.01790, entropy= 1.43032, loss=-0.01790
surrogate= 0.02747, entropy= 1.42926, loss= 0.02747
surrogate=-0.00679, entropy= 1.42597, loss=-0.00679
surrogate=-0.02441, entropy= 1.42693, loss=-0.02441
surrogate= 0.04091, entropy= 1.42533, loss= 0.04091
surrogate= 0.03774, entropy= 1.42582, loss= 0.03774
std_min= 0.32606, std_max= 0.44905, std_mean= 0.39256
val lr: [0.00014241803278688524], policy lr: [0.0001709016393442623]
Policy Loss: 0.037738, | Entropy Bonus: -0, | Value Loss: 31.958, | Advantage Loss: 5.8059
Time elapsed (s): 1.6947529315948486
Agent stdevs: 0.39256415
--------------------------------------------------------------------------------

Step 420
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2143.5
++++++++ Policy training ++++++++++
Current mean reward: 1457.070117 | mean episode length: 408.750000
val_loss=40.72637
val_loss=15.24848
val_loss=27.46666
val_loss=10.16154
val_loss=44.95464
val_loss=14.83145
val_loss=18.12413
val_loss=33.31895
val_loss=12.96371
val_loss=15.26231
adv_loss= 2.82554
adv_loss= 3.39972
adv_loss= 1.98748
adv_loss= 2.30716
adv_loss= 3.80439
adv_loss= 4.48191
adv_loss= 3.53651
adv_loss= 6.53950
adv_loss= 5.33727
adv_loss= 1.62472
surrogate= 0.03230, entropy= 1.42546, loss= 0.03230
surrogate=-0.00425, entropy= 1.42338, loss=-0.00425
surrogate=-0.01981, entropy= 1.42352, loss=-0.01981
surrogate= 0.01047, entropy= 1.42358, loss= 0.01047
surrogate=-0.00471, entropy= 1.42388, loss=-0.00471
surrogate=-0.01664, entropy= 1.42382, loss=-0.01664
surrogate= 0.02194, entropy= 1.42405, loss= 0.02194
surrogate=-0.03057, entropy= 1.42409, loss=-0.03057
surrogate=-0.02127, entropy= 1.42478, loss=-0.02127
surrogate=-0.01579, entropy= 1.42477, loss=-0.01579
std_min= 0.32602, std_max= 0.44594, std_mean= 0.39235
val lr: [0.00014216188524590162], policy lr: [0.00017059426229508194]
Policy Loss: -0.015791, | Entropy Bonus: -0, | Value Loss: 15.262, | Advantage Loss: 1.6247
Time elapsed (s): 1.658172845840454
Agent stdevs: 0.39234546
--------------------------------------------------------------------------------

Step 421
++++++++ Policy training ++++++++++
Current mean reward: 2080.559018 | mean episode length: 579.333333
val_loss=17.59678
val_loss=13.69516
val_loss=27.40202
val_loss=13.49562
val_loss=13.91884
val_loss=27.01970
val_loss=17.76298
val_loss=12.75515
val_loss= 9.52862
val_loss=19.05336
adv_loss= 2.42091
adv_loss= 1.88917
adv_loss= 3.47272
adv_loss= 2.90177
adv_loss= 4.69823
adv_loss= 0.88551
adv_loss= 1.33884
adv_loss= 2.08005
adv_loss= 2.78249
adv_loss= 1.72164
surrogate= 0.01254, entropy= 1.42456, loss= 0.01254
surrogate= 0.00487, entropy= 1.42419, loss= 0.00487
surrogate= 0.01211, entropy= 1.42326, loss= 0.01211
surrogate= 0.01060, entropy= 1.42477, loss= 0.01060
surrogate=-0.02155, entropy= 1.42451, loss=-0.02155
surrogate=-0.01911, entropy= 1.42484, loss=-0.01911
surrogate=-0.00605, entropy= 1.42577, loss=-0.00605
surrogate=-0.01897, entropy= 1.42570, loss=-0.01897
surrogate= 0.00620, entropy= 1.42384, loss= 0.00620
surrogate=-0.03631, entropy= 1.42308, loss=-0.03631
std_min= 0.32511, std_max= 0.44670, std_mean= 0.39221
val lr: [0.00014190573770491804], policy lr: [0.00017028688524590164]
Policy Loss: -0.036309, | Entropy Bonus: -0, | Value Loss: 19.053, | Advantage Loss: 1.7216
Time elapsed (s): 1.650477409362793
Agent stdevs: 0.3922143
--------------------------------------------------------------------------------

Step 422
++++++++ Policy training ++++++++++
Current mean reward: 2255.377877 | mean episode length: 649.000000
val_loss=38.09133
val_loss=30.78372
val_loss=245.08673
val_loss=55.62952
val_loss=39.50524
val_loss=71.45067
val_loss=1383.40283
val_loss=949.05255
val_loss=1455.23328
val_loss=42.23321
adv_loss=13.10166
adv_loss= 4.27554
adv_loss= 3.17173
adv_loss= 4.75074
adv_loss= 4.93984
adv_loss= 3.79786
adv_loss= 1.52074
adv_loss= 3.83902
adv_loss= 3.83111
adv_loss= 2.47071
surrogate=-0.02482, entropy= 1.42198, loss=-0.02482
surrogate= 0.01412, entropy= 1.42163, loss= 0.01412
surrogate=-0.01885, entropy= 1.42190, loss=-0.01885
surrogate=-0.00422, entropy= 1.42082, loss=-0.00422
surrogate= 0.00162, entropy= 1.41949, loss= 0.00162
surrogate=-0.02072, entropy= 1.41800, loss=-0.02072
surrogate=-0.02612, entropy= 1.41712, loss=-0.02612
surrogate=-0.01229, entropy= 1.41536, loss=-0.01229
surrogate= 0.00336, entropy= 1.41446, loss= 0.00336
surrogate=-0.01310, entropy= 1.41528, loss=-0.01310
std_min= 0.32428, std_max= 0.44806, std_mean= 0.39128
val lr: [0.00014164959016393442], policy lr: [0.00016997950819672128]
Policy Loss: -0.013103, | Entropy Bonus: -0, | Value Loss: 42.233, | Advantage Loss: 2.4707
Time elapsed (s): 1.626276969909668
Agent stdevs: 0.39127788
--------------------------------------------------------------------------------

Step 423
++++++++ Policy training ++++++++++
Current mean reward: 2622.252718 | mean episode length: 777.500000
val_loss=318.11365
val_loss=458.87082
val_loss=451.77939
val_loss=219.41272
val_loss=39.89100
val_loss=455.74384
val_loss=115.12103
val_loss=240.75334
val_loss=232.48039
val_loss=1008.22693
adv_loss= 3.81115
adv_loss= 5.82751
adv_loss= 3.93739
adv_loss= 2.75853
adv_loss= 2.05990
adv_loss= 4.32781
adv_loss= 3.02744
adv_loss= 3.67061
adv_loss=1412.86755
adv_loss= 2.59261
surrogate=-0.01108, entropy= 1.41549, loss=-0.01108
surrogate=-0.03154, entropy= 1.41499, loss=-0.03154
surrogate= 0.04657, entropy= 1.41435, loss= 0.04657
surrogate=-0.01251, entropy= 1.41360, loss=-0.01251
surrogate= 0.02210, entropy= 1.41327, loss= 0.02210
surrogate=-0.00888, entropy= 1.41243, loss=-0.00888
surrogate= 0.00545, entropy= 1.41167, loss= 0.00545
surrogate=-0.02424, entropy= 1.41247, loss=-0.02424
surrogate=-0.01865, entropy= 1.41257, loss=-0.01865
surrogate= 0.00044, entropy= 1.41157, loss= 0.00044
std_min= 0.32059, std_max= 0.45053, std_mean= 0.39117
val lr: [0.00014139344262295084], policy lr: [0.00016967213114754099]
Policy Loss: 0.00043511, | Entropy Bonus: -0, | Value Loss: 1008.2, | Advantage Loss: 2.5926
Time elapsed (s): 1.6316547393798828
Agent stdevs: 0.3911662
--------------------------------------------------------------------------------

Step 424
++++++++ Policy training ++++++++++
Current mean reward: 3307.410113 | mean episode length: 1000.000000
val_loss=544.00195
val_loss=96.68037
val_loss=429.48370
val_loss=493.11282
val_loss=874.58984
val_loss=255.80734
val_loss=336.76611
val_loss=492.10223
val_loss=1373.31958
val_loss=689.71454
adv_loss= 5.50602
adv_loss= 4.22382
adv_loss= 2.50364
adv_loss=1717.27490
adv_loss= 5.04189
adv_loss= 4.11084
adv_loss= 2.53282
adv_loss= 5.34741
adv_loss= 2.07182
adv_loss=10.55326
surrogate= 0.02665, entropy= 1.41036, loss= 0.02665
surrogate= 0.01930, entropy= 1.41146, loss= 0.01930
surrogate= 0.02279, entropy= 1.41177, loss= 0.02279
surrogate= 0.01044, entropy= 1.41251, loss= 0.01044
surrogate=-0.00530, entropy= 1.41255, loss=-0.00530
surrogate=-0.02968, entropy= 1.41216, loss=-0.02968
surrogate=-0.03250, entropy= 1.41351, loss=-0.03250
surrogate= 0.00489, entropy= 1.41430, loss= 0.00489
surrogate= 0.01286, entropy= 1.41336, loss= 0.01286
surrogate= 0.02036, entropy= 1.41154, loss= 0.02036
std_min= 0.32212, std_max= 0.44903, std_mean= 0.39098
val lr: [0.00014113729508196722], policy lr: [0.00016936475409836063]
Policy Loss: 0.020356, | Entropy Bonus: -0, | Value Loss: 689.71, | Advantage Loss: 10.553
Time elapsed (s): 1.6670427322387695
Agent stdevs: 0.39098206
--------------------------------------------------------------------------------

Step 425
++++++++ Policy training ++++++++++
Current mean reward: 3510.624187 | mean episode length: 1000.000000
val_loss=612.48431
val_loss=25.48995
val_loss=1438.91589
val_loss=1041.84375
val_loss=70.34483
val_loss=39.88866
val_loss=219.59909
val_loss=2030.68335
val_loss=2061.57886
val_loss=360.76569
adv_loss= 6.19003
adv_loss= 2.46570
adv_loss= 4.50053
adv_loss= 1.23923
adv_loss= 3.37660
adv_loss= 8.04355
adv_loss= 1.24987
adv_loss= 3.89305
adv_loss= 1.78341
adv_loss= 1.70959
surrogate= 0.03537, entropy= 1.41036, loss= 0.03537
surrogate= 0.01135, entropy= 1.41004, loss= 0.01135
surrogate=-0.00844, entropy= 1.40859, loss=-0.00844
surrogate= 0.01281, entropy= 1.40860, loss= 0.01281
surrogate= 0.01529, entropy= 1.40919, loss= 0.01529
surrogate= 0.01956, entropy= 1.40862, loss= 0.01956
surrogate=-0.03634, entropy= 1.40711, loss=-0.03634
surrogate=-0.02906, entropy= 1.40513, loss=-0.02906
surrogate=-0.01333, entropy= 1.40348, loss=-0.01333
surrogate= 0.01025, entropy= 1.40298, loss= 0.01025
std_min= 0.31989, std_max= 0.44794, std_mean= 0.38999
val lr: [0.00014088114754098362], policy lr: [0.0001690573770491803]
Policy Loss: 0.010248, | Entropy Bonus: -0, | Value Loss: 360.77, | Advantage Loss: 1.7096
Time elapsed (s): 1.6891627311706543
Agent stdevs: 0.3899875
--------------------------------------------------------------------------------

Step 426
++++++++ Policy training ++++++++++
Current mean reward: 2635.614792 | mean episode length: 763.500000
val_loss=65.13964
val_loss=110.59951
val_loss=180.09984
val_loss=625.74811
val_loss=37.37544
val_loss=237.91733
val_loss=748.60510
val_loss=304.19638
val_loss=64.47411
val_loss=85.41409
adv_loss= 3.11784
adv_loss= 2.71551
adv_loss= 4.38012
adv_loss= 7.00438
adv_loss= 3.49560
adv_loss= 3.16796
adv_loss= 7.30941
adv_loss= 3.37406
adv_loss= 4.10089
adv_loss= 4.61534
surrogate=-0.00018, entropy= 1.40366, loss=-0.00018
surrogate= 0.06068, entropy= 1.40604, loss= 0.06068
surrogate=-0.00632, entropy= 1.40830, loss=-0.00632
surrogate= 0.00048, entropy= 1.40803, loss= 0.00048
surrogate=-0.00103, entropy= 1.40945, loss=-0.00103
surrogate= 0.00018, entropy= 1.40981, loss= 0.00018
surrogate=-0.02139, entropy= 1.40990, loss=-0.02139
surrogate=-0.01533, entropy= 1.41014, loss=-0.01533
surrogate=-0.02234, entropy= 1.41120, loss=-0.02234
surrogate=-0.03670, entropy= 1.41120, loss=-0.03670
std_min= 0.32168, std_max= 0.44723, std_mean= 0.39092
val lr: [0.00014062500000000002], policy lr: [0.00016874999999999998]
Policy Loss: -0.036697, | Entropy Bonus: -0, | Value Loss: 85.414, | Advantage Loss: 4.6153
Time elapsed (s): 1.6621978282928467
Agent stdevs: 0.39091638
--------------------------------------------------------------------------------

Step 427
++++++++ Policy training ++++++++++
Current mean reward: 2546.442768 | mean episode length: 734.500000
val_loss=42.89656
val_loss=55.51777
val_loss=160.51886
val_loss=271.62543
val_loss=91.02292
val_loss=294.39529
val_loss=30.67769
val_loss=527.02222
val_loss=21.59893
val_loss=26.93249
adv_loss= 1.68086
adv_loss= 2.35229
adv_loss= 3.16624
adv_loss= 9.00807
adv_loss= 2.01844
adv_loss= 2.31814
adv_loss= 2.36375
adv_loss= 1.41312
adv_loss= 2.36697
adv_loss= 1.23477
surrogate=-0.02873, entropy= 1.41022, loss=-0.02873
surrogate=-0.01454, entropy= 1.40806, loss=-0.01454
surrogate=-0.02172, entropy= 1.40856, loss=-0.02172
surrogate=-0.04485, entropy= 1.40902, loss=-0.04485
surrogate= 0.00332, entropy= 1.40760, loss= 0.00332
surrogate=-0.03368, entropy= 1.40620, loss=-0.03368
surrogate=-0.01309, entropy= 1.40630, loss=-0.01309
surrogate=-0.03132, entropy= 1.40591, loss=-0.03132
surrogate=-0.03086, entropy= 1.40388, loss=-0.03086
surrogate=-0.02837, entropy= 1.40293, loss=-0.02837
std_min= 0.31754, std_max= 0.44998, std_mean= 0.39027
val lr: [0.0001403688524590164], policy lr: [0.00016844262295081966]
Policy Loss: -0.028369, | Entropy Bonus: -0, | Value Loss: 26.932, | Advantage Loss: 1.2348
Time elapsed (s): 1.6616246700286865
Agent stdevs: 0.39026585
--------------------------------------------------------------------------------

Step 428
++++++++ Policy training ++++++++++
Current mean reward: 2546.318266 | mean episode length: 743.500000
val_loss=295.88110
val_loss=162.67635
val_loss=361.93750
val_loss=437.57803
val_loss=277.56723
val_loss=68.63467
val_loss=48.81394
val_loss=46.17690
val_loss=314.47888
val_loss=438.35489
adv_loss= 2.84136
adv_loss= 4.67814
adv_loss= 3.77856
adv_loss= 2.87400
adv_loss= 2.06357
adv_loss= 1.95347
adv_loss= 2.08348
adv_loss= 2.84820
adv_loss= 2.61243
adv_loss=14.23404
surrogate= 0.01487, entropy= 1.40328, loss= 0.01487
surrogate=-0.01010, entropy= 1.40354, loss=-0.01010
surrogate=-0.01928, entropy= 1.40149, loss=-0.01928
surrogate= 0.00764, entropy= 1.40149, loss= 0.00764
surrogate=-0.02391, entropy= 1.40287, loss=-0.02391
surrogate=-0.02996, entropy= 1.40239, loss=-0.02996
surrogate=-0.02738, entropy= 1.40177, loss=-0.02738
surrogate=-0.02528, entropy= 1.40203, loss=-0.02528
surrogate=-0.02450, entropy= 1.40081, loss=-0.02450
surrogate=-0.01448, entropy= 1.40065, loss=-0.01448
std_min= 0.32000, std_max= 0.44505, std_mean= 0.38956
val lr: [0.0001401127049180328], policy lr: [0.00016813524590163933]
Policy Loss: -0.014484, | Entropy Bonus: -0, | Value Loss: 438.35, | Advantage Loss: 14.234
Time elapsed (s): 1.6280882358551025
Agent stdevs: 0.38956484
--------------------------------------------------------------------------------

Step 429
++++++++ Policy training ++++++++++
Current mean reward: 2384.717961 | mean episode length: 683.000000
val_loss=237.45175
val_loss=16.92925
val_loss=31.22705
val_loss=1063.83020
val_loss=16.64581
val_loss=277.26245
val_loss=16.39684
val_loss=434.53583
val_loss=206.27380
val_loss=629.87976
adv_loss= 2.25547
adv_loss= 2.46535
adv_loss= 1.93218
adv_loss= 1.57592
adv_loss= 1.20817
adv_loss= 1.87927
adv_loss= 2.71539
adv_loss= 1.04002
adv_loss= 1.27301
adv_loss= 0.86548
surrogate= 0.02014, entropy= 1.40115, loss= 0.02014
surrogate= 0.00411, entropy= 1.39994, loss= 0.00411
surrogate=-0.01862, entropy= 1.40002, loss=-0.01862
surrogate=-0.00285, entropy= 1.39880, loss=-0.00285
surrogate= 0.00682, entropy= 1.40009, loss= 0.00682
surrogate=-0.00073, entropy= 1.40219, loss=-0.00073
surrogate=-0.02214, entropy= 1.40362, loss=-0.02214
surrogate=-0.01331, entropy= 1.40372, loss=-0.01331
surrogate=-0.02424, entropy= 1.40460, loss=-0.02424
surrogate=-0.00575, entropy= 1.40440, loss=-0.00575
std_min= 0.32129, std_max= 0.44426, std_mean= 0.38993
val lr: [0.00013985655737704916], policy lr: [0.00016782786885245898]
Policy Loss: -0.0057497, | Entropy Bonus: -0, | Value Loss: 629.88, | Advantage Loss: 0.86548
Time elapsed (s): 1.6352739334106445
Agent stdevs: 0.38992563
--------------------------------------------------------------------------------

Step 430
++++++++ Policy training ++++++++++
Current mean reward: 2316.311893 | mean episode length: 654.666667
val_loss=1392.36267
val_loss=40.27147
val_loss=30.93752
val_loss=103.96786
val_loss=54.39759
val_loss=17.34350
val_loss=56.63515
val_loss=35.42182
val_loss=417.94833
val_loss=41.56133
adv_loss= 1.99100
adv_loss= 3.47622
adv_loss= 1.24988
adv_loss= 1.61821
adv_loss= 1.75161
adv_loss=10.40521
adv_loss= 5.99289
adv_loss= 4.17888
adv_loss= 2.89960
adv_loss= 2.10306
surrogate=-0.00260, entropy= 1.39746, loss=-0.00260
surrogate=-0.00455, entropy= 1.39661, loss=-0.00455
surrogate=-0.01863, entropy= 1.39678, loss=-0.01863
surrogate=-0.00033, entropy= 1.39689, loss=-0.00033
surrogate= 0.01184, entropy= 1.39714, loss= 0.01184
surrogate=-0.02493, entropy= 1.39736, loss=-0.02493
surrogate= 0.00151, entropy= 1.39510, loss= 0.00151
surrogate=-0.01151, entropy= 1.39363, loss=-0.01151
surrogate=-0.03588, entropy= 1.39316, loss=-0.03588
surrogate=-0.00466, entropy= 1.39289, loss=-0.00466
std_min= 0.32083, std_max= 0.44349, std_mean= 0.38842
val lr: [0.0001396004098360656], policy lr: [0.00016752049180327868]
Policy Loss: -0.0046646, | Entropy Bonus: -0, | Value Loss: 41.561, | Advantage Loss: 2.1031
Time elapsed (s): 1.6507742404937744
Agent stdevs: 0.38842264
--------------------------------------------------------------------------------

Step 431
++++++++ Policy training ++++++++++
Current mean reward: 3476.100686 | mean episode length: 1000.000000
val_loss=23.15288
val_loss=28.07013
val_loss=183.61230
val_loss=144.26379
val_loss=748.55945
val_loss=244.68889
val_loss=85.65903
val_loss=838.76581
val_loss=90.93257
val_loss=64.21224
adv_loss= 3.36548
adv_loss=60.67128
adv_loss= 1.57713
adv_loss= 4.26691
adv_loss= 1.85714
adv_loss= 5.02917
adv_loss= 3.57863
adv_loss= 3.09750
adv_loss= 3.32366
adv_loss= 2.30713
surrogate= 0.00814, entropy= 1.39400, loss= 0.00814
surrogate=-0.01796, entropy= 1.39167, loss=-0.01796
surrogate= 0.00746, entropy= 1.39121, loss= 0.00746
surrogate=-0.03352, entropy= 1.39191, loss=-0.03352
surrogate=-0.02644, entropy= 1.39273, loss=-0.02644
surrogate=-0.02915, entropy= 1.39149, loss=-0.02915
surrogate= 0.00850, entropy= 1.39152, loss= 0.00850
surrogate=-0.00434, entropy= 1.39226, loss=-0.00434
surrogate=-0.05443, entropy= 1.39155, loss=-0.05443
surrogate=-0.01923, entropy= 1.38968, loss=-0.01923
std_min= 0.32255, std_max= 0.44085, std_mean= 0.38776
val lr: [0.00013934426229508196], policy lr: [0.00016721311475409833]
Policy Loss: -0.019229, | Entropy Bonus: -0, | Value Loss: 64.212, | Advantage Loss: 2.3071
Time elapsed (s): 1.6785502433776855
Agent stdevs: 0.3877605
--------------------------------------------------------------------------------

Step 432
++++++++ Policy training ++++++++++
Current mean reward: 1610.336386 | mean episode length: 451.333333
val_loss=31.10630
val_loss=22.99975
val_loss=10.86712
val_loss=15.26705
val_loss= 8.87694
val_loss=18.73273
val_loss=16.68931
val_loss=10.51143
val_loss= 9.71596
val_loss= 6.16450
adv_loss= 4.45846
adv_loss= 2.21523
adv_loss= 3.98705
adv_loss= 3.73564
adv_loss= 2.60279
adv_loss= 1.19498
adv_loss= 1.73039
adv_loss= 3.64178
adv_loss= 2.23256
adv_loss= 4.39060
surrogate=-0.03038, entropy= 1.39056, loss=-0.03038
surrogate=-0.01656, entropy= 1.39157, loss=-0.01656
surrogate= 0.02083, entropy= 1.39214, loss= 0.02083
surrogate= 0.01859, entropy= 1.39273, loss= 0.01859
surrogate=-0.00354, entropy= 1.39430, loss=-0.00354
surrogate= 0.00602, entropy= 1.39488, loss= 0.00602
surrogate= 0.00352, entropy= 1.39596, loss= 0.00352
surrogate= 0.00607, entropy= 1.39348, loss= 0.00607
surrogate=-0.01631, entropy= 1.39279, loss=-0.01631
surrogate=-0.00933, entropy= 1.39265, loss=-0.00933
std_min= 0.32086, std_max= 0.44311, std_mean= 0.38836
val lr: [0.0001390881147540984], policy lr: [0.00016690573770491803]
Policy Loss: -0.0093285, | Entropy Bonus: -0, | Value Loss: 6.1645, | Advantage Loss: 4.3906
Time elapsed (s): 1.6402015686035156
Agent stdevs: 0.3883617
--------------------------------------------------------------------------------

Step 433
++++++++ Policy training ++++++++++
Current mean reward: 1102.066015 | mean episode length: 310.500000
val_loss=47.09555
val_loss=62.45902
val_loss=31.45734
val_loss=33.24275
val_loss=16.85841
val_loss=17.09812
val_loss=18.38079
val_loss=22.13385
val_loss=20.56205
val_loss=19.76058
adv_loss= 7.85185
adv_loss= 6.62868
adv_loss= 2.60415
adv_loss= 6.10814
adv_loss= 3.08751
adv_loss=10.58158
adv_loss= 3.09654
adv_loss= 4.75945
adv_loss= 4.19701
adv_loss=16.52985
surrogate= 0.02753, entropy= 1.39287, loss= 0.02753
surrogate= 0.02880, entropy= 1.39339, loss= 0.02880
surrogate=-0.02407, entropy= 1.39332, loss=-0.02407
surrogate=-0.00304, entropy= 1.39355, loss=-0.00304
surrogate=-0.03442, entropy= 1.39396, loss=-0.03442
surrogate=-0.03281, entropy= 1.39284, loss=-0.03281
surrogate=-0.01673, entropy= 1.39308, loss=-0.01673
surrogate= 0.01108, entropy= 1.39184, loss= 0.01108
surrogate=-0.03574, entropy= 1.39133, loss=-0.03574
surrogate=-0.03275, entropy= 1.39034, loss=-0.03275
std_min= 0.32106, std_max= 0.44107, std_mean= 0.38797
val lr: [0.00013883196721311476], policy lr: [0.00016659836065573768]
Policy Loss: -0.032754, | Entropy Bonus: -0, | Value Loss: 19.761, | Advantage Loss: 16.53
Time elapsed (s): 1.6439077854156494
Agent stdevs: 0.38797402
--------------------------------------------------------------------------------

Step 434
++++++++ Policy training ++++++++++
Current mean reward: 3014.472087 | mean episode length: 859.500000
val_loss=401.03018
val_loss=1382.07251
val_loss=103.96789
val_loss=912.37030
val_loss=282.04950
val_loss=680.00073
val_loss=165.14165
val_loss=336.96310
val_loss=29.17508
val_loss=43.98768
adv_loss= 1.35463
adv_loss= 2.15703
adv_loss= 2.17161
adv_loss= 1.50950
adv_loss= 1.85188
adv_loss= 1.07993
adv_loss= 3.02552
adv_loss= 2.36022
adv_loss= 2.51773
adv_loss= 5.38531
surrogate=-0.02551, entropy= 1.38999, loss=-0.02551
surrogate=-0.01202, entropy= 1.39006, loss=-0.01202
surrogate= 0.01007, entropy= 1.39002, loss= 0.01007
surrogate= 0.00441, entropy= 1.39010, loss= 0.00441
surrogate=-0.03195, entropy= 1.38933, loss=-0.03195
surrogate=-0.01867, entropy= 1.38895, loss=-0.01867
surrogate=-0.02916, entropy= 1.38805, loss=-0.02916
surrogate= 0.01373, entropy= 1.38818, loss= 0.01373
surrogate=-0.01397, entropy= 1.38836, loss=-0.01397
surrogate=-0.00018, entropy= 1.38918, loss=-0.00018
std_min= 0.32172, std_max= 0.43953, std_mean= 0.38773
val lr: [0.00013857581967213116], policy lr: [0.00016629098360655735]
Policy Loss: -0.00017894, | Entropy Bonus: -0, | Value Loss: 43.988, | Advantage Loss: 5.3853
Time elapsed (s): 1.6425037384033203
Agent stdevs: 0.3877326
--------------------------------------------------------------------------------

Step 435
++++++++ Policy training ++++++++++
Current mean reward: 2348.855613 | mean episode length: 661.666667
val_loss=31.17926
val_loss=57.87065
val_loss=211.16669
val_loss=108.55441
val_loss=303.87698
val_loss=46.12828
val_loss=427.97748
val_loss=227.09114
val_loss=35.80046
val_loss=1331.64587
adv_loss= 2.56592
adv_loss= 2.16200
adv_loss= 2.37655
adv_loss= 5.18440
adv_loss= 3.03310
adv_loss= 2.19401
adv_loss= 3.52363
adv_loss=1675.88794
adv_loss= 3.20590
adv_loss= 3.54002
surrogate=-0.00493, entropy= 1.38851, loss=-0.00493
surrogate=-0.02403, entropy= 1.38843, loss=-0.02403
surrogate=-0.00066, entropy= 1.38888, loss=-0.00066
surrogate=-0.01847, entropy= 1.38771, loss=-0.01847
surrogate= 0.00727, entropy= 1.38645, loss= 0.00727
surrogate=-0.01337, entropy= 1.38705, loss=-0.01337
surrogate=-0.02526, entropy= 1.38710, loss=-0.02526
surrogate=-0.02275, entropy= 1.38747, loss=-0.02275
surrogate=-0.03469, entropy= 1.38705, loss=-0.03469
surrogate= 0.00503, entropy= 1.38696, loss= 0.00503
std_min= 0.32039, std_max= 0.43836, std_mean= 0.38751
val lr: [0.00013831967213114753], policy lr: [0.00016598360655737702]
Policy Loss: 0.0050348, | Entropy Bonus: -0, | Value Loss: 1331.6, | Advantage Loss: 3.54
Time elapsed (s): 1.629957675933838
Agent stdevs: 0.3875138
--------------------------------------------------------------------------------

Step 436
++++++++ Policy training ++++++++++
Current mean reward: 2816.631529 | mean episode length: 799.000000
val_loss=419.45761
val_loss=24.80442
val_loss=442.68845
val_loss=1328.43872
val_loss=160.99316
val_loss=328.57120
val_loss=36.93244
val_loss=142.33228
val_loss=132.50607
val_loss=585.57861
adv_loss= 2.79017
adv_loss= 1.68941
adv_loss= 2.29790
adv_loss= 2.31696
adv_loss= 1.46221
adv_loss= 1.44659
adv_loss= 2.50036
adv_loss= 3.18678
adv_loss= 1.22412
adv_loss= 1.62869
surrogate= 0.03100, entropy= 1.38754, loss= 0.03100
surrogate=-0.01230, entropy= 1.39026, loss=-0.01230
surrogate=-0.00039, entropy= 1.39061, loss=-0.00039
surrogate=-0.01721, entropy= 1.39020, loss=-0.01721
surrogate= 0.00071, entropy= 1.38963, loss= 0.00071
surrogate=-0.01513, entropy= 1.39070, loss=-0.01513
surrogate=-0.01191, entropy= 1.39010, loss=-0.01191
surrogate=-0.02321, entropy= 1.39120, loss=-0.02321
surrogate=-0.01305, entropy= 1.39056, loss=-0.01305
surrogate=-0.03680, entropy= 1.39140, loss=-0.03680
std_min= 0.32297, std_max= 0.44114, std_mean= 0.38797
val lr: [0.00013806352459016393], policy lr: [0.0001656762295081967]
Policy Loss: -0.036802, | Entropy Bonus: -0, | Value Loss: 585.58, | Advantage Loss: 1.6287
Time elapsed (s): 1.6613142490386963
Agent stdevs: 0.3879651
--------------------------------------------------------------------------------

Step 437
++++++++ Policy training ++++++++++
Current mean reward: 2141.985568 | mean episode length: 592.500000
val_loss=45.75312
val_loss=21.88815
val_loss=17.86269
val_loss=37.82268
val_loss=29.83426
val_loss=30.37909
val_loss=24.26883
val_loss=15.26252
val_loss=22.58306
val_loss=13.96529
adv_loss= 2.21717
adv_loss= 1.51134
adv_loss= 0.95789
adv_loss= 2.22933
adv_loss= 1.43191
adv_loss= 1.85147
adv_loss= 2.05820
adv_loss= 1.39681
adv_loss= 3.03346
adv_loss= 3.67781
surrogate= 0.00035, entropy= 1.39075, loss= 0.00035
surrogate=-0.02559, entropy= 1.39023, loss=-0.02559
surrogate=-0.01852, entropy= 1.39009, loss=-0.01852
surrogate=-0.02814, entropy= 1.38950, loss=-0.02814
surrogate=-0.03323, entropy= 1.38979, loss=-0.03323
surrogate=-0.04225, entropy= 1.39037, loss=-0.04225
surrogate= 0.00535, entropy= 1.39045, loss= 0.00535
surrogate= 0.01514, entropy= 1.39029, loss= 0.01514
surrogate=-0.04081, entropy= 1.38907, loss=-0.04081
surrogate=-0.01154, entropy= 1.38802, loss=-0.01154
std_min= 0.32230, std_max= 0.43976, std_mean= 0.38753
val lr: [0.00013780737704918033], policy lr: [0.0001653688524590164]
Policy Loss: -0.011541, | Entropy Bonus: -0, | Value Loss: 13.965, | Advantage Loss: 3.6778
Time elapsed (s): 1.6780369281768799
Agent stdevs: 0.38752866
--------------------------------------------------------------------------------

Step 438
++++++++ Policy training ++++++++++
Current mean reward: 1755.933011 | mean episode length: 493.250000
val_loss=36.63619
val_loss=24.09484
val_loss=20.76097
val_loss=30.53246
val_loss=25.36703
val_loss=19.05209
val_loss=22.35691
val_loss=22.90029
val_loss=12.12819
val_loss=20.74622
adv_loss= 1.58182
adv_loss= 3.44464
adv_loss= 2.41841
adv_loss= 3.73761
adv_loss= 2.99643
adv_loss= 5.83048
adv_loss= 6.98782
adv_loss= 3.96556
adv_loss= 1.82352
adv_loss= 1.86281
surrogate=-0.00324, entropy= 1.38775, loss=-0.00324
surrogate=-0.02742, entropy= 1.38713, loss=-0.02742
surrogate=-0.03104, entropy= 1.38857, loss=-0.03104
surrogate=-0.00600, entropy= 1.38967, loss=-0.00600
surrogate=-0.00949, entropy= 1.39103, loss=-0.00949
surrogate=-0.02646, entropy= 1.39161, loss=-0.02646
surrogate= 0.03912, entropy= 1.39084, loss= 0.03912
surrogate=-0.04323, entropy= 1.39227, loss=-0.04323
surrogate=-0.02384, entropy= 1.39427, loss=-0.02384
surrogate=-0.02165, entropy= 1.39376, loss=-0.02165
std_min= 0.32459, std_max= 0.44008, std_mean= 0.38812
val lr: [0.0001375512295081967], policy lr: [0.00016506147540983602]
Policy Loss: -0.021654, | Entropy Bonus: -0, | Value Loss: 20.746, | Advantage Loss: 1.8628
Time elapsed (s): 1.6280944347381592
Agent stdevs: 0.38811848
--------------------------------------------------------------------------------

Step 439
++++++++ Policy training ++++++++++
Current mean reward: 2125.292815 | mean episode length: 589.666667
val_loss=88.79370
val_loss=55.52115
val_loss=19.44588
val_loss=59.57451
val_loss=19.72230
val_loss=65.67850
val_loss=12.07462
val_loss= 6.16016
val_loss= 7.84250
val_loss=39.47588
adv_loss= 1.26538
adv_loss=11.87768
adv_loss= 2.78248
adv_loss= 1.37453
adv_loss= 1.22894
adv_loss= 2.04358
adv_loss= 8.35991
adv_loss= 1.06468
adv_loss= 2.12310
adv_loss= 1.04981
surrogate= 0.01068, entropy= 1.39348, loss= 0.01068
surrogate= 0.04089, entropy= 1.39430, loss= 0.04089
surrogate=-0.00117, entropy= 1.39613, loss=-0.00117
surrogate=-0.00015, entropy= 1.39798, loss=-0.00015
surrogate=-0.00188, entropy= 1.39698, loss=-0.00188
surrogate=-0.03090, entropy= 1.39787, loss=-0.03090
surrogate=-0.03436, entropy= 1.39827, loss=-0.03436
surrogate= 0.00092, entropy= 1.39760, loss= 0.00092
surrogate=-0.00212, entropy= 1.39628, loss=-0.00212
surrogate=-0.01049, entropy= 1.39668, loss=-0.01049
std_min= 0.32555, std_max= 0.44044, std_mean= 0.38845
val lr: [0.00013729508196721313], policy lr: [0.00016475409836065575]
Policy Loss: -0.01049, | Entropy Bonus: -0, | Value Loss: 39.476, | Advantage Loss: 1.0498
Time elapsed (s): 1.6372852325439453
Agent stdevs: 0.388451
--------------------------------------------------------------------------------

Step 440
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2237.7
++++++++ Policy training ++++++++++
Current mean reward: 2078.288708 | mean episode length: 595.500000
val_loss=770.83264
val_loss=71.93436
val_loss=242.44585
val_loss=206.62564
val_loss=75.16142
val_loss=833.69000
val_loss=69.00689
val_loss=33.94372
val_loss=103.41199
val_loss=39.50884
adv_loss= 1.18930
adv_loss= 1.95226
adv_loss= 1.45940
adv_loss=1531.99927
adv_loss= 1.56857
adv_loss= 2.45576
adv_loss= 1.30171
adv_loss= 1.04309
adv_loss= 0.89424
adv_loss= 1.29505
surrogate= 0.01107, entropy= 1.39571, loss= 0.01107
surrogate=-0.02207, entropy= 1.39732, loss=-0.02207
surrogate= 0.00193, entropy= 1.39810, loss= 0.00193
surrogate=-0.03528, entropy= 1.39911, loss=-0.03528
surrogate=-0.02643, entropy= 1.40164, loss=-0.02643
surrogate=-0.01900, entropy= 1.40391, loss=-0.01900
surrogate=-0.03712, entropy= 1.40473, loss=-0.03712
surrogate=-0.03069, entropy= 1.40613, loss=-0.03069
surrogate=-0.04177, entropy= 1.40640, loss=-0.04177
surrogate=-0.02914, entropy= 1.40783, loss=-0.02914
std_min= 0.32504, std_max= 0.44105, std_mean= 0.39000
val lr: [0.0001370389344262295], policy lr: [0.00016444672131147537]
Policy Loss: -0.029143, | Entropy Bonus: -0, | Value Loss: 39.509, | Advantage Loss: 1.2951
Time elapsed (s): 1.6253695487976074
Agent stdevs: 0.39000085
--------------------------------------------------------------------------------

Step 441
++++++++ Policy training ++++++++++
Current mean reward: 2856.966099 | mean episode length: 803.500000
val_loss=451.53525
val_loss=42.97213
val_loss=27.28632
val_loss=24.90960
val_loss=89.66898
val_loss=25.79999
val_loss=58.92638
val_loss=24.81449
val_loss=21.18225
val_loss=28.21568
adv_loss=17.17745
adv_loss= 1.26587
adv_loss= 2.89292
adv_loss= 5.95950
adv_loss= 2.92115
adv_loss= 7.07150
adv_loss= 6.33397
adv_loss=14.95599
adv_loss= 1.85552
adv_loss= 2.37192
surrogate= 0.01502, entropy= 1.40863, loss= 0.01502
surrogate=-0.00380, entropy= 1.40951, loss=-0.00380
surrogate=-0.02409, entropy= 1.41040, loss=-0.02409
surrogate=-0.03643, entropy= 1.41167, loss=-0.03643
surrogate=-0.01638, entropy= 1.41222, loss=-0.01638
surrogate=-0.02159, entropy= 1.41266, loss=-0.02159
surrogate=-0.02980, entropy= 1.41358, loss=-0.02980
surrogate= 0.00692, entropy= 1.41268, loss= 0.00692
surrogate=-0.00599, entropy= 1.41252, loss=-0.00599
surrogate=-0.02541, entropy= 1.41360, loss=-0.02541
std_min= 0.32882, std_max= 0.43850, std_mean= 0.39041
val lr: [0.00013678278688524593], policy lr: [0.0001641393442622951]
Policy Loss: -0.025411, | Entropy Bonus: -0, | Value Loss: 28.216, | Advantage Loss: 2.3719
Time elapsed (s): 1.6277704238891602
Agent stdevs: 0.39040685
--------------------------------------------------------------------------------

Step 442
++++++++ Policy training ++++++++++
Current mean reward: 1886.675430 | mean episode length: 521.000000
val_loss=59.36254
val_loss=42.00004
val_loss=20.62313
val_loss=29.78836
val_loss=10.96349
val_loss=22.42311
val_loss=23.82973
val_loss=20.64889
val_loss=29.93972
val_loss=14.65617
adv_loss= 3.38018
adv_loss= 2.23400
adv_loss= 1.26063
adv_loss= 6.27375
adv_loss= 2.64021
adv_loss= 4.15663
adv_loss= 5.54289
adv_loss= 5.34961
adv_loss= 4.16373
adv_loss= 1.55539
surrogate= 0.01005, entropy= 1.41329, loss= 0.01005
surrogate= 0.02223, entropy= 1.41282, loss= 0.02223
surrogate=-0.01371, entropy= 1.41248, loss=-0.01371
surrogate=-0.03254, entropy= 1.41229, loss=-0.03254
surrogate= 0.00717, entropy= 1.41225, loss= 0.00717
surrogate=-0.01155, entropy= 1.41241, loss=-0.01155
surrogate=-0.03468, entropy= 1.41136, loss=-0.03468
surrogate=-0.03582, entropy= 1.41157, loss=-0.03582
surrogate=-0.01798, entropy= 1.41056, loss=-0.01798
surrogate=-0.01631, entropy= 1.41083, loss=-0.01631
std_min= 0.32780, std_max= 0.43428, std_mean= 0.39002
val lr: [0.0001365266393442623], policy lr: [0.00016383196721311475]
Policy Loss: -0.016309, | Entropy Bonus: -0, | Value Loss: 14.656, | Advantage Loss: 1.5554
Time elapsed (s): 1.651564359664917
Agent stdevs: 0.39001784
--------------------------------------------------------------------------------

Step 443
++++++++ Policy training ++++++++++
Current mean reward: 1055.156674 | mean episode length: 297.000000
val_loss=40.98206
val_loss=53.69477
val_loss=29.81541
val_loss=30.68240
val_loss=48.19886
val_loss=42.06779
val_loss=33.59569
val_loss=47.06364
val_loss=21.20745
val_loss=30.95586
adv_loss= 5.51955
adv_loss= 2.01236
adv_loss= 2.83557
adv_loss= 5.87499
adv_loss= 5.96058
adv_loss= 3.17346
adv_loss= 2.31851
adv_loss=11.84809
adv_loss= 9.99542
adv_loss=15.25657
surrogate=-0.01224, entropy= 1.41001, loss=-0.01224
surrogate= 0.02201, entropy= 1.40942, loss= 0.02201
surrogate= 0.01147, entropy= 1.40912, loss= 0.01147
surrogate=-0.00377, entropy= 1.40862, loss=-0.00377
surrogate= 0.01319, entropy= 1.40779, loss= 0.01319
surrogate= 0.00080, entropy= 1.40591, loss= 0.00080
surrogate= 0.01496, entropy= 1.40544, loss= 0.01496
surrogate=-0.03444, entropy= 1.40432, loss=-0.03444
surrogate=-0.02082, entropy= 1.40357, loss=-0.02082
surrogate=-0.02579, entropy= 1.40353, loss=-0.02579
std_min= 0.33086, std_max= 0.42929, std_mean= 0.38870
val lr: [0.00013627049180327868], policy lr: [0.00016352459016393442]
Policy Loss: -0.025786, | Entropy Bonus: -0, | Value Loss: 30.956, | Advantage Loss: 15.257
Time elapsed (s): 1.679797649383545
Agent stdevs: 0.38870165
--------------------------------------------------------------------------------

Step 444
++++++++ Policy training ++++++++++
Current mean reward: 1002.288099 | mean episode length: 283.571429
val_loss=52.98388
val_loss=27.66947
val_loss=25.72801
val_loss=34.28722
val_loss=31.58875
val_loss=29.36160
val_loss=17.23988
val_loss=26.45255
val_loss=19.32885
val_loss=14.50437
adv_loss= 5.28423
adv_loss= 4.01169
adv_loss= 6.18163
adv_loss=18.24672
adv_loss= 7.32197
adv_loss=15.12239
adv_loss=10.95971
adv_loss= 6.58656
adv_loss= 9.11733
adv_loss= 7.71762
surrogate= 0.01999, entropy= 1.40165, loss= 0.01999
surrogate=-0.03507, entropy= 1.40022, loss=-0.03507
surrogate=-0.02268, entropy= 1.39791, loss=-0.02268
surrogate=-0.02055, entropy= 1.39572, loss=-0.02055
surrogate=-0.03256, entropy= 1.39230, loss=-0.03256
surrogate=-0.04537, entropy= 1.39066, loss=-0.04537
surrogate=-0.04382, entropy= 1.39014, loss=-0.04382
surrogate=-0.03836, entropy= 1.38705, loss=-0.03836
surrogate=-0.01552, entropy= 1.38414, loss=-0.01552
surrogate=-0.03044, entropy= 1.38283, loss=-0.03044
std_min= 0.32698, std_max= 0.42754, std_mean= 0.38614
val lr: [0.00013601434426229508], policy lr: [0.0001632172131147541]
Policy Loss: -0.030437, | Entropy Bonus: -0, | Value Loss: 14.504, | Advantage Loss: 7.7176
Time elapsed (s): 1.650752067565918
Agent stdevs: 0.3861432
--------------------------------------------------------------------------------

Step 445
++++++++ Policy training ++++++++++
Current mean reward: 1188.443505 | mean episode length: 332.500000
val_loss=25.02039
val_loss=30.26477
val_loss=26.58993
val_loss=26.89141
val_loss=33.34926
val_loss=21.62306
val_loss=24.69101
val_loss=24.84834
val_loss=30.01006
val_loss=43.28117
adv_loss= 1.32692
adv_loss= 4.53582
adv_loss= 3.42679
adv_loss= 1.76504
adv_loss= 2.29401
adv_loss= 2.41145
adv_loss= 2.98766
adv_loss= 1.80331
adv_loss= 2.22391
adv_loss= 5.28777
surrogate=-0.02337, entropy= 1.38059, loss=-0.02337
surrogate=-0.00565, entropy= 1.37957, loss=-0.00565
surrogate= 0.00376, entropy= 1.37712, loss= 0.00376
surrogate=-0.03030, entropy= 1.37501, loss=-0.03030
surrogate= 0.00125, entropy= 1.37314, loss= 0.00125
surrogate=-0.01651, entropy= 1.37057, loss=-0.01651
surrogate=-0.02057, entropy= 1.36806, loss=-0.02057
surrogate=-0.01915, entropy= 1.36549, loss=-0.01915
surrogate=-0.02476, entropy= 1.36444, loss=-0.02476
surrogate=-0.03003, entropy= 1.36319, loss=-0.03003
std_min= 0.32346, std_max= 0.42467, std_mean= 0.38374
val lr: [0.00013575819672131148], policy lr: [0.00016290983606557377]
Policy Loss: -0.030027, | Entropy Bonus: -0, | Value Loss: 43.281, | Advantage Loss: 5.2878
Time elapsed (s): 1.6765987873077393
Agent stdevs: 0.38374224
--------------------------------------------------------------------------------

Step 446
++++++++ Policy training ++++++++++
Current mean reward: 2666.746903 | mean episode length: 740.000000
val_loss=29.31513
val_loss=19.18673
val_loss=11.28160
val_loss=14.41395
val_loss=33.84798
val_loss=26.83971
val_loss=13.78922
val_loss=15.91768
val_loss=17.60383
val_loss=16.52611
adv_loss= 1.19737
adv_loss= 1.39600
adv_loss= 2.36319
adv_loss= 1.73580
adv_loss= 2.88767
adv_loss= 2.26137
adv_loss= 3.50012
adv_loss= 2.30649
adv_loss= 3.06889
adv_loss= 2.17308
surrogate= 0.02401, entropy= 1.36253, loss= 0.02401
surrogate= 0.01725, entropy= 1.36141, loss= 0.01725
surrogate=-0.02147, entropy= 1.36167, loss=-0.02147
surrogate= 0.02197, entropy= 1.36166, loss= 0.02197
surrogate=-0.03450, entropy= 1.36044, loss=-0.03450
surrogate=-0.02238, entropy= 1.36053, loss=-0.02238
surrogate=-0.00560, entropy= 1.36084, loss=-0.00560
surrogate=-0.04222, entropy= 1.35920, loss=-0.04222
surrogate=-0.02102, entropy= 1.35918, loss=-0.02102
surrogate=-0.01328, entropy= 1.35889, loss=-0.01328
std_min= 0.31960, std_max= 0.42285, std_mean= 0.38347
val lr: [0.00013550204918032788], policy lr: [0.00016260245901639344]
Policy Loss: -0.013275, | Entropy Bonus: -0, | Value Loss: 16.526, | Advantage Loss: 2.1731
Time elapsed (s): 1.628338098526001
Agent stdevs: 0.3834733
--------------------------------------------------------------------------------

Step 447
++++++++ Policy training ++++++++++
Current mean reward: 1586.741341 | mean episode length: 440.250000
val_loss=14.50662
val_loss= 8.61370
val_loss=16.27508
val_loss=14.92076
val_loss= 6.85481
val_loss= 5.07800
val_loss= 9.78959
val_loss= 7.78537
val_loss= 9.29881
val_loss= 6.94654
adv_loss= 3.08534
adv_loss= 1.03138
adv_loss= 2.09282
adv_loss= 1.82414
adv_loss= 0.67082
adv_loss= 3.67833
adv_loss= 2.92982
adv_loss= 1.80559
adv_loss= 2.91095
adv_loss= 3.72580
surrogate= 0.01692, entropy= 1.35648, loss= 0.01692
surrogate= 0.00390, entropy= 1.35591, loss= 0.00390
surrogate=-0.03381, entropy= 1.35478, loss=-0.03381
surrogate=-0.04331, entropy= 1.35460, loss=-0.04331
surrogate=-0.01393, entropy= 1.35373, loss=-0.01393
surrogate=-0.02054, entropy= 1.35367, loss=-0.02054
surrogate=-0.02870, entropy= 1.35127, loss=-0.02870
surrogate=-0.00605, entropy= 1.34983, loss=-0.00605
surrogate=-0.02045, entropy= 1.34848, loss=-0.02045
surrogate=-0.02723, entropy= 1.34823, loss=-0.02723
std_min= 0.31840, std_max= 0.42278, std_mean= 0.38214
val lr: [0.00013524590163934425], policy lr: [0.0001622950819672131]
Policy Loss: -0.027228, | Entropy Bonus: -0, | Value Loss: 6.9465, | Advantage Loss: 3.7258
Time elapsed (s): 1.6298613548278809
Agent stdevs: 0.3821362
--------------------------------------------------------------------------------

Step 448
++++++++ Policy training ++++++++++
Current mean reward: 1724.257702 | mean episode length: 473.000000
val_loss=18.90080
val_loss= 9.66593
val_loss= 9.75924
val_loss= 8.32678
val_loss= 7.79397
val_loss= 5.34472
val_loss= 6.00062
val_loss= 6.81311
val_loss= 7.50573
val_loss= 7.29894
adv_loss= 1.95531
adv_loss= 1.60380
adv_loss= 3.61424
adv_loss= 1.12962
adv_loss= 1.54922
adv_loss= 1.16857
adv_loss= 1.47321
adv_loss= 1.36814
adv_loss= 3.20585
adv_loss= 1.62068
surrogate=-0.00263, entropy= 1.34476, loss=-0.00263
surrogate=-0.01909, entropy= 1.34179, loss=-0.01909
surrogate= 0.02192, entropy= 1.33814, loss= 0.02192
surrogate=-0.00836, entropy= 1.33539, loss=-0.00836
surrogate=-0.01371, entropy= 1.33234, loss=-0.01371
surrogate=-0.00492, entropy= 1.32869, loss=-0.00492
surrogate=-0.03385, entropy= 1.32547, loss=-0.03385
surrogate=-0.03475, entropy= 1.32321, loss=-0.03475
surrogate=-0.02113, entropy= 1.32095, loss=-0.02113
surrogate=-0.04010, entropy= 1.31766, loss=-0.04010
std_min= 0.31584, std_max= 0.41882, std_mean= 0.37822
val lr: [0.00013498975409836068], policy lr: [0.0001619877049180328]
Policy Loss: -0.040104, | Entropy Bonus: -0, | Value Loss: 7.2989, | Advantage Loss: 1.6207
Time elapsed (s): 1.642709732055664
Agent stdevs: 0.37821564
--------------------------------------------------------------------------------

Step 449
++++++++ Policy training ++++++++++
Current mean reward: 2671.411542 | mean episode length: 764.500000
val_loss=135.01811
val_loss=1129.93665
val_loss=24.48609
val_loss=36.17006
val_loss=41.70309
val_loss=1593.61890
val_loss=44.04619
val_loss=45.14706
val_loss=31.39075
val_loss=982.24518
adv_loss= 2.67489
adv_loss= 1.68349
adv_loss= 2.70780
adv_loss= 1.74550
adv_loss= 1.18841
adv_loss= 1.24272
adv_loss= 3.41916
adv_loss= 1.87628
adv_loss= 1.79288
adv_loss= 1.10685
surrogate= 0.01690, entropy= 1.31768, loss= 0.01690
surrogate= 0.01718, entropy= 1.31852, loss= 0.01718
surrogate= 0.00142, entropy= 1.31914, loss= 0.00142
surrogate= 0.01312, entropy= 1.31978, loss= 0.01312
surrogate=-0.00887, entropy= 1.31942, loss=-0.00887
surrogate= 0.00062, entropy= 1.31870, loss= 0.00062
surrogate= 0.03109, entropy= 1.31759, loss= 0.03109
surrogate=-0.00951, entropy= 1.31887, loss=-0.00951
surrogate=-0.01491, entropy= 1.31855, loss=-0.01491
surrogate=-0.01374, entropy= 1.31888, loss=-0.01374
std_min= 0.31800, std_max= 0.41786, std_mean= 0.37818
val lr: [0.00013473360655737705], policy lr: [0.00016168032786885244]
Policy Loss: -0.013744, | Entropy Bonus: -0, | Value Loss: 982.25, | Advantage Loss: 1.1069
Time elapsed (s): 1.6763219833374023
Agent stdevs: 0.3781794
--------------------------------------------------------------------------------

Step 450
++++++++ Policy training ++++++++++
Current mean reward: 1630.327130 | mean episode length: 453.500000
val_loss=26.11510
val_loss=22.12071
val_loss= 6.78273
val_loss=22.94374
val_loss=21.23035
val_loss= 8.86719
val_loss= 7.62475
val_loss=14.58411
val_loss=19.67369
val_loss=10.22232
adv_loss= 1.50733
adv_loss= 0.91538
adv_loss= 1.01929
adv_loss= 1.69507
adv_loss= 0.78991
adv_loss= 2.13622
adv_loss= 1.95059
adv_loss= 1.73616
adv_loss= 4.94054
adv_loss= 0.92418
surrogate=-0.00075, entropy= 1.31857, loss=-0.00075
surrogate=-0.02376, entropy= 1.31743, loss=-0.02376
surrogate=-0.02105, entropy= 1.31409, loss=-0.02105
surrogate=-0.01592, entropy= 1.31442, loss=-0.01592
surrogate=-0.01249, entropy= 1.31351, loss=-0.01249
surrogate=-0.01121, entropy= 1.31227, loss=-0.01121
surrogate=-0.01478, entropy= 1.31159, loss=-0.01478
surrogate=-0.04213, entropy= 1.31022, loss=-0.04213
surrogate=-0.00380, entropy= 1.30958, loss=-0.00380
surrogate=-0.01618, entropy= 1.30713, loss=-0.01618
std_min= 0.31567, std_max= 0.41744, std_mean= 0.37680
val lr: [0.00013447745901639345], policy lr: [0.00016137295081967211]
Policy Loss: -0.016175, | Entropy Bonus: -0, | Value Loss: 10.222, | Advantage Loss: 0.92418
Time elapsed (s): 1.6501543521881104
Agent stdevs: 0.37680474
--------------------------------------------------------------------------------

Step 451
++++++++ Policy training ++++++++++
Current mean reward: 1498.347197 | mean episode length: 419.000000
val_loss=28.37756
val_loss=12.21467
val_loss= 8.38246
val_loss= 6.28725
val_loss= 7.64984
val_loss=20.07807
val_loss=14.07576
val_loss= 9.53236
val_loss= 8.68516
val_loss=10.74897
adv_loss= 4.33415
adv_loss= 2.03380
adv_loss= 1.25519
adv_loss= 1.52561
adv_loss= 2.69216
adv_loss= 0.93082
adv_loss= 0.98364
adv_loss= 2.05463
adv_loss= 1.10969
adv_loss= 2.14720
surrogate= 0.00398, entropy= 1.30586, loss= 0.00398
surrogate=-0.00085, entropy= 1.30556, loss=-0.00085
surrogate=-0.00912, entropy= 1.30508, loss=-0.00912
surrogate=-0.00271, entropy= 1.30444, loss=-0.00271
surrogate=-0.01868, entropy= 1.30388, loss=-0.01868
surrogate= 0.00036, entropy= 1.30313, loss= 0.00036
surrogate=-0.01667, entropy= 1.30280, loss=-0.01667
surrogate=-0.02138, entropy= 1.30192, loss=-0.02138
surrogate=-0.01264, entropy= 1.30211, loss=-0.01264
surrogate=-0.01813, entropy= 1.30019, loss=-0.01813
std_min= 0.31338, std_max= 0.41846, std_mean= 0.37611
val lr: [0.00013422131147540982], policy lr: [0.0001610655737704918]
Policy Loss: -0.018127, | Entropy Bonus: -0, | Value Loss: 10.749, | Advantage Loss: 2.1472
Time elapsed (s): 1.6629011631011963
Agent stdevs: 0.37611076
--------------------------------------------------------------------------------

Step 452
++++++++ Policy training ++++++++++
Current mean reward: 1474.345471 | mean episode length: 401.666667
val_loss= 9.51944
val_loss=19.09278
val_loss=15.21244
val_loss=14.13668
val_loss=24.27763
val_loss= 5.42681
val_loss=11.15001
val_loss=10.37097
val_loss=11.08424
val_loss= 8.56808
adv_loss= 0.94479
adv_loss= 0.89085
adv_loss= 1.79127
adv_loss= 1.87654
adv_loss= 1.06128
adv_loss= 2.09638
adv_loss= 3.50885
adv_loss= 0.57945
adv_loss= 1.21971
adv_loss= 5.64658
surrogate= 0.02561, entropy= 1.30124, loss= 0.02561
surrogate=-0.00047, entropy= 1.30049, loss=-0.00047
surrogate=-0.01470, entropy= 1.30118, loss=-0.01470
surrogate=-0.01490, entropy= 1.30015, loss=-0.01490
surrogate=-0.02256, entropy= 1.30100, loss=-0.02256
surrogate= 0.02791, entropy= 1.30015, loss= 0.02791
surrogate=-0.03625, entropy= 1.29916, loss=-0.03625
surrogate=-0.01631, entropy= 1.30014, loss=-0.01631
surrogate=-0.00969, entropy= 1.29981, loss=-0.00969
surrogate=-0.02816, entropy= 1.29835, loss=-0.02816
std_min= 0.31429, std_max= 0.42097, std_mean= 0.37584
val lr: [0.00013396516393442622], policy lr: [0.00016075819672131146]
Policy Loss: -0.028161, | Entropy Bonus: -0, | Value Loss: 8.5681, | Advantage Loss: 5.6466
Time elapsed (s): 1.6492180824279785
Agent stdevs: 0.375841
--------------------------------------------------------------------------------

Step 453
++++++++ Policy training ++++++++++
Current mean reward: 2083.458138 | mean episode length: 572.666667
val_loss= 6.80381
val_loss=10.83159
val_loss= 7.22888
val_loss=12.03387
val_loss= 5.26416
val_loss= 6.63518
val_loss= 5.72638
val_loss= 5.20164
val_loss= 3.89875
val_loss= 3.44709
adv_loss= 1.17056
adv_loss= 2.21102
adv_loss= 1.06072
adv_loss= 1.13127
adv_loss= 0.83296
adv_loss= 0.81027
adv_loss= 1.95816
adv_loss= 1.47207
adv_loss= 1.21226
adv_loss= 0.56974
surrogate=-0.02719, entropy= 1.29938, loss=-0.02719
surrogate= 0.00006, entropy= 1.29888, loss= 0.00006
surrogate=-0.03438, entropy= 1.30010, loss=-0.03438
surrogate=-0.04084, entropy= 1.30115, loss=-0.04084
surrogate=-0.01673, entropy= 1.30252, loss=-0.01673
surrogate=-0.00086, entropy= 1.30374, loss=-0.00086
surrogate=-0.03491, entropy= 1.30287, loss=-0.03491
surrogate=-0.03040, entropy= 1.30247, loss=-0.03040
surrogate=-0.02475, entropy= 1.30339, loss=-0.02475
surrogate=-0.05623, entropy= 1.30181, loss=-0.05623
std_min= 0.31439, std_max= 0.42191, std_mean= 0.37631
val lr: [0.00013370901639344262], policy lr: [0.00016045081967213114]
Policy Loss: -0.056231, | Entropy Bonus: -0, | Value Loss: 3.4471, | Advantage Loss: 0.56974
Time elapsed (s): 1.6421515941619873
Agent stdevs: 0.37631235
--------------------------------------------------------------------------------

Step 454
++++++++ Policy training ++++++++++
Current mean reward: 3436.089366 | mean episode length: 1000.000000
val_loss=645.88171
val_loss=1019.06512
val_loss=950.61700
val_loss=1248.39490
val_loss=132.06010
val_loss=623.27087
val_loss=1110.60901
val_loss=376.27371
val_loss=1946.08618
val_loss=220.04886
adv_loss= 5.43946
adv_loss= 4.82588
adv_loss= 5.33298
adv_loss= 7.30336
adv_loss= 5.32790
adv_loss= 4.14882
adv_loss= 6.19205
adv_loss= 2.92406
adv_loss= 4.26336
adv_loss= 4.67638
surrogate= 0.06937, entropy= 1.30315, loss= 0.06937
surrogate= 0.00860, entropy= 1.30449, loss= 0.00860
surrogate=-0.01318, entropy= 1.30582, loss=-0.01318
surrogate=-0.00918, entropy= 1.30877, loss=-0.00918
surrogate=-0.01818, entropy= 1.30949, loss=-0.01818
surrogate= 0.03126, entropy= 1.31116, loss= 0.03126
surrogate=-0.01686, entropy= 1.31156, loss=-0.01686
surrogate= 0.01070, entropy= 1.31310, loss= 0.01070
surrogate=-0.02685, entropy= 1.31426, loss=-0.02685
surrogate=-0.02942, entropy= 1.31694, loss=-0.02942
std_min= 0.31660, std_max= 0.42254, std_mean= 0.37812
val lr: [0.00013345286885245902], policy lr: [0.0001601434426229508]
Policy Loss: -0.029419, | Entropy Bonus: -0, | Value Loss: 220.05, | Advantage Loss: 4.6764
Time elapsed (s): 1.6823859214782715
Agent stdevs: 0.37812343
--------------------------------------------------------------------------------

Step 455
++++++++ Policy training ++++++++++
Current mean reward: 1701.344675 | mean episode length: 492.500000
val_loss=61.90398
val_loss=111.47278
val_loss=112.42694
val_loss=85.64685
val_loss=126.71335
val_loss=967.62030
val_loss=70.40202
val_loss=323.56381
val_loss=53.76197
val_loss=344.92215
adv_loss= 3.97124
adv_loss= 6.17948
adv_loss=10.20901
adv_loss=10.01079
adv_loss= 4.96840
adv_loss= 4.34047
adv_loss= 8.24223
adv_loss= 3.29661
adv_loss= 4.62476
adv_loss= 2.95786
surrogate= 0.00751, entropy= 1.31865, loss= 0.00751
surrogate=-0.02265, entropy= 1.31789, loss=-0.02265
surrogate= 0.00074, entropy= 1.31668, loss= 0.00074
surrogate=-0.00101, entropy= 1.31519, loss=-0.00101
surrogate= 0.01997, entropy= 1.31411, loss= 0.01997
surrogate=-0.00080, entropy= 1.31351, loss=-0.00080
surrogate=-0.02440, entropy= 1.31315, loss=-0.02440
surrogate= 0.03922, entropy= 1.31058, loss= 0.03922
surrogate= 0.00921, entropy= 1.31183, loss= 0.00921
surrogate=-0.01949, entropy= 1.31104, loss=-0.01949
std_min= 0.31866, std_max= 0.42230, std_mean= 0.37718
val lr: [0.00013319672131147542], policy lr: [0.00015983606557377049]
Policy Loss: -0.019486, | Entropy Bonus: -0, | Value Loss: 344.92, | Advantage Loss: 2.9579
Time elapsed (s): 1.7038071155548096
Agent stdevs: 0.37717533
--------------------------------------------------------------------------------

Step 456
++++++++ Policy training ++++++++++
Current mean reward: 2115.377085 | mean episode length: 588.666667
val_loss=28.29407
val_loss=462.15112
val_loss=450.85583
val_loss=19.24258
val_loss=429.13873
val_loss=107.52042
val_loss=538.45575
val_loss=47.38499
val_loss=64.62960
val_loss=291.42242
adv_loss= 2.23527
adv_loss= 3.57368
adv_loss= 1.58204
adv_loss= 3.73323
adv_loss= 1.26292
adv_loss= 2.70144
adv_loss= 3.16154
adv_loss= 3.14346
adv_loss= 3.35365
adv_loss= 2.94203
surrogate=-0.02657, entropy= 1.30957, loss=-0.02657
surrogate=-0.00454, entropy= 1.30997, loss=-0.00454
surrogate=-0.00812, entropy= 1.30908, loss=-0.00812
surrogate=-0.01353, entropy= 1.30660, loss=-0.01353
surrogate=-0.01152, entropy= 1.30555, loss=-0.01152
surrogate=-0.02462, entropy= 1.30470, loss=-0.02462
surrogate=-0.01597, entropy= 1.30399, loss=-0.01597
surrogate=-0.01737, entropy= 1.30233, loss=-0.01737
surrogate= 0.04279, entropy= 1.30244, loss= 0.04279
surrogate=-0.01161, entropy= 1.30246, loss=-0.01161
std_min= 0.31473, std_max= 0.42263, std_mean= 0.37638
val lr: [0.0001329405737704918], policy lr: [0.00015952868852459013]
Policy Loss: -0.01161, | Entropy Bonus: -0, | Value Loss: 291.42, | Advantage Loss: 2.942
Time elapsed (s): 1.6750011444091797
Agent stdevs: 0.37638226
--------------------------------------------------------------------------------

Step 457
++++++++ Policy training ++++++++++
Current mean reward: 3372.722903 | mean episode length: 945.500000
val_loss=497.59802
val_loss=69.22093
val_loss=203.29182
val_loss=672.03430
val_loss=81.01343
val_loss=57.96479
val_loss=96.66805
val_loss=214.69476
val_loss=54.59924
val_loss=334.01877
adv_loss= 1.76403
adv_loss=1558.09253
adv_loss= 1.93805
adv_loss= 4.58175
adv_loss= 2.04965
adv_loss= 2.20767
adv_loss= 1.81407
adv_loss= 2.86803
adv_loss= 2.40690
adv_loss=1552.70398
surrogate= 0.00396, entropy= 1.30011, loss= 0.00396
surrogate=-0.00180, entropy= 1.29556, loss=-0.00180
surrogate= 0.01284, entropy= 1.29327, loss= 0.01284
surrogate=-0.02385, entropy= 1.29270, loss=-0.02385
surrogate=-0.02014, entropy= 1.29168, loss=-0.02014
surrogate=-0.02838, entropy= 1.28874, loss=-0.02838
surrogate=-0.03390, entropy= 1.28693, loss=-0.03390
surrogate=-0.02884, entropy= 1.28550, loss=-0.02884
surrogate=-0.05352, entropy= 1.28421, loss=-0.05352
surrogate=-0.02865, entropy= 1.28271, loss=-0.02865
std_min= 0.31069, std_max= 0.42249, std_mean= 0.37413
val lr: [0.00013268442622950822], policy lr: [0.00015922131147540984]
Policy Loss: -0.028649, | Entropy Bonus: -0, | Value Loss: 334.02, | Advantage Loss: 1552.7
Time elapsed (s): 1.6528980731964111
Agent stdevs: 0.37412903
--------------------------------------------------------------------------------

Step 458
++++++++ Policy training ++++++++++
Current mean reward: 1683.743969 | mean episode length: 461.250000
val_loss=22.83546
val_loss=18.77772
val_loss=31.43779
val_loss=13.71393
val_loss=13.71371
val_loss=17.15079
val_loss=12.33846
val_loss=19.43502
val_loss=15.86218
val_loss=13.93909
adv_loss= 2.36914
adv_loss= 2.83376
adv_loss= 3.67347
adv_loss= 5.80484
adv_loss= 7.27524
adv_loss= 1.64643
adv_loss= 3.45714
adv_loss= 2.38144
adv_loss= 2.30256
adv_loss= 3.28981
surrogate= 0.02769, entropy= 1.28121, loss= 0.02769
surrogate= 0.01187, entropy= 1.28051, loss= 0.01187
surrogate=-0.00108, entropy= 1.28232, loss=-0.00108
surrogate=-0.00808, entropy= 1.28157, loss=-0.00808
surrogate=-0.02599, entropy= 1.28069, loss=-0.02599
surrogate=-0.02082, entropy= 1.28115, loss=-0.02082
surrogate=-0.04651, entropy= 1.27938, loss=-0.04651
surrogate= 0.00699, entropy= 1.27870, loss= 0.00699
surrogate=-0.01858, entropy= 1.27940, loss=-0.01858
surrogate=-0.01487, entropy= 1.27810, loss=-0.01487
std_min= 0.30776, std_max= 0.42391, std_mean= 0.37384
val lr: [0.0001324282786885246], policy lr: [0.00015891393442622948]
Policy Loss: -0.014865, | Entropy Bonus: -0, | Value Loss: 13.939, | Advantage Loss: 3.2898
Time elapsed (s): 1.6502392292022705
Agent stdevs: 0.3738356
--------------------------------------------------------------------------------

Step 459
++++++++ Policy training ++++++++++
Current mean reward: 3469.626370 | mean episode length: 1000.000000
val_loss=1499.57104
val_loss=2034.32642
val_loss=697.97095
val_loss=760.13104
val_loss=32.35073
val_loss=1596.68469
val_loss=222.93466
val_loss=881.27246
val_loss=237.79189
val_loss=1042.35889
adv_loss= 2.29534
adv_loss= 7.96379
adv_loss= 4.60644
adv_loss= 8.83746
adv_loss= 6.85354
adv_loss= 9.04174
adv_loss= 3.11970
adv_loss= 4.96377
adv_loss= 5.33564
adv_loss= 2.18020
surrogate= 0.01013, entropy= 1.27886, loss= 0.01013
surrogate=-0.03307, entropy= 1.27835, loss=-0.03307
surrogate=-0.02003, entropy= 1.27876, loss=-0.02003
surrogate=-0.00118, entropy= 1.27888, loss=-0.00118
surrogate=-0.02105, entropy= 1.27992, loss=-0.02105
surrogate=-0.01337, entropy= 1.27987, loss=-0.01337
surrogate=-0.02332, entropy= 1.27865, loss=-0.02332
surrogate= 0.00090, entropy= 1.27786, loss= 0.00090
surrogate=-0.02596, entropy= 1.27779, loss=-0.02596
surrogate=-0.03322, entropy= 1.27811, loss=-0.03322
std_min= 0.30896, std_max= 0.42216, std_mean= 0.37368
val lr: [0.00013217213114754097], policy lr: [0.00015860655737704916]
Policy Loss: -0.033222, | Entropy Bonus: -0, | Value Loss: 1042.4, | Advantage Loss: 2.1802
Time elapsed (s): 1.6560933589935303
Agent stdevs: 0.37368235
--------------------------------------------------------------------------------

Step 460
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2468.6
++++++++ Policy training ++++++++++
Current mean reward: 2214.243278 | mean episode length: 634.000000
val_loss=185.05710
val_loss=1685.84656
val_loss=145.86932
val_loss=57.27509
val_loss=50.99634
val_loss=101.19076
val_loss=468.54044
val_loss=362.16766
val_loss=632.28882
val_loss=87.29662
adv_loss= 3.62972
adv_loss=12.72198
adv_loss= 3.30423
adv_loss= 3.48124
adv_loss= 5.51266
adv_loss= 8.19741
adv_loss= 3.25598
adv_loss= 5.80934
adv_loss= 7.99595
adv_loss= 9.61101
surrogate= 0.01446, entropy= 1.27777, loss= 0.01446
surrogate=-0.00113, entropy= 1.27671, loss=-0.00113
surrogate=-0.02489, entropy= 1.27634, loss=-0.02489
surrogate= 0.00168, entropy= 1.27480, loss= 0.00168
surrogate=-0.03703, entropy= 1.27421, loss=-0.03703
surrogate=-0.02199, entropy= 1.27303, loss=-0.02199
surrogate=-0.01486, entropy= 1.27243, loss=-0.01486
surrogate=-0.00644, entropy= 1.27110, loss=-0.00644
surrogate=-0.02390, entropy= 1.26886, loss=-0.02390
surrogate=-0.02523, entropy= 1.26839, loss=-0.02523
std_min= 0.30845, std_max= 0.41965, std_mean= 0.37241
val lr: [0.00013191598360655737], policy lr: [0.00015829918032786883]
Policy Loss: -0.02523, | Entropy Bonus: -0, | Value Loss: 87.297, | Advantage Loss: 9.611
Time elapsed (s): 1.6692712306976318
Agent stdevs: 0.3724073
--------------------------------------------------------------------------------

Step 461
++++++++ Policy training ++++++++++
Current mean reward: 3448.563057 | mean episode length: 1000.000000
val_loss=383.71060
val_loss=103.77634
val_loss=565.17371
val_loss=675.93054
val_loss=50.04239
val_loss=58.35085
val_loss=598.13025
val_loss=132.53958
val_loss=161.03563
val_loss=65.96519
adv_loss= 7.90069
adv_loss= 7.80208
adv_loss= 2.15369
adv_loss= 2.63839
adv_loss=21.30641
adv_loss= 3.10024
adv_loss= 2.88394
adv_loss= 8.60410
adv_loss=14.07553
adv_loss= 5.43219
surrogate= 0.00465, entropy= 1.26961, loss= 0.00465
surrogate=-0.01598, entropy= 1.27063, loss=-0.01598
surrogate=-0.00140, entropy= 1.27202, loss=-0.00140
surrogate= 0.01737, entropy= 1.27250, loss= 0.01737
surrogate=-0.02083, entropy= 1.27197, loss=-0.02083
surrogate= 0.01032, entropy= 1.27302, loss= 0.01032
surrogate=-0.02171, entropy= 1.27320, loss=-0.02171
surrogate= 0.02800, entropy= 1.27244, loss= 0.02800
surrogate=-0.00141, entropy= 1.27343, loss=-0.00141
surrogate=-0.03056, entropy= 1.27389, loss=-0.03056
std_min= 0.30970, std_max= 0.42331, std_mean= 0.37311
val lr: [0.00013165983606557377], policy lr: [0.0001579918032786885]
Policy Loss: -0.030562, | Entropy Bonus: -0, | Value Loss: 65.965, | Advantage Loss: 5.4322
Time elapsed (s): 1.718141794204712
Agent stdevs: 0.373111
--------------------------------------------------------------------------------

Step 462
++++++++ Policy training ++++++++++
Current mean reward: 3457.347067 | mean episode length: 943.500000
val_loss=60.09581
val_loss=38.59139
val_loss=23.01091
val_loss=40.28446
val_loss=20.18810
val_loss=30.21159
val_loss=29.54868
val_loss=34.70790
val_loss=20.26593
val_loss=25.74226
adv_loss= 9.55428
adv_loss=10.77902
adv_loss= 2.08023
adv_loss=10.32127
adv_loss= 2.72511
adv_loss= 6.89340
adv_loss= 2.93728
adv_loss= 2.72302
adv_loss= 6.75968
adv_loss= 4.21099
surrogate= 0.01930, entropy= 1.27609, loss= 0.01930
surrogate= 0.01564, entropy= 1.27524, loss= 0.01564
surrogate=-0.00842, entropy= 1.27415, loss=-0.00842
surrogate=-0.02653, entropy= 1.27389, loss=-0.02653
surrogate= 0.00185, entropy= 1.27323, loss= 0.00185
surrogate=-0.00776, entropy= 1.27140, loss=-0.00776
surrogate= 0.01589, entropy= 1.27106, loss= 0.01589
surrogate=-0.03055, entropy= 1.27086, loss=-0.03055
surrogate=-0.02901, entropy= 1.27041, loss=-0.02901
surrogate=-0.01987, entropy= 1.27118, loss=-0.01987
std_min= 0.31281, std_max= 0.41897, std_mean= 0.37239
val lr: [0.00013140368852459017], policy lr: [0.00015768442622950818]
Policy Loss: -0.01987, | Entropy Bonus: -0, | Value Loss: 25.742, | Advantage Loss: 4.211
Time elapsed (s): 1.6789662837982178
Agent stdevs: 0.37239087
--------------------------------------------------------------------------------

Step 463
++++++++ Policy training ++++++++++
Current mean reward: 2378.481153 | mean episode length: 678.500000
val_loss=117.69969
val_loss=61.80045
val_loss=74.00123
val_loss=142.47723
val_loss=322.10251
val_loss=77.01603
val_loss=724.70734
val_loss=516.16632
val_loss=210.11705
val_loss=1289.08521
adv_loss= 7.59674
adv_loss=11.05091
adv_loss= 3.01413
adv_loss= 4.33143
adv_loss= 3.23568
adv_loss= 3.78011
adv_loss= 1.84384
adv_loss= 3.39831
adv_loss= 4.30735
adv_loss= 9.14839
surrogate= 0.02345, entropy= 1.27058, loss= 0.02345
surrogate= 0.00594, entropy= 1.27018, loss= 0.00594
surrogate=-0.01897, entropy= 1.27239, loss=-0.01897
surrogate=-0.02904, entropy= 1.27209, loss=-0.02904
surrogate=-0.00556, entropy= 1.27160, loss=-0.00556
surrogate=-0.02430, entropy= 1.27312, loss=-0.02430
surrogate=-0.02091, entropy= 1.27305, loss=-0.02091
surrogate=-0.03577, entropy= 1.27407, loss=-0.03577
surrogate=-0.02877, entropy= 1.27386, loss=-0.02877
surrogate=-0.00852, entropy= 1.27265, loss=-0.00852
std_min= 0.31261, std_max= 0.41785, std_mean= 0.37256
val lr: [0.00013114754098360657], policy lr: [0.00015737704918032785]
Policy Loss: -0.0085232, | Entropy Bonus: -0, | Value Loss: 1289.1, | Advantage Loss: 9.1484
Time elapsed (s): 1.6776525974273682
Agent stdevs: 0.37256387
--------------------------------------------------------------------------------

Step 464
++++++++ Policy training ++++++++++
Current mean reward: 2293.749898 | mean episode length: 631.666667
val_loss=44.75045
val_loss=33.08986
val_loss=83.58996
val_loss=20.34008
val_loss=31.78457
val_loss=22.38619
val_loss=30.47149
val_loss=17.90013
val_loss=12.61776
val_loss=24.82858
adv_loss= 2.75227
adv_loss= 2.63289
adv_loss= 1.53777
adv_loss= 1.53423
adv_loss= 4.07605
adv_loss= 3.10542
adv_loss= 1.86380
adv_loss=21.46573
adv_loss= 2.34217
adv_loss= 2.98444
surrogate= 0.00043, entropy= 1.27004, loss= 0.00043
surrogate=-0.00416, entropy= 1.26637, loss=-0.00416
surrogate=-0.02853, entropy= 1.26366, loss=-0.02853
surrogate=-0.02605, entropy= 1.26048, loss=-0.02605
surrogate=-0.01129, entropy= 1.25772, loss=-0.01129
surrogate=-0.01447, entropy= 1.25429, loss=-0.01447
surrogate=-0.04989, entropy= 1.25076, loss=-0.04989
surrogate=-0.03909, entropy= 1.24911, loss=-0.03909
surrogate=-0.02723, entropy= 1.24598, loss=-0.02723
surrogate=-0.02714, entropy= 1.24359, loss=-0.02714
std_min= 0.30977, std_max= 0.41612, std_mean= 0.36900
val lr: [0.00013089139344262297], policy lr: [0.00015706967213114753]
Policy Loss: -0.02714, | Entropy Bonus: -0, | Value Loss: 24.829, | Advantage Loss: 2.9844
Time elapsed (s): 1.6674795150756836
Agent stdevs: 0.36900428
--------------------------------------------------------------------------------

Step 465
++++++++ Policy training ++++++++++
Current mean reward: 2913.924284 | mean episode length: 808.000000
val_loss=17.82861
val_loss=16.15104
val_loss=22.69683
val_loss=17.05506
val_loss=16.18341
val_loss=16.14610
val_loss= 8.79090
val_loss=15.41906
val_loss=12.44470
val_loss=17.61887
adv_loss= 1.59486
adv_loss= 3.27609
adv_loss= 1.20492
adv_loss= 2.23749
adv_loss= 4.51179
adv_loss= 1.34434
adv_loss= 2.22818
adv_loss= 8.74209
adv_loss= 2.30354
adv_loss= 2.48277
surrogate= 0.02304, entropy= 1.24290, loss= 0.02304
surrogate= 0.02690, entropy= 1.24548, loss= 0.02690
surrogate= 0.00112, entropy= 1.24461, loss= 0.00112
surrogate=-0.00807, entropy= 1.24515, loss=-0.00807
surrogate= 0.01345, entropy= 1.24549, loss= 0.01345
surrogate=-0.01300, entropy= 1.24459, loss=-0.01300
surrogate=-0.01457, entropy= 1.24550, loss=-0.01457
surrogate=-0.02424, entropy= 1.24566, loss=-0.02424
surrogate=-0.03475, entropy= 1.24549, loss=-0.03475
surrogate=-0.00775, entropy= 1.24581, loss=-0.00775
std_min= 0.31297, std_max= 0.41281, std_mean= 0.36897
val lr: [0.00013063524590163934], policy lr: [0.00015676229508196718]
Policy Loss: -0.0077525, | Entropy Bonus: -0, | Value Loss: 17.619, | Advantage Loss: 2.4828
Time elapsed (s): 1.6545298099517822
Agent stdevs: 0.368971
--------------------------------------------------------------------------------

Step 466
++++++++ Policy training ++++++++++
Current mean reward: 3220.765188 | mean episode length: 921.000000
val_loss=495.86905
val_loss=34.49302
val_loss=26.79236
val_loss=963.98340
val_loss=172.15433
val_loss=192.28613
val_loss=1129.39709
val_loss=485.81955
val_loss=71.91036
val_loss=55.67870
adv_loss= 3.56360
adv_loss= 6.21833
adv_loss= 9.47686
adv_loss= 8.89280
adv_loss= 4.15185
adv_loss= 1.72654
adv_loss=21.86934
adv_loss= 2.68134
adv_loss=11.06254
adv_loss= 3.14790
surrogate= 0.08434, entropy= 1.24653, loss= 0.08434
surrogate= 0.00728, entropy= 1.24706, loss= 0.00728
surrogate= 0.05455, entropy= 1.24662, loss= 0.05455
surrogate=-0.01789, entropy= 1.24768, loss=-0.01789
surrogate= 0.00764, entropy= 1.24738, loss= 0.00764
surrogate=-0.01019, entropy= 1.24690, loss=-0.01019
surrogate=-0.01678, entropy= 1.24813, loss=-0.01678
surrogate=-0.01573, entropy= 1.24798, loss=-0.01573
surrogate=-0.02604, entropy= 1.24886, loss=-0.02604
surrogate=-0.01812, entropy= 1.24830, loss=-0.01812
std_min= 0.31414, std_max= 0.41488, std_mean= 0.36926
val lr: [0.00013037909836065577], policy lr: [0.00015645491803278688]
Policy Loss: -0.018123, | Entropy Bonus: -0, | Value Loss: 55.679, | Advantage Loss: 3.1479
Time elapsed (s): 1.6813948154449463
Agent stdevs: 0.36925757
--------------------------------------------------------------------------------

Step 467
++++++++ Policy training ++++++++++
Current mean reward: 2057.997932 | mean episode length: 586.666667
val_loss=32.36081
val_loss=71.12064
val_loss=52.66929
val_loss=73.95017
val_loss=75.85570
val_loss=37.00729
val_loss=99.33604
val_loss=32.15882
val_loss=1033.40149
val_loss=123.28571
adv_loss= 4.28196
adv_loss= 2.07007
adv_loss= 1.74713
adv_loss= 2.75232
adv_loss= 2.50031
adv_loss= 2.35577
adv_loss= 1.70760
adv_loss= 2.50764
adv_loss= 2.95903
adv_loss=12.86883
surrogate= 0.00881, entropy= 1.24766, loss= 0.00881
surrogate=-0.01160, entropy= 1.24593, loss=-0.01160
surrogate=-0.00365, entropy= 1.24607, loss=-0.00365
surrogate=-0.03124, entropy= 1.24578, loss=-0.03124
surrogate= 0.04045, entropy= 1.24580, loss= 0.04045
surrogate=-0.01776, entropy= 1.24408, loss=-0.01776
surrogate=-0.01318, entropy= 1.24506, loss=-0.01318
surrogate=-0.02604, entropy= 1.24423, loss=-0.02604
surrogate=-0.01728, entropy= 1.24375, loss=-0.01728
surrogate=-0.02299, entropy= 1.24418, loss=-0.02299
std_min= 0.31317, std_max= 0.41270, std_mean= 0.36875
val lr: [0.0001301229508196721], policy lr: [0.00015614754098360653]
Policy Loss: -0.022988, | Entropy Bonus: -0, | Value Loss: 123.29, | Advantage Loss: 12.869
Time elapsed (s): 1.7076704502105713
Agent stdevs: 0.36874965
--------------------------------------------------------------------------------

Step 468
++++++++ Policy training ++++++++++
Current mean reward: 2477.350427 | mean episode length: 695.500000
val_loss=1497.75940
val_loss=1173.55481
val_loss=307.67181
val_loss=47.88593
val_loss=45.35815
val_loss=64.00020
val_loss=112.39793
val_loss=647.06335
val_loss=96.08344
val_loss=45.14283
adv_loss= 2.81562
adv_loss= 5.16981
adv_loss= 3.08072
adv_loss= 4.82061
adv_loss= 6.43451
adv_loss= 5.89249
adv_loss= 2.05454
adv_loss= 3.03661
adv_loss= 2.49302
adv_loss= 2.33621
surrogate= 0.03483, entropy= 1.24304, loss= 0.03483
surrogate= 0.01338, entropy= 1.24220, loss= 0.01338
surrogate=-0.01775, entropy= 1.23991, loss=-0.01775
surrogate=-0.04206, entropy= 1.23881, loss=-0.04206
surrogate=-0.01345, entropy= 1.23740, loss=-0.01345
surrogate=-0.02533, entropy= 1.23620, loss=-0.02533
surrogate=-0.02504, entropy= 1.23564, loss=-0.02504
surrogate=-0.03073, entropy= 1.23474, loss=-0.03073
surrogate=-0.01705, entropy= 1.23562, loss=-0.01705
surrogate=-0.02216, entropy= 1.23337, loss=-0.02216
std_min= 0.31126, std_max= 0.41329, std_mean= 0.36753
val lr: [0.0001298668032786885], policy lr: [0.0001558401639344262]
Policy Loss: -0.022162, | Entropy Bonus: -0, | Value Loss: 45.143, | Advantage Loss: 2.3362
Time elapsed (s): 1.656097650527954
Agent stdevs: 0.36753047
--------------------------------------------------------------------------------

Step 469
++++++++ Policy training ++++++++++
Current mean reward: 2217.709020 | mean episode length: 630.666667
val_loss=1106.27893
val_loss=81.09028
val_loss=36.88668
val_loss=556.01746
val_loss=1029.08557
val_loss=387.36853
val_loss=39.72770
val_loss=53.82246
val_loss=27.38707
val_loss=153.02623
adv_loss= 3.15849
adv_loss= 2.74832
adv_loss= 4.57954
adv_loss= 3.97073
adv_loss= 2.79788
adv_loss= 5.14627
adv_loss= 2.78999
adv_loss= 3.18554
adv_loss= 4.44144
adv_loss= 2.72280
surrogate= 0.00490, entropy= 1.23385, loss= 0.00490
surrogate=-0.00015, entropy= 1.23476, loss=-0.00015
surrogate= 0.03619, entropy= 1.23646, loss= 0.03619
surrogate=-0.01349, entropy= 1.23825, loss=-0.01349
surrogate= 0.00980, entropy= 1.23750, loss= 0.00980
surrogate=-0.01252, entropy= 1.23909, loss=-0.01252
surrogate=-0.02799, entropy= 1.23955, loss=-0.02799
surrogate=-0.01278, entropy= 1.24140, loss=-0.01278
surrogate=-0.00305, entropy= 1.24149, loss=-0.00305
surrogate=-0.02213, entropy= 1.24259, loss=-0.02213
std_min= 0.31067, std_max= 0.41533, std_mean= 0.36880
val lr: [0.0001296106557377049], policy lr: [0.00015553278688524587]
Policy Loss: -0.02213, | Entropy Bonus: -0, | Value Loss: 153.03, | Advantage Loss: 2.7228
Time elapsed (s): 1.6643269062042236
Agent stdevs: 0.3687965
--------------------------------------------------------------------------------

Step 470
++++++++ Policy training ++++++++++
Current mean reward: 1344.702030 | mean episode length: 365.800000
val_loss=54.35506
val_loss=25.44157
val_loss=25.96303
val_loss=18.67969
val_loss=27.62604
val_loss=17.94815
val_loss=14.11877
val_loss=13.80398
val_loss=11.82434
val_loss=18.83362
adv_loss= 4.85666
adv_loss= 2.87640
adv_loss= 4.40063
adv_loss= 2.93030
adv_loss= 5.52812
adv_loss= 3.84909
adv_loss= 2.86206
adv_loss= 3.49925
adv_loss= 2.95929
adv_loss= 2.09178
surrogate=-0.00733, entropy= 1.24150, loss=-0.00733
surrogate= 0.02350, entropy= 1.23989, loss= 0.02350
surrogate=-0.02125, entropy= 1.23952, loss=-0.02125
surrogate= 0.00141, entropy= 1.23798, loss= 0.00141
surrogate=-0.02948, entropy= 1.23660, loss=-0.02948
surrogate=-0.00240, entropy= 1.23498, loss=-0.00240
surrogate=-0.04365, entropy= 1.23452, loss=-0.04365
surrogate= 0.02341, entropy= 1.23265, loss= 0.02341
surrogate=-0.00597, entropy= 1.22983, loss=-0.00597
surrogate=-0.01567, entropy= 1.22913, loss=-0.01567
std_min= 0.30946, std_max= 0.41458, std_mean= 0.36717
val lr: [0.0001293545081967213], policy lr: [0.00015522540983606555]
Policy Loss: -0.015666, | Entropy Bonus: -0, | Value Loss: 18.834, | Advantage Loss: 2.0918
Time elapsed (s): 1.683800220489502
Agent stdevs: 0.36716923
--------------------------------------------------------------------------------

Step 471
++++++++ Policy training ++++++++++
Current mean reward: 2415.396362 | mean episode length: 663.333333
val_loss=16.90878
val_loss=12.97612
val_loss=12.87300
val_loss=11.06015
val_loss=15.63285
val_loss=12.37710
val_loss= 7.79094
val_loss=12.65363
val_loss=11.92715
val_loss= 8.21077
adv_loss= 2.13874
adv_loss= 4.63698
adv_loss= 1.14685
adv_loss= 1.94958
adv_loss= 4.98977
adv_loss= 2.83663
adv_loss= 8.29056
adv_loss= 1.69568
adv_loss= 1.41518
adv_loss= 3.15115
surrogate=-0.01832, entropy= 1.22810, loss=-0.01832
surrogate= 0.01601, entropy= 1.22535, loss= 0.01601
surrogate= 0.01997, entropy= 1.22188, loss= 0.01997
surrogate=-0.01731, entropy= 1.21990, loss=-0.01731
surrogate=-0.00401, entropy= 1.21772, loss=-0.00401
surrogate=-0.01176, entropy= 1.21538, loss=-0.01176
surrogate=-0.00865, entropy= 1.21388, loss=-0.00865
surrogate=-0.00970, entropy= 1.21093, loss=-0.00970
surrogate=-0.05775, entropy= 1.20898, loss=-0.05775
surrogate=-0.00172, entropy= 1.20907, loss=-0.00172
std_min= 0.30961, std_max= 0.41325, std_mean= 0.36460
val lr: [0.0001290983606557377], policy lr: [0.00015491803278688525]
Policy Loss: -0.0017202, | Entropy Bonus: -0, | Value Loss: 8.2108, | Advantage Loss: 3.1512
Time elapsed (s): 1.6937038898468018
Agent stdevs: 0.36459842
--------------------------------------------------------------------------------

Step 472
++++++++ Policy training ++++++++++
Current mean reward: 2331.631174 | mean episode length: 639.000000
val_loss=15.47665
val_loss=17.64950
val_loss=15.45402
val_loss=14.74638
val_loss= 9.11593
val_loss=10.24399
val_loss= 9.93814
val_loss= 7.89086
val_loss= 5.58931
val_loss=11.12684
adv_loss= 2.27773
adv_loss= 2.69479
adv_loss= 1.58389
adv_loss= 0.86390
adv_loss= 3.16909
adv_loss= 1.40364
adv_loss= 1.66767
adv_loss= 6.52498
adv_loss= 2.57843
adv_loss= 1.54122
surrogate=-0.01270, entropy= 1.20893, loss=-0.01270
surrogate=-0.02027, entropy= 1.21003, loss=-0.02027
surrogate=-0.02734, entropy= 1.21052, loss=-0.02734
surrogate= 0.00269, entropy= 1.21116, loss= 0.00269
surrogate=-0.02295, entropy= 1.21073, loss=-0.02295
surrogate=-0.02939, entropy= 1.21125, loss=-0.02939
surrogate=-0.02724, entropy= 1.21145, loss=-0.02724
surrogate=-0.01692, entropy= 1.21180, loss=-0.01692
surrogate=-0.02951, entropy= 1.21053, loss=-0.02951
surrogate= 0.00437, entropy= 1.21003, loss= 0.00437
std_min= 0.30780, std_max= 0.41651, std_mean= 0.36497
val lr: [0.00012884221311475409], policy lr: [0.00015461065573770487]
Policy Loss: 0.0043697, | Entropy Bonus: -0, | Value Loss: 11.127, | Advantage Loss: 1.5412
Time elapsed (s): 1.6915369033813477
Agent stdevs: 0.36496544
--------------------------------------------------------------------------------

Step 473
++++++++ Policy training ++++++++++
Current mean reward: 1494.177793 | mean episode length: 408.333333
val_loss=35.50977
val_loss=19.17801
val_loss=11.89373
val_loss=21.96566
val_loss=11.74585
val_loss=15.67940
val_loss=13.72465
val_loss= 7.95912
val_loss=13.91040
val_loss= 7.83924
adv_loss= 1.47417
adv_loss= 4.21524
adv_loss= 1.38090
adv_loss= 3.85280
adv_loss= 3.00741
adv_loss= 1.59788
adv_loss= 2.24111
adv_loss= 1.74783
adv_loss= 0.63646
adv_loss= 1.35670
surrogate= 0.02775, entropy= 1.20774, loss= 0.02775
surrogate=-0.01449, entropy= 1.20569, loss=-0.01449
surrogate=-0.01946, entropy= 1.20384, loss=-0.01946
surrogate= 0.01673, entropy= 1.20297, loss= 0.01673
surrogate=-0.01569, entropy= 1.20180, loss=-0.01569
surrogate= 0.00376, entropy= 1.20121, loss= 0.00376
surrogate= 0.00553, entropy= 1.19953, loss= 0.00553
surrogate=-0.03280, entropy= 1.19879, loss=-0.03280
surrogate=-0.00138, entropy= 1.19705, loss=-0.00138
surrogate=-0.01413, entropy= 1.19553, loss=-0.01413
std_min= 0.30669, std_max= 0.41374, std_mean= 0.36316
val lr: [0.0001285860655737705], policy lr: [0.0001543032786885246]
Policy Loss: -0.014134, | Entropy Bonus: -0, | Value Loss: 7.8392, | Advantage Loss: 1.3567
Time elapsed (s): 1.715639352798462
Agent stdevs: 0.36315754
--------------------------------------------------------------------------------

Step 474
++++++++ Policy training ++++++++++
Current mean reward: 2421.099895 | mean episode length: 669.000000
val_loss=11.29570
val_loss=10.65127
val_loss=16.78074
val_loss=14.21319
val_loss=10.08458
val_loss=11.63871
val_loss=12.23709
val_loss= 7.66257
val_loss= 6.07561
val_loss= 4.60381
adv_loss= 2.22269
adv_loss= 1.33051
adv_loss= 3.44750
adv_loss= 0.79898
adv_loss= 1.51652
adv_loss= 1.42060
adv_loss= 1.06078
adv_loss= 1.20408
adv_loss= 0.67362
adv_loss= 1.42925
surrogate= 0.03947, entropy= 1.19313, loss= 0.03947
surrogate=-0.01592, entropy= 1.18956, loss=-0.01592
surrogate=-0.01712, entropy= 1.18760, loss=-0.01712
surrogate=-0.00485, entropy= 1.18397, loss=-0.00485
surrogate= 0.01289, entropy= 1.18117, loss= 0.01289
surrogate=-0.02885, entropy= 1.17871, loss=-0.02885
surrogate=-0.01125, entropy= 1.17725, loss=-0.01125
surrogate= 0.01260, entropy= 1.17557, loss= 0.01260
surrogate=-0.00545, entropy= 1.17254, loss=-0.00545
surrogate=-0.01692, entropy= 1.17220, loss=-0.01692
std_min= 0.30385, std_max= 0.40918, std_mean= 0.36032
val lr: [0.00012832991803278689], policy lr: [0.00015399590163934422]
Policy Loss: -0.01692, | Entropy Bonus: -0, | Value Loss: 4.6038, | Advantage Loss: 1.4292
Time elapsed (s): 1.6534671783447266
Agent stdevs: 0.36031964
--------------------------------------------------------------------------------

Step 475
++++++++ Policy training ++++++++++
Current mean reward: 2504.341691 | mean episode length: 689.000000
val_loss=15.16465
val_loss=21.10513
val_loss= 7.14555
val_loss=20.24010
val_loss=10.55930
val_loss= 6.84472
val_loss=12.73639
val_loss= 6.21314
val_loss= 9.74025
val_loss=13.93178
adv_loss= 2.20756
adv_loss= 1.20411
adv_loss= 1.86184
adv_loss= 0.91119
adv_loss= 2.78916
adv_loss= 1.65354
adv_loss= 4.00513
adv_loss= 1.20226
adv_loss= 1.35742
adv_loss= 1.63764
surrogate=-0.00561, entropy= 1.17007, loss=-0.00561
surrogate= 0.00927, entropy= 1.16905, loss= 0.00927
surrogate=-0.01200, entropy= 1.16562, loss=-0.01200
surrogate=-0.00460, entropy= 1.16465, loss=-0.00460
surrogate=-0.00971, entropy= 1.16205, loss=-0.00971
surrogate= 0.00012, entropy= 1.16072, loss= 0.00012
surrogate= 0.00273, entropy= 1.15881, loss= 0.00273
surrogate=-0.00427, entropy= 1.15818, loss=-0.00427
surrogate=-0.03521, entropy= 1.15699, loss=-0.03521
surrogate= 0.00528, entropy= 1.15586, loss= 0.00528
std_min= 0.30503, std_max= 0.40779, std_mean= 0.35821
val lr: [0.00012807377049180329], policy lr: [0.00015368852459016395]
Policy Loss: 0.0052753, | Entropy Bonus: -0, | Value Loss: 13.932, | Advantage Loss: 1.6376
Time elapsed (s): 1.6835031509399414
Agent stdevs: 0.3582065
--------------------------------------------------------------------------------

Step 476
++++++++ Policy training ++++++++++
Current mean reward: 2401.790937 | mean episode length: 668.333333
val_loss=170.65929
val_loss=15.09167
val_loss=83.16430
val_loss=46.70974
val_loss=45.89977
val_loss=60.11526
val_loss=238.53525
val_loss=31.81914
val_loss=147.73904
val_loss=416.45078
adv_loss= 3.39528
adv_loss= 0.77243
adv_loss= 1.63118
adv_loss= 1.96574
adv_loss= 1.69931
adv_loss= 0.73884
adv_loss= 1.50415
adv_loss= 0.52197
adv_loss= 1.19125
adv_loss= 2.33724
surrogate=-0.00863, entropy= 1.15489, loss=-0.00863
surrogate=-0.00446, entropy= 1.15630, loss=-0.00446
surrogate= 0.02618, entropy= 1.15805, loss= 0.02618
surrogate= 0.01344, entropy= 1.15981, loss= 0.01344
surrogate=-0.01661, entropy= 1.16118, loss=-0.01661
surrogate=-0.03330, entropy= 1.16059, loss=-0.03330
surrogate=-0.00223, entropy= 1.16246, loss=-0.00223
surrogate=-0.01494, entropy= 1.16323, loss=-0.01494
surrogate=-0.01419, entropy= 1.16504, loss=-0.01419
surrogate=-0.00461, entropy= 1.16437, loss=-0.00461
std_min= 0.30599, std_max= 0.40829, std_mean= 0.35921
val lr: [0.00012781762295081966], policy lr: [0.0001533811475409836]
Policy Loss: -0.0046053, | Entropy Bonus: -0, | Value Loss: 416.45, | Advantage Loss: 2.3372
Time elapsed (s): 1.6930649280548096
Agent stdevs: 0.35920605
--------------------------------------------------------------------------------

Step 477
++++++++ Policy training ++++++++++
Current mean reward: 1494.614762 | mean episode length: 403.200000
val_loss=30.31190
val_loss=21.89871
val_loss=17.81429
val_loss=11.44897
val_loss=12.15769
val_loss=18.50280
val_loss=26.44146
val_loss=24.63586
val_loss=20.24203
val_loss=13.95309
adv_loss= 2.60898
adv_loss= 2.03008
adv_loss= 1.77051
adv_loss= 2.40836
adv_loss= 6.28426
adv_loss= 3.34704
adv_loss= 1.67818
adv_loss= 3.27133
adv_loss= 2.14746
adv_loss= 6.29390
surrogate= 0.00549, entropy= 1.16316, loss= 0.00549
surrogate=-0.02752, entropy= 1.16225, loss=-0.02752
surrogate=-0.01623, entropy= 1.16013, loss=-0.01623
surrogate=-0.01183, entropy= 1.15951, loss=-0.01183
surrogate=-0.00097, entropy= 1.15822, loss=-0.00097
surrogate=-0.03517, entropy= 1.15736, loss=-0.03517
surrogate=-0.01446, entropy= 1.15458, loss=-0.01446
surrogate= 0.02601, entropy= 1.15242, loss= 0.02601
surrogate=-0.01871, entropy= 1.15140, loss=-0.01871
surrogate=-0.00098, entropy= 1.14974, loss=-0.00098
std_min= 0.30312, std_max= 0.40923, std_mean= 0.35764
val lr: [0.00012756147540983606], policy lr: [0.00015307377049180327]
Policy Loss: -0.00097654, | Entropy Bonus: -0, | Value Loss: 13.953, | Advantage Loss: 6.2939
Time elapsed (s): 1.6642701625823975
Agent stdevs: 0.3576398
--------------------------------------------------------------------------------

Step 478
++++++++ Policy training ++++++++++
Current mean reward: 1410.037062 | mean episode length: 383.250000
val_loss=16.24203
val_loss=15.49470
val_loss=16.00444
val_loss=11.06278
val_loss=10.91080
val_loss=10.56208
val_loss= 9.52104
val_loss=10.00260
val_loss=12.95406
val_loss=14.72612
adv_loss= 2.15655
adv_loss= 1.94732
adv_loss= 3.68186
adv_loss= 0.75180
adv_loss= 1.49539
adv_loss= 0.66471
adv_loss= 3.55406
adv_loss= 1.31852
adv_loss= 3.05186
adv_loss= 0.89293
surrogate=-0.04517, entropy= 1.15030, loss=-0.04517
surrogate= 0.03114, entropy= 1.15114, loss= 0.03114
surrogate= 0.00823, entropy= 1.15231, loss= 0.00823
surrogate= 0.00113, entropy= 1.15364, loss= 0.00113
surrogate=-0.01821, entropy= 1.15441, loss=-0.01821
surrogate=-0.03185, entropy= 1.15768, loss=-0.03185
surrogate=-0.02548, entropy= 1.15836, loss=-0.02548
surrogate=-0.01537, entropy= 1.15876, loss=-0.01537
surrogate=-0.02482, entropy= 1.16005, loss=-0.02482
surrogate= 0.01736, entropy= 1.16179, loss= 0.01736
std_min= 0.30467, std_max= 0.41084, std_mean= 0.35907
val lr: [0.00012730532786885246], policy lr: [0.00015276639344262294]
Policy Loss: 0.017355, | Entropy Bonus: -0, | Value Loss: 14.726, | Advantage Loss: 0.89293
Time elapsed (s): 1.6859655380249023
Agent stdevs: 0.35907307
--------------------------------------------------------------------------------

Step 479
++++++++ Policy training ++++++++++
Current mean reward: 2074.994373 | mean episode length: 563.000000
val_loss= 8.59153
val_loss= 6.04073
val_loss= 7.95784
val_loss=21.73408
val_loss= 8.79936
val_loss=10.15252
val_loss= 9.64276
val_loss=13.92862
val_loss= 7.29781
val_loss= 4.73312
adv_loss= 1.24644
adv_loss= 2.35681
adv_loss= 1.07273
adv_loss= 0.76077
adv_loss= 2.51987
adv_loss= 1.59254
adv_loss= 0.63901
adv_loss= 1.14077
adv_loss= 0.73302
adv_loss= 0.90710
surrogate=-0.01124, entropy= 1.16190, loss=-0.01124
surrogate= 0.02552, entropy= 1.15983, loss= 0.02552
surrogate= 0.01850, entropy= 1.15787, loss= 0.01850
surrogate=-0.00726, entropy= 1.15652, loss=-0.00726
surrogate=-0.00315, entropy= 1.15437, loss=-0.00315
surrogate= 0.03445, entropy= 1.15258, loss= 0.03445
surrogate=-0.02543, entropy= 1.14987, loss=-0.02543
surrogate=-0.01078, entropy= 1.14738, loss=-0.01078
surrogate=-0.03371, entropy= 1.14395, loss=-0.03371
surrogate=-0.03064, entropy= 1.14167, loss=-0.03064
std_min= 0.30290, std_max= 0.40755, std_mean= 0.35661
val lr: [0.00012704918032786886], policy lr: [0.00015245901639344262]
Policy Loss: -0.030643, | Entropy Bonus: -0, | Value Loss: 4.7331, | Advantage Loss: 0.9071
Time elapsed (s): 1.7059106826782227
Agent stdevs: 0.35661247
--------------------------------------------------------------------------------

Step 480
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1977.2
++++++++ Policy training ++++++++++
Current mean reward: 2097.365731 | mean episode length: 592.000000
val_loss=33.51783
val_loss=55.51402
val_loss=18.34840
val_loss=34.63669
val_loss=21.04164
val_loss=40.83047
val_loss=322.48755
val_loss=11.92109
val_loss=69.46409
val_loss=24.61607
adv_loss= 1.12707
adv_loss= 5.48124
adv_loss= 1.55983
adv_loss= 2.25087
adv_loss= 8.72474
adv_loss= 6.03511
adv_loss= 1.27720
adv_loss= 1.03747
adv_loss= 5.37807
adv_loss= 6.64887
surrogate=-0.00708, entropy= 1.14092, loss=-0.00708
surrogate=-0.02053, entropy= 1.14220, loss=-0.02053
surrogate=-0.02873, entropy= 1.14267, loss=-0.02873
surrogate=-0.01013, entropy= 1.14234, loss=-0.01013
surrogate=-0.01700, entropy= 1.14134, loss=-0.01700
surrogate=-0.01468, entropy= 1.14135, loss=-0.01468
surrogate=-0.03424, entropy= 1.14314, loss=-0.03424
surrogate=-0.03441, entropy= 1.14215, loss=-0.03441
surrogate=-0.03099, entropy= 1.14220, loss=-0.03099
surrogate=-0.00027, entropy= 1.14304, loss=-0.00027
std_min= 0.30055, std_max= 0.40841, std_mean= 0.35698
val lr: [0.00012679303278688526], policy lr: [0.0001521516393442623]
Policy Loss: -0.00027125, | Entropy Bonus: -0, | Value Loss: 24.616, | Advantage Loss: 6.6489
Time elapsed (s): 1.6504414081573486
Agent stdevs: 0.35698023
--------------------------------------------------------------------------------

Step 481
++++++++ Policy training ++++++++++
Current mean reward: 1208.694181 | mean episode length: 336.600000
val_loss=45.83310
val_loss=33.13766
val_loss=28.44453
val_loss=40.75237
val_loss=28.44646
val_loss=36.11095
val_loss=35.60238
val_loss=53.52863
val_loss=27.24905
val_loss=20.48576
adv_loss=11.57674
adv_loss=14.22149
adv_loss= 1.63143
adv_loss= 4.80897
adv_loss= 2.01898
adv_loss= 3.99659
adv_loss= 4.70196
adv_loss= 6.61400
adv_loss= 3.75003
adv_loss= 8.33058
surrogate= 0.00755, entropy= 1.14315, loss= 0.00755
surrogate=-0.01371, entropy= 1.14090, loss=-0.01371
surrogate=-0.00813, entropy= 1.13931, loss=-0.00813
surrogate= 0.01887, entropy= 1.13775, loss= 0.01887
surrogate=-0.01150, entropy= 1.13605, loss=-0.01150
surrogate=-0.02672, entropy= 1.13397, loss=-0.02672
surrogate=-0.02403, entropy= 1.13245, loss=-0.02403
surrogate=-0.02921, entropy= 1.13187, loss=-0.02921
surrogate=-0.05189, entropy= 1.13038, loss=-0.05189
surrogate=-0.03302, entropy= 1.12917, loss=-0.03302
std_min= 0.29931, std_max= 0.40294, std_mean= 0.35520
val lr: [0.00012653688524590163], policy lr: [0.00015184426229508194]
Policy Loss: -0.03302, | Entropy Bonus: -0, | Value Loss: 20.486, | Advantage Loss: 8.3306
Time elapsed (s): 1.6719181537628174
Agent stdevs: 0.35519686
--------------------------------------------------------------------------------

Step 482
++++++++ Policy training ++++++++++
Current mean reward: 935.163669 | mean episode length: 262.500000
val_loss=66.02267
val_loss=38.57761
val_loss=67.31310
val_loss=25.65279
val_loss=54.18332
val_loss=75.03474
val_loss=34.65376
val_loss=39.81024
val_loss=44.64583
val_loss=58.67322
adv_loss= 6.97852
adv_loss= 4.03655
adv_loss= 4.06953
adv_loss=14.68414
adv_loss= 7.26077
adv_loss= 3.72398
adv_loss= 5.39535
adv_loss= 3.12127
adv_loss= 3.67612
adv_loss= 1.53953
surrogate=-0.00509, entropy= 1.12999, loss=-0.00509
surrogate=-0.00459, entropy= 1.12839, loss=-0.00459
surrogate= 0.00971, entropy= 1.12917, loss= 0.00971
surrogate=-0.03365, entropy= 1.12853, loss=-0.03365
surrogate= 0.02673, entropy= 1.12795, loss= 0.02673
surrogate=-0.02550, entropy= 1.12946, loss=-0.02550
surrogate=-0.02811, entropy= 1.12934, loss=-0.02811
surrogate=-0.01834, entropy= 1.12864, loss=-0.01834
surrogate=-0.04798, entropy= 1.12947, loss=-0.04798
surrogate=-0.03724, entropy= 1.12923, loss=-0.03724
std_min= 0.29877, std_max= 0.40309, std_mean= 0.35525
val lr: [0.00012628073770491806], policy lr: [0.00015153688524590164]
Policy Loss: -0.037235, | Entropy Bonus: -0, | Value Loss: 58.673, | Advantage Loss: 1.5395
Time elapsed (s): 1.6768033504486084
Agent stdevs: 0.3552468
--------------------------------------------------------------------------------

Step 483
++++++++ Policy training ++++++++++
Current mean reward: 1895.640506 | mean episode length: 522.333333
val_loss=34.82320
val_loss=15.47678
val_loss=18.31943
val_loss=14.04345
val_loss=11.25129
val_loss=15.30014
val_loss= 7.33522
val_loss=15.65016
val_loss= 9.63802
val_loss=11.71522
adv_loss= 5.62286
adv_loss= 1.78222
adv_loss= 1.93384
adv_loss= 3.88708
adv_loss= 2.19197
adv_loss= 2.64117
adv_loss= 0.54301
adv_loss= 1.82553
adv_loss= 2.12594
adv_loss= 2.01342
surrogate= 0.00117, entropy= 1.12776, loss= 0.00117
surrogate= 0.00248, entropy= 1.12677, loss= 0.00248
surrogate=-0.02065, entropy= 1.12699, loss=-0.02065
surrogate=-0.03379, entropy= 1.12525, loss=-0.03379
surrogate=-0.00001, entropy= 1.12419, loss=-0.00001
surrogate=-0.01460, entropy= 1.12278, loss=-0.01460
surrogate= 0.00361, entropy= 1.12347, loss= 0.00361
surrogate=-0.01301, entropy= 1.12230, loss=-0.01301
surrogate=-0.03277, entropy= 1.12127, loss=-0.03277
surrogate=-0.04326, entropy= 1.12051, loss=-0.04326
std_min= 0.29803, std_max= 0.40431, std_mean= 0.35428
val lr: [0.0001260245901639344], policy lr: [0.0001512295081967213]
Policy Loss: -0.043265, | Entropy Bonus: -0, | Value Loss: 11.715, | Advantage Loss: 2.0134
Time elapsed (s): 1.698106050491333
Agent stdevs: 0.35428116
--------------------------------------------------------------------------------

Step 484
++++++++ Policy training ++++++++++
Current mean reward: 1542.600491 | mean episode length: 424.000000
val_loss=12.66551
val_loss=14.37123
val_loss=12.45521
val_loss=10.21447
val_loss=10.68459
val_loss= 8.59836
val_loss=14.06320
val_loss=16.51065
val_loss=11.58062
val_loss= 8.67829
adv_loss= 5.18707
adv_loss= 1.57341
adv_loss= 1.69431
adv_loss= 1.34294
adv_loss= 2.05129
adv_loss= 0.66976
adv_loss= 6.13146
adv_loss= 1.06172
adv_loss= 4.66140
adv_loss= 1.60395
surrogate= 0.03567, entropy= 1.11948, loss= 0.03567
surrogate=-0.00293, entropy= 1.11846, loss=-0.00293
surrogate=-0.01144, entropy= 1.11624, loss=-0.01144
surrogate=-0.02636, entropy= 1.11440, loss=-0.02636
surrogate=-0.01979, entropy= 1.11214, loss=-0.01979
surrogate=-0.03843, entropy= 1.11066, loss=-0.03843
surrogate=-0.01400, entropy= 1.11066, loss=-0.01400
surrogate=-0.00758, entropy= 1.10879, loss=-0.00758
surrogate=-0.01823, entropy= 1.10598, loss=-0.01823
surrogate=-0.03349, entropy= 1.10593, loss=-0.03349
std_min= 0.29476, std_max= 0.40515, std_mean= 0.35279
val lr: [0.00012576844262295083], policy lr: [0.000150922131147541]
Policy Loss: -0.033486, | Entropy Bonus: -0, | Value Loss: 8.6783, | Advantage Loss: 1.6039
Time elapsed (s): 1.6980202198028564
Agent stdevs: 0.35279313
--------------------------------------------------------------------------------

Step 485
++++++++ Policy training ++++++++++
Current mean reward: 1447.621300 | mean episode length: 394.200000
val_loss=15.04945
val_loss=11.30914
val_loss=21.35060
val_loss=15.60185
val_loss=20.71268
val_loss=17.07275
val_loss=10.29780
val_loss=11.49046
val_loss=12.09509
val_loss= 8.83606
adv_loss= 2.72891
adv_loss= 2.84356
adv_loss= 3.17591
adv_loss= 4.18190
adv_loss= 2.77637
adv_loss= 1.26619
adv_loss= 5.14843
adv_loss= 2.72948
adv_loss= 1.56053
adv_loss= 1.88256
surrogate=-0.00897, entropy= 1.10495, loss=-0.00897
surrogate=-0.02254, entropy= 1.10507, loss=-0.02254
surrogate= 0.02858, entropy= 1.10456, loss= 0.02858
surrogate=-0.00627, entropy= 1.10519, loss=-0.00627
surrogate=-0.00975, entropy= 1.10431, loss=-0.00975
surrogate=-0.02010, entropy= 1.10353, loss=-0.02010
surrogate=-0.00657, entropy= 1.10348, loss=-0.00657
surrogate=-0.00914, entropy= 1.10353, loss=-0.00914
surrogate=-0.02285, entropy= 1.10262, loss=-0.02285
surrogate=-0.03890, entropy= 1.10338, loss=-0.03890
std_min= 0.29179, std_max= 0.40653, std_mean= 0.35277
val lr: [0.0001255122950819672], policy lr: [0.00015061475409836064]
Policy Loss: -0.038898, | Entropy Bonus: -0, | Value Loss: 8.8361, | Advantage Loss: 1.8826
Time elapsed (s): 1.709059476852417
Agent stdevs: 0.3527727
--------------------------------------------------------------------------------

Step 486
++++++++ Policy training ++++++++++
Current mean reward: 1455.319303 | mean episode length: 400.750000
val_loss=16.04325
val_loss=16.35508
val_loss=14.05070
val_loss=25.55870
val_loss=14.78197
val_loss=11.55045
val_loss=17.45955
val_loss= 7.19532
val_loss=16.47938
val_loss=10.16671
adv_loss= 3.58362
adv_loss= 2.08558
adv_loss= 1.05051
adv_loss= 2.88380
adv_loss= 2.46672
adv_loss= 3.69412
adv_loss= 1.29899
adv_loss= 3.10093
adv_loss= 1.85705
adv_loss= 1.59313
surrogate=-0.00220, entropy= 1.09887, loss=-0.00220
surrogate= 0.02884, entropy= 1.09634, loss= 0.02884
surrogate= 0.00383, entropy= 1.09129, loss= 0.00383
surrogate=-0.01411, entropy= 1.08916, loss=-0.01411
surrogate=-0.04061, entropy= 1.08636, loss=-0.04061
surrogate=-0.05005, entropy= 1.08268, loss=-0.05005
surrogate=-0.02159, entropy= 1.08018, loss=-0.02159
surrogate=-0.00688, entropy= 1.07711, loss=-0.00688
surrogate=-0.02958, entropy= 1.07555, loss=-0.02958
surrogate= 0.00430, entropy= 1.07306, loss= 0.00430
std_min= 0.28730, std_max= 0.40475, std_mean= 0.34944
val lr: [0.0001252561475409836], policy lr: [0.0001503073770491803]
Policy Loss: 0.004303, | Entropy Bonus: -0, | Value Loss: 10.167, | Advantage Loss: 1.5931
Time elapsed (s): 1.6968114376068115
Agent stdevs: 0.34944105
--------------------------------------------------------------------------------

Step 487
++++++++ Policy training ++++++++++
Current mean reward: 3284.500624 | mean episode length: 928.500000
val_loss=719.56232
val_loss=801.20428
val_loss=17.93946
val_loss=466.18658
val_loss=44.53483
val_loss=23.58570
val_loss=83.53876
val_loss=209.24638
val_loss=25.86482
val_loss=28.54693
adv_loss= 1.11845
adv_loss= 1.89331
adv_loss= 1.77210
adv_loss= 1.97505
adv_loss= 1.70831
adv_loss= 2.22218
adv_loss= 1.31701
adv_loss= 1.66051
adv_loss= 1.21523
adv_loss= 1.12367
surrogate= 0.01489, entropy= 1.07280, loss= 0.01489
surrogate=-0.03216, entropy= 1.07215, loss=-0.03216
surrogate=-0.00619, entropy= 1.07244, loss=-0.00619
surrogate= 0.03383, entropy= 1.07092, loss= 0.03383
surrogate=-0.01885, entropy= 1.06948, loss=-0.01885
surrogate=-0.00122, entropy= 1.06897, loss=-0.00122
surrogate=-0.00281, entropy= 1.06735, loss=-0.00281
surrogate=-0.02044, entropy= 1.06701, loss=-0.02044
surrogate=-0.00017, entropy= 1.06716, loss=-0.00017
surrogate=-0.02380, entropy= 1.06535, loss=-0.02380
std_min= 0.28592, std_max= 0.40673, std_mean= 0.34872
val lr: [0.000125], policy lr: [0.00015]
Policy Loss: -0.023795, | Entropy Bonus: -0, | Value Loss: 28.547, | Advantage Loss: 1.1237
Time elapsed (s): 1.66611909866333
Agent stdevs: 0.34871855
--------------------------------------------------------------------------------

Step 488
++++++++ Policy training ++++++++++
Current mean reward: 3495.393577 | mean episode length: 1000.000000
val_loss=2001.96814
val_loss=485.22476
val_loss=2058.87427
val_loss=1356.64014
val_loss=2059.23291
val_loss=1492.62231
val_loss=2473.24365
val_loss=382.12543
val_loss=2863.26001
val_loss=2157.61328
adv_loss= 1.79875
adv_loss= 2.20464
adv_loss= 3.26272
adv_loss= 3.91357
adv_loss= 1.05728
adv_loss= 1.45119
adv_loss= 1.96349
adv_loss= 3.99381
adv_loss= 1.93008
adv_loss=1749.38367
surrogate=-0.00492, entropy= 1.06530, loss=-0.00492
surrogate=-0.00164, entropy= 1.06484, loss=-0.00164
surrogate=-0.00936, entropy= 1.06222, loss=-0.00936
surrogate=-0.00477, entropy= 1.06139, loss=-0.00477
surrogate=-0.00757, entropy= 1.06199, loss=-0.00757
surrogate=-0.01315, entropy= 1.06232, loss=-0.01315
surrogate=-0.01886, entropy= 1.06242, loss=-0.01886
surrogate= 0.01497, entropy= 1.06324, loss= 0.01497
surrogate=-0.01915, entropy= 1.06405, loss=-0.01915
surrogate=-0.00631, entropy= 1.06534, loss=-0.00631
std_min= 0.28716, std_max= 0.40712, std_mean= 0.34865
val lr: [0.0001247438524590164], policy lr: [0.00014969262295081966]
Policy Loss: -0.0063145, | Entropy Bonus: -0, | Value Loss: 2157.6, | Advantage Loss: 1749.4
Time elapsed (s): 1.6904504299163818
Agent stdevs: 0.34865192
--------------------------------------------------------------------------------

Step 489
++++++++ Policy training ++++++++++
Current mean reward: 1973.827643 | mean episode length: 544.666667
val_loss= 9.63780
val_loss=14.03663
val_loss= 9.07252
val_loss=16.88688
val_loss=16.27322
val_loss=11.68363
val_loss=11.32920
val_loss= 9.03190
val_loss=10.18863
val_loss= 8.16781
adv_loss= 2.76007
adv_loss= 2.85689
adv_loss= 1.92666
adv_loss= 1.33151
adv_loss= 4.80994
adv_loss= 1.34111
adv_loss= 0.82024
adv_loss= 4.08738
adv_loss= 0.94204
adv_loss= 0.88406
surrogate= 0.00719, entropy= 1.06627, loss= 0.00719
surrogate=-0.04224, entropy= 1.06653, loss=-0.04224
surrogate=-0.00278, entropy= 1.06770, loss=-0.00278
surrogate=-0.03548, entropy= 1.06839, loss=-0.03548
surrogate=-0.01431, entropy= 1.06841, loss=-0.01431
surrogate=-0.00990, entropy= 1.06905, loss=-0.00990
surrogate= 0.00442, entropy= 1.06959, loss= 0.00442
surrogate=-0.00388, entropy= 1.07103, loss=-0.00388
surrogate=-0.03214, entropy= 1.07240, loss=-0.03214
surrogate=-0.01473, entropy= 1.07431, loss=-0.01473
std_min= 0.28782, std_max= 0.40904, std_mean= 0.34974
val lr: [0.0001244877049180328], policy lr: [0.00014938524590163934]
Policy Loss: -0.014732, | Entropy Bonus: -0, | Value Loss: 8.1678, | Advantage Loss: 0.88406
Time elapsed (s): 1.669518232345581
Agent stdevs: 0.3497391
--------------------------------------------------------------------------------

Step 490
++++++++ Policy training ++++++++++
Current mean reward: 1431.477032 | mean episode length: 393.000000
val_loss=10.35531
val_loss= 8.23176
val_loss= 6.54007
val_loss=13.01801
val_loss= 7.00533
val_loss= 8.44583
val_loss=12.05457
val_loss=11.60532
val_loss= 9.28843
val_loss= 4.04236
adv_loss= 1.34836
adv_loss= 3.89339
adv_loss= 5.42144
adv_loss= 6.38907
adv_loss= 5.05906
adv_loss= 1.55352
adv_loss= 1.44106
adv_loss= 2.08900
adv_loss= 1.90350
adv_loss= 1.10295
surrogate= 0.00691, entropy= 1.07590, loss= 0.00691
surrogate=-0.01268, entropy= 1.07595, loss=-0.01268
surrogate= 0.01941, entropy= 1.07591, loss= 0.01941
surrogate=-0.00824, entropy= 1.07655, loss=-0.00824
surrogate=-0.01991, entropy= 1.07758, loss=-0.01991
surrogate=-0.02189, entropy= 1.07762, loss=-0.02189
surrogate=-0.01401, entropy= 1.07880, loss=-0.01401
surrogate=-0.03339, entropy= 1.07994, loss=-0.03339
surrogate=-0.02481, entropy= 1.08048, loss=-0.02481
surrogate= 0.00316, entropy= 1.08007, loss= 0.00316
std_min= 0.28852, std_max= 0.40917, std_mean= 0.35038
val lr: [0.0001242315573770492], policy lr: [0.000149077868852459]
Policy Loss: 0.0031577, | Entropy Bonus: -0, | Value Loss: 4.0424, | Advantage Loss: 1.103
Time elapsed (s): 1.6865942478179932
Agent stdevs: 0.35037896
--------------------------------------------------------------------------------

Step 491
++++++++ Policy training ++++++++++
Current mean reward: 1416.608209 | mean episode length: 390.400000
val_loss= 8.87427
val_loss=12.64925
val_loss= 7.35699
val_loss= 9.21064
val_loss= 8.75886
val_loss= 9.97117
val_loss= 5.66895
val_loss=10.02976
val_loss= 9.81531
val_loss= 8.90228
adv_loss= 4.37155
adv_loss= 2.18761
adv_loss= 2.12536
adv_loss= 1.89559
adv_loss= 4.74674
adv_loss= 2.33344
adv_loss= 2.65909
adv_loss= 2.00156
adv_loss= 1.98546
adv_loss= 1.72420
surrogate= 0.00867, entropy= 1.08002, loss= 0.00867
surrogate=-0.00367, entropy= 1.08013, loss=-0.00367
surrogate=-0.01570, entropy= 1.08158, loss=-0.01570
surrogate= 0.00619, entropy= 1.08178, loss= 0.00619
surrogate=-0.02688, entropy= 1.08213, loss=-0.02688
surrogate=-0.03929, entropy= 1.08162, loss=-0.03929
surrogate=-0.03308, entropy= 1.08248, loss=-0.03308
surrogate=-0.01394, entropy= 1.08284, loss=-0.01394
surrogate=-0.02441, entropy= 1.08194, loss=-0.02441
surrogate=-0.03658, entropy= 1.08131, loss=-0.03658
std_min= 0.28881, std_max= 0.41110, std_mean= 0.35058
val lr: [0.0001239754098360656], policy lr: [0.00014877049180327868]
Policy Loss: -0.036583, | Entropy Bonus: -0, | Value Loss: 8.9023, | Advantage Loss: 1.7242
Time elapsed (s): 1.7042744159698486
Agent stdevs: 0.35058427
--------------------------------------------------------------------------------

Step 492
++++++++ Policy training ++++++++++
Current mean reward: 1630.978812 | mean episode length: 444.250000
val_loss=12.90912
val_loss=12.77292
val_loss=32.57948
val_loss=22.30153
val_loss=16.21642
val_loss=16.34317
val_loss=22.37305
val_loss=10.67883
val_loss=15.83941
val_loss=28.47968
adv_loss= 1.07432
adv_loss= 4.99390
adv_loss= 4.21014
adv_loss= 2.71771
adv_loss= 2.43023
adv_loss= 2.16967
adv_loss= 1.94039
adv_loss= 1.33064
adv_loss= 1.72725
adv_loss= 1.18918
surrogate= 0.00678, entropy= 1.08226, loss= 0.00678
surrogate=-0.00639, entropy= 1.08153, loss=-0.00639
surrogate= 0.00148, entropy= 1.08230, loss= 0.00148
surrogate=-0.03249, entropy= 1.08169, loss=-0.03249
surrogate=-0.02420, entropy= 1.08046, loss=-0.02420
surrogate=-0.02888, entropy= 1.07946, loss=-0.02888
surrogate=-0.02317, entropy= 1.07933, loss=-0.02317
surrogate=-0.01739, entropy= 1.07949, loss=-0.01739
surrogate=-0.00766, entropy= 1.07884, loss=-0.00766
surrogate=-0.02814, entropy= 1.07758, loss=-0.02814
std_min= 0.29010, std_max= 0.40974, std_mean= 0.34998
val lr: [0.00012371926229508195], policy lr: [0.00014846311475409833]
Policy Loss: -0.02814, | Entropy Bonus: -0, | Value Loss: 28.48, | Advantage Loss: 1.1892
Time elapsed (s): 1.680596113204956
Agent stdevs: 0.34998307
--------------------------------------------------------------------------------

Step 493
++++++++ Policy training ++++++++++
Current mean reward: 1996.381951 | mean episode length: 545.000000
val_loss= 7.54354
val_loss= 8.28661
val_loss= 4.87403
val_loss= 4.89239
val_loss= 5.80953
val_loss=10.54447
val_loss=14.45461
val_loss= 9.55916
val_loss= 8.07582
val_loss= 6.30573
adv_loss= 0.51489
adv_loss= 1.35313
adv_loss= 7.37640
adv_loss= 1.74390
adv_loss= 1.57279
adv_loss= 1.34742
adv_loss= 3.44002
adv_loss= 1.01362
adv_loss= 1.42755
adv_loss= 1.78243
surrogate=-0.01311, entropy= 1.07440, loss=-0.01311
surrogate= 0.00823, entropy= 1.07265, loss= 0.00823
surrogate= 0.01399, entropy= 1.07056, loss= 0.01399
surrogate= 0.01471, entropy= 1.06820, loss= 0.01471
surrogate=-0.01717, entropy= 1.06656, loss=-0.01717
surrogate= 0.01671, entropy= 1.06481, loss= 0.01671
surrogate=-0.01365, entropy= 1.06304, loss=-0.01365
surrogate=-0.02016, entropy= 1.06164, loss=-0.02016
surrogate=-0.01952, entropy= 1.06032, loss=-0.01952
surrogate=-0.03908, entropy= 1.05821, loss=-0.03908
std_min= 0.28738, std_max= 0.41090, std_mean= 0.34798
val lr: [0.00012346311475409835], policy lr: [0.000148155737704918]
Policy Loss: -0.039076, | Entropy Bonus: -0, | Value Loss: 6.3057, | Advantage Loss: 1.7824
Time elapsed (s): 1.6615979671478271
Agent stdevs: 0.3479804
--------------------------------------------------------------------------------

Step 494
++++++++ Policy training ++++++++++
Current mean reward: 1659.972411 | mean episode length: 453.000000
val_loss=13.86150
val_loss=10.27588
val_loss=18.57751
val_loss=19.36179
val_loss=13.42913
val_loss= 5.80700
val_loss= 7.84981
val_loss= 8.73583
val_loss= 9.31439
val_loss= 9.76916
adv_loss= 1.21188
adv_loss= 0.72114
adv_loss= 1.77490
adv_loss= 3.52924
adv_loss= 2.74659
adv_loss= 3.28335
adv_loss= 0.82211
adv_loss= 1.40642
adv_loss= 2.67456
adv_loss= 2.64432
surrogate=-0.00058, entropy= 1.05858, loss=-0.00058
surrogate=-0.00841, entropy= 1.06069, loss=-0.00841
surrogate=-0.02499, entropy= 1.06177, loss=-0.02499
surrogate=-0.03958, entropy= 1.06393, loss=-0.03958
surrogate=-0.01138, entropy= 1.06503, loss=-0.01138
surrogate= 0.03874, entropy= 1.06511, loss= 0.03874
surrogate= 0.01906, entropy= 1.06578, loss= 0.01906
surrogate= 0.01338, entropy= 1.06487, loss= 0.01338
surrogate= 0.02440, entropy= 1.06544, loss= 0.02440
surrogate= 0.00651, entropy= 1.06480, loss= 0.00651
std_min= 0.28854, std_max= 0.41401, std_mean= 0.34883
val lr: [0.00012320696721311475], policy lr: [0.00014784836065573768]
Policy Loss: 0.0065139, | Entropy Bonus: -0, | Value Loss: 9.7692, | Advantage Loss: 2.6443
Time elapsed (s): 1.6522588729858398
Agent stdevs: 0.34883392
--------------------------------------------------------------------------------

Step 495
++++++++ Policy training ++++++++++
Current mean reward: 2277.082086 | mean episode length: 630.000000
val_loss= 7.50606
val_loss= 4.85879
val_loss= 6.93509
val_loss= 8.82860
val_loss= 8.07537
val_loss= 5.57060
val_loss= 7.27915
val_loss= 7.26511
val_loss= 5.10941
val_loss= 6.65703
adv_loss= 6.14350
adv_loss= 1.54439
adv_loss= 0.62437
adv_loss= 1.37322
adv_loss= 0.99871
adv_loss= 1.60638
adv_loss= 2.34108
adv_loss= 1.21114
adv_loss= 2.06444
adv_loss= 1.09053
surrogate= 0.02321, entropy= 1.06298, loss= 0.02321
surrogate=-0.01382, entropy= 1.06159, loss=-0.01382
surrogate= 0.02449, entropy= 1.05888, loss= 0.02449
surrogate=-0.01272, entropy= 1.05711, loss=-0.01272
surrogate= 0.02257, entropy= 1.05487, loss= 0.02257
surrogate=-0.01344, entropy= 1.05341, loss=-0.01344
surrogate= 0.01366, entropy= 1.05205, loss= 0.01366
surrogate=-0.02269, entropy= 1.04849, loss=-0.02269
surrogate=-0.01949, entropy= 1.04592, loss=-0.01949
surrogate=-0.01460, entropy= 1.04286, loss=-0.01460
std_min= 0.28378, std_max= 0.41489, std_mean= 0.34669
val lr: [0.00012295081967213115], policy lr: [0.00014754098360655736]
Policy Loss: -0.014602, | Entropy Bonus: -0, | Value Loss: 6.657, | Advantage Loss: 1.0905
Time elapsed (s): 1.6810717582702637
Agent stdevs: 0.34669265
--------------------------------------------------------------------------------

Step 496
++++++++ Policy training ++++++++++
Current mean reward: 1259.562841 | mean episode length: 346.400000
val_loss=11.23651
val_loss=12.20065
val_loss=10.79171
val_loss=14.67915
val_loss=14.79898
val_loss= 6.38197
val_loss= 8.42893
val_loss= 9.01418
val_loss= 7.86989
val_loss= 5.21317
adv_loss= 2.13877
adv_loss= 1.60442
adv_loss= 0.93447
adv_loss= 1.23793
adv_loss= 1.44058
adv_loss= 1.92339
adv_loss= 1.42309
adv_loss= 1.36231
adv_loss= 1.24118
adv_loss= 1.44003
surrogate= 0.01048, entropy= 1.04236, loss= 0.01048
surrogate= 0.00768, entropy= 1.04234, loss= 0.00768
surrogate= 0.02022, entropy= 1.04204, loss= 0.02022
surrogate=-0.00205, entropy= 1.04081, loss=-0.00205
surrogate=-0.02902, entropy= 1.04048, loss=-0.02902
surrogate=-0.02533, entropy= 1.04016, loss=-0.02533
surrogate=-0.03738, entropy= 1.04042, loss=-0.03738
surrogate=-0.02617, entropy= 1.04001, loss=-0.02617
surrogate=-0.04138, entropy= 1.04011, loss=-0.04138
surrogate=-0.03019, entropy= 1.03949, loss=-0.03019
std_min= 0.28508, std_max= 0.41467, std_mean= 0.34621
val lr: [0.00012269467213114755], policy lr: [0.00014723360655737703]
Policy Loss: -0.030193, | Entropy Bonus: -0, | Value Loss: 5.2132, | Advantage Loss: 1.44
Time elapsed (s): 1.689274549484253
Agent stdevs: 0.34621152
--------------------------------------------------------------------------------

Step 497
++++++++ Policy training ++++++++++
Current mean reward: 1457.657591 | mean episode length: 398.500000
val_loss=14.63516
val_loss=10.71793
val_loss=17.53084
val_loss=13.90035
val_loss=10.69362
val_loss= 8.95398
val_loss= 4.07224
val_loss= 4.65525
val_loss= 8.78910
val_loss=12.67967
adv_loss= 1.30313
adv_loss= 1.16216
adv_loss= 1.62123
adv_loss= 2.30419
adv_loss= 5.73833
adv_loss= 2.07576
adv_loss= 0.99585
adv_loss= 1.00101
adv_loss= 4.26436
adv_loss= 1.96207
surrogate= 0.00833, entropy= 1.03762, loss= 0.00833
surrogate=-0.02147, entropy= 1.03693, loss=-0.02147
surrogate=-0.01075, entropy= 1.03443, loss=-0.01075
surrogate=-0.02865, entropy= 1.03279, loss=-0.02865
surrogate=-0.01510, entropy= 1.03069, loss=-0.01510
surrogate= 0.00319, entropy= 1.02838, loss= 0.00319
surrogate= 0.01542, entropy= 1.02483, loss= 0.01542
surrogate=-0.01057, entropy= 1.02320, loss=-0.01057
surrogate=-0.03849, entropy= 1.02119, loss=-0.03849
surrogate=-0.04773, entropy= 1.01890, loss=-0.04773
std_min= 0.28325, std_max= 0.41203, std_mean= 0.34384
val lr: [0.00012243852459016395], policy lr: [0.0001469262295081967]
Policy Loss: -0.047733, | Entropy Bonus: -0, | Value Loss: 12.68, | Advantage Loss: 1.9621
Time elapsed (s): 1.7004334926605225
Agent stdevs: 0.34383798
--------------------------------------------------------------------------------

Step 498
++++++++ Policy training ++++++++++
Current mean reward: 1214.858980 | mean episode length: 339.200000
val_loss=49.16678
val_loss=21.40539
val_loss=18.40322
val_loss=36.10862
val_loss=14.16259
val_loss=22.89965
val_loss=13.17359
val_loss=17.13887
val_loss=29.13343
val_loss=35.12830
adv_loss= 5.60452
adv_loss= 4.53537
adv_loss= 1.78731
adv_loss= 5.99459
adv_loss= 9.40639
adv_loss= 2.24214
adv_loss= 6.47478
adv_loss= 4.15999
adv_loss= 5.11019
adv_loss= 3.50031
surrogate=-0.00412, entropy= 1.01661, loss=-0.00412
surrogate=-0.00789, entropy= 1.01593, loss=-0.00789
surrogate= 0.00923, entropy= 1.01442, loss= 0.00923
surrogate=-0.02970, entropy= 1.01386, loss=-0.02970
surrogate=-0.00854, entropy= 1.01244, loss=-0.00854
surrogate=-0.04203, entropy= 1.01326, loss=-0.04203
surrogate=-0.03973, entropy= 1.01213, loss=-0.03973
surrogate=-0.01319, entropy= 1.01107, loss=-0.01319
surrogate=-0.02387, entropy= 1.00980, loss=-0.02387
surrogate=-0.03225, entropy= 1.00877, loss=-0.03225
std_min= 0.28236, std_max= 0.41379, std_mean= 0.34288
val lr: [0.00012218237704918035], policy lr: [0.00014661885245901638]
Policy Loss: -0.032255, | Entropy Bonus: -0, | Value Loss: 35.128, | Advantage Loss: 3.5003
Time elapsed (s): 1.677711009979248
Agent stdevs: 0.3428811
--------------------------------------------------------------------------------

Step 499
++++++++ Policy training ++++++++++
Current mean reward: 2185.463178 | mean episode length: 599.666667
val_loss=16.30064
val_loss=18.68999
val_loss=17.44666
val_loss=10.65826
val_loss= 6.82187
val_loss=10.63514
val_loss=12.85439
val_loss= 7.86180
val_loss=13.36926
val_loss= 7.03161
adv_loss= 2.49191
adv_loss= 1.25485
adv_loss= 0.85437
adv_loss= 1.36430
adv_loss= 1.77105
adv_loss= 5.20911
adv_loss= 1.50686
adv_loss= 1.82509
adv_loss= 2.27212
adv_loss= 1.24882
surrogate=-0.00840, entropy= 1.00531, loss=-0.00840
surrogate= 0.01533, entropy= 1.00355, loss= 0.01533
surrogate= 0.01104, entropy= 1.00207, loss= 0.01104
surrogate= 0.06025, entropy= 1.00098, loss= 0.06025
surrogate=-0.03077, entropy= 1.00041, loss=-0.03077
surrogate= 0.00617, entropy= 0.99789, loss= 0.00617
surrogate=-0.01368, entropy= 0.99676, loss=-0.01368
surrogate=-0.02873, entropy= 0.99416, loss=-0.02873
surrogate=-0.01855, entropy= 0.99281, loss=-0.01855
surrogate= 0.00642, entropy= 0.99045, loss= 0.00642
std_min= 0.28042, std_max= 0.40798, std_mean= 0.34060
val lr: [0.00012192622950819673], policy lr: [0.00014631147540983605]
Policy Loss: 0.0064155, | Entropy Bonus: -0, | Value Loss: 7.0316, | Advantage Loss: 1.2488
Time elapsed (s): 1.6674861907958984
Agent stdevs: 0.34060228
--------------------------------------------------------------------------------

Step 500
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1678.9
++++++++ Policy training ++++++++++
Current mean reward: 1829.862237 | mean episode length: 500.500000
val_loss=17.37563
val_loss= 8.32149
val_loss=23.48351
val_loss=15.29931
val_loss= 6.46588
val_loss=11.38645
val_loss=17.03800
val_loss=16.42861
val_loss=10.41658
val_loss=10.30596
adv_loss= 1.02093
adv_loss= 1.16379
adv_loss= 1.26837
adv_loss= 1.12671
adv_loss= 1.67304
adv_loss= 1.56048
adv_loss= 0.81391
adv_loss= 1.19953
adv_loss= 1.56404
adv_loss= 1.52324
surrogate= 0.02286, entropy= 0.99031, loss= 0.02286
surrogate=-0.03153, entropy= 0.98978, loss=-0.03153
surrogate=-0.05051, entropy= 0.98895, loss=-0.05051
surrogate=-0.01783, entropy= 0.98945, loss=-0.01783
surrogate= 0.00364, entropy= 0.98874, loss= 0.00364
surrogate=-0.02156, entropy= 0.98757, loss=-0.02156
surrogate= 0.00596, entropy= 0.98637, loss= 0.00596
surrogate=-0.01565, entropy= 0.98468, loss=-0.01565
surrogate=-0.02632, entropy= 0.98332, loss=-0.02632
surrogate=-0.04136, entropy= 0.98141, loss=-0.04136
std_min= 0.27865, std_max= 0.40905, std_mean= 0.33978
val lr: [0.00012167008196721313], policy lr: [0.00014600409836065575]
Policy Loss: -0.041357, | Entropy Bonus: -0, | Value Loss: 10.306, | Advantage Loss: 1.5232
Time elapsed (s): 1.6500542163848877
Agent stdevs: 0.33977556
--------------------------------------------------------------------------------

Step 501
++++++++ Policy training ++++++++++
Current mean reward: 3215.266610 | mean episode length: 881.500000
val_loss=10.09879
val_loss= 6.02973
val_loss= 8.88714
val_loss= 4.72828
val_loss=10.08962
val_loss= 4.12718
val_loss= 6.62767
val_loss= 4.31172
val_loss= 3.06404
val_loss= 3.84055
adv_loss= 0.58499
adv_loss= 0.70912
adv_loss= 0.39721
adv_loss= 0.78096
adv_loss= 2.02375
adv_loss= 0.55045
adv_loss= 1.12291
adv_loss= 0.57510
adv_loss= 2.48544
adv_loss= 1.75392
surrogate= 0.00515, entropy= 0.98064, loss= 0.00515
surrogate=-0.00941, entropy= 0.97997, loss=-0.00941
surrogate=-0.00497, entropy= 0.97962, loss=-0.00497
surrogate=-0.00755, entropy= 0.97823, loss=-0.00755
surrogate= 0.01854, entropy= 0.97796, loss= 0.01854
surrogate=-0.00693, entropy= 0.97809, loss=-0.00693
surrogate=-0.01243, entropy= 0.97591, loss=-0.01243
surrogate=-0.03767, entropy= 0.97612, loss=-0.03767
surrogate=-0.04552, entropy= 0.97589, loss=-0.04552
surrogate= 0.01896, entropy= 0.97563, loss= 0.01896
std_min= 0.28245, std_max= 0.40721, std_mean= 0.33881
val lr: [0.0001214139344262295], policy lr: [0.00014569672131147537]
Policy Loss: 0.018959, | Entropy Bonus: -0, | Value Loss: 3.8405, | Advantage Loss: 1.7539
Time elapsed (s): 1.666907548904419
Agent stdevs: 0.33880565
--------------------------------------------------------------------------------

Step 502
++++++++ Policy training ++++++++++
Current mean reward: 2279.636520 | mean episode length: 622.000000
val_loss=12.55744
val_loss= 8.16289
val_loss=10.04476
val_loss= 6.00267
val_loss=11.43916
val_loss= 9.05867
val_loss= 5.42097
val_loss=11.71221
val_loss= 6.37442
val_loss= 7.83770
adv_loss= 0.79223
adv_loss= 1.58560
adv_loss= 2.46366
adv_loss= 1.94269
adv_loss= 0.52895
adv_loss= 0.94941
adv_loss= 0.89988
adv_loss= 0.39293
adv_loss= 1.17740
adv_loss= 2.36337
surrogate=-0.00825, entropy= 0.97694, loss=-0.00825
surrogate= 0.00106, entropy= 0.97668, loss= 0.00106
surrogate=-0.00367, entropy= 0.97722, loss=-0.00367
surrogate= 0.00605, entropy= 0.97846, loss= 0.00605
surrogate=-0.04600, entropy= 0.97924, loss=-0.04600
surrogate= 0.01386, entropy= 0.97987, loss= 0.01386
surrogate=-0.03152, entropy= 0.98053, loss=-0.03152
surrogate=-0.02884, entropy= 0.98086, loss=-0.02884
surrogate=-0.00419, entropy= 0.98008, loss=-0.00419
surrogate=-0.01754, entropy= 0.98132, loss=-0.01754
std_min= 0.28416, std_max= 0.40998, std_mean= 0.33953
val lr: [0.00012115778688524589], policy lr: [0.00014538934426229505]
Policy Loss: -0.017536, | Entropy Bonus: -0, | Value Loss: 7.8377, | Advantage Loss: 2.3634
Time elapsed (s): 1.7011325359344482
Agent stdevs: 0.3395301
--------------------------------------------------------------------------------

Step 503
++++++++ Policy training ++++++++++
Current mean reward: 2266.285433 | mean episode length: 638.000000
val_loss=171.81383
val_loss=1678.95557
val_loss=197.97557
val_loss=228.65752
val_loss=249.69908
val_loss=2066.30469
val_loss=541.83075
val_loss=50.82220
val_loss=147.14566
val_loss=79.44659
adv_loss= 1.31835
adv_loss= 1.76451
adv_loss= 1.70779
adv_loss= 3.09965
adv_loss= 1.52782
adv_loss= 1.83441
adv_loss= 1.41066
adv_loss= 2.38286
adv_loss= 2.79469
adv_loss= 1.23835
surrogate=-0.03223, entropy= 0.98103, loss=-0.03223
surrogate= 0.01802, entropy= 0.98201, loss= 0.01802
surrogate=-0.01856, entropy= 0.98325, loss=-0.01856
surrogate=-0.00855, entropy= 0.98416, loss=-0.00855
surrogate=-0.00512, entropy= 0.98568, loss=-0.00512
surrogate= 0.04087, entropy= 0.98533, loss= 0.04087
surrogate=-0.00430, entropy= 0.98576, loss=-0.00430
surrogate=-0.00686, entropy= 0.98637, loss=-0.00686
surrogate=-0.03303, entropy= 0.98573, loss=-0.03303
surrogate=-0.01757, entropy= 0.98653, loss=-0.01757
std_min= 0.28488, std_max= 0.41196, std_mean= 0.34020
val lr: [0.00012090163934426229], policy lr: [0.00014508196721311472]
Policy Loss: -0.017569, | Entropy Bonus: -0, | Value Loss: 79.447, | Advantage Loss: 1.2383
Time elapsed (s): 1.7637202739715576
Agent stdevs: 0.3402001
--------------------------------------------------------------------------------

Step 504
++++++++ Policy training ++++++++++
Current mean reward: 2090.791082 | mean episode length: 570.333333
val_loss=17.78811
val_loss=11.77679
val_loss=14.04066
val_loss= 9.90007
val_loss=11.74018
val_loss= 9.35266
val_loss=11.20143
val_loss= 4.47863
val_loss= 7.38456
val_loss= 6.67613
adv_loss= 3.14007
adv_loss= 1.99393
adv_loss= 1.22690
adv_loss= 0.85868
adv_loss= 1.49383
adv_loss= 3.65568
adv_loss= 2.48924
adv_loss= 2.05954
adv_loss= 1.40284
adv_loss= 4.83026
surrogate= 0.01548, entropy= 0.98590, loss= 0.01548
surrogate=-0.00662, entropy= 0.98588, loss=-0.00662
surrogate=-0.02670, entropy= 0.98566, loss=-0.02670
surrogate=-0.00432, entropy= 0.98639, loss=-0.00432
surrogate=-0.03739, entropy= 0.98664, loss=-0.03739
surrogate=-0.01163, entropy= 0.98689, loss=-0.01163
surrogate=-0.01745, entropy= 0.98718, loss=-0.01745
surrogate=-0.02022, entropy= 0.98762, loss=-0.02022
surrogate=-0.01869, entropy= 0.98760, loss=-0.01869
surrogate=-0.05340, entropy= 0.98936, loss=-0.05340
std_min= 0.28496, std_max= 0.41197, std_mean= 0.34050
val lr: [0.00012064549180327869], policy lr: [0.00014477459016393443]
Policy Loss: -0.053395, | Entropy Bonus: -0, | Value Loss: 6.6761, | Advantage Loss: 4.8303
Time elapsed (s): 1.6725831031799316
Agent stdevs: 0.34049833
--------------------------------------------------------------------------------

Step 505
++++++++ Policy training ++++++++++
Current mean reward: 2894.660718 | mean episode length: 790.500000
val_loss= 7.11970
val_loss= 4.88524
val_loss= 7.70656
val_loss= 8.55009
val_loss= 7.18294
val_loss= 7.56529
val_loss= 5.95754
val_loss= 8.43909
val_loss= 5.61229
val_loss= 2.82899
adv_loss= 4.94421
adv_loss= 0.84188
adv_loss= 0.93421
adv_loss= 1.31768
adv_loss= 0.60535
adv_loss= 0.41999
adv_loss= 0.72730
adv_loss= 1.25879
adv_loss= 0.76806
adv_loss= 1.13703
surrogate=-0.00981, entropy= 0.99158, loss=-0.00981
surrogate= 0.03780, entropy= 0.99526, loss= 0.03780
surrogate= 0.01478, entropy= 0.99770, loss= 0.01478
surrogate=-0.01665, entropy= 1.00090, loss=-0.01665
surrogate=-0.03789, entropy= 1.00305, loss=-0.03789
surrogate= 0.00454, entropy= 1.00563, loss= 0.00454
surrogate=-0.03183, entropy= 1.00722, loss=-0.03183
surrogate=-0.01604, entropy= 1.00859, loss=-0.01604
surrogate=-0.01608, entropy= 1.01103, loss=-0.01608
surrogate= 0.00219, entropy= 1.01278, loss= 0.00219
std_min= 0.28855, std_max= 0.41515, std_mean= 0.34311
val lr: [0.00012038934426229509], policy lr: [0.0001444672131147541]
Policy Loss: 0.0021931, | Entropy Bonus: -0, | Value Loss: 2.829, | Advantage Loss: 1.137
Time elapsed (s): 1.6844828128814697
Agent stdevs: 0.34310922
--------------------------------------------------------------------------------

Step 506
++++++++ Policy training ++++++++++
Current mean reward: 2181.254150 | mean episode length: 608.333333
val_loss=149.84619
val_loss=47.85230
val_loss=28.74937
val_loss=334.81024
val_loss=21.75930
val_loss=137.68379
val_loss=35.78705
val_loss=126.90368
val_loss=59.67849
val_loss=13.62991
adv_loss= 1.86105
adv_loss= 4.09538
adv_loss= 1.17121
adv_loss= 0.98132
adv_loss= 3.15826
adv_loss= 0.77505
adv_loss=217.83817
adv_loss= 1.12750
adv_loss= 0.96632
adv_loss= 2.74808
surrogate= 0.01526, entropy= 1.01424, loss= 0.01526
surrogate= 0.00022, entropy= 1.01376, loss= 0.00022
surrogate=-0.00523, entropy= 1.01351, loss=-0.00523
surrogate=-0.01069, entropy= 1.01312, loss=-0.01069
surrogate=-0.01641, entropy= 1.01305, loss=-0.01641
surrogate=-0.00508, entropy= 1.01272, loss=-0.00508
surrogate=-0.02546, entropy= 1.01297, loss=-0.02546
surrogate= 0.01692, entropy= 1.01401, loss= 0.01692
surrogate=-0.02685, entropy= 1.01347, loss=-0.02685
surrogate=-0.03119, entropy= 1.01379, loss=-0.03119
std_min= 0.28926, std_max= 0.41272, std_mean= 0.34302
val lr: [0.00012013319672131148], policy lr: [0.00014415983606557377]
Policy Loss: -0.031185, | Entropy Bonus: -0, | Value Loss: 13.63, | Advantage Loss: 2.7481
Time elapsed (s): 1.6769976615905762
Agent stdevs: 0.3430151
--------------------------------------------------------------------------------

Step 507
++++++++ Policy training ++++++++++
Current mean reward: 2094.674432 | mean episode length: 571.000000
val_loss=16.72310
val_loss=19.72816
val_loss= 9.30227
val_loss= 8.86793
val_loss=11.85542
val_loss=11.40977
val_loss=15.43594
val_loss=16.01991
val_loss=13.47958
val_loss= 9.30782
adv_loss= 2.51117
adv_loss= 2.15460
adv_loss= 1.92328
adv_loss= 1.74927
adv_loss= 1.42594
adv_loss= 2.64402
adv_loss= 1.01858
adv_loss= 1.39967
adv_loss= 0.74360
adv_loss= 1.24250
surrogate=-0.00603, entropy= 1.01283, loss=-0.00603
surrogate= 0.00668, entropy= 1.01386, loss= 0.00668
surrogate=-0.01871, entropy= 1.01503, loss=-0.01871
surrogate=-0.01739, entropy= 1.01370, loss=-0.01739
surrogate= 0.00155, entropy= 1.01421, loss= 0.00155
surrogate=-0.00867, entropy= 1.01459, loss=-0.00867
surrogate=-0.02218, entropy= 1.01524, loss=-0.02218
surrogate=-0.02375, entropy= 1.01501, loss=-0.02375
surrogate= 0.00763, entropy= 1.01438, loss= 0.00763
surrogate=-0.02992, entropy= 1.01412, loss=-0.02992
std_min= 0.28818, std_max= 0.41407, std_mean= 0.34320
val lr: [0.00011987704918032788], policy lr: [0.00014385245901639345]
Policy Loss: -0.029924, | Entropy Bonus: -0, | Value Loss: 9.3078, | Advantage Loss: 1.2425
Time elapsed (s): 1.6932494640350342
Agent stdevs: 0.34319845
--------------------------------------------------------------------------------

Step 508
++++++++ Policy training ++++++++++
Current mean reward: 2033.622688 | mean episode length: 573.000000
val_loss=32.86770
val_loss=80.39461
val_loss=696.80646
val_loss=496.50339
val_loss=2134.64380
val_loss=44.31927
val_loss=317.52316
val_loss=43.54004
val_loss=1139.99707
val_loss=227.83066
adv_loss= 2.25352
adv_loss= 2.66822
adv_loss= 4.98372
adv_loss= 2.54337
adv_loss= 1.04877
adv_loss= 2.28804
adv_loss= 3.50088
adv_loss= 8.52646
adv_loss= 1.12741
adv_loss= 1.55054
surrogate= 0.02559, entropy= 1.01506, loss= 0.02559
surrogate= 0.00572, entropy= 1.01573, loss= 0.00572
surrogate= 0.03332, entropy= 1.01577, loss= 0.03332
surrogate=-0.00864, entropy= 1.01711, loss=-0.00864
surrogate=-0.02968, entropy= 1.01747, loss=-0.02968
surrogate=-0.01117, entropy= 1.01837, loss=-0.01117
surrogate=-0.01500, entropy= 1.01856, loss=-0.01500
surrogate=-0.01997, entropy= 1.01918, loss=-0.01997
surrogate=-0.01844, entropy= 1.01951, loss=-0.01844
surrogate=-0.00777, entropy= 1.02074, loss=-0.00777
std_min= 0.28796, std_max= 0.41284, std_mean= 0.34385
val lr: [0.00011962090163934428], policy lr: [0.00014354508196721312]
Policy Loss: -0.0077689, | Entropy Bonus: -0, | Value Loss: 227.83, | Advantage Loss: 1.5505
Time elapsed (s): 1.717094898223877
Agent stdevs: 0.34385267
--------------------------------------------------------------------------------

Step 509
++++++++ Policy training ++++++++++
Current mean reward: 3115.065740 | mean episode length: 866.000000
val_loss=44.09702
val_loss=83.84120
val_loss=35.12203
val_loss=72.88665
val_loss=678.03027
val_loss=20.79106
val_loss=1026.63647
val_loss=165.99922
val_loss=46.17541
val_loss=1209.86780
adv_loss= 1.56467
adv_loss= 2.77671
adv_loss= 1.39672
adv_loss= 2.44479
adv_loss= 2.98516
adv_loss= 2.91312
adv_loss= 1.41053
adv_loss= 1.94687
adv_loss= 1.38871
adv_loss= 1.75777
surrogate=-0.00220, entropy= 1.01904, loss=-0.00220
surrogate=-0.02359, entropy= 1.01901, loss=-0.02359
surrogate= 0.00941, entropy= 1.01896, loss= 0.00941
surrogate=-0.02493, entropy= 1.01815, loss=-0.02493
surrogate=-0.02383, entropy= 1.01773, loss=-0.02383
surrogate=-0.03202, entropy= 1.01726, loss=-0.03202
surrogate=-0.00087, entropy= 1.01636, loss=-0.00087
surrogate=-0.02177, entropy= 1.01578, loss=-0.02177
surrogate=-0.02438, entropy= 1.01612, loss=-0.02438
surrogate=-0.01233, entropy= 1.01483, loss=-0.01233
std_min= 0.28593, std_max= 0.41356, std_mean= 0.34334
val lr: [0.00011936475409836065], policy lr: [0.00014323770491803277]
Policy Loss: -0.01233, | Entropy Bonus: -0, | Value Loss: 1209.9, | Advantage Loss: 1.7578
Time elapsed (s): 1.6859455108642578
Agent stdevs: 0.34333947
--------------------------------------------------------------------------------

Step 510
++++++++ Policy training ++++++++++
Current mean reward: 2074.139943 | mean episode length: 580.000000
val_loss=453.53687
val_loss=24.16065
val_loss=722.09344
val_loss=38.21757
val_loss=614.53992
val_loss=29.32321
val_loss=1468.30298
val_loss=1448.95361
val_loss=45.79689
val_loss=295.60370
adv_loss= 2.12595
adv_loss= 2.38835
adv_loss= 1.73449
adv_loss= 3.52050
adv_loss= 4.13174
adv_loss= 3.26549
adv_loss=1835.81323
adv_loss=1835.73059
adv_loss= 2.76533
adv_loss= 1.96230
surrogate=-0.02116, entropy= 1.01443, loss=-0.02116
surrogate= 0.00748, entropy= 1.01538, loss= 0.00748
surrogate=-0.02599, entropy= 1.01656, loss=-0.02599
surrogate= 0.00465, entropy= 1.01633, loss= 0.00465
surrogate=-0.00245, entropy= 1.01513, loss=-0.00245
surrogate= 0.03028, entropy= 1.01581, loss= 0.03028
surrogate=-0.01865, entropy= 1.01647, loss=-0.01865
surrogate=-0.02029, entropy= 1.01724, loss=-0.02029
surrogate= 0.02406, entropy= 1.01692, loss= 0.02406
surrogate=-0.02536, entropy= 1.01737, loss=-0.02536
std_min= 0.28566, std_max= 0.41316, std_mean= 0.34362
val lr: [0.00011910860655737704], policy lr: [0.00014293032786885244]
Policy Loss: -0.025356, | Entropy Bonus: -0, | Value Loss: 295.6, | Advantage Loss: 1.9623
Time elapsed (s): 1.6657133102416992
Agent stdevs: 0.3436177
--------------------------------------------------------------------------------

Step 511
++++++++ Policy training ++++++++++
Current mean reward: 1315.758042 | mean episode length: 359.600000
val_loss=22.00633
val_loss=27.91271
val_loss=18.59499
val_loss=15.11644
val_loss=21.26186
val_loss=19.84110
val_loss=18.40435
val_loss=15.19125
val_loss=11.51138
val_loss=18.32038
adv_loss= 2.36191
adv_loss= 2.46313
adv_loss= 2.90003
adv_loss= 2.10057
adv_loss= 2.18217
adv_loss= 2.80212
adv_loss= 3.83548
adv_loss= 4.75979
adv_loss= 2.50101
adv_loss= 4.82285
surrogate= 0.02060, entropy= 1.01608, loss= 0.02060
surrogate=-0.01933, entropy= 1.01447, loss=-0.01933
surrogate=-0.01643, entropy= 1.01471, loss=-0.01643
surrogate=-0.02067, entropy= 1.01437, loss=-0.02067
surrogate=-0.01244, entropy= 1.01284, loss=-0.01244
surrogate=-0.00927, entropy= 1.01263, loss=-0.00927
surrogate=-0.00729, entropy= 1.01180, loss=-0.00729
surrogate=-0.03498, entropy= 1.00972, loss=-0.03498
surrogate=-0.02213, entropy= 1.00829, loss=-0.02213
surrogate= 0.01061, entropy= 1.00721, loss= 0.01061
std_min= 0.28280, std_max= 0.41266, std_mean= 0.34262
val lr: [0.00011885245901639344], policy lr: [0.00014262295081967212]
Policy Loss: 0.010612, | Entropy Bonus: -0, | Value Loss: 18.32, | Advantage Loss: 4.8229
Time elapsed (s): 1.656543493270874
Agent stdevs: 0.34262088
--------------------------------------------------------------------------------

Step 512
++++++++ Policy training ++++++++++
Current mean reward: 1275.871528 | mean episode length: 355.400000
val_loss=12.56500
val_loss=17.31639
val_loss=15.36766
val_loss=23.88951
val_loss=19.05939
val_loss=14.11728
val_loss=11.43836
val_loss=14.55039
val_loss=18.11416
val_loss= 9.05859
adv_loss= 3.47708
adv_loss= 3.34798
adv_loss= 2.82828
adv_loss= 1.88914
adv_loss= 4.55724
adv_loss= 2.13576
adv_loss= 5.76343
adv_loss= 2.54680
adv_loss= 2.99749
adv_loss= 4.05048
surrogate= 0.02534, entropy= 1.00794, loss= 0.02534
surrogate=-0.00171, entropy= 1.00632, loss=-0.00171
surrogate=-0.02401, entropy= 1.00428, loss=-0.02401
surrogate= 0.01819, entropy= 1.00319, loss= 0.01819
surrogate=-0.01831, entropy= 1.00165, loss=-0.01831
surrogate= 0.00295, entropy= 0.99997, loss= 0.00295
surrogate= 0.01000, entropy= 0.99869, loss= 0.01000
surrogate=-0.01548, entropy= 0.99773, loss=-0.01548
surrogate= 0.00099, entropy= 0.99588, loss= 0.00099
surrogate=-0.04521, entropy= 0.99477, loss=-0.04521
std_min= 0.28302, std_max= 0.40949, std_mean= 0.34102
val lr: [0.00011859631147540984], policy lr: [0.0001423155737704918]
Policy Loss: -0.045213, | Entropy Bonus: -0, | Value Loss: 9.0586, | Advantage Loss: 4.0505
Time elapsed (s): 1.6581730842590332
Agent stdevs: 0.34101805
--------------------------------------------------------------------------------

Step 513
++++++++ Policy training ++++++++++
Current mean reward: 2729.243356 | mean episode length: 764.000000
val_loss=261.10773
val_loss=118.59538
val_loss=1800.89551
val_loss=196.25990
val_loss=1656.23010
val_loss=151.79604
val_loss=134.68954
val_loss=117.35770
val_loss=1287.87317
val_loss=401.20975
adv_loss= 2.75717
adv_loss= 1.19789
adv_loss= 1.91730
adv_loss= 1.70464
adv_loss= 0.94529
adv_loss= 0.97278
adv_loss= 1.50667
adv_loss= 1.18289
adv_loss= 1.81968
adv_loss= 1.31066
surrogate=-0.01149, entropy= 0.99360, loss=-0.01149
surrogate=-0.01006, entropy= 0.99340, loss=-0.01006
surrogate= 0.01321, entropy= 0.99188, loss= 0.01321
surrogate= 0.00234, entropy= 0.99108, loss= 0.00234
surrogate=-0.02190, entropy= 0.99027, loss=-0.02190
surrogate=-0.02948, entropy= 0.99011, loss=-0.02948
surrogate= 0.01357, entropy= 0.98901, loss= 0.01357
surrogate=-0.01845, entropy= 0.98854, loss=-0.01845
surrogate=-0.01562, entropy= 0.98878, loss=-0.01562
surrogate=-0.02024, entropy= 0.98849, loss=-0.02024
std_min= 0.28104, std_max= 0.40955, std_mean= 0.34045
val lr: [0.00011834016393442624], policy lr: [0.00014200819672131147]
Policy Loss: -0.02024, | Entropy Bonus: -0, | Value Loss: 401.21, | Advantage Loss: 1.3107
Time elapsed (s): 1.6669797897338867
Agent stdevs: 0.34044847
--------------------------------------------------------------------------------

Step 514
++++++++ Policy training ++++++++++
Current mean reward: 2469.897470 | mean episode length: 677.500000
val_loss= 7.10165
val_loss= 7.29802
val_loss= 9.09935
val_loss= 4.32845
val_loss= 5.57374
val_loss= 5.42206
val_loss= 4.60656
val_loss= 3.61294
val_loss= 2.77091
val_loss= 4.57403
adv_loss= 1.09230
adv_loss= 2.38541
adv_loss= 2.64924
adv_loss= 2.36500
adv_loss= 0.69804
adv_loss= 2.74603
adv_loss= 0.64695
adv_loss= 1.15474
adv_loss= 1.68305
adv_loss= 1.19615
surrogate= 0.00416, entropy= 0.98861, loss= 0.00416
surrogate=-0.00974, entropy= 0.98825, loss=-0.00974
surrogate= 0.00633, entropy= 0.98856, loss= 0.00633
surrogate= 0.00699, entropy= 0.98829, loss= 0.00699
surrogate=-0.01960, entropy= 0.98807, loss=-0.01960
surrogate= 0.01286, entropy= 0.98805, loss= 0.01286
surrogate=-0.00611, entropy= 0.98838, loss=-0.00611
surrogate= 0.02784, entropy= 0.98802, loss= 0.02784
surrogate=-0.02371, entropy= 0.98628, loss=-0.02371
surrogate=-0.01303, entropy= 0.98685, loss=-0.01303
std_min= 0.28010, std_max= 0.40833, std_mean= 0.34025
val lr: [0.00011808401639344262], policy lr: [0.00014170081967213114]
Policy Loss: -0.013031, | Entropy Bonus: -0, | Value Loss: 4.574, | Advantage Loss: 1.1961
Time elapsed (s): 1.6996853351593018
Agent stdevs: 0.3402455
--------------------------------------------------------------------------------

Step 515
++++++++ Policy training ++++++++++
Current mean reward: 1193.419976 | mean episode length: 330.400000
val_loss=12.93753
val_loss=10.43147
val_loss=21.98362
val_loss=37.48170
val_loss=23.50130
val_loss=23.91462
val_loss=19.48153
val_loss=16.31373
val_loss= 8.79716
val_loss=13.52714
adv_loss= 1.40972
adv_loss= 1.14749
adv_loss= 8.19419
adv_loss= 6.36944
adv_loss= 5.82770
adv_loss= 1.49122
adv_loss= 2.50039
adv_loss= 1.40252
adv_loss= 1.62899
adv_loss= 3.98664
surrogate= 0.00081, entropy= 0.98702, loss= 0.00081
surrogate=-0.01750, entropy= 0.98679, loss=-0.01750
surrogate=-0.00095, entropy= 0.98667, loss=-0.00095
surrogate= 0.01798, entropy= 0.98655, loss= 0.01798
surrogate=-0.03580, entropy= 0.98497, loss=-0.03580
surrogate=-0.01277, entropy= 0.98454, loss=-0.01277
surrogate=-0.00650, entropy= 0.98447, loss=-0.00650
surrogate=-0.00257, entropy= 0.98286, loss=-0.00257
surrogate=-0.04623, entropy= 0.98284, loss=-0.04623
surrogate=-0.03516, entropy= 0.98123, loss=-0.03516
std_min= 0.27990, std_max= 0.40828, std_mean= 0.33962
val lr: [0.00011782786885245902], policy lr: [0.00014139344262295082]
Policy Loss: -0.035157, | Entropy Bonus: -0, | Value Loss: 13.527, | Advantage Loss: 3.9866
Time elapsed (s): 1.6784138679504395
Agent stdevs: 0.33962497
--------------------------------------------------------------------------------

Step 516
++++++++ Policy training ++++++++++
Current mean reward: 1925.888089 | mean episode length: 528.000000
val_loss=10.05864
val_loss=10.00688
val_loss= 8.76864
val_loss=13.19770
val_loss=10.08955
val_loss=12.72477
val_loss= 8.73690
val_loss= 3.51433
val_loss=16.08121
val_loss=12.29443
adv_loss= 1.09849
adv_loss= 4.84958
adv_loss= 2.50744
adv_loss= 1.76560
adv_loss= 5.97141
adv_loss= 1.21058
adv_loss= 1.71768
adv_loss= 5.82638
adv_loss= 7.45545
adv_loss= 1.19534
surrogate= 0.00937, entropy= 0.97930, loss= 0.00937
surrogate=-0.02542, entropy= 0.97907, loss=-0.02542
surrogate=-0.02787, entropy= 0.97903, loss=-0.02787
surrogate=-0.02557, entropy= 0.97893, loss=-0.02557
surrogate=-0.02178, entropy= 0.97684, loss=-0.02178
surrogate=-0.02443, entropy= 0.97545, loss=-0.02443
surrogate=-0.03458, entropy= 0.97415, loss=-0.03458
surrogate=-0.00368, entropy= 0.97351, loss=-0.00368
surrogate=-0.01360, entropy= 0.97288, loss=-0.01360
surrogate=-0.01813, entropy= 0.97089, loss=-0.01813
std_min= 0.27853, std_max= 0.40823, std_mean= 0.33858
val lr: [0.00011757172131147542], policy lr: [0.0001410860655737705]
Policy Loss: -0.018134, | Entropy Bonus: -0, | Value Loss: 12.294, | Advantage Loss: 1.1953
Time elapsed (s): 1.6796247959136963
Agent stdevs: 0.3385755
--------------------------------------------------------------------------------

Step 517
++++++++ Policy training ++++++++++
Current mean reward: 2395.089334 | mean episode length: 651.500000
val_loss=14.93464
val_loss= 4.93392
val_loss= 9.57159
val_loss= 4.88291
val_loss= 5.12973
val_loss= 5.95339
val_loss= 9.02183
val_loss= 4.21517
val_loss= 3.51370
val_loss= 6.87416
adv_loss= 1.59941
adv_loss= 0.60331
adv_loss= 1.38405
adv_loss= 0.40105
adv_loss= 0.39931
adv_loss= 0.66813
adv_loss= 1.28956
adv_loss= 1.76847
adv_loss= 1.18973
adv_loss= 0.92378
surrogate= 0.04051, entropy= 0.96943, loss= 0.04051
surrogate=-0.01309, entropy= 0.96987, loss=-0.01309
surrogate=-0.01004, entropy= 0.96860, loss=-0.01004
surrogate=-0.03856, entropy= 0.96708, loss=-0.03856
surrogate= 0.02941, entropy= 0.96581, loss= 0.02941
surrogate= 0.00563, entropy= 0.96569, loss= 0.00563
surrogate=-0.04108, entropy= 0.96420, loss=-0.04108
surrogate=-0.01047, entropy= 0.96177, loss=-0.01047
surrogate=-0.01685, entropy= 0.96246, loss=-0.01685
surrogate= 0.00967, entropy= 0.96072, loss= 0.00967
std_min= 0.27709, std_max= 0.40577, std_mean= 0.33739
val lr: [0.00011731557377049182], policy lr: [0.00014077868852459017]
Policy Loss: 0.00967, | Entropy Bonus: -0, | Value Loss: 6.8742, | Advantage Loss: 0.92378
Time elapsed (s): 1.6617965698242188
Agent stdevs: 0.33738694
--------------------------------------------------------------------------------

Step 518
++++++++ Policy training ++++++++++
Current mean reward: 2658.132872 | mean episode length: 757.000000
val_loss=855.51257
val_loss=20.16177
val_loss=28.24214
val_loss=188.80898
val_loss=1122.43628
val_loss=25.91612
val_loss=35.70791
val_loss=913.39929
val_loss=966.49402
val_loss=3245.27563
adv_loss= 2.99099
adv_loss= 2.65959
adv_loss= 2.89490
adv_loss= 2.21537
adv_loss= 3.39297
adv_loss= 2.03503
adv_loss= 2.05157
adv_loss= 4.69642
adv_loss= 6.55113
adv_loss= 3.49660
surrogate= 0.00009, entropy= 0.95938, loss= 0.00009
surrogate=-0.02709, entropy= 0.95837, loss=-0.02709
surrogate=-0.02518, entropy= 0.95729, loss=-0.02518
surrogate=-0.01077, entropy= 0.95461, loss=-0.01077
surrogate= 0.01810, entropy= 0.95189, loss= 0.01810
surrogate= 0.01851, entropy= 0.95103, loss= 0.01851
surrogate= 0.02271, entropy= 0.94925, loss= 0.02271
surrogate=-0.00143, entropy= 0.94790, loss=-0.00143
surrogate=-0.00550, entropy= 0.94696, loss=-0.00550
surrogate= 0.01991, entropy= 0.94524, loss= 0.01991
std_min= 0.27424, std_max= 0.40662, std_mean= 0.33593
val lr: [0.00011705942622950818], policy lr: [0.0001404713114754098]
Policy Loss: 0.019914, | Entropy Bonus: -0, | Value Loss: 3245.3, | Advantage Loss: 3.4966
Time elapsed (s): 1.6959528923034668
Agent stdevs: 0.33592772
--------------------------------------------------------------------------------

Step 519
++++++++ Policy training ++++++++++
Current mean reward: 3217.336845 | mean episode length: 900.000000
val_loss=24.16306
val_loss=21.25061
val_loss=48.64648
val_loss=22.40092
val_loss=25.48432
val_loss=155.43887
val_loss=334.41803
val_loss=43.96316
val_loss=535.84552
val_loss=226.44368
adv_loss= 1.17952
adv_loss= 1.76474
adv_loss= 1.32756
adv_loss= 1.06752
adv_loss= 1.29905
adv_loss= 2.21371
adv_loss= 2.18738
adv_loss= 2.72952
adv_loss= 2.65063
adv_loss= 1.55408
surrogate= 0.01433, entropy= 0.94456, loss= 0.01433
surrogate=-0.02994, entropy= 0.94631, loss=-0.02994
surrogate=-0.02046, entropy= 0.94970, loss=-0.02046
surrogate=-0.00237, entropy= 0.95132, loss=-0.00237
surrogate= 0.04739, entropy= 0.95095, loss= 0.04739
surrogate= 0.00221, entropy= 0.95240, loss= 0.00221
surrogate=-0.01113, entropy= 0.95373, loss=-0.01113
surrogate=-0.01592, entropy= 0.95394, loss=-0.01592
surrogate=-0.04257, entropy= 0.95486, loss=-0.04257
surrogate=-0.01902, entropy= 0.95609, loss=-0.01902
std_min= 0.27447, std_max= 0.41048, std_mean= 0.33736
val lr: [0.00011680327868852458], policy lr: [0.0001401639344262295]
Policy Loss: -0.019023, | Entropy Bonus: -0, | Value Loss: 226.44, | Advantage Loss: 1.5541
Time elapsed (s): 1.6969974040985107
Agent stdevs: 0.33736253
--------------------------------------------------------------------------------

Step 520
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2278
++++++++ Policy training ++++++++++
Current mean reward: 2910.984478 | mean episode length: 799.000000
val_loss=11.69812
val_loss= 9.23623
val_loss=12.49316
val_loss=11.77851
val_loss= 9.65211
val_loss= 9.03453
val_loss= 6.36209
val_loss= 6.27070
val_loss= 7.38935
val_loss=11.36736
adv_loss= 1.22044
adv_loss= 1.60146
adv_loss= 1.70898
adv_loss= 1.26965
adv_loss= 2.61088
adv_loss= 1.66678
adv_loss= 1.51684
adv_loss= 0.73192
adv_loss= 1.58173
adv_loss= 3.77658
surrogate= 0.02102, entropy= 0.95603, loss= 0.02102
surrogate= 0.01647, entropy= 0.95767, loss= 0.01647
surrogate=-0.01757, entropy= 0.95828, loss=-0.01757
surrogate=-0.02874, entropy= 0.95875, loss=-0.02874
surrogate= 0.00455, entropy= 0.95957, loss= 0.00455
surrogate=-0.03493, entropy= 0.96098, loss=-0.03493
surrogate=-0.02583, entropy= 0.96145, loss=-0.02583
surrogate=-0.01967, entropy= 0.96187, loss=-0.01967
surrogate=-0.02701, entropy= 0.96237, loss=-0.02701
surrogate= 0.00720, entropy= 0.96284, loss= 0.00720
std_min= 0.27521, std_max= 0.41188, std_mean= 0.33815
val lr: [0.00011654713114754098], policy lr: [0.00013985655737704916]
Policy Loss: 0.0072036, | Entropy Bonus: -0, | Value Loss: 11.367, | Advantage Loss: 3.7766
Time elapsed (s): 1.698549747467041
Agent stdevs: 0.3381492
--------------------------------------------------------------------------------

Step 521
++++++++ Policy training ++++++++++
Current mean reward: 3221.584981 | mean episode length: 902.000000
val_loss=221.94376
val_loss=356.36758
val_loss=560.61499
val_loss=499.02438
val_loss=1890.94788
val_loss=31.87330
val_loss=29.47978
val_loss=208.79387
val_loss=31.31669
val_loss=46.90421
adv_loss=1528.79199
adv_loss= 3.50371
adv_loss= 3.04226
adv_loss=1520.20386
adv_loss= 6.30656
adv_loss= 2.93375
adv_loss= 2.47208
adv_loss= 6.04052
adv_loss= 1.45942
adv_loss= 1.98797
surrogate=-0.02302, entropy= 0.96048, loss=-0.02302
surrogate= 0.01238, entropy= 0.95992, loss= 0.01238
surrogate=-0.00395, entropy= 0.95735, loss=-0.00395
surrogate=-0.01729, entropy= 0.95599, loss=-0.01729
surrogate=-0.01835, entropy= 0.95606, loss=-0.01835
surrogate=-0.03803, entropy= 0.95515, loss=-0.03803
surrogate=-0.02540, entropy= 0.95456, loss=-0.02540
surrogate=-0.01282, entropy= 0.95442, loss=-0.01282
surrogate=-0.02491, entropy= 0.95452, loss=-0.02491
surrogate=-0.04282, entropy= 0.95374, loss=-0.04282
std_min= 0.27547, std_max= 0.41166, std_mean= 0.33713
val lr: [0.00011629098360655738], policy lr: [0.00013954918032786884]
Policy Loss: -0.042822, | Entropy Bonus: -0, | Value Loss: 46.904, | Advantage Loss: 1.988
Time elapsed (s): 1.671692132949829
Agent stdevs: 0.3371272
--------------------------------------------------------------------------------

Step 522
++++++++ Policy training ++++++++++
Current mean reward: 1592.970201 | mean episode length: 436.500000
val_loss=10.82297
val_loss= 5.95085
val_loss= 8.52612
val_loss=11.80993
val_loss= 3.65308
val_loss= 4.90197
val_loss= 5.44258
val_loss= 4.41555
val_loss= 6.04076
val_loss= 4.01261
adv_loss= 2.44833
adv_loss= 2.00476
adv_loss= 1.94220
adv_loss= 1.16721
adv_loss= 1.96750
adv_loss= 2.36671
adv_loss= 2.05945
adv_loss= 4.82621
adv_loss= 0.99417
adv_loss= 1.65883
surrogate= 0.05214, entropy= 0.95370, loss= 0.05214
surrogate= 0.01515, entropy= 0.95467, loss= 0.01515
surrogate= 0.01501, entropy= 0.95362, loss= 0.01501
surrogate=-0.00926, entropy= 0.95369, loss=-0.00926
surrogate=-0.02545, entropy= 0.95261, loss=-0.02545
surrogate=-0.01042, entropy= 0.95182, loss=-0.01042
surrogate=-0.03413, entropy= 0.95113, loss=-0.03413
surrogate=-0.02478, entropy= 0.95004, loss=-0.02478
surrogate= 0.00064, entropy= 0.95010, loss= 0.00064
surrogate= 0.00230, entropy= 0.94957, loss= 0.00230
std_min= 0.27742, std_max= 0.41080, std_mean= 0.33651
val lr: [0.00011603483606557377], policy lr: [0.0001392418032786885]
Policy Loss: 0.0023031, | Entropy Bonus: -0, | Value Loss: 4.0126, | Advantage Loss: 1.6588
Time elapsed (s): 1.6746349334716797
Agent stdevs: 0.33650652
--------------------------------------------------------------------------------

Step 523
++++++++ Policy training ++++++++++
Current mean reward: 2773.364354 | mean episode length: 781.500000
val_loss=30.72771
val_loss=960.99786
val_loss=60.96569
val_loss=51.80635
val_loss=34.19302
val_loss=31.18845
val_loss=331.22131
val_loss=109.83683
val_loss=24.74336
val_loss=344.06384
adv_loss= 2.18660
adv_loss= 3.25290
adv_loss= 1.14944
adv_loss= 1.69910
adv_loss= 1.36171
adv_loss= 3.69604
adv_loss= 1.67842
adv_loss= 1.75921
adv_loss= 1.75160
adv_loss= 3.08083
surrogate=-0.01140, entropy= 0.95047, loss=-0.01140
surrogate=-0.01995, entropy= 0.95010, loss=-0.01995
surrogate= 0.03046, entropy= 0.95122, loss= 0.03046
surrogate=-0.02717, entropy= 0.95261, loss=-0.02717
surrogate= 0.00679, entropy= 0.95249, loss= 0.00679
surrogate=-0.02420, entropy= 0.95288, loss=-0.02420
surrogate=-0.02404, entropy= 0.95400, loss=-0.02404
surrogate=-0.00818, entropy= 0.95397, loss=-0.00818
surrogate=-0.01420, entropy= 0.95486, loss=-0.01420
surrogate=-0.03223, entropy= 0.95614, loss=-0.03223
std_min= 0.27869, std_max= 0.41023, std_mean= 0.33711
val lr: [0.00011577868852459017], policy lr: [0.00013893442622950819]
Policy Loss: -0.032232, | Entropy Bonus: -0, | Value Loss: 344.06, | Advantage Loss: 3.0808
Time elapsed (s): 1.6601746082305908
Agent stdevs: 0.3371098
--------------------------------------------------------------------------------

Step 524
++++++++ Policy training ++++++++++
Current mean reward: 2971.396685 | mean episode length: 827.500000
val_loss=67.17809
val_loss=913.58966
val_loss=34.73479
val_loss=12.78186
val_loss=10.04951
val_loss=102.51913
val_loss=534.60950
val_loss=137.61156
val_loss= 9.83116
val_loss=247.13020
adv_loss= 2.63167
adv_loss= 2.54526
adv_loss= 1.71623
adv_loss= 2.81075
adv_loss= 3.12597
adv_loss= 5.76461
adv_loss= 3.65459
adv_loss= 1.59870
adv_loss= 4.53444
adv_loss= 2.63551
surrogate= 0.02037, entropy= 0.95694, loss= 0.02037
surrogate= 0.00930, entropy= 0.95721, loss= 0.00930
surrogate= 0.02838, entropy= 0.95697, loss= 0.02838
surrogate= 0.00660, entropy= 0.95710, loss= 0.00660
surrogate=-0.02330, entropy= 0.95865, loss=-0.02330
surrogate=-0.02144, entropy= 0.95936, loss=-0.02144
surrogate= 0.00193, entropy= 0.95830, loss= 0.00193
surrogate=-0.02738, entropy= 0.95964, loss=-0.02738
surrogate= 0.00375, entropy= 0.95848, loss= 0.00375
surrogate= 0.04859, entropy= 0.95766, loss= 0.04859
std_min= 0.27833, std_max= 0.41074, std_mean= 0.33733
val lr: [0.00011552254098360657], policy lr: [0.00013862704918032786]
Policy Loss: 0.048591, | Entropy Bonus: -0, | Value Loss: 247.13, | Advantage Loss: 2.6355
Time elapsed (s): 1.6730575561523438
Agent stdevs: 0.33732787
--------------------------------------------------------------------------------

Step 525
++++++++ Policy training ++++++++++
Current mean reward: 2250.518765 | mean episode length: 630.000000
val_loss=1891.05322
val_loss=210.03984
val_loss=19.24603
val_loss=1613.59668
val_loss=837.87738
val_loss=1093.67798
val_loss=593.42047
val_loss=117.70615
val_loss=265.21054
val_loss=580.60181
adv_loss= 3.13009
adv_loss= 3.33917
adv_loss= 3.43702
adv_loss= 6.73442
adv_loss= 4.16056
adv_loss= 2.89414
adv_loss= 3.46380
adv_loss= 2.79719
adv_loss=15.06483
adv_loss= 5.48762
surrogate= 0.01073, entropy= 0.95829, loss= 0.01073
surrogate=-0.00675, entropy= 0.95808, loss=-0.00675
surrogate=-0.01107, entropy= 0.95665, loss=-0.01107
surrogate= 0.02284, entropy= 0.95631, loss= 0.02284
surrogate= 0.00599, entropy= 0.95546, loss= 0.00599
surrogate=-0.03081, entropy= 0.95358, loss=-0.03081
surrogate=-0.03416, entropy= 0.95379, loss=-0.03416
surrogate=-0.02538, entropy= 0.95230, loss=-0.02538
surrogate=-0.01608, entropy= 0.95043, loss=-0.01608
surrogate=-0.02454, entropy= 0.95075, loss=-0.02454
std_min= 0.27727, std_max= 0.41212, std_mean= 0.33673
val lr: [0.00011526639344262297], policy lr: [0.00013831967213114753]
Policy Loss: -0.024536, | Entropy Bonus: -0, | Value Loss: 580.6, | Advantage Loss: 5.4876
Time elapsed (s): 1.6826605796813965
Agent stdevs: 0.33673334
--------------------------------------------------------------------------------

Step 526
++++++++ Policy training ++++++++++
Current mean reward: 1288.169894 | mean episode length: 357.750000
val_loss=36.82425
val_loss=24.17982
val_loss=32.56268
val_loss=19.99133
val_loss=29.36021
val_loss=22.80806
val_loss=32.67389
val_loss=29.33671
val_loss=26.42748
val_loss=17.87251
adv_loss= 6.14648
adv_loss= 3.78884
adv_loss= 2.67032
adv_loss= 4.17093
adv_loss= 3.54631
adv_loss= 3.27022
adv_loss= 2.40538
adv_loss= 5.96881
adv_loss= 1.62850
adv_loss= 5.99148
surrogate= 0.00747, entropy= 0.94676, loss= 0.00747
surrogate=-0.00360, entropy= 0.94255, loss=-0.00360
surrogate=-0.01116, entropy= 0.93839, loss=-0.01116
surrogate=-0.00007, entropy= 0.93555, loss=-0.00007
surrogate=-0.02221, entropy= 0.93206, loss=-0.02221
surrogate= 0.01090, entropy= 0.92946, loss= 0.01090
surrogate=-0.02705, entropy= 0.92636, loss=-0.02705
surrogate=-0.03921, entropy= 0.92308, loss=-0.03921
surrogate=-0.02366, entropy= 0.92033, loss=-0.02366
surrogate=-0.04277, entropy= 0.91805, loss=-0.04277
std_min= 0.27449, std_max= 0.40737, std_mean= 0.33305
val lr: [0.00011501024590163935], policy lr: [0.0001380122950819672]
Policy Loss: -0.042775, | Entropy Bonus: -0, | Value Loss: 17.873, | Advantage Loss: 5.9915
Time elapsed (s): 1.6970698833465576
Agent stdevs: 0.3330451
--------------------------------------------------------------------------------

Step 527
++++++++ Policy training ++++++++++
Current mean reward: 2981.119777 | mean episode length: 846.000000
val_loss=42.09458
val_loss=99.19137
val_loss=273.92767
val_loss=113.41886
val_loss=122.96740
val_loss=403.44418
val_loss=234.11423
val_loss=98.43700
val_loss=614.09180
val_loss=154.24570
adv_loss= 2.38468
adv_loss= 1.71167
adv_loss= 6.59088
adv_loss=13.64628
adv_loss= 4.14136
adv_loss= 2.82479
adv_loss= 2.76459
adv_loss= 1.85854
adv_loss= 4.87016
adv_loss= 3.21209
surrogate=-0.00322, entropy= 0.91675, loss=-0.00322
surrogate=-0.00262, entropy= 0.91919, loss=-0.00262
surrogate=-0.02074, entropy= 0.92084, loss=-0.02074
surrogate=-0.02915, entropy= 0.92245, loss=-0.02915
surrogate=-0.01736, entropy= 0.92415, loss=-0.01736
surrogate=-0.01327, entropy= 0.92488, loss=-0.01327
surrogate=-0.01618, entropy= 0.92581, loss=-0.01618
surrogate=-0.01208, entropy= 0.92581, loss=-0.01208
surrogate=-0.01821, entropy= 0.92582, loss=-0.01821
surrogate=-0.02918, entropy= 0.92673, loss=-0.02918
std_min= 0.27689, std_max= 0.40704, std_mean= 0.33383
val lr: [0.00011475409836065573], policy lr: [0.00013770491803278686]
Policy Loss: -0.029184, | Entropy Bonus: -0, | Value Loss: 154.25, | Advantage Loss: 3.2121
Time elapsed (s): 1.6931090354919434
Agent stdevs: 0.33382726
--------------------------------------------------------------------------------

Step 528
++++++++ Policy training ++++++++++
Current mean reward: 1633.765998 | mean episode length: 457.333333
val_loss=25.79208
val_loss=64.75222
val_loss=24.70345
val_loss=22.48467
val_loss=27.70268
val_loss=16.90734
val_loss=23.72627
val_loss=20.87165
val_loss=13.42781
val_loss=12.12604
adv_loss= 2.85977
adv_loss= 2.91106
adv_loss= 1.57562
adv_loss= 2.44354
adv_loss= 2.87727
adv_loss= 2.63025
adv_loss= 4.30647
adv_loss= 4.69773
adv_loss= 3.99657
adv_loss= 3.43234
surrogate=-0.03413, entropy= 0.92542, loss=-0.03413
surrogate= 0.01847, entropy= 0.92525, loss= 0.01847
surrogate=-0.01083, entropy= 0.92516, loss=-0.01083
surrogate=-0.01742, entropy= 0.92457, loss=-0.01742
surrogate=-0.00141, entropy= 0.92182, loss=-0.00141
surrogate= 0.00270, entropy= 0.92013, loss= 0.00270
surrogate= 0.03074, entropy= 0.92063, loss= 0.03074
surrogate= 0.01377, entropy= 0.91903, loss= 0.01377
surrogate=-0.01458, entropy= 0.91791, loss=-0.01458
surrogate=-0.04101, entropy= 0.91625, loss=-0.04101
std_min= 0.27560, std_max= 0.40383, std_mean= 0.33254
val lr: [0.00011449795081967213], policy lr: [0.00013739754098360653]
Policy Loss: -0.041008, | Entropy Bonus: -0, | Value Loss: 12.126, | Advantage Loss: 3.4323
Time elapsed (s): 1.6629929542541504
Agent stdevs: 0.33254287
--------------------------------------------------------------------------------

Step 529
++++++++ Policy training ++++++++++
Current mean reward: 1995.295948 | mean episode length: 559.000000
val_loss=89.89849
val_loss=56.63078
val_loss=784.93213
val_loss=1270.25293
val_loss=58.69966
val_loss=31.99467
val_loss=191.23657
val_loss=142.76157
val_loss=25.96533
val_loss=58.62585
adv_loss= 4.12997
adv_loss= 6.50799
adv_loss= 4.25272
adv_loss= 5.43602
adv_loss= 5.15372
adv_loss= 5.32900
adv_loss= 7.35312
adv_loss= 9.18730
adv_loss= 5.82473
adv_loss= 5.39552
surrogate=-0.01626, entropy= 0.91645, loss=-0.01626
surrogate=-0.00463, entropy= 0.91689, loss=-0.00463
surrogate= 0.00464, entropy= 0.91866, loss= 0.00464
surrogate=-0.02025, entropy= 0.92018, loss=-0.02025
surrogate=-0.03449, entropy= 0.92155, loss=-0.03449
surrogate=-0.03158, entropy= 0.92251, loss=-0.03158
surrogate= 0.01560, entropy= 0.92309, loss= 0.01560
surrogate=-0.02967, entropy= 0.92307, loss=-0.02967
surrogate=-0.03045, entropy= 0.92381, loss=-0.03045
surrogate=-0.00352, entropy= 0.92527, loss=-0.00352
std_min= 0.27701, std_max= 0.40278, std_mean= 0.33336
val lr: [0.00011424180327868853], policy lr: [0.0001370901639344262]
Policy Loss: -0.0035225, | Entropy Bonus: -0, | Value Loss: 58.626, | Advantage Loss: 5.3955
Time elapsed (s): 1.6542062759399414
Agent stdevs: 0.3333597
--------------------------------------------------------------------------------

Step 530
++++++++ Policy training ++++++++++
Current mean reward: 2370.750989 | mean episode length: 674.500000
val_loss=26.42420
val_loss=640.76166
val_loss=345.11441
val_loss=23.93918
val_loss=1021.95160
val_loss=1377.19812
val_loss=311.52426
val_loss=63.19322
val_loss=35.15888
val_loss=2058.85400
adv_loss= 3.11093
adv_loss= 1.89545
adv_loss= 1.68074
adv_loss= 5.40596
adv_loss= 1.80792
adv_loss= 6.23812
adv_loss= 1.17106
adv_loss= 1.76243
adv_loss= 2.64720
adv_loss= 1.52357
surrogate=-0.01361, entropy= 0.92377, loss=-0.01361
surrogate=-0.01178, entropy= 0.92378, loss=-0.01178
surrogate= 0.01507, entropy= 0.92411, loss= 0.01507
surrogate= 0.02001, entropy= 0.92158, loss= 0.02001
surrogate=-0.04202, entropy= 0.92029, loss=-0.04202
surrogate=-0.00868, entropy= 0.92056, loss=-0.00868
surrogate=-0.01317, entropy= 0.92091, loss=-0.01317
surrogate=-0.03343, entropy= 0.92006, loss=-0.03343
surrogate= 0.01412, entropy= 0.91959, loss= 0.01412
surrogate=-0.02788, entropy= 0.91976, loss=-0.02788
std_min= 0.27611, std_max= 0.40265, std_mean= 0.33282
val lr: [0.00011398565573770491], policy lr: [0.00013678278688524588]
Policy Loss: -0.027877, | Entropy Bonus: -0, | Value Loss: 2058.9, | Advantage Loss: 1.5236
Time elapsed (s): 1.6487743854522705
Agent stdevs: 0.33281806
--------------------------------------------------------------------------------

Step 531
++++++++ Policy training ++++++++++
Current mean reward: 1553.320529 | mean episode length: 432.666667
val_loss=36.40215
val_loss=14.78220
val_loss=20.59660
val_loss=27.25702
val_loss=14.99999
val_loss=13.01793
val_loss=12.75469
val_loss=28.48512
val_loss= 8.75261
val_loss=18.12073
adv_loss= 3.41813
adv_loss=13.84619
adv_loss= 2.86020
adv_loss= 4.94860
adv_loss= 1.30152
adv_loss= 1.75993
adv_loss= 1.74554
adv_loss= 2.38810
adv_loss= 6.42292
adv_loss= 0.90996
surrogate= 0.00417, entropy= 0.92065, loss= 0.00417
surrogate=-0.00168, entropy= 0.92066, loss=-0.00168
surrogate= 0.00795, entropy= 0.92022, loss= 0.00795
surrogate=-0.01446, entropy= 0.92053, loss=-0.01446
surrogate=-0.01646, entropy= 0.92028, loss=-0.01646
surrogate=-0.03121, entropy= 0.92057, loss=-0.03121
surrogate=-0.03769, entropy= 0.92145, loss=-0.03769
surrogate=-0.05351, entropy= 0.92340, loss=-0.05351
surrogate=-0.01593, entropy= 0.92298, loss=-0.01593
surrogate=-0.01026, entropy= 0.92349, loss=-0.01026
std_min= 0.27535, std_max= 0.40219, std_mean= 0.33323
val lr: [0.00011372950819672131], policy lr: [0.00013647540983606555]
Policy Loss: -0.010261, | Entropy Bonus: -0, | Value Loss: 18.121, | Advantage Loss: 0.90996
Time elapsed (s): 1.6571807861328125
Agent stdevs: 0.33322918
--------------------------------------------------------------------------------

Step 532
++++++++ Policy training ++++++++++
Current mean reward: 2807.046576 | mean episode length: 771.000000
val_loss=11.78866
val_loss=18.57729
val_loss= 4.38841
val_loss= 5.56056
val_loss=11.17963
val_loss= 6.71438
val_loss= 4.60457
val_loss=10.72630
val_loss= 5.59342
val_loss= 6.92474
adv_loss= 1.04805
adv_loss= 3.60865
adv_loss= 1.31939
adv_loss= 0.78210
adv_loss= 4.46051
adv_loss= 2.70984
adv_loss= 1.51680
adv_loss= 1.13941
adv_loss= 3.00302
adv_loss= 1.85915
surrogate=-0.00137, entropy= 0.92186, loss=-0.00137
surrogate= 0.01050, entropy= 0.92188, loss= 0.01050
surrogate=-0.01361, entropy= 0.91903, loss=-0.01361
surrogate=-0.00732, entropy= 0.91726, loss=-0.00732
surrogate=-0.00308, entropy= 0.91547, loss=-0.00308
surrogate=-0.02692, entropy= 0.91204, loss=-0.02692
surrogate=-0.04033, entropy= 0.90967, loss=-0.04033
surrogate= 0.01188, entropy= 0.90761, loss= 0.01188
surrogate=-0.01719, entropy= 0.90561, loss=-0.01719
surrogate=-0.01901, entropy= 0.90318, loss=-0.01901
std_min= 0.27235, std_max= 0.40150, std_mean= 0.33117
val lr: [0.00011347336065573771], policy lr: [0.00013616803278688523]
Policy Loss: -0.019014, | Entropy Bonus: -0, | Value Loss: 6.9247, | Advantage Loss: 1.8591
Time elapsed (s): 1.7191197872161865
Agent stdevs: 0.33117077
--------------------------------------------------------------------------------

Step 533
++++++++ Policy training ++++++++++
Current mean reward: 3148.888051 | mean episode length: 898.000000
val_loss=137.27110
val_loss=85.99179
val_loss=359.84058
val_loss=310.58371
val_loss=987.88580
val_loss=177.20781
val_loss=133.61243
val_loss=947.82416
val_loss=90.25749
val_loss=195.63155
adv_loss= 1.62575
adv_loss=10.00851
adv_loss= 3.69804
adv_loss= 0.90730
adv_loss= 9.27213
adv_loss= 3.01428
adv_loss= 3.30982
adv_loss=1327.96179
adv_loss= 2.87344
adv_loss= 2.15696
surrogate= 0.00842, entropy= 0.90228, loss= 0.00842
surrogate=-0.00900, entropy= 0.90125, loss=-0.00900
surrogate=-0.04323, entropy= 0.90090, loss=-0.04323
surrogate=-0.00164, entropy= 0.90076, loss=-0.00164
surrogate= 0.00116, entropy= 0.90089, loss= 0.00116
surrogate=-0.00291, entropy= 0.90098, loss=-0.00291
surrogate=-0.02948, entropy= 0.90072, loss=-0.02948
surrogate=-0.03406, entropy= 0.90144, loss=-0.03406
surrogate=-0.02991, entropy= 0.90189, loss=-0.02991
surrogate= 0.00837, entropy= 0.90127, loss= 0.00837
std_min= 0.27239, std_max= 0.40136, std_mean= 0.33097
val lr: [0.00011321721311475411], policy lr: [0.0001358606557377049]
Policy Loss: 0.0083672, | Entropy Bonus: -0, | Value Loss: 195.63, | Advantage Loss: 2.157
Time elapsed (s): 1.6901178359985352
Agent stdevs: 0.33096638
--------------------------------------------------------------------------------

Step 534
++++++++ Policy training ++++++++++
Current mean reward: 2334.944070 | mean episode length: 662.333333
val_loss=29.19085
val_loss=47.45860
val_loss=38.06466
val_loss=139.42850
val_loss=23.27788
val_loss=780.30554
val_loss=1159.59045
val_loss=422.58975
val_loss=144.80444
val_loss=1150.45557
adv_loss= 3.97064
adv_loss= 3.47744
adv_loss= 3.07807
adv_loss= 1.49574
adv_loss= 3.52504
adv_loss= 6.43589
adv_loss= 7.25450
adv_loss= 5.59351
adv_loss=1499.41370
adv_loss= 3.36995
surrogate= 0.02448, entropy= 0.90085, loss= 0.02448
surrogate= 0.02822, entropy= 0.90181, loss= 0.02822
surrogate=-0.01785, entropy= 0.90332, loss=-0.01785
surrogate= 0.00750, entropy= 0.90344, loss= 0.00750
surrogate=-0.04082, entropy= 0.90252, loss=-0.04082
surrogate= 0.02081, entropy= 0.90232, loss= 0.02081
surrogate=-0.01790, entropy= 0.90193, loss=-0.01790
surrogate=-0.01587, entropy= 0.90177, loss=-0.01587
surrogate= 0.00063, entropy= 0.90045, loss= 0.00063
surrogate=-0.02626, entropy= 0.89970, loss=-0.02626
std_min= 0.27243, std_max= 0.40046, std_mean= 0.33074
val lr: [0.0001129610655737705], policy lr: [0.0001355532786885246]
Policy Loss: -0.026264, | Entropy Bonus: -0, | Value Loss: 1150.5, | Advantage Loss: 3.3699
Time elapsed (s): 1.693178415298462
Agent stdevs: 0.33073968
--------------------------------------------------------------------------------

Step 535
++++++++ Policy training ++++++++++
Current mean reward: 2249.451743 | mean episode length: 624.333333
val_loss=36.44456
val_loss=21.50310
val_loss=11.05508
val_loss=15.46457
val_loss=16.61835
val_loss=10.95985
val_loss=15.33655
val_loss=10.27591
val_loss=11.63416
val_loss=16.44811
adv_loss= 5.46377
adv_loss= 2.61921
adv_loss= 2.63491
adv_loss= 2.74023
adv_loss= 1.78938
adv_loss= 3.70502
adv_loss= 4.24661
adv_loss= 8.55444
adv_loss= 8.80777
adv_loss= 3.11176
surrogate= 0.00511, entropy= 0.90084, loss= 0.00511
surrogate= 0.00762, entropy= 0.90113, loss= 0.00762
surrogate=-0.04709, entropy= 0.90079, loss=-0.04709
surrogate=-0.00411, entropy= 0.89926, loss=-0.00411
surrogate=-0.02617, entropy= 0.89819, loss=-0.02617
surrogate= 0.02240, entropy= 0.89686, loss= 0.02240
surrogate= 0.00807, entropy= 0.89700, loss= 0.00807
surrogate=-0.03527, entropy= 0.89755, loss=-0.03527
surrogate=-0.00669, entropy= 0.89701, loss=-0.00669
surrogate=-0.01916, entropy= 0.89553, loss=-0.01916
std_min= 0.27115, std_max= 0.40378, std_mean= 0.33059
val lr: [0.00011270491803278687], policy lr: [0.00013524590163934422]
Policy Loss: -0.019161, | Entropy Bonus: -0, | Value Loss: 16.448, | Advantage Loss: 3.1118
Time elapsed (s): 1.674473762512207
Agent stdevs: 0.33059192
--------------------------------------------------------------------------------

Step 536
++++++++ Policy training ++++++++++
Current mean reward: 2121.668206 | mean episode length: 584.666667
val_loss=23.27794
val_loss=17.92163
val_loss=21.22533
val_loss=19.75543
val_loss=12.74100
val_loss=13.24952
val_loss=16.48794
val_loss=12.50529
val_loss=10.30871
val_loss= 7.85229
adv_loss= 4.09394
adv_loss= 3.54785
adv_loss= 2.30987
adv_loss= 1.57371
adv_loss= 5.51150
adv_loss= 2.49899
adv_loss= 2.03096
adv_loss= 3.91380
adv_loss= 2.14066
adv_loss= 2.40748
surrogate= 0.00981, entropy= 0.89223, loss= 0.00981
surrogate=-0.01582, entropy= 0.88983, loss=-0.01582
surrogate= 0.00111, entropy= 0.88613, loss= 0.00111
surrogate=-0.00444, entropy= 0.88191, loss=-0.00444
surrogate=-0.02373, entropy= 0.87875, loss=-0.02373
surrogate=-0.01631, entropy= 0.87576, loss=-0.01631
surrogate=-0.02127, entropy= 0.87239, loss=-0.02127
surrogate=-0.00564, entropy= 0.86887, loss=-0.00564
surrogate=-0.03771, entropy= 0.86629, loss=-0.03771
surrogate=-0.01734, entropy= 0.86517, loss=-0.01734
std_min= 0.26820, std_max= 0.40065, std_mean= 0.32734
val lr: [0.00011244877049180327], policy lr: [0.0001349385245901639]
Policy Loss: -0.017338, | Entropy Bonus: -0, | Value Loss: 7.8523, | Advantage Loss: 2.4075
Time elapsed (s): 1.676262378692627
Agent stdevs: 0.32733527
--------------------------------------------------------------------------------

Step 537
++++++++ Policy training ++++++++++
Current mean reward: 1434.621598 | mean episode length: 397.500000
val_loss=25.40461
val_loss=27.63415
val_loss=13.10248
val_loss=12.33053
val_loss=20.66399
val_loss= 6.56813
val_loss=10.90591
val_loss=10.10587
val_loss=15.60980
val_loss= 9.93409
adv_loss= 2.38539
adv_loss= 2.76184
adv_loss= 2.68686
adv_loss= 4.36846
adv_loss= 4.88802
adv_loss= 1.83882
adv_loss= 4.44642
adv_loss= 2.58759
adv_loss= 1.44709
adv_loss= 6.18526
surrogate=-0.01709, entropy= 0.86473, loss=-0.01709
surrogate=-0.00292, entropy= 0.86380, loss=-0.00292
surrogate=-0.01751, entropy= 0.86388, loss=-0.01751
surrogate=-0.03060, entropy= 0.86362, loss=-0.03060
surrogate=-0.02444, entropy= 0.86353, loss=-0.02444
surrogate=-0.01676, entropy= 0.86383, loss=-0.01676
surrogate= 0.01699, entropy= 0.86268, loss= 0.01699
surrogate=-0.02086, entropy= 0.86242, loss=-0.02086
surrogate=-0.04650, entropy= 0.86272, loss=-0.04650
surrogate=-0.03099, entropy= 0.86226, loss=-0.03099
std_min= 0.26898, std_max= 0.40033, std_mean= 0.32698
val lr: [0.00011219262295081967], policy lr: [0.00013463114754098357]
Policy Loss: -0.030993, | Entropy Bonus: -0, | Value Loss: 9.9341, | Advantage Loss: 6.1853
Time elapsed (s): 1.6829843521118164
Agent stdevs: 0.32697573
--------------------------------------------------------------------------------

Step 538
++++++++ Policy training ++++++++++
Current mean reward: 2584.015623 | mean episode length: 716.500000
val_loss=32.46638
val_loss=30.47986
val_loss=23.65452
val_loss=17.55248
val_loss=36.05900
val_loss=18.50990
val_loss=33.54228
val_loss=23.80264
val_loss=29.68234
val_loss=17.35643
adv_loss= 1.82191
adv_loss= 3.03514
adv_loss= 2.48522
adv_loss= 2.01282
adv_loss= 3.76639
adv_loss= 1.28805
adv_loss= 2.93410
adv_loss= 2.30564
adv_loss= 4.08651
adv_loss= 1.62776
surrogate= 0.01964, entropy= 0.86146, loss= 0.01964
surrogate=-0.00679, entropy= 0.86146, loss=-0.00679
surrogate=-0.00370, entropy= 0.86144, loss=-0.00370
surrogate=-0.02214, entropy= 0.86141, loss=-0.02214
surrogate=-0.03291, entropy= 0.86142, loss=-0.03291
surrogate=-0.01961, entropy= 0.86179, loss=-0.01961
surrogate=-0.01549, entropy= 0.86348, loss=-0.01549
surrogate=-0.00780, entropy= 0.86341, loss=-0.00780
surrogate=-0.01825, entropy= 0.86322, loss=-0.01825
surrogate=-0.02337, entropy= 0.86294, loss=-0.02337
std_min= 0.26910, std_max= 0.40128, std_mean= 0.32711
val lr: [0.00011193647540983606], policy lr: [0.00013432377049180327]
Policy Loss: -0.023369, | Entropy Bonus: -0, | Value Loss: 17.356, | Advantage Loss: 1.6278
Time elapsed (s): 1.7054057121276855
Agent stdevs: 0.32711485
--------------------------------------------------------------------------------

Step 539
++++++++ Policy training ++++++++++
Current mean reward: 2287.696998 | mean episode length: 631.500000
val_loss=10.06565
val_loss=15.32496
val_loss= 6.69999
val_loss= 8.70591
val_loss= 8.38490
val_loss= 7.89050
val_loss= 4.94658
val_loss= 5.53871
val_loss= 9.87815
val_loss= 6.25995
adv_loss= 2.10338
adv_loss= 0.70581
adv_loss= 1.55385
adv_loss= 1.70939
adv_loss= 0.84219
adv_loss= 1.16557
adv_loss= 0.99551
adv_loss= 1.09606
adv_loss= 1.15362
adv_loss= 1.92001
surrogate= 0.00845, entropy= 0.86283, loss= 0.00845
surrogate=-0.00491, entropy= 0.86195, loss=-0.00491
surrogate= 0.00315, entropy= 0.86173, loss= 0.00315
surrogate=-0.00569, entropy= 0.86154, loss=-0.00569
surrogate= 0.01993, entropy= 0.86178, loss= 0.01993
surrogate=-0.02551, entropy= 0.86188, loss=-0.02551
surrogate=-0.01191, entropy= 0.86219, loss=-0.01191
surrogate=-0.00812, entropy= 0.86270, loss=-0.00812
surrogate=-0.04277, entropy= 0.86331, loss=-0.04277
surrogate=-0.02142, entropy= 0.86379, loss=-0.02142
std_min= 0.26948, std_max= 0.39807, std_mean= 0.32695
val lr: [0.00011168032786885246], policy lr: [0.00013401639344262295]
Policy Loss: -0.021425, | Entropy Bonus: -0, | Value Loss: 6.2599, | Advantage Loss: 1.92
Time elapsed (s): 1.6909816265106201
Agent stdevs: 0.32694772
--------------------------------------------------------------------------------

Step 540
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2135.5
++++++++ Policy training ++++++++++
Current mean reward: 3330.601176 | mean episode length: 920.500000
val_loss=14.64607
val_loss=14.32604
val_loss= 8.40069
val_loss=10.94991
val_loss=10.78262
val_loss= 6.50886
val_loss= 6.48613
val_loss=12.96382
val_loss=10.05072
val_loss= 7.67992
adv_loss= 0.94811
adv_loss= 1.47429
adv_loss= 0.93023
adv_loss= 4.22584
adv_loss= 8.30683
adv_loss= 1.41177
adv_loss= 4.43282
adv_loss= 1.67861
adv_loss= 4.39951
adv_loss= 2.10931
surrogate= 0.01102, entropy= 0.86196, loss= 0.01102
surrogate= 0.00966, entropy= 0.85999, loss= 0.00966
surrogate= 0.02951, entropy= 0.85852, loss= 0.02951
surrogate=-0.02428, entropy= 0.85778, loss=-0.02428
surrogate= 0.00418, entropy= 0.85508, loss= 0.00418
surrogate=-0.02255, entropy= 0.85479, loss=-0.02255
surrogate=-0.01496, entropy= 0.85310, loss=-0.01496
surrogate=-0.04836, entropy= 0.85147, loss=-0.04836
surrogate=-0.02590, entropy= 0.85095, loss=-0.02590
surrogate=-0.02782, entropy= 0.84987, loss=-0.02782
std_min= 0.26963, std_max= 0.39270, std_mean= 0.32511
val lr: [0.00011142418032786886], policy lr: [0.00013370901639344262]
Policy Loss: -0.027822, | Entropy Bonus: -0, | Value Loss: 7.6799, | Advantage Loss: 2.1093
Time elapsed (s): 1.6906929016113281
Agent stdevs: 0.32510734
--------------------------------------------------------------------------------

Step 541
++++++++ Policy training ++++++++++
Current mean reward: 1618.441085 | mean episode length: 443.250000
val_loss= 6.40834
val_loss=12.09085
val_loss= 6.26888
val_loss= 8.93253
val_loss=10.60748
val_loss= 9.27503
val_loss=11.32806
val_loss=11.00430
val_loss= 5.29317
val_loss= 9.53586
adv_loss= 4.78405
adv_loss= 2.05194
adv_loss= 2.41342
adv_loss= 1.70282
adv_loss= 1.90065
adv_loss= 1.65702
adv_loss= 5.57067
adv_loss= 2.36656
adv_loss= 2.05312
adv_loss= 0.82344
surrogate= 0.00557, entropy= 0.84867, loss= 0.00557
surrogate= 0.00090, entropy= 0.84674, loss= 0.00090
surrogate= 0.00872, entropy= 0.84455, loss= 0.00872
surrogate=-0.03067, entropy= 0.84414, loss=-0.03067
surrogate= 0.00469, entropy= 0.84357, loss= 0.00469
surrogate=-0.04001, entropy= 0.84196, loss=-0.04001
surrogate=-0.00053, entropy= 0.84031, loss=-0.00053
surrogate= 0.02236, entropy= 0.83857, loss= 0.02236
surrogate=-0.00424, entropy= 0.83756, loss=-0.00424
surrogate=-0.02365, entropy= 0.83624, loss=-0.02365
std_min= 0.26854, std_max= 0.39031, std_mean= 0.32358
val lr: [0.00011116803278688526], policy lr: [0.0001334016393442623]
Policy Loss: -0.023652, | Entropy Bonus: -0, | Value Loss: 9.5359, | Advantage Loss: 0.82344
Time elapsed (s): 1.668480634689331
Agent stdevs: 0.32358393
--------------------------------------------------------------------------------

Step 542
++++++++ Policy training ++++++++++
Current mean reward: 2029.420063 | mean episode length: 574.333333
val_loss=465.83734
val_loss=336.45468
val_loss=384.25287
val_loss=74.71854
val_loss=583.97064
val_loss=79.73502
val_loss=226.45061
val_loss=170.55602
val_loss=1057.85791
val_loss=41.26667
adv_loss= 1.44736
adv_loss= 1.31879
adv_loss= 1.03105
adv_loss= 2.95493
adv_loss= 1.33950
adv_loss= 2.45250
adv_loss= 1.07685
adv_loss= 2.14833
adv_loss= 1.36862
adv_loss=1718.15918
surrogate= 0.00489, entropy= 0.83526, loss= 0.00489
surrogate=-0.01218, entropy= 0.83426, loss=-0.01218
surrogate=-0.01201, entropy= 0.83512, loss=-0.01201
surrogate= 0.00352, entropy= 0.83436, loss= 0.00352
surrogate= 0.00421, entropy= 0.83407, loss= 0.00421
surrogate= 0.02650, entropy= 0.83310, loss= 0.02650
surrogate= 0.02728, entropy= 0.83411, loss= 0.02728
surrogate=-0.00775, entropy= 0.83300, loss=-0.00775
surrogate=-0.02796, entropy= 0.83364, loss=-0.02796
surrogate=-0.03443, entropy= 0.83384, loss=-0.03443
std_min= 0.26859, std_max= 0.38951, std_mean= 0.32328
val lr: [0.00011091188524590164], policy lr: [0.00013309426229508197]
Policy Loss: -0.034425, | Entropy Bonus: -0, | Value Loss: 41.267, | Advantage Loss: 1718.2
Time elapsed (s): 1.6793034076690674
Agent stdevs: 0.32328442
--------------------------------------------------------------------------------

Step 543
++++++++ Policy training ++++++++++
Current mean reward: 3444.873173 | mean episode length: 1000.000000
val_loss=247.69440
val_loss=48.02342
val_loss=915.09912
val_loss=496.13953
val_loss=71.56317
val_loss=51.41845
val_loss=429.79703
val_loss=760.54303
val_loss=61.10730
val_loss=809.05737
adv_loss= 2.64272
adv_loss= 2.22051
adv_loss= 1.74828
adv_loss= 2.13032
adv_loss= 0.99751
adv_loss= 1.50892
adv_loss= 1.81347
adv_loss= 1.73969
adv_loss= 2.39722
adv_loss= 2.48554
surrogate= 0.01605, entropy= 0.83517, loss= 0.01605
surrogate=-0.00284, entropy= 0.83800, loss=-0.00284
surrogate=-0.00152, entropy= 0.84110, loss=-0.00152
surrogate= 0.00807, entropy= 0.84358, loss= 0.00807
surrogate=-0.02438, entropy= 0.84714, loss=-0.02438
surrogate=-0.00376, entropy= 0.84848, loss=-0.00376
surrogate=-0.00057, entropy= 0.85113, loss=-0.00057
surrogate=-0.00088, entropy= 0.85210, loss=-0.00088
surrogate=-0.01358, entropy= 0.85404, loss=-0.01358
surrogate=-0.01055, entropy= 0.85573, loss=-0.01055
std_min= 0.26903, std_max= 0.39346, std_mean= 0.32582
val lr: [0.00011065573770491804], policy lr: [0.00013278688524590165]
Policy Loss: -0.01055, | Entropy Bonus: -0, | Value Loss: 809.06, | Advantage Loss: 2.4855
Time elapsed (s): 1.6864776611328125
Agent stdevs: 0.3258182
--------------------------------------------------------------------------------

Step 544
++++++++ Policy training ++++++++++
Current mean reward: 2298.288708 | mean episode length: 654.500000
val_loss=16.94713
val_loss=90.72038
val_loss=1439.77112
val_loss=25.74583
val_loss=2167.19653
val_loss=1556.27075
val_loss=19.34825
val_loss=17.67729
val_loss=418.88742
val_loss=405.19080
adv_loss= 3.73350
adv_loss= 1.87298
adv_loss= 2.68110
adv_loss= 2.48090
adv_loss= 1.63809
adv_loss= 1.55339
adv_loss= 1.73144
adv_loss= 1.29179
adv_loss= 1.52514
adv_loss= 4.51598
surrogate= 0.00469, entropy= 0.85724, loss= 0.00469
surrogate=-0.02497, entropy= 0.85864, loss=-0.02497
surrogate=-0.03284, entropy= 0.85936, loss=-0.03284
surrogate= 0.03795, entropy= 0.85922, loss= 0.03795
surrogate= 0.02408, entropy= 0.85883, loss= 0.02408
surrogate=-0.03365, entropy= 0.85841, loss=-0.03365
surrogate=-0.01745, entropy= 0.85850, loss=-0.01745
surrogate=-0.02366, entropy= 0.85851, loss=-0.02366
surrogate=-0.01533, entropy= 0.85890, loss=-0.01533
surrogate=-0.03564, entropy= 0.85887, loss=-0.03564
std_min= 0.26879, std_max= 0.39308, std_mean= 0.32613
val lr: [0.00011039959016393442], policy lr: [0.0001324795081967213]
Policy Loss: -0.035639, | Entropy Bonus: -0, | Value Loss: 405.19, | Advantage Loss: 4.516
Time elapsed (s): 1.7083120346069336
Agent stdevs: 0.32612517
--------------------------------------------------------------------------------

Step 545
++++++++ Policy training ++++++++++
Current mean reward: 3115.173840 | mean episode length: 873.500000
val_loss=459.20132
val_loss=44.86884
val_loss=1976.61670
val_loss=41.42663
val_loss=106.85132
val_loss=331.26651
val_loss=167.85690
val_loss=49.31211
val_loss=38.91644
val_loss=185.22620
adv_loss= 1.08839
adv_loss= 1.34992
adv_loss= 2.08235
adv_loss= 3.51901
adv_loss= 5.09045
adv_loss= 1.63372
adv_loss= 3.26004
adv_loss= 3.25581
adv_loss= 1.39957
adv_loss= 1.63364
surrogate= 0.00010, entropy= 0.86116, loss= 0.00010
surrogate=-0.01267, entropy= 0.86264, loss=-0.01267
surrogate=-0.00731, entropy= 0.86418, loss=-0.00731
surrogate= 0.01130, entropy= 0.86602, loss= 0.01130
surrogate=-0.01427, entropy= 0.86757, loss=-0.01427
surrogate= 0.01561, entropy= 0.86891, loss= 0.01561
surrogate=-0.01372, entropy= 0.87009, loss=-0.01372
surrogate=-0.00301, entropy= 0.87102, loss=-0.00301
surrogate=-0.01058, entropy= 0.87237, loss=-0.01058
surrogate=-0.02714, entropy= 0.87407, loss=-0.02714
std_min= 0.26891, std_max= 0.39759, std_mean= 0.32804
val lr: [0.00011014344262295082], policy lr: [0.00013217213114754097]
Policy Loss: -0.027143, | Entropy Bonus: -0, | Value Loss: 185.23, | Advantage Loss: 1.6336
Time elapsed (s): 1.6782190799713135
Agent stdevs: 0.32803783
--------------------------------------------------------------------------------

Step 546
++++++++ Policy training ++++++++++
Current mean reward: 3318.063724 | mean episode length: 936.000000
val_loss=1402.96021
val_loss=54.21970
val_loss=33.29970
val_loss=99.51103
val_loss=51.36213
val_loss=1121.52881
val_loss=149.18181
val_loss=39.48592
val_loss=582.62585
val_loss=43.51933
adv_loss= 4.60352
adv_loss= 5.91058
adv_loss= 5.19895
adv_loss= 3.37176
adv_loss= 2.99878
adv_loss= 4.24157
adv_loss= 2.00573
adv_loss= 4.63852
adv_loss= 4.75995
adv_loss= 4.24199
surrogate=-0.02320, entropy= 0.87562, loss=-0.02320
surrogate=-0.00769, entropy= 0.87630, loss=-0.00769
surrogate=-0.00035, entropy= 0.87745, loss=-0.00035
surrogate= 0.00417, entropy= 0.87866, loss= 0.00417
surrogate=-0.01272, entropy= 0.88094, loss=-0.01272
surrogate=-0.02823, entropy= 0.88245, loss=-0.02823
surrogate=-0.00658, entropy= 0.88490, loss=-0.00658
surrogate=-0.02594, entropy= 0.88631, loss=-0.02594
surrogate=-0.01933, entropy= 0.88810, loss=-0.01933
surrogate=-0.02260, entropy= 0.88880, loss=-0.02260
std_min= 0.27149, std_max= 0.39930, std_mean= 0.32956
val lr: [0.0001098872950819672], policy lr: [0.00013186475409836064]
Policy Loss: -0.022599, | Entropy Bonus: -0, | Value Loss: 43.519, | Advantage Loss: 4.242
Time elapsed (s): 1.6773715019226074
Agent stdevs: 0.32955918
--------------------------------------------------------------------------------

Step 547
++++++++ Policy training ++++++++++
Current mean reward: 2277.741466 | mean episode length: 632.000000
val_loss=34.41296
val_loss=10.29412
val_loss=22.12473
val_loss=15.20088
val_loss=19.76458
val_loss=11.50612
val_loss=16.16463
val_loss=12.89115
val_loss=19.50953
val_loss=19.11933
adv_loss= 1.72373
adv_loss= 3.45621
adv_loss= 3.42723
adv_loss= 1.74949
adv_loss= 2.68660
adv_loss= 2.04176
adv_loss= 2.19617
adv_loss= 1.46105
adv_loss= 2.13119
adv_loss= 2.11793
surrogate= 0.00882, entropy= 0.88920, loss= 0.00882
surrogate= 0.00219, entropy= 0.89010, loss= 0.00219
surrogate=-0.00566, entropy= 0.89096, loss=-0.00566
surrogate=-0.02498, entropy= 0.88977, loss=-0.02498
surrogate=-0.02969, entropy= 0.88930, loss=-0.02969
surrogate=-0.03649, entropy= 0.88880, loss=-0.03649
surrogate= 0.02584, entropy= 0.88835, loss= 0.02584
surrogate=-0.00698, entropy= 0.88819, loss=-0.00698
surrogate=-0.01309, entropy= 0.88872, loss=-0.01309
surrogate= 0.01569, entropy= 0.88762, loss= 0.01569
std_min= 0.27009, std_max= 0.39828, std_mean= 0.32945
val lr: [0.0001096311475409836], policy lr: [0.00013155737704918032]
Policy Loss: 0.015692, | Entropy Bonus: -0, | Value Loss: 19.119, | Advantage Loss: 2.1179
Time elapsed (s): 1.668609619140625
Agent stdevs: 0.32944664
--------------------------------------------------------------------------------

Step 548
++++++++ Policy training ++++++++++
Current mean reward: 2006.701578 | mean episode length: 554.000000
val_loss=15.52948
val_loss=19.08123
val_loss= 8.27672
val_loss= 9.72704
val_loss=12.49183
val_loss= 5.34991
val_loss= 8.92012
val_loss= 5.76269
val_loss= 8.14040
val_loss=13.21412
adv_loss= 1.49542
adv_loss= 2.15915
adv_loss= 2.44107
adv_loss= 2.32385
adv_loss= 2.46305
adv_loss= 0.61366
adv_loss= 3.19109
adv_loss= 2.99438
adv_loss= 1.51625
adv_loss= 1.25139
surrogate= 0.00167, entropy= 0.88774, loss= 0.00167
surrogate=-0.02220, entropy= 0.88579, loss=-0.02220
surrogate=-0.01703, entropy= 0.88573, loss=-0.01703
surrogate=-0.02932, entropy= 0.88550, loss=-0.02932
surrogate=-0.01592, entropy= 0.88557, loss=-0.01592
surrogate=-0.02566, entropy= 0.88591, loss=-0.02566
surrogate=-0.02126, entropy= 0.88565, loss=-0.02126
surrogate=-0.04919, entropy= 0.88475, loss=-0.04919
surrogate=-0.03569, entropy= 0.88406, loss=-0.03569
surrogate= 0.01767, entropy= 0.88372, loss= 0.01767
std_min= 0.26885, std_max= 0.39726, std_mean= 0.32904
val lr: [0.000109375], policy lr: [0.00013125]
Policy Loss: 0.017666, | Entropy Bonus: -0, | Value Loss: 13.214, | Advantage Loss: 1.2514
Time elapsed (s): 1.6632680892944336
Agent stdevs: 0.32903937
--------------------------------------------------------------------------------

Step 549
++++++++ Policy training ++++++++++
Current mean reward: 1579.950842 | mean episode length: 443.500000
val_loss=71.27284
val_loss=32.92358
val_loss=35.52358
val_loss=51.23958
val_loss=1005.54584
val_loss=28.87545
val_loss=77.46381
val_loss=299.43054
val_loss=156.03346
val_loss=22.74868
adv_loss= 2.97552
adv_loss= 1.39272
adv_loss= 2.18615
adv_loss= 4.38695
adv_loss= 3.55970
adv_loss= 3.51459
adv_loss= 4.79233
adv_loss= 2.11933
adv_loss= 3.19955
adv_loss= 5.02425
surrogate= 0.03466, entropy= 0.88462, loss= 0.03466
surrogate= 0.00509, entropy= 0.88560, loss= 0.00509
surrogate=-0.01126, entropy= 0.88602, loss=-0.01126
surrogate= 0.00658, entropy= 0.88579, loss= 0.00658
surrogate=-0.01219, entropy= 0.88546, loss=-0.01219
surrogate= 0.01259, entropy= 0.88509, loss= 0.01259
surrogate=-0.00993, entropy= 0.88560, loss=-0.00993
surrogate=-0.01919, entropy= 0.88558, loss=-0.01919
surrogate=-0.03755, entropy= 0.88577, loss=-0.03755
surrogate=-0.03133, entropy= 0.88698, loss=-0.03133
std_min= 0.26689, std_max= 0.39962, std_mean= 0.32967
val lr: [0.0001091188524590164], policy lr: [0.00013094262295081967]
Policy Loss: -0.031331, | Entropy Bonus: -0, | Value Loss: 22.749, | Advantage Loss: 5.0242
Time elapsed (s): 1.673593521118164
Agent stdevs: 0.32966614
--------------------------------------------------------------------------------

Step 550
++++++++ Policy training ++++++++++
Current mean reward: 1356.416883 | mean episode length: 372.000000
val_loss=29.37907
val_loss=24.88731
val_loss=11.35285
val_loss=11.53270
val_loss=10.07681
val_loss=18.52371
val_loss= 9.46690
val_loss=10.40475
val_loss=12.24655
val_loss=19.75714
adv_loss= 1.64372
adv_loss= 4.31594
adv_loss= 2.52493
adv_loss= 1.86526
adv_loss= 1.60099
adv_loss= 6.50426
adv_loss= 5.99510
adv_loss= 5.37005
adv_loss=25.03153
adv_loss= 1.44205
surrogate= 0.00980, entropy= 0.88482, loss= 0.00980
surrogate=-0.02318, entropy= 0.88289, loss=-0.02318
surrogate=-0.00360, entropy= 0.88009, loss=-0.00360
surrogate=-0.02774, entropy= 0.87958, loss=-0.02774
surrogate= 0.00385, entropy= 0.87688, loss= 0.00385
surrogate=-0.02159, entropy= 0.87542, loss=-0.02159
surrogate=-0.02676, entropy= 0.87315, loss=-0.02676
surrogate=-0.02480, entropy= 0.87071, loss=-0.02480
surrogate=-0.02848, entropy= 0.86943, loss=-0.02848
surrogate= 0.00945, entropy= 0.86826, loss= 0.00945
std_min= 0.26502, std_max= 0.39926, std_mean= 0.32777
val lr: [0.0001088627049180328], policy lr: [0.00013063524590163934]
Policy Loss: 0.0094469, | Entropy Bonus: -0, | Value Loss: 19.757, | Advantage Loss: 1.442
Time elapsed (s): 1.7059392929077148
Agent stdevs: 0.32776767
--------------------------------------------------------------------------------

Step 551
++++++++ Policy training ++++++++++
Current mean reward: 1651.930803 | mean episode length: 451.750000
val_loss=27.78597
val_loss=36.65006
val_loss=17.83125
val_loss= 6.36416
val_loss=12.52659
val_loss=13.47739
val_loss= 7.38231
val_loss=10.57132
val_loss= 9.69825
val_loss=10.80325
adv_loss= 2.82324
adv_loss= 1.52413
adv_loss= 2.30681
adv_loss= 1.60755
adv_loss= 2.29606
adv_loss= 2.04045
adv_loss= 4.13473
adv_loss= 2.96475
adv_loss= 5.30747
adv_loss= 2.00461
surrogate= 0.00059, entropy= 0.86948, loss= 0.00059
surrogate=-0.01422, entropy= 0.87090, loss=-0.01422
surrogate= 0.03969, entropy= 0.87277, loss= 0.03969
surrogate=-0.01383, entropy= 0.87475, loss=-0.01383
surrogate=-0.02787, entropy= 0.87619, loss=-0.02787
surrogate= 0.01813, entropy= 0.87750, loss= 0.01813
surrogate= 0.00414, entropy= 0.87757, loss= 0.00414
surrogate= 0.00236, entropy= 0.87745, loss= 0.00236
surrogate=-0.02926, entropy= 0.87829, loss=-0.02926
surrogate=-0.06120, entropy= 0.87990, loss=-0.06120
std_min= 0.26677, std_max= 0.39895, std_mean= 0.32888
val lr: [0.00010860655737704919], policy lr: [0.00013032786885245902]
Policy Loss: -0.061196, | Entropy Bonus: -0, | Value Loss: 10.803, | Advantage Loss: 2.0046
Time elapsed (s): 1.693986177444458
Agent stdevs: 0.32887682
--------------------------------------------------------------------------------

Step 552
++++++++ Policy training ++++++++++
Current mean reward: 3136.808189 | mean episode length: 860.000000
val_loss=12.27240
val_loss=11.32261
val_loss=20.24376
val_loss=17.05961
val_loss=11.66032
val_loss=23.35243
val_loss=10.22590
val_loss= 8.50162
val_loss= 9.12585
val_loss=11.63152
adv_loss= 1.18133
adv_loss= 3.09239
adv_loss= 1.26827
adv_loss= 1.02242
adv_loss= 3.19406
adv_loss= 3.04482
adv_loss= 5.13565
adv_loss= 5.55522
adv_loss= 1.36448
adv_loss= 4.35549
surrogate=-0.00599, entropy= 0.88040, loss=-0.00599
surrogate= 0.02067, entropy= 0.88117, loss= 0.02067
surrogate=-0.00626, entropy= 0.88197, loss=-0.00626
surrogate=-0.00274, entropy= 0.88144, loss=-0.00274
surrogate=-0.04204, entropy= 0.88077, loss=-0.04204
surrogate=-0.02186, entropy= 0.88148, loss=-0.02186
surrogate=-0.00977, entropy= 0.88124, loss=-0.00977
surrogate=-0.00957, entropy= 0.88049, loss=-0.00957
surrogate=-0.02366, entropy= 0.88008, loss=-0.02366
surrogate=-0.03808, entropy= 0.87919, loss=-0.03808
std_min= 0.26640, std_max= 0.39778, std_mean= 0.32875
val lr: [0.00010835040983606559], policy lr: [0.0001300204918032787]
Policy Loss: -0.038078, | Entropy Bonus: -0, | Value Loss: 11.632, | Advantage Loss: 4.3555
Time elapsed (s): 1.6850271224975586
Agent stdevs: 0.3287463
--------------------------------------------------------------------------------

Step 553
++++++++ Policy training ++++++++++
Current mean reward: 2952.264986 | mean episode length: 807.000000
val_loss= 8.38307
val_loss=17.21098
val_loss= 4.11955
val_loss= 5.39736
val_loss= 5.80498
val_loss= 7.89089
val_loss= 5.51414
val_loss= 4.88470
val_loss= 7.22786
val_loss= 5.05159
adv_loss= 2.38204
adv_loss= 2.07583
adv_loss= 2.86156
adv_loss= 2.89113
adv_loss= 0.97679
adv_loss= 0.40503
adv_loss= 0.66848
adv_loss= 1.10713
adv_loss= 6.66094
adv_loss= 1.25786
surrogate=-0.01001, entropy= 0.87806, loss=-0.01001
surrogate=-0.01143, entropy= 0.87725, loss=-0.01143
surrogate=-0.01123, entropy= 0.87735, loss=-0.01123
surrogate=-0.01645, entropy= 0.87633, loss=-0.01645
surrogate=-0.01506, entropy= 0.87462, loss=-0.01506
surrogate=-0.01370, entropy= 0.87506, loss=-0.01370
surrogate=-0.02082, entropy= 0.87426, loss=-0.02082
surrogate=-0.03183, entropy= 0.87379, loss=-0.03183
surrogate=-0.01881, entropy= 0.87377, loss=-0.01881
surrogate=-0.01925, entropy= 0.87325, loss=-0.01925
std_min= 0.26534, std_max= 0.39988, std_mean= 0.32832
val lr: [0.00010809426229508196], policy lr: [0.00012971311475409834]
Policy Loss: -0.019255, | Entropy Bonus: -0, | Value Loss: 5.0516, | Advantage Loss: 1.2579
Time elapsed (s): 1.6737217903137207
Agent stdevs: 0.3283203
--------------------------------------------------------------------------------

Step 554
++++++++ Policy training ++++++++++
Current mean reward: 3527.443001 | mean episode length: 1000.000000
val_loss=208.09416
val_loss=2081.88623
val_loss=121.50946
val_loss=625.82275
val_loss=491.37048
val_loss=507.36719
val_loss=217.97595
val_loss=2853.14551
val_loss=100.97024
val_loss=461.39911
adv_loss=1905.29968
adv_loss= 1.68581
adv_loss= 1.85173
adv_loss= 1.30515
adv_loss= 3.16607
adv_loss= 1.34570
adv_loss= 2.41137
adv_loss= 2.17477
adv_loss= 1.35810
adv_loss= 1.24787
surrogate=-0.02145, entropy= 0.87281, loss=-0.02145
surrogate=-0.00796, entropy= 0.87395, loss=-0.00796
surrogate= 0.00330, entropy= 0.87460, loss= 0.00330
surrogate=-0.00400, entropy= 0.87548, loss=-0.00400
surrogate=-0.01613, entropy= 0.87557, loss=-0.01613
surrogate=-0.02720, entropy= 0.87566, loss=-0.02720
surrogate=-0.00696, entropy= 0.87711, loss=-0.00696
surrogate=-0.00929, entropy= 0.87727, loss=-0.00929
surrogate= 0.01067, entropy= 0.87794, loss= 0.01067
surrogate=-0.00794, entropy= 0.87832, loss=-0.00794
std_min= 0.26670, std_max= 0.39820, std_mean= 0.32866
val lr: [0.00010783811475409836], policy lr: [0.000129405737704918]
Policy Loss: -0.007941, | Entropy Bonus: -0, | Value Loss: 461.4, | Advantage Loss: 1.2479
Time elapsed (s): 1.6671168804168701
Agent stdevs: 0.32865945
--------------------------------------------------------------------------------

Step 555
++++++++ Policy training ++++++++++
Current mean reward: 1817.502389 | mean episode length: 495.250000
val_loss=18.74685
val_loss=17.13748
val_loss=18.02182
val_loss=18.39324
val_loss=10.50241
val_loss=16.70872
val_loss= 5.40942
val_loss= 9.73150
val_loss=16.80276
val_loss=12.45131
adv_loss= 2.28895
adv_loss= 8.49399
adv_loss= 0.97300
adv_loss= 1.82865
adv_loss= 2.68060
adv_loss= 1.63739
adv_loss= 1.66613
adv_loss= 1.17543
adv_loss= 1.16149
adv_loss= 3.26501
surrogate= 0.02604, entropy= 0.87738, loss= 0.02604
surrogate=-0.00664, entropy= 0.87692, loss=-0.00664
surrogate=-0.00466, entropy= 0.87701, loss=-0.00466
surrogate=-0.01417, entropy= 0.87635, loss=-0.01417
surrogate= 0.02443, entropy= 0.87668, loss= 0.02443
surrogate=-0.01677, entropy= 0.87804, loss=-0.01677
surrogate= 0.00498, entropy= 0.87736, loss= 0.00498
surrogate= 0.00516, entropy= 0.87659, loss= 0.00516
surrogate= 0.01004, entropy= 0.87702, loss= 0.01004
surrogate=-0.01545, entropy= 0.87632, loss=-0.01545
std_min= 0.26835, std_max= 0.39666, std_mean= 0.32825
val lr: [0.00010758196721311475], policy lr: [0.00012909836065573769]
Policy Loss: -0.015445, | Entropy Bonus: -0, | Value Loss: 12.451, | Advantage Loss: 3.265
Time elapsed (s): 1.6841819286346436
Agent stdevs: 0.32824942
--------------------------------------------------------------------------------

Step 556
++++++++ Policy training ++++++++++
Current mean reward: 2246.032798 | mean episode length: 611.333333
val_loss=15.86501
val_loss=13.84226
val_loss=10.20265
val_loss=11.01429
val_loss=12.01739
val_loss= 6.91691
val_loss= 8.45253
val_loss= 7.04363
val_loss= 6.34258
val_loss= 6.73225
adv_loss= 3.33577
adv_loss= 1.45274
adv_loss= 1.08311
adv_loss= 1.40482
adv_loss= 9.16559
adv_loss= 0.96210
adv_loss= 1.18718
adv_loss= 1.00291
adv_loss= 1.03711
adv_loss= 1.16687
surrogate= 0.00781, entropy= 0.87779, loss= 0.00781
surrogate= 0.02609, entropy= 0.87851, loss= 0.02609
surrogate=-0.02002, entropy= 0.87989, loss=-0.02002
surrogate=-0.00634, entropy= 0.88096, loss=-0.00634
surrogate=-0.02047, entropy= 0.88190, loss=-0.02047
surrogate=-0.02455, entropy= 0.88358, loss=-0.02455
surrogate=-0.02248, entropy= 0.88391, loss=-0.02248
surrogate=-0.01784, entropy= 0.88408, loss=-0.01784
surrogate=-0.04350, entropy= 0.88600, loss=-0.04350
surrogate=-0.01116, entropy= 0.88573, loss=-0.01116
std_min= 0.27087, std_max= 0.39647, std_mean= 0.32908
val lr: [0.00010732581967213115], policy lr: [0.00012879098360655736]
Policy Loss: -0.011163, | Entropy Bonus: -0, | Value Loss: 6.7323, | Advantage Loss: 1.1669
Time elapsed (s): 1.7250425815582275
Agent stdevs: 0.32907602
--------------------------------------------------------------------------------

Step 557
++++++++ Policy training ++++++++++
Current mean reward: 2240.183573 | mean episode length: 611.000000
val_loss= 8.89086
val_loss= 5.74567
val_loss= 5.77753
val_loss= 6.18394
val_loss= 5.65336
val_loss= 5.89409
val_loss= 7.10235
val_loss= 4.04624
val_loss= 4.51983
val_loss= 7.87553
adv_loss= 7.75962
adv_loss= 1.74357
adv_loss= 2.34091
adv_loss= 1.36600
adv_loss= 0.80582
adv_loss= 0.49435
adv_loss= 1.99037
adv_loss= 0.73291
adv_loss= 0.90878
adv_loss= 1.81088
surrogate= 0.00480, entropy= 0.88507, loss= 0.00480
surrogate= 0.01491, entropy= 0.88358, loss= 0.01491
surrogate=-0.01474, entropy= 0.88334, loss=-0.01474
surrogate=-0.02289, entropy= 0.88105, loss=-0.02289
surrogate= 0.00170, entropy= 0.88046, loss= 0.00170
surrogate= 0.00376, entropy= 0.88055, loss= 0.00376
surrogate=-0.00157, entropy= 0.87852, loss=-0.00157
surrogate=-0.02561, entropy= 0.87776, loss=-0.02561
surrogate=-0.01601, entropy= 0.87753, loss=-0.01601
surrogate=-0.02415, entropy= 0.87663, loss=-0.02415
std_min= 0.26755, std_max= 0.39921, std_mean= 0.32849
val lr: [0.00010706967213114755], policy lr: [0.00012848360655737703]
Policy Loss: -0.024153, | Entropy Bonus: -0, | Value Loss: 7.8755, | Advantage Loss: 1.8109
Time elapsed (s): 1.6807355880737305
Agent stdevs: 0.32848904
--------------------------------------------------------------------------------

Step 558
++++++++ Policy training ++++++++++
Current mean reward: 2894.672486 | mean episode length: 809.000000
val_loss= 9.01322
val_loss=836.63879
val_loss=823.35010
val_loss=823.66614
val_loss=242.41464
val_loss=467.01141
val_loss=32.82884
val_loss=23.19994
val_loss=50.58436
val_loss=1856.56226
adv_loss= 0.91328
adv_loss= 1.15044
adv_loss= 0.93463
adv_loss= 1.37572
adv_loss= 1.14070
adv_loss= 0.86947
adv_loss=1906.73889
adv_loss= 0.94970
adv_loss= 2.39499
adv_loss= 0.97731
surrogate= 0.01237, entropy= 0.87488, loss= 0.01237
surrogate=-0.01070, entropy= 0.87491, loss=-0.01070
surrogate=-0.00486, entropy= 0.87429, loss=-0.00486
surrogate=-0.01736, entropy= 0.87482, loss=-0.01736
surrogate=-0.00212, entropy= 0.87544, loss=-0.00212
surrogate= 0.00855, entropy= 0.87499, loss= 0.00855
surrogate= 0.02833, entropy= 0.87424, loss= 0.02833
surrogate=-0.03052, entropy= 0.87334, loss=-0.03052
surrogate=-0.03102, entropy= 0.87354, loss=-0.03102
surrogate=-0.01965, entropy= 0.87338, loss=-0.01965
std_min= 0.26751, std_max= 0.39751, std_mean= 0.32803
val lr: [0.00010681352459016395], policy lr: [0.0001281762295081967]
Policy Loss: -0.019655, | Entropy Bonus: -0, | Value Loss: 1856.6, | Advantage Loss: 0.97731
Time elapsed (s): 1.6675727367401123
Agent stdevs: 0.32803425
--------------------------------------------------------------------------------

Step 559
++++++++ Policy training ++++++++++
Current mean reward: 1211.564593 | mean episode length: 333.400000
val_loss=37.46162
val_loss= 6.94068
val_loss=10.39384
val_loss=24.78532
val_loss=11.58002
val_loss=10.92574
val_loss= 7.96300
val_loss= 7.94732
val_loss=10.99643
val_loss= 8.94083
adv_loss= 4.24009
adv_loss= 3.59331
adv_loss= 3.33715
adv_loss= 2.60818
adv_loss= 1.68679
adv_loss= 2.22643
adv_loss= 8.68176
adv_loss= 2.68984
adv_loss= 3.26183
adv_loss= 3.17621
surrogate= 0.00029, entropy= 0.87193, loss= 0.00029
surrogate=-0.01987, entropy= 0.87215, loss=-0.01987
surrogate=-0.01584, entropy= 0.87193, loss=-0.01584
surrogate= 0.00656, entropy= 0.87042, loss= 0.00656
surrogate=-0.02545, entropy= 0.86913, loss=-0.02545
surrogate=-0.01816, entropy= 0.86841, loss=-0.01816
surrogate= 0.02663, entropy= 0.86705, loss= 0.02663
surrogate=-0.01197, entropy= 0.86698, loss=-0.01197
surrogate=-0.01053, entropy= 0.86654, loss=-0.01053
surrogate=-0.01010, entropy= 0.86649, loss=-0.01010
std_min= 0.26541, std_max= 0.39746, std_mean= 0.32743
val lr: [0.00010655737704918033], policy lr: [0.00012786885245901638]
Policy Loss: -0.010097, | Entropy Bonus: -0, | Value Loss: 8.9408, | Advantage Loss: 3.1762
Time elapsed (s): 1.6673102378845215
Agent stdevs: 0.3274337
--------------------------------------------------------------------------------

Step 560
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2082
++++++++ Policy training ++++++++++
Current mean reward: 1701.815643 | mean episode length: 458.000000
val_loss=13.43333
val_loss= 9.20926
val_loss= 7.25382
val_loss= 4.46976
val_loss= 9.80063
val_loss= 6.11735
val_loss=13.60095
val_loss= 5.90170
val_loss= 5.50876
val_loss= 6.28020
adv_loss= 4.93093
adv_loss= 0.95455
adv_loss= 0.45604
adv_loss= 0.68366
adv_loss= 1.81282
adv_loss= 5.73746
adv_loss= 7.52072
adv_loss= 1.13165
adv_loss= 1.94092
adv_loss= 0.82168
surrogate=-0.00313, entropy= 0.86426, loss=-0.00313
surrogate=-0.01443, entropy= 0.86026, loss=-0.01443
surrogate= 0.00731, entropy= 0.85893, loss= 0.00731
surrogate=-0.02944, entropy= 0.85677, loss=-0.02944
surrogate=-0.01699, entropy= 0.85541, loss=-0.01699
surrogate=-0.04280, entropy= 0.85295, loss=-0.04280
surrogate= 0.00953, entropy= 0.85027, loss= 0.00953
surrogate= 0.01597, entropy= 0.84818, loss= 0.01597
surrogate=-0.03248, entropy= 0.84665, loss=-0.03248
surrogate=-0.01575, entropy= 0.84459, loss=-0.01575
std_min= 0.26411, std_max= 0.39373, std_mean= 0.32495
val lr: [0.00010630122950819673], policy lr: [0.00012756147540983606]
Policy Loss: -0.015751, | Entropy Bonus: -0, | Value Loss: 6.2802, | Advantage Loss: 0.82168
Time elapsed (s): 1.684324026107788
Agent stdevs: 0.32494637
--------------------------------------------------------------------------------

Step 561
++++++++ Policy training ++++++++++
Current mean reward: 2125.988816 | mean episode length: 592.333333
val_loss=36.29551
val_loss=617.67200
val_loss=37.37975
val_loss=1070.53662
val_loss=16.25125
val_loss=770.87177
val_loss=72.40326
val_loss=997.50592
val_loss=57.12244
val_loss=1847.47791
adv_loss= 5.50874
adv_loss= 5.35348
adv_loss=1311.30334
adv_loss= 4.11725
adv_loss= 4.72374
adv_loss= 3.44345
adv_loss=13.64312
adv_loss=1305.14661
adv_loss= 3.65785
adv_loss= 3.63028
surrogate= 0.01383, entropy= 0.84355, loss= 0.01383
surrogate=-0.02757, entropy= 0.84360, loss=-0.02757
surrogate=-0.02155, entropy= 0.84419, loss=-0.02155
surrogate=-0.03452, entropy= 0.84386, loss=-0.03452
surrogate=-0.00973, entropy= 0.84379, loss=-0.00973
surrogate=-0.02860, entropy= 0.84383, loss=-0.02860
surrogate=-0.02869, entropy= 0.84279, loss=-0.02869
surrogate=-0.02240, entropy= 0.84349, loss=-0.02240
surrogate=-0.02147, entropy= 0.84310, loss=-0.02147
surrogate=-0.02282, entropy= 0.84270, loss=-0.02282
std_min= 0.26377, std_max= 0.39343, std_mean= 0.32476
val lr: [0.00010604508196721313], policy lr: [0.00012725409836065573]
Policy Loss: -0.02282, | Entropy Bonus: -0, | Value Loss: 1847.5, | Advantage Loss: 3.6303
Time elapsed (s): 1.671250820159912
Agent stdevs: 0.32476124
--------------------------------------------------------------------------------

Step 562
++++++++ Policy training ++++++++++
Current mean reward: 1852.917315 | mean episode length: 500.250000
val_loss=25.35354
val_loss= 4.60479
val_loss=10.48652
val_loss= 5.89918
val_loss= 7.74199
val_loss= 7.54970
val_loss= 5.32134
val_loss= 5.72564
val_loss= 6.97493
val_loss= 8.36493
adv_loss= 2.62585
adv_loss= 2.51785
adv_loss= 2.14968
adv_loss= 1.26277
adv_loss= 1.27513
adv_loss= 1.43040
adv_loss= 1.11808
adv_loss= 2.34092
adv_loss= 2.08763
adv_loss= 5.85633
surrogate=-0.00243, entropy= 0.84159, loss=-0.00243
surrogate= 0.03110, entropy= 0.83985, loss= 0.03110
surrogate=-0.03422, entropy= 0.83889, loss=-0.03422
surrogate= 0.00040, entropy= 0.83621, loss= 0.00040
surrogate= 0.00539, entropy= 0.83550, loss= 0.00539
surrogate=-0.02504, entropy= 0.83286, loss=-0.02504
surrogate= 0.01232, entropy= 0.83115, loss= 0.01232
surrogate=-0.03813, entropy= 0.82952, loss=-0.03813
surrogate=-0.02238, entropy= 0.82745, loss=-0.02238
surrogate=-0.02652, entropy= 0.82585, loss=-0.02652
std_min= 0.26247, std_max= 0.39042, std_mean= 0.32287
val lr: [0.0001057889344262295], policy lr: [0.00012694672131147538]
Policy Loss: -0.026521, | Entropy Bonus: -0, | Value Loss: 8.3649, | Advantage Loss: 5.8563
Time elapsed (s): 1.7194323539733887
Agent stdevs: 0.32287198
--------------------------------------------------------------------------------

Step 563
++++++++ Policy training ++++++++++
Current mean reward: 1498.208649 | mean episode length: 408.333333
val_loss=25.63716
val_loss= 9.31815
val_loss=13.67364
val_loss=13.50520
val_loss=11.79041
val_loss=12.38594
val_loss=11.52600
val_loss= 8.58096
val_loss=12.20065
val_loss= 4.69687
adv_loss= 0.81104
adv_loss= 4.51353
adv_loss= 3.83727
adv_loss= 6.72428
adv_loss= 1.60418
adv_loss= 1.10918
adv_loss= 0.64538
adv_loss= 2.04700
adv_loss= 5.38276
adv_loss= 1.82703
surrogate= 0.03158, entropy= 0.82352, loss= 0.03158
surrogate= 0.01051, entropy= 0.82141, loss= 0.01051
surrogate=-0.01474, entropy= 0.81918, loss=-0.01474
surrogate=-0.02495, entropy= 0.81671, loss=-0.02495
surrogate=-0.00952, entropy= 0.81382, loss=-0.00952
surrogate=-0.01430, entropy= 0.81190, loss=-0.01430
surrogate=-0.02051, entropy= 0.80940, loss=-0.02051
surrogate=-0.05282, entropy= 0.80652, loss=-0.05282
surrogate=-0.00065, entropy= 0.80561, loss=-0.00065
surrogate=-0.03757, entropy= 0.80268, loss=-0.03757
std_min= 0.26137, std_max= 0.38498, std_mean= 0.32017
val lr: [0.00010553278688524589], policy lr: [0.00012663934426229505]
Policy Loss: -0.037574, | Entropy Bonus: -0, | Value Loss: 4.6969, | Advantage Loss: 1.827
Time elapsed (s): 1.673238754272461
Agent stdevs: 0.3201749
--------------------------------------------------------------------------------

Step 564
++++++++ Policy training ++++++++++
Current mean reward: 1364.594006 | mean episode length: 374.200000
val_loss=43.64618
val_loss=28.77932
val_loss=33.41632
val_loss=18.52174
val_loss=17.13693
val_loss=14.67484
val_loss=25.74495
val_loss=18.50587
val_loss=19.20433
val_loss=20.71668
adv_loss= 4.39386
adv_loss= 1.33914
adv_loss= 9.14965
adv_loss= 9.43261
adv_loss=11.96976
adv_loss= 3.31385
adv_loss= 6.73943
adv_loss= 3.38934
adv_loss= 4.27339
adv_loss= 2.80142
surrogate= 0.03133, entropy= 0.80091, loss= 0.03133
surrogate= 0.01215, entropy= 0.79966, loss= 0.01215
surrogate= 0.00060, entropy= 0.79855, loss= 0.00060
surrogate= 0.00941, entropy= 0.79619, loss= 0.00941
surrogate=-0.02270, entropy= 0.79557, loss=-0.02270
surrogate= 0.01357, entropy= 0.79397, loss= 0.01357
surrogate=-0.04384, entropy= 0.79211, loss=-0.04384
surrogate=-0.03835, entropy= 0.79086, loss=-0.03835
surrogate=-0.02787, entropy= 0.79080, loss=-0.02787
surrogate=-0.03684, entropy= 0.79000, loss=-0.03684
std_min= 0.26156, std_max= 0.38095, std_mean= 0.31860
val lr: [0.00010527663934426229], policy lr: [0.00012633196721311473]
Policy Loss: -0.036842, | Entropy Bonus: -0, | Value Loss: 20.717, | Advantage Loss: 2.8014
Time elapsed (s): 1.6677587032318115
Agent stdevs: 0.31859806
--------------------------------------------------------------------------------

Step 565
++++++++ Policy training ++++++++++
Current mean reward: 3218.647047 | mean episode length: 888.000000
val_loss=150.55635
val_loss=3317.07104
val_loss=70.12678
val_loss=98.95345
val_loss=67.05865
val_loss=26.46437
val_loss=123.46497
val_loss=21.21381
val_loss=178.58633
val_loss=54.38529
adv_loss= 0.82553
adv_loss= 1.17296
adv_loss= 1.52313
adv_loss= 0.68021
adv_loss= 0.74632
adv_loss= 2.60478
adv_loss= 0.62863
adv_loss=1931.21875
adv_loss= 1.10389
adv_loss= 1.45070
surrogate=-0.01490, entropy= 0.79076, loss=-0.01490
surrogate= 0.01364, entropy= 0.79118, loss= 0.01364
surrogate=-0.02366, entropy= 0.79257, loss=-0.02366
surrogate=-0.00530, entropy= 0.79301, loss=-0.00530
surrogate=-0.00134, entropy= 0.79436, loss=-0.00134
surrogate=-0.02182, entropy= 0.79490, loss=-0.02182
surrogate=-0.00646, entropy= 0.79419, loss=-0.00646
surrogate=-0.02857, entropy= 0.79416, loss=-0.02857
surrogate=-0.03187, entropy= 0.79475, loss=-0.03187
surrogate=-0.01642, entropy= 0.79477, loss=-0.01642
std_min= 0.26262, std_max= 0.38211, std_mean= 0.31910
val lr: [0.00010502049180327869], policy lr: [0.0001260245901639344]
Policy Loss: -0.016416, | Entropy Bonus: -0, | Value Loss: 54.385, | Advantage Loss: 1.4507
Time elapsed (s): 1.646644115447998
Agent stdevs: 0.3191013
--------------------------------------------------------------------------------

Step 566
++++++++ Policy training ++++++++++
Current mean reward: 2190.496946 | mean episode length: 602.000000
val_loss=1547.03857
val_loss=206.62785
val_loss=15.91154
val_loss=234.80902
val_loss=47.37400
val_loss=1033.15259
val_loss=1462.50879
val_loss=1626.31360
val_loss=243.39839
val_loss=17.91627
adv_loss= 1.49883
adv_loss= 0.88710
adv_loss= 0.72865
adv_loss= 1.05722
adv_loss= 0.93761
adv_loss= 1.44392
adv_loss= 0.46729
adv_loss= 2.02899
adv_loss= 1.51346
adv_loss= 3.27380
surrogate= 0.01793, entropy= 0.79480, loss= 0.01793
surrogate= 0.00527, entropy= 0.79526, loss= 0.00527
surrogate= 0.01180, entropy= 0.79517, loss= 0.01180
surrogate=-0.00711, entropy= 0.79409, loss=-0.00711
surrogate= 0.02801, entropy= 0.79343, loss= 0.02801
surrogate=-0.01237, entropy= 0.79208, loss=-0.01237
surrogate=-0.03185, entropy= 0.79159, loss=-0.03185
surrogate=-0.03330, entropy= 0.79174, loss=-0.03330
surrogate=-0.01366, entropy= 0.79145, loss=-0.01366
surrogate=-0.00947, entropy= 0.79156, loss=-0.00947
std_min= 0.26121, std_max= 0.38253, std_mean= 0.31889
val lr: [0.00010476434426229509], policy lr: [0.00012571721311475408]
Policy Loss: -0.0094704, | Entropy Bonus: -0, | Value Loss: 17.916, | Advantage Loss: 3.2738
Time elapsed (s): 1.676252841949463
Agent stdevs: 0.31888515
--------------------------------------------------------------------------------

Step 567
++++++++ Policy training ++++++++++
Current mean reward: 1760.787470 | mean episode length: 485.250000
val_loss=10.40023
val_loss= 8.19898
val_loss= 8.52676
val_loss= 7.04602
val_loss=15.91038
val_loss= 7.55419
val_loss=10.42691
val_loss=12.45226
val_loss= 7.06950
val_loss= 6.75677
adv_loss= 1.31150
adv_loss= 1.32324
adv_loss= 2.84249
adv_loss= 2.46893
adv_loss= 1.42595
adv_loss= 3.72298
adv_loss= 2.86822
adv_loss= 2.44978
adv_loss= 2.99393
adv_loss= 2.39491
surrogate=-0.00940, entropy= 0.79158, loss=-0.00940
surrogate= 0.00749, entropy= 0.79103, loss= 0.00749
surrogate=-0.02378, entropy= 0.79117, loss=-0.02378
surrogate= 0.00731, entropy= 0.79040, loss= 0.00731
surrogate= 0.02988, entropy= 0.79093, loss= 0.02988
surrogate=-0.03778, entropy= 0.78987, loss=-0.03778
surrogate= 0.00862, entropy= 0.79025, loss= 0.00862
surrogate=-0.03581, entropy= 0.78999, loss=-0.03581
surrogate= 0.00156, entropy= 0.78961, loss= 0.00156
surrogate=-0.04366, entropy= 0.78931, loss=-0.04366
std_min= 0.26148, std_max= 0.38257, std_mean= 0.31863
val lr: [0.00010450819672131148], policy lr: [0.00012540983606557378]
Policy Loss: -0.043655, | Entropy Bonus: -0, | Value Loss: 6.7568, | Advantage Loss: 2.3949
Time elapsed (s): 1.6593718528747559
Agent stdevs: 0.31863058
--------------------------------------------------------------------------------

Step 568
++++++++ Policy training ++++++++++
Current mean reward: 2327.057647 | mean episode length: 646.666667
val_loss=42.75991
val_loss=291.80862
val_loss=126.39029
val_loss=1943.22925
val_loss=608.03137
val_loss=983.04431
val_loss=38.59513
val_loss=27.47229
val_loss=229.05203
val_loss=995.42047
adv_loss= 1.83360
adv_loss= 1.80252
adv_loss= 1.96998
adv_loss= 1.81971
adv_loss= 1.52654
adv_loss= 1.51193
adv_loss= 1.32274
adv_loss= 1.30198
adv_loss= 1.95845
adv_loss= 1.25996
surrogate= 0.00741, entropy= 0.78862, loss= 0.00741
surrogate=-0.01255, entropy= 0.78877, loss=-0.01255
surrogate= 0.00804, entropy= 0.78851, loss= 0.00804
surrogate= 0.00760, entropy= 0.78733, loss= 0.00760
surrogate=-0.01677, entropy= 0.78843, loss=-0.01677
surrogate=-0.01162, entropy= 0.78832, loss=-0.01162
surrogate=-0.01210, entropy= 0.78877, loss=-0.01210
surrogate=-0.00318, entropy= 0.78851, loss=-0.00318
surrogate=-0.01918, entropy= 0.78784, loss=-0.01918
surrogate=-0.02420, entropy= 0.78783, loss=-0.02420
std_min= 0.26190, std_max= 0.38082, std_mean= 0.31835
val lr: [0.00010425204918032788], policy lr: [0.00012510245901639345]
Policy Loss: -0.024198, | Entropy Bonus: -0, | Value Loss: 995.42, | Advantage Loss: 1.26
Time elapsed (s): 1.6918821334838867
Agent stdevs: 0.31834948
--------------------------------------------------------------------------------

Step 569
++++++++ Policy training ++++++++++
Current mean reward: 2214.872074 | mean episode length: 601.666667
val_loss=19.47003
val_loss=13.86095
val_loss=22.99282
val_loss= 8.00109
val_loss=27.68646
val_loss= 8.58587
val_loss= 5.29452
val_loss=11.52771
val_loss=10.93887
val_loss= 6.53208
adv_loss= 1.16694
adv_loss= 2.73214
adv_loss= 1.02898
adv_loss= 0.73894
adv_loss= 7.65768
adv_loss= 0.91215
adv_loss= 1.13139
adv_loss= 2.06819
adv_loss= 0.74188
adv_loss= 1.25572
surrogate= 0.00089, entropy= 0.78847, loss= 0.00089
surrogate= 0.00141, entropy= 0.78833, loss= 0.00141
surrogate=-0.02228, entropy= 0.78827, loss=-0.02228
surrogate=-0.01180, entropy= 0.78733, loss=-0.01180
surrogate=-0.02414, entropy= 0.78602, loss=-0.02414
surrogate=-0.02189, entropy= 0.78483, loss=-0.02189
surrogate=-0.04138, entropy= 0.78436, loss=-0.04138
surrogate=-0.02221, entropy= 0.78309, loss=-0.02221
surrogate=-0.01476, entropy= 0.78152, loss=-0.01476
surrogate=-0.02499, entropy= 0.78106, loss=-0.02499
std_min= 0.26229, std_max= 0.37952, std_mean= 0.31753
val lr: [0.00010399590163934428], policy lr: [0.00012479508196721313]
Policy Loss: -0.024993, | Entropy Bonus: -0, | Value Loss: 6.5321, | Advantage Loss: 1.2557
Time elapsed (s): 1.669233798980713
Agent stdevs: 0.3175319
--------------------------------------------------------------------------------

Step 570
++++++++ Policy training ++++++++++
Current mean reward: 1863.935289 | mean episode length: 507.250000
val_loss=14.10346
val_loss= 7.77375
val_loss= 6.25867
val_loss=30.96959
val_loss= 6.25602
val_loss= 6.16405
val_loss= 7.80797
val_loss=23.11941
val_loss= 6.60561
val_loss= 3.57115
adv_loss= 3.91334
adv_loss= 2.85712
adv_loss= 1.45475
adv_loss= 0.70768
adv_loss= 2.82676
adv_loss= 2.60106
adv_loss= 2.39893
adv_loss= 0.97370
adv_loss= 8.08167
adv_loss= 1.39878
surrogate= 0.03049, entropy= 0.78140, loss= 0.03049
surrogate=-0.01702, entropy= 0.78383, loss=-0.01702
surrogate= 0.00614, entropy= 0.78507, loss= 0.00614
surrogate=-0.02998, entropy= 0.78590, loss=-0.02998
surrogate= 0.00920, entropy= 0.78614, loss= 0.00920
surrogate=-0.02949, entropy= 0.78617, loss=-0.02949
surrogate=-0.02720, entropy= 0.78704, loss=-0.02720
surrogate=-0.02605, entropy= 0.78865, loss=-0.02605
surrogate=-0.05356, entropy= 0.78959, loss=-0.05356
surrogate=-0.01868, entropy= 0.78907, loss=-0.01868
std_min= 0.26204, std_max= 0.38123, std_mean= 0.31849
val lr: [0.00010373975409836065], policy lr: [0.00012448770491803275]
Policy Loss: -0.018679, | Entropy Bonus: -0, | Value Loss: 3.5712, | Advantage Loss: 1.3988
Time elapsed (s): 1.6654598712921143
Agent stdevs: 0.3184868
--------------------------------------------------------------------------------

Step 571
++++++++ Policy training ++++++++++
Current mean reward: 1461.510786 | mean episode length: 400.400000
val_loss=21.81930
val_loss=22.83261
val_loss=22.49334
val_loss=13.18787
val_loss=23.29224
val_loss= 9.12488
val_loss=20.79328
val_loss= 8.81725
val_loss=13.23300
val_loss=17.35170
adv_loss= 3.37714
adv_loss= 1.46774
adv_loss= 1.86090
adv_loss= 1.33341
adv_loss= 1.88917
adv_loss= 4.04998
adv_loss= 2.84740
adv_loss= 4.52614
adv_loss= 1.87858
adv_loss=14.82325
surrogate=-0.02048, entropy= 0.78809, loss=-0.02048
surrogate= 0.02490, entropy= 0.78786, loss= 0.02490
surrogate= 0.00342, entropy= 0.78619, loss= 0.00342
surrogate=-0.00625, entropy= 0.78667, loss=-0.00625
surrogate=-0.01581, entropy= 0.78776, loss=-0.01581
surrogate= 0.01175, entropy= 0.78768, loss= 0.01175
surrogate=-0.00836, entropy= 0.78645, loss=-0.00836
surrogate=-0.04417, entropy= 0.78676, loss=-0.04417
surrogate=-0.00450, entropy= 0.78619, loss=-0.00450
surrogate=-0.02210, entropy= 0.78486, loss=-0.02210
std_min= 0.26133, std_max= 0.37830, std_mean= 0.31792
val lr: [0.00010348360655737704], policy lr: [0.00012418032786885242]
Policy Loss: -0.022103, | Entropy Bonus: -0, | Value Loss: 17.352, | Advantage Loss: 14.823
Time elapsed (s): 1.6371448040008545
Agent stdevs: 0.3179212
--------------------------------------------------------------------------------

Step 572
++++++++ Policy training ++++++++++
Current mean reward: 1373.240115 | mean episode length: 371.800000
val_loss=12.02437
val_loss= 9.50591
val_loss= 5.95063
val_loss= 5.99349
val_loss= 7.36699
val_loss= 5.88853
val_loss= 5.18916
val_loss= 7.55096
val_loss= 3.60913
val_loss= 5.04015
adv_loss= 1.68880
adv_loss= 1.14745
adv_loss= 1.33969
adv_loss= 0.69896
adv_loss= 1.97274
adv_loss= 5.15798
adv_loss= 3.59045
adv_loss= 2.08578
adv_loss= 4.08378
adv_loss= 4.79456
surrogate= 0.02305, entropy= 0.78415, loss= 0.02305
surrogate=-0.00044, entropy= 0.78405, loss=-0.00044
surrogate=-0.01579, entropy= 0.78370, loss=-0.01579
surrogate=-0.01060, entropy= 0.78387, loss=-0.01060
surrogate=-0.02690, entropy= 0.78376, loss=-0.02690
surrogate= 0.00004, entropy= 0.78344, loss= 0.00004
surrogate=-0.03419, entropy= 0.78329, loss=-0.03419
surrogate=-0.03115, entropy= 0.78361, loss=-0.03115
surrogate=-0.01330, entropy= 0.78384, loss=-0.01330
surrogate=-0.03976, entropy= 0.78363, loss=-0.03976
std_min= 0.25947, std_max= 0.37742, std_mean= 0.31788
val lr: [0.00010322745901639344], policy lr: [0.00012387295081967212]
Policy Loss: -0.039765, | Entropy Bonus: -0, | Value Loss: 5.0402, | Advantage Loss: 4.7946
Time elapsed (s): 1.646118402481079
Agent stdevs: 0.31787798
--------------------------------------------------------------------------------

Step 573
++++++++ Policy training ++++++++++
Current mean reward: 1762.432965 | mean episode length: 476.250000
val_loss= 7.01043
val_loss= 6.35436
val_loss= 6.82902
val_loss= 9.73623
val_loss= 6.99837
val_loss= 7.26463
val_loss= 7.68483
val_loss= 7.51478
val_loss= 8.10664
val_loss= 6.77979
adv_loss= 1.41389
adv_loss= 1.99481
adv_loss= 1.96206
adv_loss= 1.87195
adv_loss= 1.97128
adv_loss= 3.36291
adv_loss= 0.94062
adv_loss= 1.15800
adv_loss= 0.34788
adv_loss= 1.13907
surrogate= 0.03209, entropy= 0.78398, loss= 0.03209
surrogate=-0.02315, entropy= 0.78414, loss=-0.02315
surrogate=-0.01634, entropy= 0.78444, loss=-0.01634
surrogate=-0.03204, entropy= 0.78430, loss=-0.03204
surrogate= 0.00081, entropy= 0.78460, loss= 0.00081
surrogate=-0.03802, entropy= 0.78371, loss=-0.03802
surrogate=-0.03750, entropy= 0.78277, loss=-0.03750
surrogate=-0.03887, entropy= 0.78229, loss=-0.03887
surrogate=-0.04186, entropy= 0.78078, loss=-0.04186
surrogate=-0.02329, entropy= 0.78072, loss=-0.02329
std_min= 0.25863, std_max= 0.37820, std_mean= 0.31768
val lr: [0.00010297131147540984], policy lr: [0.0001235655737704918]
Policy Loss: -0.023286, | Entropy Bonus: -0, | Value Loss: 6.7798, | Advantage Loss: 1.1391
Time elapsed (s): 1.6688945293426514
Agent stdevs: 0.31767836
--------------------------------------------------------------------------------

Step 574
++++++++ Policy training ++++++++++
Current mean reward: 1921.037261 | mean episode length: 534.000000
val_loss=237.81885
val_loss=552.50323
val_loss=30.05919
val_loss=299.12512
val_loss=421.42126
val_loss=95.29165
val_loss=35.90497
val_loss=66.97928
val_loss=268.38562
val_loss=430.69675
adv_loss= 1.75452
adv_loss= 3.68396
adv_loss= 2.37485
adv_loss= 2.09492
adv_loss= 1.28881
adv_loss= 0.99531
adv_loss= 1.65583
adv_loss= 2.22151
adv_loss= 0.79033
adv_loss= 1.05076
surrogate=-0.01419, entropy= 0.78006, loss=-0.01419
surrogate= 0.00696, entropy= 0.77951, loss= 0.00696
surrogate= 0.03855, entropy= 0.77892, loss= 0.03855
surrogate=-0.01050, entropy= 0.77779, loss=-0.01050
surrogate=-0.01079, entropy= 0.77770, loss=-0.01079
surrogate=-0.01645, entropy= 0.77745, loss=-0.01645
surrogate= 0.00609, entropy= 0.77665, loss= 0.00609
surrogate=-0.01178, entropy= 0.77559, loss=-0.01178
surrogate= 0.03725, entropy= 0.77418, loss= 0.03725
surrogate=-0.01253, entropy= 0.77394, loss=-0.01253
std_min= 0.25802, std_max= 0.37881, std_mean= 0.31704
val lr: [0.00010271516393442624], policy lr: [0.00012325819672131147]
Policy Loss: -0.012535, | Entropy Bonus: -0, | Value Loss: 430.7, | Advantage Loss: 1.0508
Time elapsed (s): 1.6926600933074951
Agent stdevs: 0.3170396
--------------------------------------------------------------------------------

Step 575
++++++++ Policy training ++++++++++
Current mean reward: 1853.522580 | mean episode length: 504.000000
val_loss=54.42519
val_loss=13.63645
val_loss=18.34693
val_loss=17.97573
val_loss=11.82078
val_loss=14.09989
val_loss=13.23327
val_loss=10.48536
val_loss= 7.88842
val_loss=10.88222
adv_loss= 4.93740
adv_loss= 1.77654
adv_loss= 2.86036
adv_loss= 1.36814
adv_loss= 3.65611
adv_loss= 4.69333
adv_loss= 2.23153
adv_loss= 2.72926
adv_loss= 0.95799
adv_loss= 1.17105
surrogate= 0.01405, entropy= 0.77345, loss= 0.01405
surrogate= 0.02388, entropy= 0.77093, loss= 0.02388
surrogate=-0.01172, entropy= 0.77049, loss=-0.01172
surrogate=-0.03309, entropy= 0.76847, loss=-0.03309
surrogate=-0.03197, entropy= 0.76781, loss=-0.03197
surrogate=-0.02797, entropy= 0.76558, loss=-0.02797
surrogate=-0.01957, entropy= 0.76562, loss=-0.01957
surrogate= 0.00927, entropy= 0.76500, loss= 0.00927
surrogate=-0.01050, entropy= 0.76416, loss=-0.01050
surrogate=-0.01337, entropy= 0.76328, loss=-0.01337
std_min= 0.25721, std_max= 0.37576, std_mean= 0.31580
val lr: [0.00010245901639344262], policy lr: [0.00012295081967213115]
Policy Loss: -0.013373, | Entropy Bonus: -0, | Value Loss: 10.882, | Advantage Loss: 1.171
Time elapsed (s): 1.664916753768921
Agent stdevs: 0.3158035
--------------------------------------------------------------------------------

Step 576
++++++++ Policy training ++++++++++
Current mean reward: 1214.736660 | mean episode length: 326.833333
val_loss=18.88107
val_loss=28.16272
val_loss=21.46469
val_loss=12.77328
val_loss=13.64873
val_loss=18.31385
val_loss=20.08004
val_loss=22.71182
val_loss=26.75054
val_loss=10.04173
adv_loss= 6.70991
adv_loss=16.15619
adv_loss= 5.40334
adv_loss= 1.48436
adv_loss= 1.73208
adv_loss= 4.67541
adv_loss= 6.10608
adv_loss= 4.23421
adv_loss= 9.81246
adv_loss= 5.86959
surrogate=-0.01547, entropy= 0.76315, loss=-0.01547
surrogate= 0.00703, entropy= 0.76320, loss= 0.00703
surrogate= 0.03456, entropy= 0.76347, loss= 0.03456
surrogate= 0.00179, entropy= 0.76379, loss= 0.00179
surrogate=-0.00009, entropy= 0.76519, loss=-0.00009
surrogate=-0.00544, entropy= 0.76655, loss=-0.00544
surrogate=-0.00216, entropy= 0.76721, loss=-0.00216
surrogate=-0.00683, entropy= 0.76703, loss=-0.00683
surrogate=-0.00322, entropy= 0.76767, loss=-0.00322
surrogate=-0.03828, entropy= 0.76843, loss=-0.03828
std_min= 0.25706, std_max= 0.37811, std_mean= 0.31650
val lr: [0.00010220286885245902], policy lr: [0.00012264344262295082]
Policy Loss: -0.03828, | Entropy Bonus: -0, | Value Loss: 10.042, | Advantage Loss: 5.8696
Time elapsed (s): 1.674539566040039
Agent stdevs: 0.31649628
--------------------------------------------------------------------------------

Step 577
++++++++ Policy training ++++++++++
Current mean reward: 2024.206635 | mean episode length: 545.333333
val_loss=21.95012
val_loss=15.95536
val_loss=11.42616
val_loss=11.11518
val_loss= 8.25990
val_loss= 7.82316
val_loss= 9.36787
val_loss=15.53642
val_loss= 8.90738
val_loss=10.07737
adv_loss= 0.84487
adv_loss= 0.95753
adv_loss= 0.59549
adv_loss= 0.61669
adv_loss= 1.22362
adv_loss= 1.40402
adv_loss= 1.13899
adv_loss= 1.21249
adv_loss= 0.84179
adv_loss= 1.02496
surrogate= 0.00794, entropy= 0.76979, loss= 0.00794
surrogate=-0.00259, entropy= 0.77255, loss=-0.00259
surrogate= 0.00883, entropy= 0.77360, loss= 0.00883
surrogate=-0.00153, entropy= 0.77527, loss=-0.00153
surrogate=-0.02310, entropy= 0.77632, loss=-0.02310
surrogate=-0.02679, entropy= 0.77771, loss=-0.02679
surrogate=-0.02097, entropy= 0.77830, loss=-0.02097
surrogate=-0.03110, entropy= 0.77895, loss=-0.03110
surrogate=-0.04283, entropy= 0.78015, loss=-0.04283
surrogate=-0.05772, entropy= 0.78138, loss=-0.05772
std_min= 0.25748, std_max= 0.37996, std_mean= 0.31793
val lr: [0.00010194672131147542], policy lr: [0.0001223360655737705]
Policy Loss: -0.057718, | Entropy Bonus: -0, | Value Loss: 10.077, | Advantage Loss: 1.025
Time elapsed (s): 1.667452096939087
Agent stdevs: 0.3179332
--------------------------------------------------------------------------------

Step 578
++++++++ Policy training ++++++++++
Current mean reward: 2912.966189 | mean episode length: 782.500000
val_loss= 5.03183
val_loss= 7.77180
val_loss= 7.36192
val_loss=11.52818
val_loss= 9.13127
val_loss= 4.37705
val_loss= 4.80008
val_loss= 4.89039
val_loss= 8.49792
val_loss= 7.31132
adv_loss= 0.64266
adv_loss= 0.81740
adv_loss= 1.99196
adv_loss= 0.51826
adv_loss= 1.11328
adv_loss= 0.45127
adv_loss= 0.51730
adv_loss= 1.17026
adv_loss= 0.65657
adv_loss= 2.41143
surrogate= 0.00422, entropy= 0.78113, loss= 0.00422
surrogate=-0.02365, entropy= 0.77884, loss=-0.02365
surrogate= 0.00478, entropy= 0.77740, loss= 0.00478
surrogate=-0.02221, entropy= 0.77651, loss=-0.02221
surrogate= 0.00743, entropy= 0.77478, loss= 0.00743
surrogate=-0.01693, entropy= 0.77390, loss=-0.01693
surrogate=-0.03065, entropy= 0.77193, loss=-0.03065
surrogate=-0.00254, entropy= 0.77054, loss=-0.00254
surrogate=-0.04832, entropy= 0.76868, loss=-0.04832
surrogate=-0.02322, entropy= 0.76838, loss=-0.02322
std_min= 0.25752, std_max= 0.37761, std_mean= 0.31642
val lr: [0.00010169057377049182], policy lr: [0.00012202868852459017]
Policy Loss: -0.023219, | Entropy Bonus: -0, | Value Loss: 7.3113, | Advantage Loss: 2.4114
Time elapsed (s): 1.6850662231445312
Agent stdevs: 0.316417
--------------------------------------------------------------------------------

Step 579
++++++++ Policy training ++++++++++
Current mean reward: 1794.179275 | mean episode length: 483.000000
val_loss=14.16704
val_loss=20.63866
val_loss=11.31462
val_loss= 8.69316
val_loss=10.35330
val_loss= 9.28675
val_loss= 5.98031
val_loss= 9.72927
val_loss= 8.03492
val_loss= 8.86210
adv_loss= 0.97548
adv_loss= 3.36057
adv_loss= 1.54282
adv_loss= 1.24009
adv_loss= 0.97035
adv_loss= 1.17886
adv_loss= 2.17765
adv_loss= 3.13553
adv_loss= 0.72507
adv_loss= 0.74279
surrogate= 0.01588, entropy= 0.76658, loss= 0.01588
surrogate=-0.02448, entropy= 0.76386, loss=-0.02448
surrogate=-0.02163, entropy= 0.76428, loss=-0.02163
surrogate=-0.00546, entropy= 0.76470, loss=-0.00546
surrogate= 0.00961, entropy= 0.76344, loss= 0.00961
surrogate=-0.00575, entropy= 0.76279, loss=-0.00575
surrogate=-0.02995, entropy= 0.76231, loss=-0.02995
surrogate= 0.00101, entropy= 0.76248, loss= 0.00101
surrogate= 0.02710, entropy= 0.76115, loss= 0.02710
surrogate=-0.02344, entropy= 0.76136, loss=-0.02344
std_min= 0.25535, std_max= 0.37700, std_mean= 0.31582
val lr: [0.00010143442622950818], policy lr: [0.0001217213114754098]
Policy Loss: -0.023439, | Entropy Bonus: -0, | Value Loss: 8.8621, | Advantage Loss: 0.74279
Time elapsed (s): 1.6780004501342773
Agent stdevs: 0.31581852
--------------------------------------------------------------------------------

Step 580
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1959.9
++++++++ Policy training ++++++++++
Current mean reward: 1849.775679 | mean episode length: 502.666667
val_loss= 6.54531
val_loss= 9.52113
val_loss= 7.99569
val_loss= 9.92499
val_loss= 6.28059
val_loss= 7.53103
val_loss= 6.31590
val_loss= 5.25420
val_loss= 7.13561
val_loss= 7.17081
adv_loss= 1.45166
adv_loss= 0.25308
adv_loss= 0.90671
adv_loss= 1.12714
adv_loss= 0.59843
adv_loss= 1.46719
adv_loss= 1.03791
adv_loss= 1.01779
adv_loss= 0.33846
adv_loss= 1.55447
surrogate= 0.04185, entropy= 0.75938, loss= 0.04185
surrogate= 0.01883, entropy= 0.75654, loss= 0.01883
surrogate=-0.02057, entropy= 0.75458, loss=-0.02057
surrogate=-0.00948, entropy= 0.75255, loss=-0.00948
surrogate=-0.04770, entropy= 0.75138, loss=-0.04770
surrogate=-0.02485, entropy= 0.74948, loss=-0.02485
surrogate= 0.00906, entropy= 0.74853, loss= 0.00906
surrogate=-0.02645, entropy= 0.74787, loss=-0.02645
surrogate=-0.01005, entropy= 0.74654, loss=-0.01005
surrogate= 0.00212, entropy= 0.74525, loss= 0.00212
std_min= 0.25486, std_max= 0.37471, std_mean= 0.31404
val lr: [0.00010117827868852458], policy lr: [0.00012141393442622949]
Policy Loss: 0.0021167, | Entropy Bonus: -0, | Value Loss: 7.1708, | Advantage Loss: 1.5545
Time elapsed (s): 1.7070415019989014
Agent stdevs: 0.31403896
--------------------------------------------------------------------------------

Step 581
++++++++ Policy training ++++++++++
Current mean reward: 2273.479538 | mean episode length: 631.333333
val_loss=1255.65051
val_loss=11.65202
val_loss=16.77130
val_loss=667.45874
val_loss=647.37964
val_loss=2210.44849
val_loss=110.02749
val_loss=28.79978
val_loss=589.02203
val_loss=66.92759
adv_loss= 1.62943
adv_loss= 0.72357
adv_loss= 1.68090
adv_loss= 1.09490
adv_loss= 2.83776
adv_loss= 1.98580
adv_loss= 1.40953
adv_loss= 1.95632
adv_loss= 1.66773
adv_loss= 2.95865
surrogate= 0.02705, entropy= 0.74390, loss= 0.02705
surrogate=-0.00376, entropy= 0.74319, loss=-0.00376
surrogate= 0.01342, entropy= 0.74217, loss= 0.01342
surrogate= 0.00700, entropy= 0.74112, loss= 0.00700
surrogate=-0.01693, entropy= 0.74083, loss=-0.01693
surrogate= 0.00816, entropy= 0.74071, loss= 0.00816
surrogate=-0.02122, entropy= 0.74013, loss=-0.02122
surrogate=-0.01572, entropy= 0.73970, loss=-0.01572
surrogate= 0.00220, entropy= 0.74021, loss= 0.00220
surrogate=-0.00701, entropy= 0.74024, loss=-0.00701
std_min= 0.25498, std_max= 0.37319, std_mean= 0.31344
val lr: [0.00010092213114754098], policy lr: [0.00012110655737704917]
Policy Loss: -0.0070111, | Entropy Bonus: -0, | Value Loss: 66.928, | Advantage Loss: 2.9586
Time elapsed (s): 1.6711256504058838
Agent stdevs: 0.3134353
--------------------------------------------------------------------------------

Step 582
++++++++ Policy training ++++++++++
Current mean reward: 1802.384020 | mean episode length: 483.250000
val_loss=15.80586
val_loss= 7.38602
val_loss= 7.74286
val_loss= 8.87468
val_loss= 4.51271
val_loss= 6.41389
val_loss= 5.44311
val_loss= 4.31297
val_loss=10.91913
val_loss= 5.77473
adv_loss= 2.41188
adv_loss= 5.47241
adv_loss= 1.14841
adv_loss= 2.89440
adv_loss= 0.99734
adv_loss= 0.76484
adv_loss= 1.07631
adv_loss= 2.14256
adv_loss= 1.49929
adv_loss= 4.17745
surrogate= 0.02958, entropy= 0.74038, loss= 0.02958
surrogate=-0.02259, entropy= 0.74063, loss=-0.02259
surrogate=-0.02629, entropy= 0.74109, loss=-0.02629
surrogate=-0.01608, entropy= 0.74018, loss=-0.01608
surrogate= 0.00428, entropy= 0.74007, loss= 0.00428
surrogate=-0.00803, entropy= 0.74033, loss=-0.00803
surrogate=-0.01423, entropy= 0.73945, loss=-0.01423
surrogate= 0.00161, entropy= 0.73810, loss= 0.00161
surrogate= 0.01278, entropy= 0.73872, loss= 0.01278
surrogate= 0.00215, entropy= 0.73784, loss= 0.00215
std_min= 0.25415, std_max= 0.37210, std_mean= 0.31319
val lr: [0.00010066598360655738], policy lr: [0.00012079918032786884]
Policy Loss: 0.0021467, | Entropy Bonus: -0, | Value Loss: 5.7747, | Advantage Loss: 4.1775
Time elapsed (s): 1.654836893081665
Agent stdevs: 0.31318575
--------------------------------------------------------------------------------

Step 583
++++++++ Policy training ++++++++++
Current mean reward: 2758.587227 | mean episode length: 777.000000
val_loss=1564.77112
val_loss=3433.07397
val_loss=916.26190
val_loss=261.71918
val_loss=345.08047
val_loss=24.48486
val_loss=838.10101
val_loss=26.10368
val_loss=39.67652
val_loss=18.36273
adv_loss= 2.72655
adv_loss= 0.97077
adv_loss= 2.08427
adv_loss= 1.26648
adv_loss= 3.51132
adv_loss= 3.51690
adv_loss= 2.00657
adv_loss= 1.00331
adv_loss= 0.97125
adv_loss= 1.57042
surrogate= 0.00031, entropy= 0.73734, loss= 0.00031
surrogate=-0.00209, entropy= 0.73679, loss=-0.00209
surrogate=-0.01266, entropy= 0.73686, loss=-0.01266
surrogate=-0.00698, entropy= 0.73671, loss=-0.00698
surrogate=-0.02847, entropy= 0.73716, loss=-0.02847
surrogate=-0.00893, entropy= 0.73747, loss=-0.00893
surrogate=-0.02024, entropy= 0.73820, loss=-0.02024
surrogate= 0.00729, entropy= 0.73903, loss= 0.00729
surrogate=-0.01369, entropy= 0.74079, loss=-0.01369
surrogate= 0.01823, entropy= 0.74162, loss= 0.01823
std_min= 0.25412, std_max= 0.37234, std_mean= 0.31360
val lr: [0.00010040983606557377], policy lr: [0.00012049180327868852]
Policy Loss: 0.018231, | Entropy Bonus: -0, | Value Loss: 18.363, | Advantage Loss: 1.5704
Time elapsed (s): 1.6514832973480225
Agent stdevs: 0.31359854
--------------------------------------------------------------------------------

Step 584
++++++++ Policy training ++++++++++
Current mean reward: 1422.711676 | mean episode length: 388.750000
val_loss=20.10515
val_loss=12.88445
val_loss=16.98408
val_loss= 7.86096
val_loss=13.56385
val_loss= 9.37549
val_loss=16.44723
val_loss= 6.22139
val_loss= 8.67086
val_loss=11.16448
adv_loss= 0.84618
adv_loss= 0.39273
adv_loss= 1.22396
adv_loss= 0.75658
adv_loss= 4.52866
adv_loss= 0.74353
adv_loss= 2.41454
adv_loss= 0.81482
adv_loss= 1.03704
adv_loss= 2.77609
surrogate= 0.01848, entropy= 0.74123, loss= 0.01848
surrogate=-0.01077, entropy= 0.74067, loss=-0.01077
surrogate=-0.00745, entropy= 0.74000, loss=-0.00745
surrogate= 0.00497, entropy= 0.74015, loss= 0.00497
surrogate=-0.00209, entropy= 0.74125, loss=-0.00209
surrogate=-0.02332, entropy= 0.74129, loss=-0.02332
surrogate=-0.00362, entropy= 0.74201, loss=-0.00362
surrogate=-0.02575, entropy= 0.74117, loss=-0.02575
surrogate=-0.01473, entropy= 0.74069, loss=-0.01473
surrogate=-0.02524, entropy= 0.73983, loss=-0.02524
std_min= 0.25396, std_max= 0.37051, std_mean= 0.31334
val lr: [0.00010015368852459017], policy lr: [0.00012018442622950819]
Policy Loss: -0.025242, | Entropy Bonus: -0, | Value Loss: 11.164, | Advantage Loss: 2.7761
Time elapsed (s): 1.661564826965332
Agent stdevs: 0.3133354
--------------------------------------------------------------------------------

Step 585
++++++++ Policy training ++++++++++
Current mean reward: 2190.460600 | mean episode length: 588.333333
val_loss= 8.01920
val_loss= 7.92745
val_loss=10.85458
val_loss= 7.72122
val_loss= 2.13805
val_loss= 5.24141
val_loss= 4.29500
val_loss= 8.27656
val_loss= 2.88121
val_loss= 5.73431
adv_loss= 1.33007
adv_loss= 1.11449
adv_loss= 0.54348
adv_loss= 0.93202
adv_loss= 2.07655
adv_loss= 1.60133
adv_loss= 1.62848
adv_loss= 0.97501
adv_loss= 0.82854
adv_loss= 0.45894
surrogate= 0.01919, entropy= 0.74041, loss= 0.01919
surrogate=-0.03964, entropy= 0.74048, loss=-0.03964
surrogate= 0.01115, entropy= 0.74048, loss= 0.01115
surrogate= 0.02019, entropy= 0.74047, loss= 0.02019
surrogate=-0.01610, entropy= 0.74090, loss=-0.01610
surrogate=-0.00752, entropy= 0.74108, loss=-0.00752
surrogate=-0.00298, entropy= 0.74119, loss=-0.00298
surrogate=-0.01796, entropy= 0.74046, loss=-0.01796
surrogate=-0.04075, entropy= 0.74020, loss=-0.04075
surrogate=-0.01321, entropy= 0.73968, loss=-0.01321
std_min= 0.25388, std_max= 0.37047, std_mean= 0.31332
val lr: [9.989754098360657e-05], policy lr: [0.00011987704918032786]
Policy Loss: -0.013207, | Entropy Bonus: -0, | Value Loss: 5.7343, | Advantage Loss: 0.45894
Time elapsed (s): 1.6656203269958496
Agent stdevs: 0.3133193
--------------------------------------------------------------------------------

Step 586
++++++++ Policy training ++++++++++
Current mean reward: 1811.303793 | mean episode length: 491.250000
val_loss=16.66203
val_loss= 8.98207
val_loss=12.72848
val_loss=14.17001
val_loss= 9.01903
val_loss=11.73650
val_loss=11.93711
val_loss=11.32567
val_loss= 8.50229
val_loss= 8.15606
adv_loss= 2.18253
adv_loss= 1.19987
adv_loss= 0.84973
adv_loss= 0.93101
adv_loss= 1.21574
adv_loss= 0.50759
adv_loss= 1.68305
adv_loss= 1.41745
adv_loss= 0.85861
adv_loss= 3.71054
surrogate=-0.00858, entropy= 0.73998, loss=-0.00858
surrogate=-0.00093, entropy= 0.74114, loss=-0.00093
surrogate=-0.01900, entropy= 0.74190, loss=-0.01900
surrogate=-0.00113, entropy= 0.74222, loss=-0.00113
surrogate= 0.01092, entropy= 0.74269, loss= 0.01092
surrogate= 0.01494, entropy= 0.74427, loss= 0.01494
surrogate=-0.03883, entropy= 0.74402, loss=-0.03883
surrogate=-0.02992, entropy= 0.74410, loss=-0.02992
surrogate=-0.01601, entropy= 0.74496, loss=-0.01601
surrogate=-0.02546, entropy= 0.74423, loss=-0.02546
std_min= 0.25298, std_max= 0.37207, std_mean= 0.31396
val lr: [9.964139344262297e-05], policy lr: [0.00011956967213114754]
Policy Loss: -0.025458, | Entropy Bonus: -0, | Value Loss: 8.1561, | Advantage Loss: 3.7105
Time elapsed (s): 1.6904520988464355
Agent stdevs: 0.31395623
--------------------------------------------------------------------------------

Step 587
++++++++ Policy training ++++++++++
Current mean reward: 1695.974284 | mean episode length: 457.750000
val_loss= 7.33919
val_loss= 7.36851
val_loss= 6.08276
val_loss= 5.86650
val_loss= 4.04616
val_loss= 4.61324
val_loss=10.07011
val_loss= 4.38376
val_loss= 4.24218
val_loss= 6.47211
adv_loss= 1.02773
adv_loss= 0.52509
adv_loss= 0.78395
adv_loss= 0.99219
adv_loss= 0.90075
adv_loss= 0.78473
adv_loss= 0.84939
adv_loss= 0.96340
adv_loss= 1.54082
adv_loss= 1.51891
surrogate= 0.00409, entropy= 0.74535, loss= 0.00409
surrogate=-0.01494, entropy= 0.74608, loss=-0.01494
surrogate=-0.01067, entropy= 0.74535, loss=-0.01067
surrogate=-0.01210, entropy= 0.74498, loss=-0.01210
surrogate=-0.03005, entropy= 0.74431, loss=-0.03005
surrogate=-0.04599, entropy= 0.74364, loss=-0.04599
surrogate=-0.02440, entropy= 0.74343, loss=-0.02440
surrogate=-0.03740, entropy= 0.74287, loss=-0.03740
surrogate=-0.00656, entropy= 0.74341, loss=-0.00656
surrogate=-0.00416, entropy= 0.74323, loss=-0.00416
std_min= 0.25330, std_max= 0.37158, std_mean= 0.31379
val lr: [9.938524590163935e-05], policy lr: [0.00011926229508196721]
Policy Loss: -0.0041581, | Entropy Bonus: -0, | Value Loss: 6.4721, | Advantage Loss: 1.5189
Time elapsed (s): 1.6627395153045654
Agent stdevs: 0.31379354
--------------------------------------------------------------------------------

Step 588
++++++++ Policy training ++++++++++
Current mean reward: 2425.781344 | mean episode length: 655.000000
val_loss=12.43795
val_loss=12.91843
val_loss= 8.27207
val_loss= 6.81074
val_loss= 2.83802
val_loss= 4.04738
val_loss= 8.38825
val_loss= 8.25484
val_loss= 7.38194
val_loss= 8.10680
adv_loss= 1.05828
adv_loss= 0.89209
adv_loss= 0.89665
adv_loss= 3.00321
adv_loss= 1.08850
adv_loss= 1.36530
adv_loss= 0.89524
adv_loss= 0.63150
adv_loss= 0.71648
adv_loss= 1.49822
surrogate= 0.01246, entropy= 0.74429, loss= 0.01246
surrogate= 0.01680, entropy= 0.74599, loss= 0.01680
surrogate=-0.02541, entropy= 0.74784, loss=-0.02541
surrogate=-0.00067, entropy= 0.74979, loss=-0.00067
surrogate=-0.01082, entropy= 0.74983, loss=-0.01082
surrogate=-0.01671, entropy= 0.75204, loss=-0.01671
surrogate=-0.04213, entropy= 0.75266, loss=-0.04213
surrogate=-0.02654, entropy= 0.75461, loss=-0.02654
surrogate=-0.00451, entropy= 0.75589, loss=-0.00451
surrogate=-0.02114, entropy= 0.75662, loss=-0.02114
std_min= 0.25461, std_max= 0.37285, std_mean= 0.31516
val lr: [9.912909836065573e-05], policy lr: [0.00011895491803278686]
Policy Loss: -0.021137, | Entropy Bonus: -0, | Value Loss: 8.1068, | Advantage Loss: 1.4982
Time elapsed (s): 1.6764569282531738
Agent stdevs: 0.3151615
--------------------------------------------------------------------------------

Step 589
++++++++ Policy training ++++++++++
Current mean reward: 1886.759956 | mean episode length: 513.333333
val_loss=10.83274
val_loss=12.26053
val_loss= 7.46168
val_loss= 9.02158
val_loss= 7.72003
val_loss= 8.52665
val_loss= 9.60410
val_loss= 6.29818
val_loss=14.97480
val_loss= 4.10991
adv_loss= 0.46452
adv_loss= 0.54224
adv_loss= 7.33384
adv_loss= 0.63662
adv_loss= 1.06618
adv_loss= 7.41099
adv_loss= 2.82496
adv_loss= 1.24838
adv_loss= 0.61945
adv_loss= 2.19125
surrogate= 0.01747, entropy= 0.75522, loss= 0.01747
surrogate= 0.00395, entropy= 0.75598, loss= 0.00395
surrogate=-0.01801, entropy= 0.75604, loss=-0.01801
surrogate=-0.02041, entropy= 0.75575, loss=-0.02041
surrogate=-0.01463, entropy= 0.75650, loss=-0.01463
surrogate=-0.01914, entropy= 0.75682, loss=-0.01914
surrogate=-0.02168, entropy= 0.75661, loss=-0.02168
surrogate=-0.03583, entropy= 0.75676, loss=-0.03583
surrogate=-0.03710, entropy= 0.75687, loss=-0.03710
surrogate=-0.02652, entropy= 0.75634, loss=-0.02652
std_min= 0.25408, std_max= 0.37178, std_mean= 0.31513
val lr: [9.887295081967213e-05], policy lr: [0.00011864754098360654]
Policy Loss: -0.02652, | Entropy Bonus: -0, | Value Loss: 4.1099, | Advantage Loss: 2.1912
Time elapsed (s): 1.6559233665466309
Agent stdevs: 0.31513014
--------------------------------------------------------------------------------

Step 590
++++++++ Policy training ++++++++++
Current mean reward: 1442.334468 | mean episode length: 392.000000
val_loss= 5.45951
val_loss= 5.01464
val_loss= 6.65131
val_loss= 5.19952
val_loss= 5.02180
val_loss= 9.81893
val_loss= 5.51233
val_loss= 7.84919
val_loss= 5.48599
val_loss= 3.81761
adv_loss= 1.20803
adv_loss= 0.43852
adv_loss= 0.70035
adv_loss= 0.85885
adv_loss= 3.73560
adv_loss= 0.86261
adv_loss= 1.06067
adv_loss= 2.23823
adv_loss= 1.67976
adv_loss= 0.95992
surrogate=-0.00610, entropy= 0.75660, loss=-0.00610
surrogate=-0.02932, entropy= 0.75684, loss=-0.02932
surrogate=-0.01365, entropy= 0.75711, loss=-0.01365
surrogate=-0.01481, entropy= 0.75879, loss=-0.01481
surrogate= 0.00703, entropy= 0.75919, loss= 0.00703
surrogate=-0.02530, entropy= 0.76027, loss=-0.02530
surrogate= 0.00278, entropy= 0.76163, loss= 0.00278
surrogate=-0.02192, entropy= 0.76307, loss=-0.02192
surrogate=-0.02184, entropy= 0.76333, loss=-0.02184
surrogate=-0.00962, entropy= 0.76477, loss=-0.00962
std_min= 0.25511, std_max= 0.37406, std_mean= 0.31604
val lr: [9.861680327868853e-05], policy lr: [0.00011834016393442621]
Policy Loss: -0.0096209, | Entropy Bonus: -0, | Value Loss: 3.8176, | Advantage Loss: 0.95992
Time elapsed (s): 1.667090654373169
Agent stdevs: 0.3160421
--------------------------------------------------------------------------------

Step 591
++++++++ Policy training ++++++++++
Current mean reward: 1789.210279 | mean episode length: 484.500000
val_loss=12.69355
val_loss= 9.80011
val_loss= 9.39242
val_loss=12.81156
val_loss= 7.07722
val_loss= 7.54352
val_loss= 6.49746
val_loss= 6.57450
val_loss= 5.72026
val_loss= 4.68540
adv_loss= 1.59816
adv_loss= 1.17019
adv_loss= 1.27715
adv_loss= 1.86794
adv_loss= 0.60485
adv_loss= 1.49238
adv_loss= 2.43120
adv_loss= 0.96299
adv_loss= 3.43459
adv_loss= 8.38182
surrogate= 0.00066, entropy= 0.76332, loss= 0.00066
surrogate= 0.00462, entropy= 0.76237, loss= 0.00462
surrogate= 0.01127, entropy= 0.76094, loss= 0.01127
surrogate=-0.01177, entropy= 0.75926, loss=-0.01177
surrogate=-0.04334, entropy= 0.75674, loss=-0.04334
surrogate= 0.01571, entropy= 0.75437, loss= 0.01571
surrogate=-0.02556, entropy= 0.75384, loss=-0.02556
surrogate=-0.00395, entropy= 0.75195, loss=-0.00395
surrogate=-0.04095, entropy= 0.75010, loss=-0.04095
surrogate=-0.01856, entropy= 0.74920, loss=-0.01856
std_min= 0.25521, std_max= 0.37277, std_mean= 0.31433
val lr: [9.836065573770491e-05], policy lr: [0.00011803278688524588]
Policy Loss: -0.018555, | Entropy Bonus: -0, | Value Loss: 4.6854, | Advantage Loss: 8.3818
Time elapsed (s): 1.6933414936065674
Agent stdevs: 0.3143324
--------------------------------------------------------------------------------

Step 592
++++++++ Policy training ++++++++++
Current mean reward: 1145.591001 | mean episode length: 312.500000
val_loss=20.47188
val_loss=12.03903
val_loss=16.99885
val_loss= 9.94743
val_loss=12.71850
val_loss=14.41371
val_loss=13.92947
val_loss= 9.53032
val_loss=16.19727
val_loss=11.06594
adv_loss= 0.83825
adv_loss= 0.98811
adv_loss= 1.11545
adv_loss= 1.13794
adv_loss= 1.07289
adv_loss= 1.23202
adv_loss= 1.16556
adv_loss= 1.71588
adv_loss=12.72126
adv_loss= 1.09997
surrogate= 0.01550, entropy= 0.75027, loss= 0.01550
surrogate=-0.01009, entropy= 0.75285, loss=-0.01009
surrogate=-0.02478, entropy= 0.75437, loss=-0.02478
surrogate=-0.01511, entropy= 0.75567, loss=-0.01511
surrogate= 0.00747, entropy= 0.75760, loss= 0.00747
surrogate=-0.05662, entropy= 0.75897, loss=-0.05662
surrogate=-0.02730, entropy= 0.76015, loss=-0.02730
surrogate=-0.00470, entropy= 0.76145, loss=-0.00470
surrogate=-0.05290, entropy= 0.76282, loss=-0.05290
surrogate=-0.00644, entropy= 0.76473, loss=-0.00644
std_min= 0.25725, std_max= 0.37587, std_mean= 0.31597
val lr: [9.810450819672131e-05], policy lr: [0.00011772540983606556]
Policy Loss: -0.0064362, | Entropy Bonus: -0, | Value Loss: 11.066, | Advantage Loss: 1.1
Time elapsed (s): 1.7181568145751953
Agent stdevs: 0.31597474
--------------------------------------------------------------------------------

Step 593
++++++++ Policy training ++++++++++
Current mean reward: 2901.933013 | mean episode length: 791.500000
val_loss=16.40818
val_loss=13.43844
val_loss= 9.67379
val_loss=16.14124
val_loss= 8.97876
val_loss= 8.37717
val_loss=16.96540
val_loss=13.28809
val_loss= 9.62855
val_loss= 5.74289
adv_loss= 0.70038
adv_loss= 0.81343
adv_loss= 0.72472
adv_loss= 1.09449
adv_loss= 0.49756
adv_loss= 1.15428
adv_loss= 1.37330
adv_loss= 0.66063
adv_loss= 0.89017
adv_loss= 0.69146
surrogate= 0.00874, entropy= 0.76567, loss= 0.00874
surrogate=-0.02410, entropy= 0.76629, loss=-0.02410
surrogate= 0.00854, entropy= 0.76673, loss= 0.00854
surrogate=-0.01102, entropy= 0.76741, loss=-0.01102
surrogate=-0.03737, entropy= 0.76881, loss=-0.03737
surrogate=-0.00064, entropy= 0.77000, loss=-0.00064
surrogate=-0.02978, entropy= 0.77001, loss=-0.02978
surrogate=-0.01296, entropy= 0.77115, loss=-0.01296
surrogate=-0.02141, entropy= 0.77136, loss=-0.02141
surrogate=-0.00930, entropy= 0.77100, loss=-0.00930
std_min= 0.25795, std_max= 0.37453, std_mean= 0.31651
val lr: [9.784836065573771e-05], policy lr: [0.00011741803278688525]
Policy Loss: -0.0093018, | Entropy Bonus: -0, | Value Loss: 5.7429, | Advantage Loss: 0.69146
Time elapsed (s): 1.6588916778564453
Agent stdevs: 0.316511
--------------------------------------------------------------------------------

Step 594
++++++++ Policy training ++++++++++
Current mean reward: 3483.945925 | mean episode length: 981.500000
val_loss=1124.31763
val_loss=37.21071
val_loss=45.68437
val_loss=880.39917
val_loss=925.46515
val_loss=48.20071
val_loss=31.29078
val_loss=814.19580
val_loss=347.82349
val_loss=289.33688
adv_loss= 1.45912
adv_loss= 1.89277
adv_loss= 1.47266
adv_loss= 1.03339
adv_loss=1769.17029
adv_loss= 4.00439
adv_loss= 1.09752
adv_loss= 1.09770
adv_loss= 0.88421
adv_loss=1762.51562
surrogate= 0.05583, entropy= 0.77042, loss= 0.05583
surrogate=-0.01416, entropy= 0.76935, loss=-0.01416
surrogate=-0.01924, entropy= 0.77032, loss=-0.01924
surrogate= 0.01346, entropy= 0.76858, loss= 0.01346
surrogate= 0.00323, entropy= 0.76749, loss= 0.00323
surrogate=-0.00452, entropy= 0.76619, loss=-0.00452
surrogate=-0.02521, entropy= 0.76627, loss=-0.02521
surrogate= 0.00774, entropy= 0.76528, loss= 0.00774
surrogate=-0.01695, entropy= 0.76385, loss=-0.01695
surrogate=-0.00360, entropy= 0.76373, loss=-0.00360
std_min= 0.25740, std_max= 0.37643, std_mean= 0.31589
val lr: [9.759221311475411e-05], policy lr: [0.00011711065573770492]
Policy Loss: -0.0035971, | Entropy Bonus: -0, | Value Loss: 289.34, | Advantage Loss: 1762.5
Time elapsed (s): 1.6511685848236084
Agent stdevs: 0.3158866
--------------------------------------------------------------------------------

Step 595
++++++++ Policy training ++++++++++
Current mean reward: 2196.201681 | mean episode length: 609.500000
val_loss=26.88269
val_loss=28.10383
val_loss=22.81964
val_loss=10.52441
val_loss=12.70159
val_loss= 6.63812
val_loss=11.45759
val_loss=10.75335
val_loss= 8.53385
val_loss= 8.29874
adv_loss= 1.64192
adv_loss= 1.09048
adv_loss= 1.35184
adv_loss= 1.35281
adv_loss= 2.82945
adv_loss= 1.76683
adv_loss= 2.09681
adv_loss= 0.87076
adv_loss= 0.76255
adv_loss= 7.40373
surrogate= 0.05451, entropy= 0.76414, loss= 0.05451
surrogate= 0.01539, entropy= 0.76459, loss= 0.01539
surrogate=-0.01414, entropy= 0.76425, loss=-0.01414
surrogate=-0.01223, entropy= 0.76490, loss=-0.01223
surrogate=-0.00411, entropy= 0.76362, loss=-0.00411
surrogate= 0.00306, entropy= 0.76256, loss= 0.00306
surrogate=-0.03466, entropy= 0.76135, loss=-0.03466
surrogate=-0.03049, entropy= 0.76132, loss=-0.03049
surrogate=-0.01253, entropy= 0.76032, loss=-0.01253
surrogate=-0.03672, entropy= 0.75921, loss=-0.03672
std_min= 0.25739, std_max= 0.37411, std_mean= 0.31528
val lr: [9.73360655737705e-05], policy lr: [0.0001168032786885246]
Policy Loss: -0.036718, | Entropy Bonus: -0, | Value Loss: 8.2987, | Advantage Loss: 7.4037
Time elapsed (s): 1.673940658569336
Agent stdevs: 0.3152816
--------------------------------------------------------------------------------

Step 596
++++++++ Policy training ++++++++++
Current mean reward: 2099.260585 | mean episode length: 562.666667
val_loss=11.10982
val_loss= 6.54342
val_loss= 8.56144
val_loss= 8.50675
val_loss= 5.56956
val_loss= 7.91152
val_loss= 4.51681
val_loss= 4.14783
val_loss= 5.14809
val_loss= 5.68947
adv_loss= 1.03464
adv_loss= 1.64381
adv_loss= 1.19660
adv_loss= 1.48355
adv_loss= 1.72673
adv_loss= 0.84630
adv_loss= 1.14869
adv_loss= 1.02491
adv_loss= 1.01905
adv_loss= 1.17332
surrogate= 0.00666, entropy= 0.75726, loss= 0.00666
surrogate= 0.01473, entropy= 0.75651, loss= 0.01473
surrogate=-0.01246, entropy= 0.75400, loss=-0.01246
surrogate=-0.01632, entropy= 0.75199, loss=-0.01632
surrogate=-0.01943, entropy= 0.74940, loss=-0.01943
surrogate=-0.02808, entropy= 0.74769, loss=-0.02808
surrogate=-0.00617, entropy= 0.74411, loss=-0.00617
surrogate=-0.00905, entropy= 0.74333, loss=-0.00905
surrogate=-0.02264, entropy= 0.74103, loss=-0.02264
surrogate=-0.06222, entropy= 0.73903, loss=-0.06222
std_min= 0.25622, std_max= 0.37207, std_mean= 0.31315
val lr: [9.707991803278687e-05], policy lr: [0.00011649590163934423]
Policy Loss: -0.062219, | Entropy Bonus: -0, | Value Loss: 5.6895, | Advantage Loss: 1.1733
Time elapsed (s): 1.6601083278656006
Agent stdevs: 0.31315014
--------------------------------------------------------------------------------

Step 597
++++++++ Policy training ++++++++++
Current mean reward: 2330.524954 | mean episode length: 640.500000
val_loss=14.51284
val_loss=14.19964
val_loss=10.64619
val_loss= 6.57580
val_loss= 7.53587
val_loss= 7.48535
val_loss= 9.77853
val_loss= 4.51525
val_loss=12.51141
val_loss= 4.54426
adv_loss= 0.66398
adv_loss= 3.43053
adv_loss= 0.81391
adv_loss= 1.46382
adv_loss= 1.11964
adv_loss= 0.56357
adv_loss= 1.56106
adv_loss= 0.60958
adv_loss= 0.65732
adv_loss= 0.39073
surrogate= 0.00877, entropy= 0.73754, loss= 0.00877
surrogate=-0.03806, entropy= 0.73612, loss=-0.03806
surrogate= 0.02079, entropy= 0.73602, loss= 0.02079
surrogate=-0.01067, entropy= 0.73564, loss=-0.01067
surrogate= 0.00167, entropy= 0.73405, loss= 0.00167
surrogate=-0.02808, entropy= 0.73398, loss=-0.02808
surrogate=-0.02208, entropy= 0.73383, loss=-0.02208
surrogate=-0.03249, entropy= 0.73241, loss=-0.03249
surrogate=-0.02580, entropy= 0.73239, loss=-0.02580
surrogate=-0.01000, entropy= 0.73174, loss=-0.01000
std_min= 0.25528, std_max= 0.37122, std_mean= 0.31242
val lr: [9.682377049180327e-05], policy lr: [0.00011618852459016392]
Policy Loss: -0.0099973, | Entropy Bonus: -0, | Value Loss: 4.5443, | Advantage Loss: 0.39073
Time elapsed (s): 1.6624886989593506
Agent stdevs: 0.31242287
--------------------------------------------------------------------------------

Step 598
++++++++ Policy training ++++++++++
Current mean reward: 2844.477184 | mean episode length: 787.500000
val_loss=62.99135
val_loss=890.07788
val_loss=130.59122
val_loss=197.51782
val_loss=736.92297
val_loss=132.71677
val_loss=92.81773
val_loss=45.34145
val_loss=968.66882
val_loss=54.18145
adv_loss= 3.58021
adv_loss=15.15787
adv_loss= 2.70586
adv_loss= 3.06837
adv_loss= 2.65385
adv_loss= 1.07291
adv_loss= 2.87699
adv_loss= 6.10480
adv_loss= 2.37446
adv_loss= 2.46153
surrogate=-0.00314, entropy= 0.73155, loss=-0.00314
surrogate=-0.01853, entropy= 0.73062, loss=-0.01853
surrogate= 0.00019, entropy= 0.73096, loss= 0.00019
surrogate=-0.01402, entropy= 0.73126, loss=-0.01402
surrogate= 0.01017, entropy= 0.73151, loss= 0.01017
surrogate=-0.02228, entropy= 0.73219, loss=-0.02228
surrogate=-0.02017, entropy= 0.73325, loss=-0.02017
surrogate=-0.03673, entropy= 0.73324, loss=-0.03673
surrogate= 0.00248, entropy= 0.73464, loss= 0.00248
surrogate= 0.01210, entropy= 0.73377, loss= 0.01210
std_min= 0.25448, std_max= 0.37084, std_mean= 0.31267
val lr: [9.656762295081967e-05], policy lr: [0.00011588114754098359]
Policy Loss: 0.012104, | Entropy Bonus: -0, | Value Loss: 54.181, | Advantage Loss: 2.4615
Time elapsed (s): 1.7056615352630615
Agent stdevs: 0.31266966
--------------------------------------------------------------------------------

Step 599
++++++++ Policy training ++++++++++
Current mean reward: 3478.261448 | mean episode length: 1000.000000
val_loss=1442.12378
val_loss=604.68695
val_loss=1533.79578
val_loss=203.01260
val_loss=756.66895
val_loss=78.44226
val_loss=956.90894
val_loss=139.71756
val_loss=316.02914
val_loss=559.93597
adv_loss=11.13596
adv_loss= 9.70491
adv_loss= 4.56347
adv_loss= 2.86027
adv_loss= 1.90302
adv_loss= 8.33594
adv_loss=11.44034
adv_loss= 3.48862
adv_loss= 7.87187
adv_loss= 2.25717
surrogate= 0.00410, entropy= 0.73413, loss= 0.00410
surrogate=-0.00785, entropy= 0.73456, loss=-0.00785
surrogate= 0.01708, entropy= 0.73531, loss= 0.01708
surrogate= 0.01470, entropy= 0.73622, loss= 0.01470
surrogate=-0.00867, entropy= 0.73793, loss=-0.00867
surrogate= 0.01291, entropy= 0.73836, loss= 0.01291
surrogate=-0.01943, entropy= 0.73882, loss=-0.01943
surrogate=-0.01981, entropy= 0.73924, loss=-0.01981
surrogate= 0.00481, entropy= 0.73912, loss= 0.00481
surrogate=-0.00070, entropy= 0.73974, loss=-0.00070
std_min= 0.25392, std_max= 0.37424, std_mean= 0.31352
val lr: [9.631147540983606e-05], policy lr: [0.00011557377049180327]
Policy Loss: -0.00069545, | Entropy Bonus: -0, | Value Loss: 559.94, | Advantage Loss: 2.2572
Time elapsed (s): 1.6570310592651367
Agent stdevs: 0.31352443
--------------------------------------------------------------------------------

Step 600
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2589.7
++++++++ Policy training ++++++++++
Current mean reward: 3429.607924 | mean episode length: 1000.000000
val_loss=84.88721
val_loss=1593.60632
val_loss=946.17273
val_loss=1535.02832
val_loss=1893.93896
val_loss=1771.15430
val_loss=833.38452
val_loss=311.47589
val_loss=1213.72498
val_loss=297.75491
adv_loss= 1.49092
adv_loss= 2.65760
adv_loss= 3.46881
adv_loss= 2.09070
adv_loss= 1.64055
adv_loss= 2.42162
adv_loss=1865.23340
adv_loss= 4.46873
adv_loss= 4.10439
adv_loss= 3.42657
surrogate= 0.04324, entropy= 0.73945, loss= 0.04324
surrogate= 0.02466, entropy= 0.73827, loss= 0.02466
surrogate=-0.01800, entropy= 0.73671, loss=-0.01800
surrogate=-0.01884, entropy= 0.73580, loss=-0.01884
surrogate=-0.03945, entropy= 0.73643, loss=-0.03945
surrogate=-0.02506, entropy= 0.73614, loss=-0.02506
surrogate=-0.02356, entropy= 0.73576, loss=-0.02356
surrogate=-0.02560, entropy= 0.73455, loss=-0.02560
surrogate=-0.03489, entropy= 0.73538, loss=-0.03489
surrogate= 0.00015, entropy= 0.73539, loss= 0.00015
std_min= 0.25261, std_max= 0.37497, std_mean= 0.31321
val lr: [9.605532786885246e-05], policy lr: [0.00011526639344262294]
Policy Loss: 0.00015095, | Entropy Bonus: -0, | Value Loss: 297.75, | Advantage Loss: 3.4266
Time elapsed (s): 1.641453504562378
Agent stdevs: 0.3132061
--------------------------------------------------------------------------------

Step 601
++++++++ Policy training ++++++++++
Current mean reward: 3509.739026 | mean episode length: 1000.000000
val_loss=1060.25281
val_loss=1694.47522
val_loss=36.23788
val_loss=304.57248
val_loss=38.98977
val_loss=507.21222
val_loss=74.47070
val_loss=31.31245
val_loss=316.68713
val_loss=114.07847
adv_loss= 2.92342
adv_loss= 1.28070
adv_loss= 2.75928
adv_loss= 4.11763
adv_loss= 4.01677
adv_loss= 1.23761
adv_loss= 1.96976
adv_loss= 1.87701
adv_loss= 3.80617
adv_loss= 2.31163
surrogate= 0.00364, entropy= 0.73544, loss= 0.00364
surrogate=-0.01439, entropy= 0.73574, loss=-0.01439
surrogate=-0.01050, entropy= 0.73658, loss=-0.01050
surrogate= 0.01167, entropy= 0.73789, loss= 0.01167
surrogate=-0.01779, entropy= 0.73914, loss=-0.01779
surrogate= 0.00346, entropy= 0.73992, loss= 0.00346
surrogate=-0.02413, entropy= 0.74213, loss=-0.02413
surrogate=-0.02580, entropy= 0.74354, loss=-0.02580
surrogate= 0.00587, entropy= 0.74424, loss= 0.00587
surrogate= 0.02687, entropy= 0.74492, loss= 0.02687
std_min= 0.25418, std_max= 0.37686, std_mean= 0.31419
val lr: [9.579918032786886e-05], policy lr: [0.00011495901639344261]
Policy Loss: 0.026872, | Entropy Bonus: -0, | Value Loss: 114.08, | Advantage Loss: 2.3116
Time elapsed (s): 1.6530647277832031
Agent stdevs: 0.31419396
--------------------------------------------------------------------------------

Step 602
++++++++ Policy training ++++++++++
Current mean reward: 3473.257404 | mean episode length: 1000.000000
val_loss=219.52498
val_loss=2048.70239
val_loss=1042.40857
val_loss=588.07770
val_loss=953.55359
val_loss=663.59900
val_loss=493.83173
val_loss=288.14935
val_loss=1245.90283
val_loss=276.26831
adv_loss= 3.02978
adv_loss= 3.73180
adv_loss= 2.92048
adv_loss= 1.71333
adv_loss= 3.03840
adv_loss= 2.41279
adv_loss= 2.21822
adv_loss= 2.12010
adv_loss=1757.26160
adv_loss= 2.50878
surrogate=-0.01922, entropy= 0.74639, loss=-0.01922
surrogate=-0.00319, entropy= 0.74767, loss=-0.00319
surrogate=-0.01731, entropy= 0.74859, loss=-0.01731
surrogate=-0.00135, entropy= 0.75026, loss=-0.00135
surrogate= 0.00220, entropy= 0.75289, loss= 0.00220
surrogate=-0.02086, entropy= 0.75372, loss=-0.02086
surrogate= 0.01441, entropy= 0.75571, loss= 0.01441
surrogate=-0.01921, entropy= 0.75706, loss=-0.01921
surrogate=-0.02605, entropy= 0.75816, loss=-0.02605
surrogate=-0.00211, entropy= 0.75918, loss=-0.00211
std_min= 0.25619, std_max= 0.37902, std_mean= 0.31565
val lr: [9.554303278688526e-05], policy lr: [0.00011465163934426229]
Policy Loss: -0.0021094, | Entropy Bonus: -0, | Value Loss: 276.27, | Advantage Loss: 2.5088
Time elapsed (s): 1.6819801330566406
Agent stdevs: 0.31565127
--------------------------------------------------------------------------------

Step 603
++++++++ Policy training ++++++++++
Current mean reward: 3482.475266 | mean episode length: 1000.000000
val_loss=641.61700
val_loss=107.44895
val_loss=124.57047
val_loss=101.34393
val_loss=278.96014
val_loss=1438.50757
val_loss=1742.15723
val_loss=145.32104
val_loss=135.84514
val_loss=674.10730
adv_loss= 3.48464
adv_loss= 2.08877
adv_loss= 3.60025
adv_loss= 3.09372
adv_loss= 2.59701
adv_loss= 4.36135
adv_loss= 2.62430
adv_loss= 2.19528
adv_loss= 1.66843
adv_loss= 2.20865
surrogate=-0.01449, entropy= 0.75984, loss=-0.01449
surrogate= 0.02867, entropy= 0.75863, loss= 0.02867
surrogate= 0.00704, entropy= 0.75734, loss= 0.00704
surrogate= 0.00123, entropy= 0.75721, loss= 0.00123
surrogate= 0.04024, entropy= 0.75639, loss= 0.04024
surrogate= 0.01345, entropy= 0.75579, loss= 0.01345
surrogate= 0.00377, entropy= 0.75627, loss= 0.00377
surrogate= 0.01811, entropy= 0.75698, loss= 0.01811
surrogate=-0.01855, entropy= 0.75594, loss=-0.01855
surrogate=-0.02554, entropy= 0.75580, loss=-0.02554
std_min= 0.25626, std_max= 0.37692, std_mean= 0.31517
val lr: [9.528688524590164e-05], policy lr: [0.00011434426229508196]
Policy Loss: -0.025538, | Entropy Bonus: -0, | Value Loss: 674.11, | Advantage Loss: 2.2086
Time elapsed (s): 1.6614668369293213
Agent stdevs: 0.31516913
--------------------------------------------------------------------------------

Step 604
++++++++ Policy training ++++++++++
Current mean reward: 3482.347245 | mean episode length: 1000.000000
val_loss=91.94778
val_loss=81.19599
val_loss=824.74469
val_loss=2278.36353
val_loss=41.09116
val_loss=687.39026
val_loss=83.26080
val_loss=42.79487
val_loss=348.69516
val_loss=136.45108
adv_loss= 3.16046
adv_loss= 5.35098
adv_loss=10.60368
adv_loss= 3.62996
adv_loss= 3.66128
adv_loss= 9.44996
adv_loss=1316.53027
adv_loss= 4.33827
adv_loss= 3.36220
adv_loss= 9.96624
surrogate= 0.00162, entropy= 0.75775, loss= 0.00162
surrogate=-0.01856, entropy= 0.76068, loss=-0.01856
surrogate=-0.01731, entropy= 0.76155, loss=-0.01731
surrogate= 0.00653, entropy= 0.76437, loss= 0.00653
surrogate=-0.02446, entropy= 0.76632, loss=-0.02446
surrogate=-0.02369, entropy= 0.76761, loss=-0.02369
surrogate= 0.00959, entropy= 0.76881, loss= 0.00959
surrogate=-0.02169, entropy= 0.77053, loss=-0.02169
surrogate=-0.03325, entropy= 0.77114, loss=-0.03325
surrogate=-0.01883, entropy= 0.77257, loss=-0.01883
std_min= 0.25958, std_max= 0.37843, std_mean= 0.31677
val lr: [9.503073770491804e-05], policy lr: [0.00011403688524590164]
Policy Loss: -0.018832, | Entropy Bonus: -0, | Value Loss: 136.45, | Advantage Loss: 9.9662
Time elapsed (s): 1.7182307243347168
Agent stdevs: 0.31677496
--------------------------------------------------------------------------------

Step 605
++++++++ Policy training ++++++++++
Current mean reward: 3498.008735 | mean episode length: 1000.000000
val_loss=40.90576
val_loss=122.47901
val_loss=1429.74744
val_loss=98.76064
val_loss=241.52193
val_loss=1705.31665
val_loss=1742.66895
val_loss=321.23587
val_loss=134.35887
val_loss=1285.29138
adv_loss=1546.55237
adv_loss=11.83444
adv_loss= 5.58605
adv_loss= 2.08747
adv_loss=10.60949
adv_loss= 5.25762
adv_loss= 2.03509
adv_loss= 2.06723
adv_loss= 1.93486
adv_loss= 8.14658
surrogate=-0.00556, entropy= 0.77425, loss=-0.00556
surrogate=-0.00275, entropy= 0.77669, loss=-0.00275
surrogate= 0.02157, entropy= 0.77889, loss= 0.02157
surrogate= 0.00889, entropy= 0.77938, loss= 0.00889
surrogate= 0.00577, entropy= 0.77951, loss= 0.00577
surrogate=-0.01106, entropy= 0.78079, loss=-0.01106
surrogate=-0.02537, entropy= 0.78135, loss=-0.02537
surrogate=-0.01333, entropy= 0.78108, loss=-0.01333
surrogate= 0.01883, entropy= 0.78234, loss= 0.01883
surrogate=-0.00665, entropy= 0.78370, loss=-0.00665
std_min= 0.26104, std_max= 0.37965, std_mean= 0.31790
val lr: [9.477459016393442e-05], policy lr: [0.00011372950819672129]
Policy Loss: -0.0066481, | Entropy Bonus: -0, | Value Loss: 1285.3, | Advantage Loss: 8.1466
Time elapsed (s): 1.7285192012786865
Agent stdevs: 0.3179027
--------------------------------------------------------------------------------

Step 606
++++++++ Policy training ++++++++++
Current mean reward: 3025.679701 | mean episode length: 826.000000
val_loss=59.60690
val_loss=52.30769
val_loss=51.44181
val_loss=58.10294
val_loss=41.62328
val_loss=32.70300
val_loss=31.48618
val_loss=14.55597
val_loss=23.99657
val_loss=14.87472
adv_loss= 3.65187
adv_loss= 2.92955
adv_loss= 2.70251
adv_loss= 3.69552
adv_loss=10.70590
adv_loss= 5.24572
adv_loss= 2.28801
adv_loss= 2.19138
adv_loss= 1.40654
adv_loss= 3.31967
surrogate=-0.00791, entropy= 0.78347, loss=-0.00791
surrogate=-0.00487, entropy= 0.78115, loss=-0.00487
surrogate=-0.02510, entropy= 0.77940, loss=-0.02510
surrogate=-0.00487, entropy= 0.77841, loss=-0.00487
surrogate=-0.02976, entropy= 0.77629, loss=-0.02976
surrogate=-0.02893, entropy= 0.77382, loss=-0.02893
surrogate=-0.01984, entropy= 0.77251, loss=-0.01984
surrogate=-0.01041, entropy= 0.77093, loss=-0.01041
surrogate= 0.00618, entropy= 0.77023, loss= 0.00618
surrogate=-0.00708, entropy= 0.76895, loss=-0.00708
std_min= 0.25908, std_max= 0.37632, std_mean= 0.31630
val lr: [9.451844262295082e-05], policy lr: [0.00011342213114754096]
Policy Loss: -0.0070755, | Entropy Bonus: -0, | Value Loss: 14.875, | Advantage Loss: 3.3197
Time elapsed (s): 1.6500120162963867
Agent stdevs: 0.31630087
--------------------------------------------------------------------------------

Step 607
++++++++ Policy training ++++++++++
Current mean reward: 3478.698240 | mean episode length: 1000.000000
val_loss=1208.73669
val_loss=383.92603
val_loss=769.41705
val_loss=169.26225
val_loss=2466.51416
val_loss=646.07654
val_loss=502.02573
val_loss=1756.72412
val_loss=754.61035
val_loss=2843.22534
adv_loss= 6.16987
adv_loss= 1.65961
adv_loss= 2.68039
adv_loss= 7.87089
adv_loss= 4.09053
adv_loss= 1.42362
adv_loss= 2.29631
adv_loss= 1.10972
adv_loss= 2.61593
adv_loss= 2.41558
surrogate= 0.00092, entropy= 0.76892, loss= 0.00092
surrogate=-0.00021, entropy= 0.76905, loss=-0.00021
surrogate=-0.02606, entropy= 0.76972, loss=-0.02606
surrogate=-0.01096, entropy= 0.76812, loss=-0.01096
surrogate=-0.01990, entropy= 0.76894, loss=-0.01990
surrogate=-0.01940, entropy= 0.76893, loss=-0.01940
surrogate=-0.01199, entropy= 0.76940, loss=-0.01199
surrogate=-0.02297, entropy= 0.76845, loss=-0.02297
surrogate=-0.03213, entropy= 0.76832, loss=-0.03213
surrogate=-0.03491, entropy= 0.76758, loss=-0.03491
std_min= 0.26006, std_max= 0.37677, std_mean= 0.31612
val lr: [9.42622950819672e-05], policy lr: [0.00011311475409836063]
Policy Loss: -0.034908, | Entropy Bonus: -0, | Value Loss: 2843.2, | Advantage Loss: 2.4156
Time elapsed (s): 1.6821975708007812
Agent stdevs: 0.31612158
--------------------------------------------------------------------------------

Step 608
++++++++ Policy training ++++++++++
Current mean reward: 2071.711532 | mean episode length: 564.000000
val_loss=66.43320
val_loss=24.81825
val_loss=55.12869
val_loss=37.38573
val_loss=36.36514
val_loss=39.88515
val_loss=32.50582
val_loss=24.30320
val_loss=22.52047
val_loss=28.35298
adv_loss= 7.18097
adv_loss= 3.55790
adv_loss= 2.51802
adv_loss= 3.00121
adv_loss= 2.33739
adv_loss= 2.77381
adv_loss= 2.82995
adv_loss= 8.48331
adv_loss= 3.37201
adv_loss= 2.59621
surrogate= 0.00758, entropy= 0.76677, loss= 0.00758
surrogate= 0.02037, entropy= 0.76502, loss= 0.02037
surrogate=-0.03425, entropy= 0.76338, loss=-0.03425
surrogate=-0.02044, entropy= 0.76155, loss=-0.02044
surrogate=-0.02347, entropy= 0.76044, loss=-0.02347
surrogate=-0.02911, entropy= 0.75889, loss=-0.02911
surrogate=-0.01488, entropy= 0.75737, loss=-0.01488
surrogate=-0.00128, entropy= 0.75522, loss=-0.00128
surrogate=-0.01269, entropy= 0.75482, loss=-0.01269
surrogate=-0.01469, entropy= 0.75255, loss=-0.01469
std_min= 0.25872, std_max= 0.37421, std_mean= 0.31450
val lr: [9.40061475409836e-05], policy lr: [0.00011280737704918031]
Policy Loss: -0.014687, | Entropy Bonus: -0, | Value Loss: 28.353, | Advantage Loss: 2.5962
Time elapsed (s): 1.6802222728729248
Agent stdevs: 0.31449965
--------------------------------------------------------------------------------

Step 609
++++++++ Policy training ++++++++++
Current mean reward: 3562.423278 | mean episode length: 990.500000
val_loss=61.40004
val_loss=670.82825
val_loss=28.42419
val_loss=37.56720
val_loss=25.16228
val_loss=43.82109
val_loss=511.86664
val_loss=58.74329
val_loss=1714.68811
val_loss=65.01765
adv_loss= 3.35754
adv_loss= 1.74136
adv_loss= 5.02157
adv_loss= 3.90492
adv_loss= 1.07820
adv_loss= 2.54137
adv_loss= 1.65614
adv_loss= 1.13496
adv_loss= 1.09872
adv_loss= 1.05931
surrogate= 0.00313, entropy= 0.75109, loss= 0.00313
surrogate=-0.00338, entropy= 0.75131, loss=-0.00338
surrogate=-0.02379, entropy= 0.75059, loss=-0.02379
surrogate= 0.00456, entropy= 0.75016, loss= 0.00456
surrogate=-0.00935, entropy= 0.74930, loss=-0.00935
surrogate= 0.00122, entropy= 0.74997, loss= 0.00122
surrogate= 0.00239, entropy= 0.75079, loss= 0.00239
surrogate= 0.00851, entropy= 0.74962, loss= 0.00851
surrogate= 0.01804, entropy= 0.74931, loss= 0.01804
surrogate=-0.02778, entropy= 0.74864, loss=-0.02778
std_min= 0.25962, std_max= 0.37267, std_mean= 0.31394
val lr: [9.375e-05], policy lr: [0.0001125]
Policy Loss: -0.027783, | Entropy Bonus: -0, | Value Loss: 65.018, | Advantage Loss: 1.0593
Time elapsed (s): 1.6568183898925781
Agent stdevs: 0.3139439
--------------------------------------------------------------------------------

Step 610
++++++++ Policy training ++++++++++
Current mean reward: 3140.629928 | mean episode length: 891.000000
val_loss=115.57613
val_loss=34.52769
val_loss=149.20683
val_loss=55.36627
val_loss=35.82319
val_loss=792.71936
val_loss=383.84369
val_loss=935.77527
val_loss=2481.50928
val_loss=21.66544
adv_loss= 1.41067
adv_loss= 3.47542
adv_loss= 2.29817
adv_loss= 2.98731
adv_loss= 3.14208
adv_loss= 2.32893
adv_loss= 0.99240
adv_loss= 2.64439
adv_loss= 2.52940
adv_loss= 2.61613
surrogate=-0.01606, entropy= 0.74694, loss=-0.01606
surrogate= 0.01735, entropy= 0.74607, loss= 0.01735
surrogate= 0.00582, entropy= 0.74557, loss= 0.00582
surrogate=-0.02773, entropy= 0.74521, loss=-0.02773
surrogate=-0.01980, entropy= 0.74495, loss=-0.01980
surrogate=-0.00887, entropy= 0.74447, loss=-0.00887
surrogate=-0.03979, entropy= 0.74385, loss=-0.03979
surrogate=-0.02287, entropy= 0.74407, loss=-0.02287
surrogate= 0.01538, entropy= 0.74525, loss= 0.01538
surrogate=-0.03208, entropy= 0.74642, loss=-0.03208
std_min= 0.26013, std_max= 0.37003, std_mean= 0.31354
val lr: [9.34938524590164e-05], policy lr: [0.00011219262295081967]
Policy Loss: -0.032076, | Entropy Bonus: -0, | Value Loss: 21.665, | Advantage Loss: 2.6161
Time elapsed (s): 1.6755805015563965
Agent stdevs: 0.31354138
--------------------------------------------------------------------------------

Step 611
++++++++ Policy training ++++++++++
Current mean reward: 2158.816595 | mean episode length: 591.666667
val_loss=22.99697
val_loss=29.13275
val_loss=30.42484
val_loss=17.81385
val_loss=27.34625
val_loss=12.50504
val_loss= 9.76159
val_loss=22.62293
val_loss=16.71668
val_loss=15.74316
adv_loss= 4.50227
adv_loss= 0.89605
adv_loss= 4.91024
adv_loss= 3.86904
adv_loss= 2.47706
adv_loss= 1.33582
adv_loss= 1.69407
adv_loss= 1.98341
adv_loss= 1.71114
adv_loss= 1.79652
surrogate=-0.00697, entropy= 0.74474, loss=-0.00697
surrogate=-0.02345, entropy= 0.74263, loss=-0.02345
surrogate=-0.02824, entropy= 0.74097, loss=-0.02824
surrogate= 0.00972, entropy= 0.73940, loss= 0.00972
surrogate= 0.01135, entropy= 0.73823, loss= 0.01135
surrogate=-0.02761, entropy= 0.73709, loss=-0.02761
surrogate=-0.02844, entropy= 0.73599, loss=-0.02844
surrogate= 0.00780, entropy= 0.73452, loss= 0.00780
surrogate=-0.03330, entropy= 0.73314, loss=-0.03330
surrogate=-0.00826, entropy= 0.73080, loss=-0.00826
std_min= 0.25924, std_max= 0.36595, std_mean= 0.31177
val lr: [9.32377049180328e-05], policy lr: [0.00011188524590163935]
Policy Loss: -0.0082647, | Entropy Bonus: -0, | Value Loss: 15.743, | Advantage Loss: 1.7965
Time elapsed (s): 1.6787660121917725
Agent stdevs: 0.31177187
--------------------------------------------------------------------------------

Step 612
++++++++ Policy training ++++++++++
Current mean reward: 1617.429760 | mean episode length: 459.000000
val_loss=64.12585
val_loss=121.76867
val_loss=392.93689
val_loss=96.07626
val_loss=122.92548
val_loss=97.85488
val_loss=93.79887
val_loss=74.60815
val_loss=130.04044
val_loss=86.75095
adv_loss= 6.79373
adv_loss= 1.73850
adv_loss= 6.12723
adv_loss= 2.17617
adv_loss= 2.37133
adv_loss= 5.39082
adv_loss= 3.49407
adv_loss= 2.13410
adv_loss=82.85374
adv_loss= 3.65502
surrogate= 0.00267, entropy= 0.73167, loss= 0.00267
surrogate=-0.00878, entropy= 0.73251, loss=-0.00878
surrogate= 0.02128, entropy= 0.73376, loss= 0.02128
surrogate=-0.01139, entropy= 0.73484, loss=-0.01139
surrogate= 0.02117, entropy= 0.73501, loss= 0.02117
surrogate=-0.03005, entropy= 0.73698, loss=-0.03005
surrogate= 0.02864, entropy= 0.73932, loss= 0.02864
surrogate=-0.00935, entropy= 0.74086, loss=-0.00935
surrogate=-0.03474, entropy= 0.74126, loss=-0.03474
surrogate=-0.00406, entropy= 0.74127, loss=-0.00406
std_min= 0.25995, std_max= 0.36792, std_mean= 0.31291
val lr: [9.298155737704919e-05], policy lr: [0.00011157786885245902]
Policy Loss: -0.0040615, | Entropy Bonus: -0, | Value Loss: 86.751, | Advantage Loss: 3.655
Time elapsed (s): 1.6703627109527588
Agent stdevs: 0.31290796
--------------------------------------------------------------------------------

Step 613
++++++++ Policy training ++++++++++
Current mean reward: 3505.942838 | mean episode length: 1000.000000
val_loss=1685.29456
val_loss=171.29337
val_loss=1599.19092
val_loss=1198.24463
val_loss=777.60706
val_loss=88.42500
val_loss=281.18353
val_loss=445.31488
val_loss=1595.13354
val_loss=207.41357
adv_loss= 1.98380
adv_loss= 1.31887
adv_loss= 3.59722
adv_loss= 1.11311
adv_loss= 2.70464
adv_loss= 2.09260
adv_loss=1755.58740
adv_loss= 4.91029
adv_loss= 1.35562
adv_loss= 1.57842
surrogate= 0.01019, entropy= 0.73993, loss= 0.01019
surrogate= 0.00918, entropy= 0.74040, loss= 0.00918
surrogate=-0.01116, entropy= 0.74035, loss=-0.01116
surrogate= 0.01000, entropy= 0.74042, loss= 0.01000
surrogate= 0.03133, entropy= 0.74126, loss= 0.03133
surrogate= 0.00503, entropy= 0.74114, loss= 0.00503
surrogate=-0.00349, entropy= 0.74150, loss=-0.00349
surrogate= 0.02365, entropy= 0.74159, loss= 0.02365
surrogate=-0.02680, entropy= 0.74167, loss=-0.02680
surrogate=-0.01767, entropy= 0.74187, loss=-0.01767
std_min= 0.25929, std_max= 0.36897, std_mean= 0.31307
val lr: [9.272540983606559e-05], policy lr: [0.0001112704918032787]
Policy Loss: -0.017672, | Entropy Bonus: -0, | Value Loss: 207.41, | Advantage Loss: 1.5784
Time elapsed (s): 1.6629557609558105
Agent stdevs: 0.31307474
--------------------------------------------------------------------------------

Step 614
++++++++ Policy training ++++++++++
Current mean reward: 1638.782663 | mean episode length: 448.500000
val_loss=63.27918
val_loss=29.41337
val_loss=27.62623
val_loss=19.10209
val_loss=25.66198
val_loss=12.75800
val_loss=20.65026
val_loss=17.42592
val_loss=16.39385
val_loss=20.61633
adv_loss= 2.12398
adv_loss= 2.13025
adv_loss= 1.71131
adv_loss= 4.99640
adv_loss= 4.77407
adv_loss= 2.10044
adv_loss= 4.94054
adv_loss= 2.46450
adv_loss= 6.96275
adv_loss= 6.60456
surrogate= 0.00263, entropy= 0.74197, loss= 0.00263
surrogate= 0.00704, entropy= 0.74199, loss= 0.00704
surrogate=-0.02905, entropy= 0.74275, loss=-0.02905
surrogate=-0.02800, entropy= 0.74185, loss=-0.02800
surrogate=-0.01452, entropy= 0.74266, loss=-0.01452
surrogate= 0.02065, entropy= 0.74351, loss= 0.02065
surrogate=-0.02842, entropy= 0.74309, loss=-0.02842
surrogate=-0.02280, entropy= 0.74342, loss=-0.02280
surrogate=-0.04182, entropy= 0.74381, loss=-0.04182
surrogate=-0.00613, entropy= 0.74353, loss=-0.00613
std_min= 0.25972, std_max= 0.36769, std_mean= 0.31315
val lr: [9.246926229508196e-05], policy lr: [0.00011096311475409834]
Policy Loss: -0.0061293, | Entropy Bonus: -0, | Value Loss: 20.616, | Advantage Loss: 6.6046
Time elapsed (s): 1.6833415031433105
Agent stdevs: 0.31314948
--------------------------------------------------------------------------------

Step 615
++++++++ Policy training ++++++++++
Current mean reward: 1196.161297 | mean episode length: 331.333333
val_loss=41.07692
val_loss=37.39920
val_loss=60.41188
val_loss=28.80296
val_loss=48.81697
val_loss=28.15114
val_loss=37.24202
val_loss=32.20604
val_loss=44.44491
val_loss=39.56337
adv_loss=32.98985
adv_loss= 4.78306
adv_loss= 3.21037
adv_loss= 1.08464
adv_loss=12.16951
adv_loss= 8.43989
adv_loss= 2.07901
adv_loss= 4.27022
adv_loss= 4.97191
adv_loss= 5.49471
surrogate=-0.00424, entropy= 0.74317, loss=-0.00424
surrogate=-0.02127, entropy= 0.74410, loss=-0.02127
surrogate=-0.01457, entropy= 0.74509, loss=-0.01457
surrogate=-0.03555, entropy= 0.74529, loss=-0.03555
surrogate= 0.00109, entropy= 0.74666, loss= 0.00109
surrogate=-0.00226, entropy= 0.74702, loss=-0.00226
surrogate=-0.01266, entropy= 0.74730, loss=-0.01266
surrogate= 0.00052, entropy= 0.74791, loss= 0.00052
surrogate=-0.00914, entropy= 0.74912, loss=-0.00914
surrogate=-0.03146, entropy= 0.74855, loss=-0.03146
std_min= 0.26121, std_max= 0.36711, std_mean= 0.31355
val lr: [9.221311475409836e-05], policy lr: [0.00011065573770491802]
Policy Loss: -0.031456, | Entropy Bonus: -0, | Value Loss: 39.563, | Advantage Loss: 5.4947
Time elapsed (s): 1.6681437492370605
Agent stdevs: 0.31355205
--------------------------------------------------------------------------------

Step 616
++++++++ Policy training ++++++++++
Current mean reward: 1364.346292 | mean episode length: 383.500000
val_loss=27.97112
val_loss=31.49878
val_loss=22.37387
val_loss=22.74977
val_loss=52.37152
val_loss=29.65060
val_loss=74.24110
val_loss=71.53325
val_loss=37.01947
val_loss=13.38576
adv_loss= 2.91607
adv_loss= 5.52294
adv_loss= 4.05137
adv_loss= 3.65199
adv_loss= 3.23911
adv_loss=21.13497
adv_loss= 1.26608
adv_loss= 0.50386
adv_loss= 4.03185
adv_loss= 6.31708
surrogate=-0.01014, entropy= 0.75073, loss=-0.01014
surrogate= 0.02382, entropy= 0.75307, loss= 0.02382
surrogate=-0.01535, entropy= 0.75398, loss=-0.01535
surrogate=-0.00919, entropy= 0.75610, loss=-0.00919
surrogate= 0.01173, entropy= 0.75816, loss= 0.01173
surrogate=-0.01090, entropy= 0.75987, loss=-0.01090
surrogate=-0.01057, entropy= 0.76088, loss=-0.01057
surrogate=-0.01665, entropy= 0.76171, loss=-0.01665
surrogate=-0.02628, entropy= 0.76378, loss=-0.02628
surrogate=-0.01680, entropy= 0.76557, loss=-0.01680
std_min= 0.26201, std_max= 0.36845, std_mean= 0.31534
val lr: [9.195696721311475e-05], policy lr: [0.00011034836065573769]
Policy Loss: -0.016798, | Entropy Bonus: -0, | Value Loss: 13.386, | Advantage Loss: 6.3171
Time elapsed (s): 1.727670669555664
Agent stdevs: 0.3153446
--------------------------------------------------------------------------------

Step 617
++++++++ Policy training ++++++++++
Current mean reward: 2029.795506 | mean episode length: 552.000000
val_loss=23.36716
val_loss=16.87819
val_loss=16.57769
val_loss=17.75032
val_loss=15.55815
val_loss=17.05721
val_loss=16.22350
val_loss=13.94028
val_loss= 9.75919
val_loss=13.53752
adv_loss= 1.91141
adv_loss= 1.26516
adv_loss= 5.03510
adv_loss= 4.14363
adv_loss= 1.36505
adv_loss= 2.22553
adv_loss= 2.86781
adv_loss= 2.10672
adv_loss= 3.81387
adv_loss= 3.66901
surrogate= 0.00420, entropy= 0.76541, loss= 0.00420
surrogate=-0.01302, entropy= 0.76391, loss=-0.01302
surrogate=-0.00528, entropy= 0.76154, loss=-0.00528
surrogate= 0.02479, entropy= 0.75987, loss= 0.02479
surrogate=-0.01633, entropy= 0.75795, loss=-0.01633
surrogate=-0.02048, entropy= 0.75610, loss=-0.02048
surrogate=-0.02865, entropy= 0.75429, loss=-0.02865
surrogate=-0.00567, entropy= 0.75309, loss=-0.00567
surrogate=-0.01870, entropy= 0.75160, loss=-0.01870
surrogate=-0.03338, entropy= 0.74973, loss=-0.03338
std_min= 0.26047, std_max= 0.36538, std_mean= 0.31363
val lr: [9.170081967213115e-05], policy lr: [0.00011004098360655737]
Policy Loss: -0.033384, | Entropy Bonus: -0, | Value Loss: 13.538, | Advantage Loss: 3.669
Time elapsed (s): 1.6792511940002441
Agent stdevs: 0.31363115
--------------------------------------------------------------------------------

Step 618
++++++++ Policy training ++++++++++
Current mean reward: 2357.001221 | mean episode length: 657.333333
val_loss=1965.37329
val_loss=367.99554
val_loss=49.43524
val_loss=43.33346
val_loss=57.43608
val_loss=47.53198
val_loss=73.50481
val_loss=79.98509
val_loss=91.20735
val_loss=684.88879
adv_loss= 3.05763
adv_loss= 5.21145
adv_loss= 2.14681
adv_loss= 3.57195
adv_loss= 5.39477
adv_loss= 3.51431
adv_loss= 5.08840
adv_loss= 2.20691
adv_loss= 4.23374
adv_loss= 2.27743
surrogate= 0.01394, entropy= 0.75050, loss= 0.01394
surrogate= 0.08483, entropy= 0.75230, loss= 0.08483
surrogate=-0.01360, entropy= 0.75334, loss=-0.01360
surrogate= 0.01069, entropy= 0.75446, loss= 0.01069
surrogate=-0.01381, entropy= 0.75553, loss=-0.01381
surrogate=-0.01725, entropy= 0.75709, loss=-0.01725
surrogate=-0.01907, entropy= 0.75782, loss=-0.01907
surrogate= 0.02633, entropy= 0.75944, loss= 0.02633
surrogate=-0.02347, entropy= 0.76001, loss=-0.02347
surrogate= 0.00375, entropy= 0.76089, loss= 0.00375
std_min= 0.25976, std_max= 0.36777, std_mean= 0.31497
val lr: [9.144467213114755e-05], policy lr: [0.00010973360655737704]
Policy Loss: 0.0037453, | Entropy Bonus: -0, | Value Loss: 684.89, | Advantage Loss: 2.2774
Time elapsed (s): 1.6603991985321045
Agent stdevs: 0.31497422
--------------------------------------------------------------------------------

Step 619
++++++++ Policy training ++++++++++
Current mean reward: 1683.872644 | mean episode length: 472.000000
val_loss=412.85577
val_loss=1301.79785
val_loss=592.50940
val_loss=38.37234
val_loss=118.84618
val_loss=47.15742
val_loss=72.64785
val_loss=1710.18311
val_loss=598.75055
val_loss=793.20007
adv_loss= 2.00241
adv_loss= 3.53650
adv_loss= 2.01017
adv_loss=1681.07788
adv_loss= 2.57205
adv_loss= 1.76059
adv_loss= 2.19589
adv_loss= 4.88075
adv_loss= 4.28023
adv_loss= 2.43889
surrogate=-0.02352, entropy= 0.76013, loss=-0.02352
surrogate=-0.00275, entropy= 0.75849, loss=-0.00275
surrogate=-0.00661, entropy= 0.75632, loss=-0.00661
surrogate=-0.01148, entropy= 0.75494, loss=-0.01148
surrogate=-0.00690, entropy= 0.75388, loss=-0.00690
surrogate=-0.00913, entropy= 0.75364, loss=-0.00913
surrogate= 0.00205, entropy= 0.75165, loss= 0.00205
surrogate=-0.02907, entropy= 0.75103, loss=-0.02907
surrogate=-0.01204, entropy= 0.75043, loss=-0.01204
surrogate=-0.01515, entropy= 0.75126, loss=-0.01515
std_min= 0.25803, std_max= 0.36625, std_mean= 0.31402
val lr: [9.118852459016395e-05], policy lr: [0.00010942622950819671]
Policy Loss: -0.01515, | Entropy Bonus: -0, | Value Loss: 793.2, | Advantage Loss: 2.4389
Time elapsed (s): 1.6414625644683838
Agent stdevs: 0.31401938
--------------------------------------------------------------------------------

Step 620
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1726.2
++++++++ Policy training ++++++++++
Current mean reward: 2456.246524 | mean episode length: 679.666667
val_loss=119.10782
val_loss=330.62082
val_loss=1908.41052
val_loss=106.72916
val_loss=368.09006
val_loss=288.56311
val_loss=62.08359
val_loss=936.11591
val_loss=237.53241
val_loss=77.25341
adv_loss= 1.27907
adv_loss= 6.34972
adv_loss= 2.23494
adv_loss= 2.19311
adv_loss= 5.03791
adv_loss= 5.39175
adv_loss= 3.70622
adv_loss= 2.58826
adv_loss= 5.95137
adv_loss= 5.26139
surrogate= 0.01285, entropy= 0.75282, loss= 0.01285
surrogate=-0.01219, entropy= 0.75375, loss=-0.01219
surrogate= 0.01584, entropy= 0.75417, loss= 0.01584
surrogate=-0.02326, entropy= 0.75594, loss=-0.02326
surrogate=-0.02805, entropy= 0.75658, loss=-0.02805
surrogate=-0.00378, entropy= 0.75749, loss=-0.00378
surrogate=-0.02585, entropy= 0.75830, loss=-0.02585
surrogate=-0.02131, entropy= 0.75915, loss=-0.02131
surrogate= 0.03809, entropy= 0.76012, loss= 0.03809
surrogate=-0.00896, entropy= 0.76182, loss=-0.00896
std_min= 0.25918, std_max= 0.36696, std_mean= 0.31508
val lr: [9.093237704918033e-05], policy lr: [0.00010911885245901639]
Policy Loss: -0.0089552, | Entropy Bonus: -0, | Value Loss: 77.253, | Advantage Loss: 5.2614
Time elapsed (s): 1.6495811939239502
Agent stdevs: 0.3150796
--------------------------------------------------------------------------------

Step 621
++++++++ Policy training ++++++++++
Current mean reward: 1774.266505 | mean episode length: 482.500000
val_loss=20.07530
val_loss=13.93100
val_loss=10.01824
val_loss=14.15382
val_loss=13.44615
val_loss= 6.78625
val_loss= 7.10039
val_loss= 6.95242
val_loss= 5.61954
val_loss= 5.51530
adv_loss= 1.03731
adv_loss= 1.78399
adv_loss= 7.18700
adv_loss= 1.76481
adv_loss= 1.22662
adv_loss= 2.32942
adv_loss= 1.33004
adv_loss= 1.45858
adv_loss= 6.13151
adv_loss= 6.28137
surrogate= 0.00201, entropy= 0.76167, loss= 0.00201
surrogate=-0.00435, entropy= 0.76142, loss=-0.00435
surrogate=-0.00240, entropy= 0.76110, loss=-0.00240
surrogate= 0.00832, entropy= 0.76217, loss= 0.00832
surrogate=-0.00641, entropy= 0.76222, loss=-0.00641
surrogate=-0.00267, entropy= 0.76154, loss=-0.00267
surrogate=-0.00819, entropy= 0.76084, loss=-0.00819
surrogate=-0.00347, entropy= 0.76119, loss=-0.00347
surrogate= 0.00257, entropy= 0.76005, loss= 0.00257
surrogate=-0.02295, entropy= 0.76035, loss=-0.02295
std_min= 0.25869, std_max= 0.36778, std_mean= 0.31499
val lr: [9.067622950819673e-05], policy lr: [0.00010881147540983606]
Policy Loss: -0.022949, | Entropy Bonus: -0, | Value Loss: 5.5153, | Advantage Loss: 6.2814
Time elapsed (s): 1.6449363231658936
Agent stdevs: 0.31499404
--------------------------------------------------------------------------------

Step 622
++++++++ Policy training ++++++++++
Current mean reward: 1494.086470 | mean episode length: 403.600000
val_loss=12.66321
val_loss=12.57077
val_loss=11.86438
val_loss=13.69665
val_loss=14.95834
val_loss= 8.96559
val_loss=13.50885
val_loss=10.08061
val_loss= 9.38692
val_loss= 8.76503
adv_loss= 2.10218
adv_loss= 1.86432
adv_loss= 3.71650
adv_loss= 2.43217
adv_loss= 2.87286
adv_loss= 0.87979
adv_loss= 1.84899
adv_loss= 1.71322
adv_loss= 3.11839
adv_loss= 3.37499
surrogate=-0.00290, entropy= 0.76021, loss=-0.00290
surrogate= 0.01766, entropy= 0.76089, loss= 0.01766
surrogate= 0.00458, entropy= 0.76177, loss= 0.00458
surrogate=-0.00106, entropy= 0.76289, loss=-0.00106
surrogate=-0.00858, entropy= 0.76308, loss=-0.00858
surrogate=-0.03776, entropy= 0.76464, loss=-0.03776
surrogate=-0.00470, entropy= 0.76451, loss=-0.00470
surrogate=-0.02560, entropy= 0.76476, loss=-0.02560
surrogate=-0.00651, entropy= 0.76506, loss=-0.00651
surrogate=-0.03607, entropy= 0.76679, loss=-0.03607
std_min= 0.25650, std_max= 0.37128, std_mean= 0.31601
val lr: [9.042008196721313e-05], policy lr: [0.00010850409836065575]
Policy Loss: -0.036075, | Entropy Bonus: -0, | Value Loss: 8.765, | Advantage Loss: 3.375
Time elapsed (s): 1.6829109191894531
Agent stdevs: 0.31601098
--------------------------------------------------------------------------------

Step 623
++++++++ Policy training ++++++++++
Current mean reward: 1608.376205 | mean episode length: 443.750000
val_loss=16.91047
val_loss=23.69164
val_loss=16.31350
val_loss= 6.30037
val_loss=24.09535
val_loss=34.27930
val_loss=11.08968
val_loss= 8.87550
val_loss= 9.16293
val_loss= 8.69329
adv_loss= 4.40405
adv_loss= 1.41756
adv_loss=15.12226
adv_loss= 3.45859
adv_loss= 2.01405
adv_loss= 1.52023
adv_loss= 1.24370
adv_loss= 2.01067
adv_loss= 1.53119
adv_loss= 2.02601
surrogate=-0.00418, entropy= 0.76775, loss=-0.00418
surrogate= 0.00385, entropy= 0.76870, loss= 0.00385
surrogate= 0.00378, entropy= 0.77046, loss= 0.00378
surrogate=-0.02831, entropy= 0.77132, loss=-0.02831
surrogate=-0.02434, entropy= 0.77164, loss=-0.02434
surrogate= 0.01249, entropy= 0.77202, loss= 0.01249
surrogate= 0.00863, entropy= 0.77285, loss= 0.00863
surrogate=-0.02984, entropy= 0.77330, loss=-0.02984
surrogate=-0.01337, entropy= 0.77374, loss=-0.01337
surrogate=-0.00689, entropy= 0.77444, loss=-0.00689
std_min= 0.25658, std_max= 0.37327, std_mean= 0.31692
val lr: [9.01639344262295e-05], policy lr: [0.00010819672131147538]
Policy Loss: -0.0068867, | Entropy Bonus: -0, | Value Loss: 8.6933, | Advantage Loss: 2.026
Time elapsed (s): 1.6579749584197998
Agent stdevs: 0.31692007
--------------------------------------------------------------------------------

Step 624
++++++++ Policy training ++++++++++
Current mean reward: 2392.806575 | mean episode length: 653.666667
val_loss=12.40484
val_loss=11.40801
val_loss= 6.63130
val_loss= 7.84409
val_loss= 3.76694
val_loss= 6.69809
val_loss= 9.11892
val_loss= 7.78386
val_loss= 7.16285
val_loss= 7.98401
adv_loss= 1.31686
adv_loss= 1.57361
adv_loss= 1.84788
adv_loss= 0.88576
adv_loss= 2.20605
adv_loss= 1.25733
adv_loss= 2.11503
adv_loss= 1.29531
adv_loss= 0.82343
adv_loss= 1.10870
surrogate= 0.03500, entropy= 0.77413, loss= 0.03500
surrogate= 0.00022, entropy= 0.77315, loss= 0.00022
surrogate=-0.00452, entropy= 0.77159, loss=-0.00452
surrogate=-0.02412, entropy= 0.77036, loss=-0.02412
surrogate=-0.00274, entropy= 0.76905, loss=-0.00274
surrogate= 0.00986, entropy= 0.76849, loss= 0.00986
surrogate= 0.00752, entropy= 0.76830, loss= 0.00752
surrogate=-0.00076, entropy= 0.76796, loss=-0.00076
surrogate= 0.00232, entropy= 0.76640, loss= 0.00232
surrogate=-0.04198, entropy= 0.76612, loss=-0.04198
std_min= 0.25677, std_max= 0.37303, std_mean= 0.31600
val lr: [8.990778688524589e-05], policy lr: [0.00010788934426229506]
Policy Loss: -0.041983, | Entropy Bonus: -0, | Value Loss: 7.984, | Advantage Loss: 1.1087
Time elapsed (s): 1.6455318927764893
Agent stdevs: 0.3159952
--------------------------------------------------------------------------------

Step 625
++++++++ Policy training ++++++++++
Current mean reward: 1352.460195 | mean episode length: 367.600000
val_loss=16.10135
val_loss= 9.49869
val_loss= 7.22589
val_loss= 7.12258
val_loss= 8.12687
val_loss=12.97250
val_loss=17.83762
val_loss= 5.58423
val_loss= 8.98387
val_loss= 7.56452
adv_loss= 2.98448
adv_loss= 6.45142
adv_loss= 0.96626
adv_loss= 1.07675
adv_loss= 4.22876
adv_loss= 3.20944
adv_loss= 1.42511
adv_loss= 0.65497
adv_loss= 2.25041
adv_loss= 5.31692
surrogate=-0.00693, entropy= 0.76481, loss=-0.00693
surrogate=-0.01700, entropy= 0.76459, loss=-0.01700
surrogate=-0.02378, entropy= 0.76402, loss=-0.02378
surrogate=-0.03330, entropy= 0.76331, loss=-0.03330
surrogate=-0.02345, entropy= 0.76274, loss=-0.02345
surrogate=-0.01137, entropy= 0.76169, loss=-0.01137
surrogate=-0.02499, entropy= 0.76101, loss=-0.02499
surrogate=-0.02035, entropy= 0.76051, loss=-0.02035
surrogate=-0.04751, entropy= 0.75911, loss=-0.04751
surrogate=-0.01267, entropy= 0.75820, loss=-0.01267
std_min= 0.25575, std_max= 0.37094, std_mean= 0.31515
val lr: [8.965163934426229e-05], policy lr: [0.00010758196721311473]
Policy Loss: -0.012674, | Entropy Bonus: -0, | Value Loss: 7.5645, | Advantage Loss: 5.3169
Time elapsed (s): 1.6355388164520264
Agent stdevs: 0.31514528
--------------------------------------------------------------------------------

Step 626
++++++++ Policy training ++++++++++
Current mean reward: 1809.102206 | mean episode length: 491.000000
val_loss= 8.30019
val_loss= 7.52695
val_loss= 9.60536
val_loss=10.85335
val_loss= 6.50423
val_loss= 9.71578
val_loss= 9.19059
val_loss= 5.65794
val_loss= 6.15313
val_loss= 6.81683
adv_loss= 0.53950
adv_loss= 1.02333
adv_loss= 0.79889
adv_loss= 1.66179
adv_loss= 0.53204
adv_loss= 2.30378
adv_loss= 0.77455
adv_loss= 1.91551
adv_loss= 0.60516
adv_loss= 1.25216
surrogate=-0.00288, entropy= 0.75688, loss=-0.00288
surrogate=-0.00534, entropy= 0.75662, loss=-0.00534
surrogate=-0.02212, entropy= 0.75670, loss=-0.02212
surrogate= 0.00243, entropy= 0.75654, loss= 0.00243
surrogate= 0.00942, entropy= 0.75671, loss= 0.00942
surrogate= 0.00315, entropy= 0.75610, loss= 0.00315
surrogate=-0.00265, entropy= 0.75618, loss=-0.00265
surrogate=-0.02250, entropy= 0.75625, loss=-0.02250
surrogate=-0.04389, entropy= 0.75508, loss=-0.04389
surrogate=-0.02325, entropy= 0.75451, loss=-0.02325
std_min= 0.25587, std_max= 0.36879, std_mean= 0.31464
val lr: [8.939549180327869e-05], policy lr: [0.00010727459016393442]
Policy Loss: -0.02325, | Entropy Bonus: -0, | Value Loss: 6.8168, | Advantage Loss: 1.2522
Time elapsed (s): 1.637634515762329
Agent stdevs: 0.3146439
--------------------------------------------------------------------------------

Step 627
++++++++ Policy training ++++++++++
Current mean reward: 2136.678365 | mean episode length: 593.666667
val_loss=321.48849
val_loss=261.39056
val_loss=44.02176
val_loss=46.92912
val_loss=540.26379
val_loss=635.71637
val_loss=211.97983
val_loss=104.70656
val_loss=14.97165
val_loss=19.85946
adv_loss= 5.63534
adv_loss= 2.14689
adv_loss= 2.15008
adv_loss= 4.63762
adv_loss= 7.70453
adv_loss= 2.53517
adv_loss= 1.38300
adv_loss= 5.61893
adv_loss= 4.18089
adv_loss= 3.16700
surrogate=-0.00794, entropy= 0.75279, loss=-0.00794
surrogate= 0.00827, entropy= 0.75401, loss= 0.00827
surrogate= 0.00419, entropy= 0.75327, loss= 0.00419
surrogate= 0.02054, entropy= 0.75335, loss= 0.02054
surrogate=-0.01441, entropy= 0.75260, loss=-0.01441
surrogate=-0.02965, entropy= 0.75246, loss=-0.02965
surrogate= 0.00643, entropy= 0.75240, loss= 0.00643
surrogate=-0.01436, entropy= 0.75267, loss=-0.01436
surrogate=-0.01619, entropy= 0.75410, loss=-0.01619
surrogate= 0.04993, entropy= 0.75357, loss= 0.04993
std_min= 0.25561, std_max= 0.36798, std_mean= 0.31453
val lr: [8.913934426229509e-05], policy lr: [0.0001069672131147541]
Policy Loss: 0.049929, | Entropy Bonus: -0, | Value Loss: 19.859, | Advantage Loss: 3.167
Time elapsed (s): 1.6390783786773682
Agent stdevs: 0.3145279
--------------------------------------------------------------------------------

Step 628
++++++++ Policy training ++++++++++
Current mean reward: 1574.322817 | mean episode length: 430.250000
val_loss=32.19701
val_loss=16.06857
val_loss=25.99469
val_loss=13.60274
val_loss=16.30529
val_loss=18.87504
val_loss=14.61466
val_loss= 7.10363
val_loss=10.25833
val_loss=10.87164
adv_loss= 1.91892
adv_loss= 7.50431
adv_loss= 1.37696
adv_loss= 2.32672
adv_loss= 2.64384
adv_loss= 1.56693
adv_loss= 4.46446
adv_loss= 0.85479
adv_loss= 3.72940
adv_loss= 1.04841
surrogate=-0.00704, entropy= 0.75364, loss=-0.00704
surrogate=-0.01316, entropy= 0.75533, loss=-0.01316
surrogate=-0.00524, entropy= 0.75707, loss=-0.00524
surrogate=-0.01431, entropy= 0.75856, loss=-0.01431
surrogate=-0.01181, entropy= 0.75907, loss=-0.01181
surrogate=-0.01830, entropy= 0.75997, loss=-0.01830
surrogate=-0.01213, entropy= 0.76130, loss=-0.01213
surrogate=-0.01491, entropy= 0.76285, loss=-0.01491
surrogate= 0.01522, entropy= 0.76304, loss= 0.01522
surrogate=-0.00994, entropy= 0.76413, loss=-0.00994
std_min= 0.25759, std_max= 0.37111, std_mean= 0.31565
val lr: [8.888319672131148e-05], policy lr: [0.00010665983606557377]
Policy Loss: -0.0099371, | Entropy Bonus: -0, | Value Loss: 10.872, | Advantage Loss: 1.0484
Time elapsed (s): 1.6676025390625
Agent stdevs: 0.3156483
--------------------------------------------------------------------------------

Step 629
++++++++ Policy training ++++++++++
Current mean reward: 2079.719784 | mean episode length: 569.000000
val_loss=12.08991
val_loss=11.15092
val_loss=19.85249
val_loss=16.17829
val_loss= 9.41402
val_loss= 7.53047
val_loss=10.36576
val_loss= 5.30229
val_loss= 3.41302
val_loss= 9.33783
adv_loss= 0.81045
adv_loss= 1.38012
adv_loss= 0.99257
adv_loss= 1.59890
adv_loss= 1.62988
adv_loss= 1.22711
adv_loss= 1.28572
adv_loss= 1.02335
adv_loss= 0.90355
adv_loss= 1.11743
surrogate= 0.00568, entropy= 0.76602, loss= 0.00568
surrogate=-0.00795, entropy= 0.76685, loss=-0.00795
surrogate= 0.01406, entropy= 0.76774, loss= 0.01406
surrogate= 0.00244, entropy= 0.76804, loss= 0.00244
surrogate=-0.02067, entropy= 0.76913, loss=-0.02067
surrogate=-0.01387, entropy= 0.77019, loss=-0.01387
surrogate=-0.00737, entropy= 0.76977, loss=-0.00737
surrogate= 0.00110, entropy= 0.76989, loss= 0.00110
surrogate=-0.02138, entropy= 0.76965, loss=-0.02138
surrogate=-0.00221, entropy= 0.77012, loss=-0.00221
std_min= 0.25912, std_max= 0.36968, std_mean= 0.31609
val lr: [8.862704918032788e-05], policy lr: [0.00010635245901639344]
Policy Loss: -0.0022113, | Entropy Bonus: -0, | Value Loss: 9.3378, | Advantage Loss: 1.1174
Time elapsed (s): 1.6510381698608398
Agent stdevs: 0.3160877
--------------------------------------------------------------------------------

Step 630
++++++++ Policy training ++++++++++
Current mean reward: 1644.047889 | mean episode length: 445.500000
val_loss=12.05789
val_loss= 8.49401
val_loss=11.36207
val_loss= 9.44098
val_loss= 8.22168
val_loss=10.15214
val_loss= 5.50723
val_loss= 5.91925
val_loss= 4.74274
val_loss= 7.70419
adv_loss= 2.60178
adv_loss= 2.05748
adv_loss= 3.83754
adv_loss= 0.84385
adv_loss= 1.91648
adv_loss= 2.66081
adv_loss= 2.86727
adv_loss= 1.43799
adv_loss= 1.27472
adv_loss= 0.82862
surrogate= 0.00630, entropy= 0.76942, loss= 0.00630
surrogate=-0.01874, entropy= 0.76922, loss=-0.01874
surrogate=-0.01329, entropy= 0.76897, loss=-0.01329
surrogate= 0.01908, entropy= 0.76866, loss= 0.01908
surrogate=-0.01031, entropy= 0.76777, loss=-0.01031
surrogate=-0.00512, entropy= 0.76690, loss=-0.00512
surrogate=-0.01970, entropy= 0.76705, loss=-0.01970
surrogate= 0.00238, entropy= 0.76653, loss= 0.00238
surrogate=-0.02217, entropy= 0.76574, loss=-0.02217
surrogate=-0.03973, entropy= 0.76599, loss=-0.03973
std_min= 0.25663, std_max= 0.37056, std_mean= 0.31589
val lr: [8.837090163934428e-05], policy lr: [0.00010604508196721312]
Policy Loss: -0.039726, | Entropy Bonus: -0, | Value Loss: 7.7042, | Advantage Loss: 0.82862
Time elapsed (s): 1.6438183784484863
Agent stdevs: 0.31588596
--------------------------------------------------------------------------------

Step 631
++++++++ Policy training ++++++++++
Current mean reward: 1659.141403 | mean episode length: 468.000000
val_loss=40.62707
val_loss=176.65013
val_loss=988.96545
val_loss=693.13000
val_loss=290.05334
val_loss=430.10840
val_loss=84.02513
val_loss=84.13399
val_loss=1267.35461
val_loss=345.93124
adv_loss= 3.16082
adv_loss= 5.82205
adv_loss= 2.71532
adv_loss= 2.77600
adv_loss= 4.11127
adv_loss= 2.91770
adv_loss= 8.02721
adv_loss=1561.81189
adv_loss= 2.39867
adv_loss= 2.12245
surrogate= 0.00533, entropy= 0.76504, loss= 0.00533
surrogate=-0.01735, entropy= 0.76546, loss=-0.01735
surrogate= 0.00247, entropy= 0.76500, loss= 0.00247
surrogate= 0.00154, entropy= 0.76421, loss= 0.00154
surrogate=-0.02634, entropy= 0.76339, loss=-0.02634
surrogate= 0.00559, entropy= 0.76317, loss= 0.00559
surrogate=-0.02206, entropy= 0.76187, loss=-0.02206
surrogate=-0.02473, entropy= 0.76135, loss=-0.02473
surrogate=-0.02555, entropy= 0.75953, loss=-0.02555
surrogate=-0.00444, entropy= 0.75883, loss=-0.00444
std_min= 0.25463, std_max= 0.37053, std_mean= 0.31528
val lr: [8.811475409836065e-05], policy lr: [0.00010573770491803277]
Policy Loss: -0.0044411, | Entropy Bonus: -0, | Value Loss: 345.93, | Advantage Loss: 2.1224
Time elapsed (s): 1.6595768928527832
Agent stdevs: 0.31528488
--------------------------------------------------------------------------------

Step 632
++++++++ Policy training ++++++++++
Current mean reward: 1545.269364 | mean episode length: 423.250000
val_loss=22.51230
val_loss=17.11005
val_loss=13.95094
val_loss=16.99866
val_loss= 9.75458
val_loss=13.13929
val_loss= 7.83494
val_loss=13.79041
val_loss= 6.20950
val_loss=11.13008
adv_loss= 3.00077
adv_loss= 2.66119
adv_loss= 1.16857
adv_loss= 1.84810
adv_loss= 1.89343
adv_loss= 2.45848
adv_loss= 1.51377
adv_loss= 1.80608
adv_loss= 1.33729
adv_loss= 2.03199
surrogate= 0.01573, entropy= 0.75719, loss= 0.01573
surrogate=-0.02347, entropy= 0.75683, loss=-0.02347
surrogate=-0.00909, entropy= 0.75627, loss=-0.00909
surrogate=-0.02023, entropy= 0.75605, loss=-0.02023
surrogate=-0.02082, entropy= 0.75447, loss=-0.02082
surrogate=-0.02052, entropy= 0.75350, loss=-0.02052
surrogate= 0.00920, entropy= 0.75218, loss= 0.00920
surrogate=-0.03228, entropy= 0.75176, loss=-0.03228
surrogate=-0.02507, entropy= 0.75103, loss=-0.02507
surrogate=-0.03267, entropy= 0.75025, loss=-0.03267
std_min= 0.25464, std_max= 0.36822, std_mean= 0.31427
val lr: [8.785860655737704e-05], policy lr: [0.00010543032786885244]
Policy Loss: -0.032666, | Entropy Bonus: -0, | Value Loss: 11.13, | Advantage Loss: 2.032
Time elapsed (s): 1.6360974311828613
Agent stdevs: 0.31427205
--------------------------------------------------------------------------------

Step 633
++++++++ Policy training ++++++++++
Current mean reward: 1485.486011 | mean episode length: 406.750000
val_loss=35.63291
val_loss=23.35662
val_loss=18.24110
val_loss=15.56653
val_loss=27.32192
val_loss=11.83933
val_loss=15.29453
val_loss=18.91972
val_loss=10.23865
val_loss=11.60180
adv_loss= 1.14432
adv_loss= 0.98086
adv_loss= 3.11039
adv_loss= 4.40238
adv_loss= 1.02561
adv_loss= 2.07450
adv_loss= 2.92625
adv_loss= 1.52266
adv_loss= 1.61012
adv_loss= 2.84950
surrogate= 0.03003, entropy= 0.75045, loss= 0.03003
surrogate=-0.01887, entropy= 0.75185, loss=-0.01887
surrogate=-0.04303, entropy= 0.75276, loss=-0.04303
surrogate=-0.01844, entropy= 0.75355, loss=-0.01844
surrogate= 0.01256, entropy= 0.75448, loss= 0.01256
surrogate=-0.02952, entropy= 0.75475, loss=-0.02952
surrogate=-0.00798, entropy= 0.75499, loss=-0.00798
surrogate=-0.04520, entropy= 0.75583, loss=-0.04520
surrogate= 0.00470, entropy= 0.75464, loss= 0.00470
surrogate=-0.03437, entropy= 0.75463, loss=-0.03437
std_min= 0.25511, std_max= 0.36893, std_mean= 0.31473
val lr: [8.760245901639344e-05], policy lr: [0.00010512295081967212]
Policy Loss: -0.034373, | Entropy Bonus: -0, | Value Loss: 11.602, | Advantage Loss: 2.8495
Time elapsed (s): 1.6429636478424072
Agent stdevs: 0.3147313
--------------------------------------------------------------------------------

Step 634
++++++++ Policy training ++++++++++
Current mean reward: 1366.025533 | mean episode length: 373.600000
val_loss=11.39056
val_loss=12.47333
val_loss= 9.48633
val_loss= 9.89474
val_loss= 7.42454
val_loss= 6.35894
val_loss=10.81777
val_loss=14.01448
val_loss= 7.05720
val_loss=10.51915
adv_loss= 1.21198
adv_loss= 1.28679
adv_loss= 4.20913
adv_loss= 2.34484
adv_loss= 0.97420
adv_loss= 2.86327
adv_loss= 1.91702
adv_loss= 1.57049
adv_loss= 0.86862
adv_loss= 1.93350
surrogate=-0.00153, entropy= 0.75351, loss=-0.00153
surrogate= 0.01597, entropy= 0.75140, loss= 0.01597
surrogate=-0.01502, entropy= 0.75077, loss=-0.01502
surrogate=-0.00066, entropy= 0.75004, loss=-0.00066
surrogate=-0.00782, entropy= 0.74951, loss=-0.00782
surrogate=-0.01937, entropy= 0.74899, loss=-0.01937
surrogate=-0.01547, entropy= 0.74744, loss=-0.01547
surrogate=-0.01168, entropy= 0.74640, loss=-0.01168
surrogate=-0.01268, entropy= 0.74539, loss=-0.01268
surrogate=-0.03448, entropy= 0.74504, loss=-0.03448
std_min= 0.25501, std_max= 0.36799, std_mean= 0.31367
val lr: [8.734631147540984e-05], policy lr: [0.00010481557377049179]
Policy Loss: -0.034476, | Entropy Bonus: -0, | Value Loss: 10.519, | Advantage Loss: 1.9335
Time elapsed (s): 1.686309576034546
Agent stdevs: 0.31367254
--------------------------------------------------------------------------------

Step 635
++++++++ Policy training ++++++++++
Current mean reward: 2384.981972 | mean episode length: 666.000000
val_loss=19.64976
val_loss=134.04324
val_loss=509.90375
val_loss=607.09888
val_loss=23.38018
val_loss=1284.22803
val_loss=197.16672
val_loss=26.02930
val_loss=57.69912
val_loss=814.50201
adv_loss= 2.28416
adv_loss= 1.54792
adv_loss= 2.72711
adv_loss= 0.88381
adv_loss= 1.26440
adv_loss= 0.99994
adv_loss= 2.30839
adv_loss= 4.74285
adv_loss= 0.99590
adv_loss= 0.99565
surrogate=-0.00648, entropy= 0.74678, loss=-0.00648
surrogate= 0.01756, entropy= 0.74934, loss= 0.01756
surrogate=-0.00113, entropy= 0.75272, loss=-0.00113
surrogate= 0.00294, entropy= 0.75434, loss= 0.00294
surrogate=-0.02788, entropy= 0.75654, loss=-0.02788
surrogate=-0.00950, entropy= 0.75800, loss=-0.00950
surrogate=-0.01344, entropy= 0.75961, loss=-0.01344
surrogate=-0.03592, entropy= 0.76161, loss=-0.03592
surrogate=-0.00228, entropy= 0.76366, loss=-0.00228
surrogate=-0.01570, entropy= 0.76514, loss=-0.01570
std_min= 0.25699, std_max= 0.37013, std_mean= 0.31575
val lr: [8.709016393442624e-05], policy lr: [0.00010450819672131146]
Policy Loss: -0.015705, | Entropy Bonus: -0, | Value Loss: 814.5, | Advantage Loss: 0.99565
Time elapsed (s): 1.6479346752166748
Agent stdevs: 0.31575426
--------------------------------------------------------------------------------

Step 636
++++++++ Policy training ++++++++++
Current mean reward: 1462.177623 | mean episode length: 399.200000
val_loss=17.12013
val_loss=13.48688
val_loss=27.99029
val_loss=17.37369
val_loss= 7.86873
val_loss=16.11754
val_loss=10.42561
val_loss= 9.07550
val_loss= 8.03095
val_loss=23.45742
adv_loss= 1.07308
adv_loss= 2.29981
adv_loss= 1.52336
adv_loss= 1.14211
adv_loss= 1.38763
adv_loss= 0.87896
adv_loss= 1.14658
adv_loss= 1.61432
adv_loss= 2.06078
adv_loss= 1.43654
surrogate= 0.00293, entropy= 0.76496, loss= 0.00293
surrogate=-0.01458, entropy= 0.76565, loss=-0.01458
surrogate= 0.00122, entropy= 0.76530, loss= 0.00122
surrogate=-0.00519, entropy= 0.76661, loss=-0.00519
surrogate= 0.00184, entropy= 0.76627, loss= 0.00184
surrogate=-0.02673, entropy= 0.76631, loss=-0.02673
surrogate=-0.05143, entropy= 0.76654, loss=-0.05143
surrogate=-0.02120, entropy= 0.76720, loss=-0.02120
surrogate=-0.01748, entropy= 0.76648, loss=-0.01748
surrogate=-0.02444, entropy= 0.76734, loss=-0.02444
std_min= 0.25801, std_max= 0.36962, std_mean= 0.31588
val lr: [8.683401639344262e-05], policy lr: [0.00010420081967213114]
Policy Loss: -0.024437, | Entropy Bonus: -0, | Value Loss: 23.457, | Advantage Loss: 1.4365
Time elapsed (s): 1.6522784233093262
Agent stdevs: 0.31587765
--------------------------------------------------------------------------------

Step 637
++++++++ Policy training ++++++++++
Current mean reward: 2315.399238 | mean episode length: 655.000000
val_loss=27.09077
val_loss=688.35132
val_loss=75.62621
val_loss=152.04662
val_loss=175.50459
val_loss=98.58984
val_loss=45.09608
val_loss=239.77383
val_loss=155.90765
val_loss=833.37164
adv_loss= 0.89388
adv_loss= 1.11121
adv_loss= 1.26836
adv_loss= 1.37278
adv_loss= 0.96648
adv_loss= 0.90725
adv_loss= 1.41596
adv_loss= 1.94647
adv_loss= 1.40963
adv_loss= 2.43696
surrogate= 0.03142, entropy= 0.76894, loss= 0.03142
surrogate= 0.00105, entropy= 0.76873, loss= 0.00105
surrogate=-0.00923, entropy= 0.76908, loss=-0.00923
surrogate= 0.00738, entropy= 0.77030, loss= 0.00738
surrogate= 0.00733, entropy= 0.77123, loss= 0.00733
surrogate= 0.00113, entropy= 0.77141, loss= 0.00113
surrogate=-0.02872, entropy= 0.77258, loss=-0.02872
surrogate=-0.02512, entropy= 0.77393, loss=-0.02512
surrogate=-0.01267, entropy= 0.77478, loss=-0.01267
surrogate=-0.02335, entropy= 0.77508, loss=-0.02335
std_min= 0.25744, std_max= 0.37210, std_mean= 0.31686
val lr: [8.657786885245902e-05], policy lr: [0.00010389344262295081]
Policy Loss: -0.023349, | Entropy Bonus: -0, | Value Loss: 833.37, | Advantage Loss: 2.437
Time elapsed (s): 1.629572868347168
Agent stdevs: 0.31686196
--------------------------------------------------------------------------------

Step 638
++++++++ Policy training ++++++++++
Current mean reward: 1958.828483 | mean episode length: 549.666667
val_loss=193.26569
val_loss=562.46899
val_loss=161.61841
val_loss=857.95154
val_loss=98.41876
val_loss=110.58974
val_loss=29.11275
val_loss=40.26381
val_loss=1572.48511
val_loss=39.08131
adv_loss= 5.71955
adv_loss= 4.63343
adv_loss=13.82912
adv_loss= 5.81612
adv_loss= 5.37673
adv_loss= 3.43443
adv_loss= 5.05420
adv_loss= 3.87756
adv_loss= 5.69986
adv_loss= 4.02385
surrogate=-0.00097, entropy= 0.77405, loss=-0.00097
surrogate=-0.00417, entropy= 0.77338, loss=-0.00417
surrogate=-0.00188, entropy= 0.77302, loss=-0.00188
surrogate=-0.00490, entropy= 0.77295, loss=-0.00490
surrogate=-0.00318, entropy= 0.77134, loss=-0.00318
surrogate=-0.03249, entropy= 0.77095, loss=-0.03249
surrogate=-0.02035, entropy= 0.77001, loss=-0.02035
surrogate=-0.03631, entropy= 0.76946, loss=-0.03631
surrogate=-0.02555, entropy= 0.76755, loss=-0.02555
surrogate=-0.01957, entropy= 0.76749, loss=-0.01957
std_min= 0.25639, std_max= 0.37121, std_mean= 0.31609
val lr: [8.632172131147542e-05], policy lr: [0.00010358606557377049]
Policy Loss: -0.019566, | Entropy Bonus: -0, | Value Loss: 39.081, | Advantage Loss: 4.0239
Time elapsed (s): 1.6447796821594238
Agent stdevs: 0.316093
--------------------------------------------------------------------------------

Step 639
++++++++ Policy training ++++++++++
Current mean reward: 1460.663995 | mean episode length: 393.750000
val_loss=18.52656
val_loss=17.35679
val_loss=13.65883
val_loss=10.94638
val_loss=10.80640
val_loss= 9.10073
val_loss=12.62619
val_loss=12.32438
val_loss= 9.09232
val_loss= 9.31337
adv_loss= 3.11450
adv_loss= 2.86083
adv_loss= 2.11593
adv_loss= 2.67882
adv_loss= 3.60408
adv_loss= 2.00037
adv_loss= 1.70676
adv_loss= 1.66344
adv_loss= 4.12834
adv_loss= 1.69133
surrogate= 0.00974, entropy= 0.76620, loss= 0.00974
surrogate=-0.01370, entropy= 0.76554, loss=-0.01370
surrogate=-0.00072, entropy= 0.76440, loss=-0.00072
surrogate= 0.00571, entropy= 0.76342, loss= 0.00571
surrogate=-0.01803, entropy= 0.76275, loss=-0.01803
surrogate=-0.01246, entropy= 0.76177, loss=-0.01246
surrogate=-0.04409, entropy= 0.76075, loss=-0.04409
surrogate=-0.02222, entropy= 0.76029, loss=-0.02222
surrogate=-0.05223, entropy= 0.75961, loss=-0.05223
surrogate= 0.00529, entropy= 0.75995, loss= 0.00529
std_min= 0.25515, std_max= 0.37053, std_mean= 0.31537
val lr: [8.606557377049182e-05], policy lr: [0.00010327868852459018]
Policy Loss: 0.0052863, | Entropy Bonus: -0, | Value Loss: 9.3134, | Advantage Loss: 1.6913
Time elapsed (s): 1.6520004272460938
Agent stdevs: 0.31536546
--------------------------------------------------------------------------------

Step 640
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1916.4
++++++++ Policy training ++++++++++
Current mean reward: 2374.850411 | mean episode length: 652.000000
val_loss=19.62336
val_loss=12.87520
val_loss=14.75313
val_loss=14.85315
val_loss=15.09973
val_loss=14.07393
val_loss= 6.54502
val_loss= 6.99496
val_loss=10.03186
val_loss= 8.72587
adv_loss= 2.33531
adv_loss= 1.25230
adv_loss= 2.60459
adv_loss= 1.77927
adv_loss= 2.41809
adv_loss= 1.66403
adv_loss= 2.54170
adv_loss= 1.22823
adv_loss= 1.14569
adv_loss= 1.29062
surrogate= 0.02651, entropy= 0.76050, loss= 0.02651
surrogate=-0.00450, entropy= 0.76176, loss=-0.00450
surrogate= 0.00895, entropy= 0.76215, loss= 0.00895
surrogate= 0.00004, entropy= 0.76228, loss= 0.00004
surrogate=-0.01277, entropy= 0.76269, loss=-0.01277
surrogate=-0.01312, entropy= 0.76323, loss=-0.01312
surrogate=-0.00751, entropy= 0.76402, loss=-0.00751
surrogate=-0.00826, entropy= 0.76379, loss=-0.00826
surrogate=-0.01758, entropy= 0.76417, loss=-0.01758
surrogate=-0.01643, entropy= 0.76455, loss=-0.01643
std_min= 0.25583, std_max= 0.36956, std_mean= 0.31576
val lr: [8.580942622950818e-05], policy lr: [0.00010297131147540981]
Policy Loss: -0.016433, | Entropy Bonus: -0, | Value Loss: 8.7259, | Advantage Loss: 1.2906
Time elapsed (s): 1.682222604751587
Agent stdevs: 0.31575838
--------------------------------------------------------------------------------

Step 641
++++++++ Policy training ++++++++++
Current mean reward: 1994.624515 | mean episode length: 546.666667
val_loss=16.40026
val_loss=22.89648
val_loss=18.09304
val_loss=18.32366
val_loss=23.46838
val_loss=25.44020
val_loss=20.67704
val_loss=13.72289
val_loss= 8.46026
val_loss=17.73602
adv_loss= 2.02440
adv_loss= 2.15745
adv_loss= 2.29175
adv_loss= 1.67234
adv_loss= 1.82898
adv_loss= 2.14519
adv_loss= 1.08713
adv_loss= 1.42577
adv_loss= 1.81438
adv_loss= 1.66909
surrogate= 0.00195, entropy= 0.76487, loss= 0.00195
surrogate=-0.01843, entropy= 0.76666, loss=-0.01843
surrogate= 0.00159, entropy= 0.76667, loss= 0.00159
surrogate=-0.00780, entropy= 0.76813, loss=-0.00780
surrogate=-0.02637, entropy= 0.76857, loss=-0.02637
surrogate=-0.03275, entropy= 0.76876, loss=-0.03275
surrogate=-0.02259, entropy= 0.76961, loss=-0.02259
surrogate=-0.02854, entropy= 0.76990, loss=-0.02854
surrogate=-0.00438, entropy= 0.76975, loss=-0.00438
surrogate=-0.00914, entropy= 0.76981, loss=-0.00914
std_min= 0.25510, std_max= 0.36875, std_mean= 0.31636
val lr: [8.555327868852458e-05], policy lr: [0.00010266393442622948]
Policy Loss: -0.0091419, | Entropy Bonus: -0, | Value Loss: 17.736, | Advantage Loss: 1.6691
Time elapsed (s): 1.6541004180908203
Agent stdevs: 0.31635877
--------------------------------------------------------------------------------

Step 642
++++++++ Policy training ++++++++++
Current mean reward: 2077.798766 | mean episode length: 574.666667
val_loss=86.61372
val_loss=92.22201
val_loss=50.44869
val_loss=51.76018
val_loss=18.84324
val_loss=11.96108
val_loss=36.60141
val_loss= 7.32805
val_loss=30.16977
val_loss=11.76979
adv_loss= 1.67666
adv_loss= 1.77464
adv_loss= 1.45892
adv_loss= 1.26272
adv_loss= 0.94062
adv_loss= 1.52991
adv_loss= 2.27210
adv_loss= 0.77845
adv_loss= 1.47023
adv_loss= 2.25383
surrogate= 0.01660, entropy= 0.77033, loss= 0.01660
surrogate=-0.01700, entropy= 0.77100, loss=-0.01700
surrogate=-0.01447, entropy= 0.77186, loss=-0.01447
surrogate=-0.01766, entropy= 0.77171, loss=-0.01766
surrogate= 0.00205, entropy= 0.77174, loss= 0.00205
surrogate=-0.01377, entropy= 0.77188, loss=-0.01377
surrogate=-0.00146, entropy= 0.77256, loss=-0.00146
surrogate=-0.02112, entropy= 0.77288, loss=-0.02112
surrogate= 0.00352, entropy= 0.77278, loss= 0.00352
surrogate=-0.00646, entropy= 0.77364, loss=-0.00646
std_min= 0.25672, std_max= 0.36771, std_mean= 0.31658
val lr: [8.529713114754098e-05], policy lr: [0.00010235655737704916]
Policy Loss: -0.0064576, | Entropy Bonus: -0, | Value Loss: 11.77, | Advantage Loss: 2.2538
Time elapsed (s): 1.6662702560424805
Agent stdevs: 0.31658378
--------------------------------------------------------------------------------

Step 643
++++++++ Policy training ++++++++++
Current mean reward: 1675.793066 | mean episode length: 455.750000
val_loss=27.50607
val_loss= 9.06033
val_loss= 6.69024
val_loss=18.55214
val_loss=13.80650
val_loss=11.05595
val_loss=12.43985
val_loss=10.23391
val_loss=11.92463
val_loss=13.77979
adv_loss= 1.91593
adv_loss= 1.16020
adv_loss= 1.00208
adv_loss= 1.41999
adv_loss= 1.19605
adv_loss= 3.02358
adv_loss= 1.76150
adv_loss= 3.47455
adv_loss= 1.02358
adv_loss= 1.28647
surrogate=-0.00435, entropy= 0.77278, loss=-0.00435
surrogate=-0.01235, entropy= 0.77093, loss=-0.01235
surrogate= 0.02479, entropy= 0.76967, loss= 0.02479
surrogate=-0.00122, entropy= 0.76788, loss=-0.00122
surrogate=-0.03058, entropy= 0.76593, loss=-0.03058
surrogate=-0.00164, entropy= 0.76496, loss=-0.00164
surrogate= 0.00749, entropy= 0.76356, loss= 0.00749
surrogate=-0.02588, entropy= 0.76232, loss=-0.02588
surrogate=-0.03121, entropy= 0.76064, loss=-0.03121
surrogate=-0.01267, entropy= 0.75949, loss=-0.01267
std_min= 0.25573, std_max= 0.36758, std_mean= 0.31513
val lr: [8.504098360655738e-05], policy lr: [0.00010204918032786885]
Policy Loss: -0.012672, | Entropy Bonus: -0, | Value Loss: 13.78, | Advantage Loss: 1.2865
Time elapsed (s): 1.6416010856628418
Agent stdevs: 0.31513375
--------------------------------------------------------------------------------

Step 644
++++++++ Policy training ++++++++++
Current mean reward: 1843.456553 | mean episode length: 510.750000
val_loss=14.79873
val_loss=11.86301
val_loss=16.31248
val_loss=17.79662
val_loss=11.19441
val_loss= 5.52713
val_loss=17.60616
val_loss= 8.76376
val_loss= 8.06859
val_loss= 6.76899
adv_loss= 1.24726
adv_loss= 1.91454
adv_loss= 1.76028
adv_loss= 3.14440
adv_loss= 1.10002
adv_loss= 2.92819
adv_loss= 0.68787
adv_loss= 2.14761
adv_loss= 1.96102
adv_loss= 2.05476
surrogate= 0.03491, entropy= 0.76016, loss= 0.03491
surrogate=-0.01437, entropy= 0.76068, loss=-0.01437
surrogate=-0.00836, entropy= 0.76170, loss=-0.00836
surrogate= 0.01624, entropy= 0.76148, loss= 0.01624
surrogate= 0.00388, entropy= 0.76212, loss= 0.00388
surrogate=-0.03949, entropy= 0.76279, loss=-0.03949
surrogate= 0.01294, entropy= 0.76277, loss= 0.01294
surrogate=-0.00776, entropy= 0.76384, loss=-0.00776
surrogate=-0.00988, entropy= 0.76471, loss=-0.00988
surrogate=-0.02607, entropy= 0.76459, loss=-0.02607
std_min= 0.25588, std_max= 0.36909, std_mean= 0.31574
val lr: [8.478483606557377e-05], policy lr: [0.00010174180327868852]
Policy Loss: -0.026074, | Entropy Bonus: -0, | Value Loss: 6.769, | Advantage Loss: 2.0548
Time elapsed (s): 1.6790640354156494
Agent stdevs: 0.31573722
--------------------------------------------------------------------------------

Step 645
++++++++ Policy training ++++++++++
Current mean reward: 1964.542969 | mean episode length: 548.000000
val_loss=59.87772
val_loss=1353.20178
val_loss=692.34180
val_loss=49.89880
val_loss=1144.01099
val_loss=37.56071
val_loss=348.59842
val_loss=104.25816
val_loss=1330.37000
val_loss=560.70453
adv_loss= 7.34574
adv_loss= 6.12991
adv_loss= 6.19514
adv_loss=10.96632
adv_loss= 6.74887
adv_loss= 7.25687
adv_loss= 4.21915
adv_loss= 3.52511
adv_loss=16.36015
adv_loss=1730.03918
surrogate=-0.02287, entropy= 0.76404, loss=-0.02287
surrogate=-0.02750, entropy= 0.76383, loss=-0.02750
surrogate= 0.01573, entropy= 0.76347, loss= 0.01573
surrogate= 0.00128, entropy= 0.76251, loss= 0.00128
surrogate=-0.03182, entropy= 0.76148, loss=-0.03182
surrogate=-0.01074, entropy= 0.76191, loss=-0.01074
surrogate=-0.02591, entropy= 0.76154, loss=-0.02591
surrogate=-0.01685, entropy= 0.76098, loss=-0.01685
surrogate= 0.01053, entropy= 0.76100, loss= 0.01053
surrogate= 0.00428, entropy= 0.76061, loss= 0.00428
std_min= 0.25439, std_max= 0.36999, std_mean= 0.31547
val lr: [8.452868852459017e-05], policy lr: [0.0001014344262295082]
Policy Loss: 0.0042807, | Entropy Bonus: -0, | Value Loss: 560.7, | Advantage Loss: 1730
Time elapsed (s): 1.667841911315918
Agent stdevs: 0.31546986
--------------------------------------------------------------------------------

Step 646
++++++++ Policy training ++++++++++
Current mean reward: 2796.229283 | mean episode length: 769.500000
val_loss=46.35262
val_loss=13.46369
val_loss=19.46640
val_loss=17.50532
val_loss=22.40770
val_loss=18.92282
val_loss=13.41244
val_loss=15.61794
val_loss=16.10857
val_loss=12.54733
adv_loss= 3.02057
adv_loss= 2.46506
adv_loss= 2.42583
adv_loss= 1.36668
adv_loss= 1.46467
adv_loss= 1.02943
adv_loss= 2.42884
adv_loss= 1.79416
adv_loss= 1.80697
adv_loss= 1.25362
surrogate=-0.00225, entropy= 0.76127, loss=-0.00225
surrogate=-0.01787, entropy= 0.76149, loss=-0.01787
surrogate= 0.01898, entropy= 0.76105, loss= 0.01898
surrogate=-0.01143, entropy= 0.76173, loss=-0.01143
surrogate=-0.02429, entropy= 0.76132, loss=-0.02429
surrogate=-0.03150, entropy= 0.76199, loss=-0.03150
surrogate=-0.00228, entropy= 0.76238, loss=-0.00228
surrogate=-0.00357, entropy= 0.76300, loss=-0.00357
surrogate=-0.02702, entropy= 0.76213, loss=-0.02702
surrogate= 0.00544, entropy= 0.76244, loss= 0.00544
std_min= 0.25483, std_max= 0.36717, std_mean= 0.31552
val lr: [8.427254098360657e-05], policy lr: [0.00010112704918032787]
Policy Loss: 0.0054355, | Entropy Bonus: -0, | Value Loss: 12.547, | Advantage Loss: 1.2536
Time elapsed (s): 1.6950619220733643
Agent stdevs: 0.31551963
--------------------------------------------------------------------------------

Step 647
++++++++ Policy training ++++++++++
Current mean reward: 1240.863608 | mean episode length: 338.833333
val_loss=26.79112
val_loss=29.57061
val_loss=23.33511
val_loss=25.26792
val_loss=12.48005
val_loss=18.37847
val_loss=23.77362
val_loss=21.91569
val_loss=25.26196
val_loss=18.89336
adv_loss= 3.81932
adv_loss= 2.41074
adv_loss= 5.47318
adv_loss= 1.98904
adv_loss= 2.00705
adv_loss= 1.41045
adv_loss= 2.26218
adv_loss= 1.44025
adv_loss= 1.38101
adv_loss= 1.04145
surrogate= 0.04377, entropy= 0.76183, loss= 0.04377
surrogate= 0.00480, entropy= 0.76120, loss= 0.00480
surrogate= 0.01716, entropy= 0.76036, loss= 0.01716
surrogate=-0.00715, entropy= 0.76041, loss=-0.00715
surrogate=-0.02317, entropy= 0.76061, loss=-0.02317
surrogate=-0.03103, entropy= 0.75905, loss=-0.03103
surrogate=-0.04551, entropy= 0.75914, loss=-0.04551
surrogate=-0.03933, entropy= 0.75841, loss=-0.03933
surrogate=-0.01907, entropy= 0.75824, loss=-0.01907
surrogate=-0.01014, entropy= 0.75784, loss=-0.01014
std_min= 0.25527, std_max= 0.36576, std_mean= 0.31493
val lr: [8.401639344262297e-05], policy lr: [0.00010081967213114754]
Policy Loss: -0.010143, | Entropy Bonus: -0, | Value Loss: 18.893, | Advantage Loss: 1.0415
Time elapsed (s): 1.6782803535461426
Agent stdevs: 0.3149264
--------------------------------------------------------------------------------

Step 648
++++++++ Policy training ++++++++++
Current mean reward: 2504.055790 | mean episode length: 675.500000
val_loss=11.66094
val_loss=13.71794
val_loss= 9.59313
val_loss=11.45857
val_loss= 6.08227
val_loss= 7.76461
val_loss= 7.31267
val_loss= 5.43635
val_loss= 5.45549
val_loss= 6.68789
adv_loss= 1.19525
adv_loss= 1.54113
adv_loss= 0.72902
adv_loss= 1.50546
adv_loss= 2.59767
adv_loss= 0.62499
adv_loss= 1.26390
adv_loss= 1.28192
adv_loss= 1.27536
adv_loss= 1.30127
surrogate= 0.01348, entropy= 0.75580, loss= 0.01348
surrogate=-0.05027, entropy= 0.75442, loss=-0.05027
surrogate=-0.02173, entropy= 0.75304, loss=-0.02173
surrogate=-0.00206, entropy= 0.75174, loss=-0.00206
surrogate= 0.00398, entropy= 0.75005, loss= 0.00398
surrogate=-0.03822, entropy= 0.74923, loss=-0.03822
surrogate=-0.03860, entropy= 0.74825, loss=-0.03860
surrogate=-0.04202, entropy= 0.74707, loss=-0.04202
surrogate=-0.02456, entropy= 0.74598, loss=-0.02456
surrogate=-0.01945, entropy= 0.74520, loss=-0.01945
std_min= 0.25636, std_max= 0.36273, std_mean= 0.31335
val lr: [8.376024590163935e-05], policy lr: [0.00010051229508196722]
Policy Loss: -0.019454, | Entropy Bonus: -0, | Value Loss: 6.6879, | Advantage Loss: 1.3013
Time elapsed (s): 1.6718432903289795
Agent stdevs: 0.31335193
--------------------------------------------------------------------------------

Step 649
++++++++ Policy training ++++++++++
Current mean reward: 1623.381778 | mean episode length: 447.500000
val_loss=28.75861
val_loss=27.41086
val_loss=19.13396
val_loss=17.29833
val_loss=26.69624
val_loss=10.48165
val_loss=16.37012
val_loss=16.69319
val_loss=13.38956
val_loss=30.55441
adv_loss= 1.92962
adv_loss= 3.62808
adv_loss= 1.71991
adv_loss= 2.24141
adv_loss= 5.17559
adv_loss= 0.78087
adv_loss= 3.16154
adv_loss= 1.54626
adv_loss= 1.10590
adv_loss= 1.62073
surrogate= 0.03014, entropy= 0.74337, loss= 0.03014
surrogate= 0.00156, entropy= 0.74375, loss= 0.00156
surrogate=-0.01740, entropy= 0.74221, loss=-0.01740
surrogate= 0.00951, entropy= 0.74189, loss= 0.00951
surrogate=-0.00832, entropy= 0.74064, loss=-0.00832
surrogate= 0.00355, entropy= 0.73905, loss= 0.00355
surrogate=-0.02107, entropy= 0.73873, loss=-0.02107
surrogate=-0.03368, entropy= 0.73741, loss=-0.03368
surrogate=-0.00722, entropy= 0.73619, loss=-0.00722
surrogate=-0.05430, entropy= 0.73496, loss=-0.05430
std_min= 0.25660, std_max= 0.35966, std_mean= 0.31213
val lr: [8.350409836065573e-05], policy lr: [0.00010020491803278687]
Policy Loss: -0.054297, | Entropy Bonus: -0, | Value Loss: 30.554, | Advantage Loss: 1.6207
Time elapsed (s): 1.6593828201293945
Agent stdevs: 0.3121309
--------------------------------------------------------------------------------

Step 650
++++++++ Policy training ++++++++++
Current mean reward: 1940.087991 | mean episode length: 547.666667
val_loss=126.41553
val_loss=123.25199
val_loss=342.96655
val_loss=296.40915
val_loss=863.92487
val_loss=682.63574
val_loss=588.25177
val_loss=40.17405
val_loss=168.72852
val_loss=300.11273
adv_loss= 3.05549
adv_loss= 3.80384
adv_loss=148.55829
adv_loss= 3.96052
adv_loss= 1.81575
adv_loss= 4.52234
adv_loss= 2.33560
adv_loss= 3.10556
adv_loss= 6.85741
adv_loss= 4.20728
surrogate=-0.00447, entropy= 0.73476, loss=-0.00447
surrogate= 0.01752, entropy= 0.73448, loss= 0.01752
surrogate= 0.00603, entropy= 0.73442, loss= 0.00603
surrogate=-0.00676, entropy= 0.73539, loss=-0.00676
surrogate= 0.00506, entropy= 0.73540, loss= 0.00506
surrogate= 0.00503, entropy= 0.73620, loss= 0.00503
surrogate=-0.02845, entropy= 0.73752, loss=-0.02845
surrogate= 0.00214, entropy= 0.73819, loss= 0.00214
surrogate=-0.01011, entropy= 0.73940, loss=-0.01011
surrogate=-0.01276, entropy= 0.73967, loss=-0.01276
std_min= 0.25614, std_max= 0.36090, std_mean= 0.31272
val lr: [8.324795081967213e-05], policy lr: [9.989754098360654e-05]
Policy Loss: -0.012763, | Entropy Bonus: -0, | Value Loss: 300.11, | Advantage Loss: 4.2073
Time elapsed (s): 1.680844783782959
Agent stdevs: 0.3127245
--------------------------------------------------------------------------------

Step 651
++++++++ Policy training ++++++++++
Current mean reward: 1516.308894 | mean episode length: 415.750000
val_loss=26.20968
val_loss=48.04121
val_loss=56.27003
val_loss=23.59400
val_loss=26.05280
val_loss=63.86853
val_loss=16.83880
val_loss=30.84226
val_loss=12.51647
val_loss=10.73391
adv_loss= 1.99877
adv_loss= 1.31089
adv_loss= 1.46610
adv_loss= 2.55293
adv_loss= 1.65016
adv_loss= 2.35515
adv_loss= 6.45417
adv_loss=13.78142
adv_loss= 6.33813
adv_loss= 0.72329
surrogate=-0.00635, entropy= 0.74047, loss=-0.00635
surrogate=-0.02716, entropy= 0.74054, loss=-0.02716
surrogate=-0.02770, entropy= 0.74045, loss=-0.02770
surrogate=-0.00840, entropy= 0.74102, loss=-0.00840
surrogate= 0.00630, entropy= 0.74116, loss= 0.00630
surrogate=-0.02258, entropy= 0.74173, loss=-0.02258
surrogate=-0.00200, entropy= 0.74279, loss=-0.00200
surrogate=-0.03711, entropy= 0.74305, loss=-0.03711
surrogate=-0.00751, entropy= 0.74223, loss=-0.00751
surrogate=-0.05681, entropy= 0.74208, loss=-0.05681
std_min= 0.25611, std_max= 0.36198, std_mean= 0.31302
val lr: [8.299180327868853e-05], policy lr: [9.959016393442621e-05]
Policy Loss: -0.056814, | Entropy Bonus: -0, | Value Loss: 10.734, | Advantage Loss: 0.72329
Time elapsed (s): 1.6783473491668701
Agent stdevs: 0.313019
--------------------------------------------------------------------------------

Step 652
++++++++ Policy training ++++++++++
Current mean reward: 1747.874874 | mean episode length: 469.000000
val_loss= 8.86415
val_loss=23.53405
val_loss= 9.63982
val_loss=21.09843
val_loss= 5.78899
val_loss=15.88892
val_loss=22.65152
val_loss= 9.22607
val_loss= 8.05209
val_loss= 9.61416
adv_loss= 3.33606
adv_loss= 1.77206
adv_loss= 1.19276
adv_loss= 1.13236
adv_loss= 1.11837
adv_loss= 2.08846
adv_loss= 0.72956
adv_loss= 1.08584
adv_loss= 0.60797
adv_loss= 1.53975
surrogate= 0.00728, entropy= 0.74083, loss= 0.00728
surrogate=-0.01507, entropy= 0.73923, loss=-0.01507
surrogate=-0.00938, entropy= 0.73766, loss=-0.00938
surrogate=-0.01785, entropy= 0.73539, loss=-0.01785
surrogate=-0.03083, entropy= 0.73383, loss=-0.03083
surrogate=-0.01317, entropy= 0.73227, loss=-0.01317
surrogate=-0.01451, entropy= 0.73065, loss=-0.01451
surrogate=-0.00929, entropy= 0.72882, loss=-0.00929
surrogate=-0.02306, entropy= 0.72694, loss=-0.02306
surrogate=-0.02389, entropy= 0.72576, loss=-0.02389
std_min= 0.25360, std_max= 0.36085, std_mean= 0.31145
val lr: [8.273565573770491e-05], policy lr: [9.928278688524589e-05]
Policy Loss: -0.023889, | Entropy Bonus: -0, | Value Loss: 9.6142, | Advantage Loss: 1.5398
Time elapsed (s): 1.6825170516967773
Agent stdevs: 0.31144544
--------------------------------------------------------------------------------

Step 653
++++++++ Policy training ++++++++++
Current mean reward: 1104.307483 | mean episode length: 308.333333
val_loss=284.31958
val_loss=1797.02795
val_loss=655.75665
val_loss=56.88560
val_loss=58.65882
val_loss=1415.55286
val_loss=28.11635
val_loss=1966.50854
val_loss=696.76636
val_loss=168.59517
adv_loss= 2.18523
adv_loss= 4.75504
adv_loss= 1.74842
adv_loss= 5.74465
adv_loss= 1.48761
adv_loss= 3.07530
adv_loss= 2.96430
adv_loss=14.97164
adv_loss= 2.16211
adv_loss= 7.68200
surrogate= 0.00960, entropy= 0.72488, loss= 0.00960
surrogate=-0.01914, entropy= 0.72405, loss=-0.01914
surrogate=-0.02620, entropy= 0.72371, loss=-0.02620
surrogate=-0.01593, entropy= 0.72269, loss=-0.01593
surrogate=-0.02016, entropy= 0.72118, loss=-0.02016
surrogate=-0.02253, entropy= 0.72149, loss=-0.02253
surrogate=-0.02242, entropy= 0.72105, loss=-0.02242
surrogate=-0.00677, entropy= 0.72097, loss=-0.00677
surrogate=-0.03462, entropy= 0.72079, loss=-0.03462
surrogate=-0.00262, entropy= 0.72014, loss=-0.00262
std_min= 0.25316, std_max= 0.36059, std_mean= 0.31088
val lr: [8.247950819672131e-05], policy lr: [9.897540983606556e-05]
Policy Loss: -0.0026225, | Entropy Bonus: -0, | Value Loss: 168.6, | Advantage Loss: 7.682
Time elapsed (s): 1.6747372150421143
Agent stdevs: 0.3108772
--------------------------------------------------------------------------------

Step 654
++++++++ Policy training ++++++++++
Current mean reward: 1402.575166 | mean episode length: 382.600000
val_loss= 8.97952
val_loss=22.02906
val_loss=20.18258
val_loss=11.63344
val_loss=13.02793
val_loss= 9.97634
val_loss=12.97038
val_loss=16.29440
val_loss= 8.18697
val_loss=15.86255
adv_loss= 1.41368
adv_loss= 2.76967
adv_loss= 3.75984
adv_loss= 1.27641
adv_loss= 1.82265
adv_loss= 2.08317
adv_loss= 1.92353
adv_loss= 2.68318
adv_loss= 2.72304
adv_loss= 3.19184
surrogate= 0.00865, entropy= 0.72023, loss= 0.00865
surrogate=-0.01862, entropy= 0.72113, loss=-0.01862
surrogate=-0.01020, entropy= 0.72168, loss=-0.01020
surrogate=-0.00785, entropy= 0.72139, loss=-0.00785
surrogate=-0.02494, entropy= 0.72197, loss=-0.02494
surrogate=-0.03878, entropy= 0.72285, loss=-0.03878
surrogate= 0.00935, entropy= 0.72260, loss= 0.00935
surrogate= 0.03606, entropy= 0.72293, loss= 0.03606
surrogate=-0.03206, entropy= 0.72381, loss=-0.03206
surrogate=-0.04266, entropy= 0.72522, loss=-0.04266
std_min= 0.25325, std_max= 0.35969, std_mean= 0.31138
val lr: [8.222336065573771e-05], policy lr: [9.866803278688524e-05]
Policy Loss: -0.042661, | Entropy Bonus: -0, | Value Loss: 15.863, | Advantage Loss: 3.1918
Time elapsed (s): 1.6860017776489258
Agent stdevs: 0.31138062
--------------------------------------------------------------------------------

Step 655
++++++++ Policy training ++++++++++
Current mean reward: 1701.174371 | mean episode length: 466.500000
val_loss=16.15080
val_loss=11.11036
val_loss= 8.54964
val_loss=18.70203
val_loss=14.67918
val_loss=13.04140
val_loss= 8.56692
val_loss= 9.71843
val_loss= 9.07076
val_loss=11.45724
adv_loss= 1.74854
adv_loss= 2.51186
adv_loss= 2.47818
adv_loss= 3.31797
adv_loss= 1.48734
adv_loss= 2.92897
adv_loss= 6.17740
adv_loss= 2.58922
adv_loss= 2.85964
adv_loss= 1.19856
surrogate=-0.01182, entropy= 0.72337, loss=-0.01182
surrogate= 0.00854, entropy= 0.72009, loss= 0.00854
surrogate=-0.01699, entropy= 0.71786, loss=-0.01699
surrogate= 0.00085, entropy= 0.71493, loss= 0.00085
surrogate= 0.01445, entropy= 0.71311, loss= 0.01445
surrogate=-0.00340, entropy= 0.70962, loss=-0.00340
surrogate=-0.00585, entropy= 0.70738, loss=-0.00585
surrogate=-0.03040, entropy= 0.70550, loss=-0.03040
surrogate=-0.05185, entropy= 0.70239, loss=-0.05185
surrogate=-0.01353, entropy= 0.70041, loss=-0.01353
std_min= 0.25094, std_max= 0.35666, std_mean= 0.30883
val lr: [8.196721311475411e-05], policy lr: [9.836065573770491e-05]
Policy Loss: -0.013528, | Entropy Bonus: -0, | Value Loss: 11.457, | Advantage Loss: 1.1986
Time elapsed (s): 1.6723217964172363
Agent stdevs: 0.30882734
--------------------------------------------------------------------------------

Step 656
++++++++ Policy training ++++++++++
Current mean reward: 1633.048263 | mean episode length: 439.000000
val_loss= 8.77087
val_loss= 7.18931
val_loss=10.28113
val_loss= 9.00528
val_loss= 9.47430
val_loss= 7.57377
val_loss= 5.54811
val_loss= 8.10611
val_loss= 4.61087
val_loss= 5.92540
adv_loss= 1.47473
adv_loss= 2.41546
adv_loss= 2.03598
adv_loss= 0.93441
adv_loss= 1.15593
adv_loss= 0.79525
adv_loss= 1.03311
adv_loss= 3.01193
adv_loss= 1.55979
adv_loss= 1.14882
surrogate=-0.00429, entropy= 0.69911, loss=-0.00429
surrogate=-0.01866, entropy= 0.69776, loss=-0.01866
surrogate=-0.01460, entropy= 0.69839, loss=-0.01460
surrogate= 0.02412, entropy= 0.69798, loss= 0.02412
surrogate= 0.01060, entropy= 0.69626, loss= 0.01060
surrogate= 0.02035, entropy= 0.69505, loss= 0.02035
surrogate=-0.01791, entropy= 0.69384, loss=-0.01791
surrogate=-0.00327, entropy= 0.69336, loss=-0.00327
surrogate=-0.03019, entropy= 0.69251, loss=-0.03019
surrogate=-0.00636, entropy= 0.69228, loss=-0.00636
std_min= 0.25030, std_max= 0.35665, std_mean= 0.30802
val lr: [8.17110655737705e-05], policy lr: [9.80532786885246e-05]
Policy Loss: -0.0063638, | Entropy Bonus: -0, | Value Loss: 5.9254, | Advantage Loss: 1.1488
Time elapsed (s): 1.6596629619598389
Agent stdevs: 0.30802354
--------------------------------------------------------------------------------

Step 657
++++++++ Policy training ++++++++++
Current mean reward: 1362.869201 | mean episode length: 374.600000
val_loss=33.89502
val_loss=32.27361
val_loss=19.52281
val_loss=28.21237
val_loss=21.00796
val_loss=10.52205
val_loss=23.58923
val_loss=15.89929
val_loss=20.41232
val_loss=31.17916
adv_loss= 1.54566
adv_loss=13.96803
adv_loss= 1.22478
adv_loss= 0.94046
adv_loss= 3.55391
adv_loss= 0.75902
adv_loss= 1.11284
adv_loss= 2.00637
adv_loss= 4.26635
adv_loss= 2.70867
surrogate= 0.03622, entropy= 0.69168, loss= 0.03622
surrogate=-0.00805, entropy= 0.69288, loss=-0.00805
surrogate=-0.01176, entropy= 0.69258, loss=-0.01176
surrogate=-0.01059, entropy= 0.69321, loss=-0.01059
surrogate=-0.01274, entropy= 0.69258, loss=-0.01274
surrogate=-0.00546, entropy= 0.69249, loss=-0.00546
surrogate=-0.02101, entropy= 0.69245, loss=-0.02101
surrogate=-0.00863, entropy= 0.69244, loss=-0.00863
surrogate=-0.00944, entropy= 0.69182, loss=-0.00944
surrogate=-0.02684, entropy= 0.69224, loss=-0.02684
std_min= 0.24927, std_max= 0.35847, std_mean= 0.30818
val lr: [8.145491803278687e-05], policy lr: [9.774590163934423e-05]
Policy Loss: -0.026836, | Entropy Bonus: -0, | Value Loss: 31.179, | Advantage Loss: 2.7087
Time elapsed (s): 1.6835696697235107
Agent stdevs: 0.30818322
--------------------------------------------------------------------------------

Step 658
++++++++ Policy training ++++++++++
Current mean reward: 1466.299778 | mean episode length: 396.600000
val_loss=22.56336
val_loss=16.19277
val_loss=14.23335
val_loss=11.43724
val_loss=14.17688
val_loss=19.12992
val_loss=11.77818
val_loss=14.23275
val_loss= 4.33905
val_loss=10.44567
adv_loss= 1.74601
adv_loss= 2.35614
adv_loss= 1.37231
adv_loss= 2.17053
adv_loss= 1.97909
adv_loss= 2.17772
adv_loss= 1.89481
adv_loss= 1.41525
adv_loss= 0.87372
adv_loss= 4.27215
surrogate=-0.00039, entropy= 0.69153, loss=-0.00039
surrogate= 0.00468, entropy= 0.69036, loss= 0.00468
surrogate=-0.02650, entropy= 0.68903, loss=-0.02650
surrogate= 0.00757, entropy= 0.68726, loss= 0.00757
surrogate=-0.04011, entropy= 0.68626, loss=-0.04011
surrogate=-0.02914, entropy= 0.68479, loss=-0.02914
surrogate=-0.02701, entropy= 0.68328, loss=-0.02701
surrogate=-0.04097, entropy= 0.68116, loss=-0.04097
surrogate=-0.02689, entropy= 0.68054, loss=-0.02689
surrogate=-0.02949, entropy= 0.67860, loss=-0.02949
std_min= 0.24931, std_max= 0.35615, std_mean= 0.30665
val lr: [8.119877049180327e-05], policy lr: [9.743852459016391e-05]
Policy Loss: -0.029488, | Entropy Bonus: -0, | Value Loss: 10.446, | Advantage Loss: 4.2722
Time elapsed (s): 1.7012383937835693
Agent stdevs: 0.30664748
--------------------------------------------------------------------------------

Step 659
++++++++ Policy training ++++++++++
Current mean reward: 1370.596818 | mean episode length: 375.400000
val_loss=13.99913
val_loss=11.02055
val_loss=12.24302
val_loss=23.20584
val_loss=25.38299
val_loss= 6.30569
val_loss= 8.93247
val_loss= 9.39577
val_loss=12.24120
val_loss=30.40762
adv_loss= 1.57470
adv_loss= 1.23195
adv_loss= 1.52618
adv_loss= 3.42398
adv_loss= 0.97832
adv_loss= 2.62536
adv_loss= 1.77069
adv_loss= 5.25143
adv_loss= 2.39861
adv_loss= 0.77910
surrogate=-0.00653, entropy= 0.67657, loss=-0.00653
surrogate= 0.00436, entropy= 0.67675, loss= 0.00436
surrogate= 0.02887, entropy= 0.67614, loss= 0.02887
surrogate=-0.02250, entropy= 0.67462, loss=-0.02250
surrogate= 0.00075, entropy= 0.67460, loss= 0.00075
surrogate=-0.03177, entropy= 0.67430, loss=-0.03177
surrogate=-0.00493, entropy= 0.67377, loss=-0.00493
surrogate=-0.01865, entropy= 0.67426, loss=-0.01865
surrogate=-0.04233, entropy= 0.67378, loss=-0.04233
surrogate=-0.01573, entropy= 0.67289, loss=-0.01573
std_min= 0.24898, std_max= 0.35535, std_mean= 0.30605
val lr: [8.094262295081967e-05], policy lr: [9.713114754098358e-05]
Policy Loss: -0.015728, | Entropy Bonus: -0, | Value Loss: 30.408, | Advantage Loss: 0.7791
Time elapsed (s): 1.6791250705718994
Agent stdevs: 0.30605182
--------------------------------------------------------------------------------

Step 660
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1506.8
++++++++ Policy training ++++++++++
Current mean reward: 1770.337935 | mean episode length: 495.500000
val_loss=539.06012
val_loss=1586.60840
val_loss=51.75256
val_loss=90.92908
val_loss=1026.63086
val_loss=327.74457
val_loss=113.59360
val_loss=1211.74426
val_loss=55.26869
val_loss=219.61475
adv_loss= 1.90576
adv_loss= 4.37818
adv_loss= 7.23161
adv_loss= 3.02079
adv_loss=16.19094
adv_loss= 4.78875
adv_loss= 2.25205
adv_loss= 4.35063
adv_loss= 2.51736
adv_loss= 4.09412
surrogate= 0.01165, entropy= 0.67184, loss= 0.01165
surrogate= 0.00724, entropy= 0.67184, loss= 0.00724
surrogate= 0.00916, entropy= 0.67233, loss= 0.00916
surrogate=-0.02139, entropy= 0.67252, loss=-0.02139
surrogate=-0.02191, entropy= 0.67046, loss=-0.02191
surrogate=-0.03654, entropy= 0.67002, loss=-0.03654
surrogate= 0.00605, entropy= 0.66888, loss= 0.00605
surrogate=-0.03218, entropy= 0.66896, loss=-0.03218
surrogate=-0.02782, entropy= 0.66872, loss=-0.02782
surrogate=-0.00964, entropy= 0.66905, loss=-0.00964
std_min= 0.24951, std_max= 0.35408, std_mean= 0.30556
val lr: [8.068647540983606e-05], policy lr: [9.682377049180327e-05]
Policy Loss: -0.0096356, | Entropy Bonus: -0, | Value Loss: 219.61, | Advantage Loss: 4.0941
Time elapsed (s): 1.6830060482025146
Agent stdevs: 0.30555615
--------------------------------------------------------------------------------

Step 661
++++++++ Policy training ++++++++++
Current mean reward: 1745.531601 | mean episode length: 474.750000
val_loss=16.54836
val_loss=15.47802
val_loss=10.36996
val_loss=12.29673
val_loss=10.30144
val_loss= 9.69638
val_loss=12.22527
val_loss=10.22622
val_loss= 7.63466
val_loss= 6.13678
adv_loss= 2.14380
adv_loss= 1.42267
adv_loss= 1.73059
adv_loss= 1.41544
adv_loss= 2.43814
adv_loss= 0.78041
adv_loss= 1.66428
adv_loss= 1.60145
adv_loss= 1.08089
adv_loss= 1.77538
surrogate=-0.00246, entropy= 0.66915, loss=-0.00246
surrogate= 0.02468, entropy= 0.66822, loss= 0.02468
surrogate=-0.00141, entropy= 0.66708, loss=-0.00141
surrogate=-0.01243, entropy= 0.66621, loss=-0.01243
surrogate= 0.00578, entropy= 0.66479, loss= 0.00578
surrogate=-0.02635, entropy= 0.66408, loss=-0.02635
surrogate=-0.01427, entropy= 0.66310, loss=-0.01427
surrogate=-0.01497, entropy= 0.66166, loss=-0.01497
surrogate= 0.00031, entropy= 0.66049, loss= 0.00031
surrogate=-0.00818, entropy= 0.65893, loss=-0.00818
std_min= 0.24896, std_max= 0.35354, std_mean= 0.30453
val lr: [8.043032786885246e-05], policy lr: [9.651639344262295e-05]
Policy Loss: -0.0081799, | Entropy Bonus: -0, | Value Loss: 6.1368, | Advantage Loss: 1.7754
Time elapsed (s): 1.690674066543579
Agent stdevs: 0.30452713
--------------------------------------------------------------------------------

Step 662
++++++++ Policy training ++++++++++
Current mean reward: 2356.974466 | mean episode length: 660.333333
val_loss=62.03250
val_loss=53.80113
val_loss=34.30851
val_loss=31.30734
val_loss=724.46814
val_loss=290.17596
val_loss=34.45989
val_loss=539.27502
val_loss=36.52619
val_loss=67.88033
adv_loss= 1.66710
adv_loss= 2.55861
adv_loss= 6.07835
adv_loss= 1.54242
adv_loss= 4.90263
adv_loss= 2.97021
adv_loss= 2.83967
adv_loss= 1.49192
adv_loss= 1.73784
adv_loss= 5.16387
surrogate=-0.00206, entropy= 0.65746, loss=-0.00206
surrogate=-0.00941, entropy= 0.65740, loss=-0.00941
surrogate=-0.01709, entropy= 0.65680, loss=-0.01709
surrogate= 0.00243, entropy= 0.65626, loss= 0.00243
surrogate=-0.02188, entropy= 0.65602, loss=-0.02188
surrogate=-0.01759, entropy= 0.65626, loss=-0.01759
surrogate=-0.00990, entropy= 0.65552, loss=-0.00990
surrogate=-0.02636, entropy= 0.65522, loss=-0.02636
surrogate=-0.00440, entropy= 0.65428, loss=-0.00440
surrogate=-0.01724, entropy= 0.65359, loss=-0.01724
std_min= 0.24791, std_max= 0.35371, std_mean= 0.30407
val lr: [8.017418032786886e-05], policy lr: [9.620901639344262e-05]
Policy Loss: -0.017235, | Entropy Bonus: -0, | Value Loss: 67.88, | Advantage Loss: 5.1639
Time elapsed (s): 1.6688933372497559
Agent stdevs: 0.3040721
--------------------------------------------------------------------------------

Step 663
++++++++ Policy training ++++++++++
Current mean reward: 1412.567314 | mean episode length: 383.666667
val_loss=16.80495
val_loss=13.14382
val_loss= 7.55374
val_loss= 8.31213
val_loss= 8.98313
val_loss= 8.26460
val_loss= 5.79699
val_loss=11.92975
val_loss= 6.87201
val_loss= 8.64778
adv_loss= 3.25540
adv_loss= 0.61356
adv_loss= 0.93104
adv_loss= 2.22463
adv_loss= 1.83545
adv_loss= 0.96718
adv_loss= 0.92546
adv_loss= 0.89350
adv_loss= 1.42122
adv_loss= 1.24642
surrogate= 0.01847, entropy= 0.65289, loss= 0.01847
surrogate= 0.00191, entropy= 0.65344, loss= 0.00191
surrogate=-0.01804, entropy= 0.65354, loss=-0.01804
surrogate=-0.00695, entropy= 0.65431, loss=-0.00695
surrogate= 0.00979, entropy= 0.65357, loss= 0.00979
surrogate=-0.00913, entropy= 0.65466, loss=-0.00913
surrogate= 0.00465, entropy= 0.65475, loss= 0.00465
surrogate=-0.00819, entropy= 0.65489, loss=-0.00819
surrogate=-0.02026, entropy= 0.65444, loss=-0.02026
surrogate=-0.01216, entropy= 0.65424, loss=-0.01216
std_min= 0.24734, std_max= 0.35360, std_mean= 0.30418
val lr: [7.991803278688526e-05], policy lr: [9.59016393442623e-05]
Policy Loss: -0.012156, | Entropy Bonus: -0, | Value Loss: 8.6478, | Advantage Loss: 1.2464
Time elapsed (s): 1.6625983715057373
Agent stdevs: 0.3041842
--------------------------------------------------------------------------------

Step 664
++++++++ Policy training ++++++++++
Current mean reward: 2252.909669 | mean episode length: 609.666667
val_loss= 7.86326
val_loss= 5.13874
val_loss= 6.59484
val_loss= 7.20352
val_loss= 5.78139
val_loss= 6.03453
val_loss= 9.50705
val_loss= 3.88810
val_loss= 7.65459
val_loss= 6.14403
adv_loss= 2.03951
adv_loss= 0.93591
adv_loss= 0.99922
adv_loss= 1.01253
adv_loss= 1.48872
adv_loss= 1.67699
adv_loss= 0.76807
adv_loss= 1.02244
adv_loss= 1.14415
adv_loss= 0.90174
surrogate= 0.03187, entropy= 0.65386, loss= 0.03187
surrogate=-0.02583, entropy= 0.65128, loss=-0.02583
surrogate=-0.01607, entropy= 0.65117, loss=-0.01607
surrogate= 0.00994, entropy= 0.65042, loss= 0.00994
surrogate=-0.01896, entropy= 0.64899, loss=-0.01896
surrogate= 0.00081, entropy= 0.64739, loss= 0.00081
surrogate= 0.00496, entropy= 0.64632, loss= 0.00496
surrogate=-0.02796, entropy= 0.64627, loss=-0.02796
surrogate=-0.01905, entropy= 0.64567, loss=-0.01905
surrogate= 0.00377, entropy= 0.64482, loss= 0.00377
std_min= 0.24710, std_max= 0.35261, std_mean= 0.30319
val lr: [7.966188524590164e-05], policy lr: [9.559426229508197e-05]
Policy Loss: 0.0037666, | Entropy Bonus: -0, | Value Loss: 6.144, | Advantage Loss: 0.90174
Time elapsed (s): 1.6747796535491943
Agent stdevs: 0.30318847
--------------------------------------------------------------------------------

Step 665
++++++++ Policy training ++++++++++
Current mean reward: 1799.903617 | mean episode length: 486.500000
val_loss=10.26863
val_loss= 4.71350
val_loss= 7.03027
val_loss= 7.55814
val_loss= 5.81147
val_loss= 3.16207
val_loss= 4.48137
val_loss= 6.02161
val_loss= 4.20192
val_loss= 6.29419
adv_loss= 0.91835
adv_loss= 0.69437
adv_loss= 1.08546
adv_loss= 2.48778
adv_loss= 1.16612
adv_loss= 1.02279
adv_loss= 0.92086
adv_loss= 1.13651
adv_loss= 0.95986
adv_loss= 1.91824
surrogate=-0.01796, entropy= 0.64366, loss=-0.01796
surrogate= 0.01180, entropy= 0.64367, loss= 0.01180
surrogate= 0.02712, entropy= 0.64255, loss= 0.02712
surrogate=-0.00568, entropy= 0.64265, loss=-0.00568
surrogate=-0.01552, entropy= 0.64182, loss=-0.01552
surrogate=-0.00841, entropy= 0.64125, loss=-0.00841
surrogate=-0.01499, entropy= 0.64072, loss=-0.01499
surrogate=-0.01406, entropy= 0.63936, loss=-0.01406
surrogate=-0.01308, entropy= 0.63850, loss=-0.01308
surrogate= 0.01358, entropy= 0.63784, loss= 0.01358
std_min= 0.24581, std_max= 0.35189, std_mean= 0.30255
val lr: [7.940573770491804e-05], policy lr: [9.528688524590164e-05]
Policy Loss: 0.013577, | Entropy Bonus: -0, | Value Loss: 6.2942, | Advantage Loss: 1.9182
Time elapsed (s): 1.6649284362792969
Agent stdevs: 0.3025508
--------------------------------------------------------------------------------

Step 666
++++++++ Policy training ++++++++++
Current mean reward: 2309.867299 | mean episode length: 633.666667
val_loss= 6.06420
val_loss= 9.26598
val_loss=10.44687
val_loss=12.93627
val_loss= 9.66417
val_loss=13.87886
val_loss= 7.89514
val_loss=11.22641
val_loss= 8.67074
val_loss= 6.24365
adv_loss= 0.98662
adv_loss= 0.58799
adv_loss= 4.21167
adv_loss= 0.95447
adv_loss= 1.04174
adv_loss= 1.26215
adv_loss= 1.20015
adv_loss= 0.52608
adv_loss= 0.59326
adv_loss= 1.96333
surrogate=-0.00047, entropy= 0.63889, loss=-0.00047
surrogate=-0.00115, entropy= 0.63948, loss=-0.00115
surrogate=-0.00829, entropy= 0.64016, loss=-0.00829
surrogate= 0.01415, entropy= 0.64041, loss= 0.01415
surrogate=-0.00677, entropy= 0.64147, loss=-0.00677
surrogate=-0.01556, entropy= 0.64251, loss=-0.01556
surrogate=-0.03107, entropy= 0.64357, loss=-0.03107
surrogate=-0.04278, entropy= 0.64474, loss=-0.04278
surrogate=-0.02780, entropy= 0.64559, loss=-0.02780
surrogate= 0.00738, entropy= 0.64599, loss= 0.00738
std_min= 0.24474, std_max= 0.35537, std_mean= 0.30364
val lr: [7.914959016393442e-05], policy lr: [9.497950819672129e-05]
Policy Loss: 0.0073799, | Entropy Bonus: -0, | Value Loss: 6.2437, | Advantage Loss: 1.9633
Time elapsed (s): 1.6674230098724365
Agent stdevs: 0.30363768
--------------------------------------------------------------------------------

Step 667
++++++++ Policy training ++++++++++
Current mean reward: 1642.812235 | mean episode length: 447.000000
val_loss=22.08750
val_loss=14.21125
val_loss= 5.05379
val_loss=19.14843
val_loss=10.87820
val_loss= 6.77165
val_loss= 6.82766
val_loss=21.63976
val_loss= 8.66271
val_loss=21.94647
adv_loss= 1.33936
adv_loss= 0.99299
adv_loss= 0.51809
adv_loss= 0.85827
adv_loss= 0.31000
adv_loss= 0.66144
adv_loss= 1.63734
adv_loss= 0.55592
adv_loss= 1.04110
adv_loss= 1.44970
surrogate= 0.00262, entropy= 0.64642, loss= 0.00262
surrogate=-0.00234, entropy= 0.64580, loss=-0.00234
surrogate=-0.00703, entropy= 0.64497, loss=-0.00703
surrogate=-0.02882, entropy= 0.64336, loss=-0.02882
surrogate=-0.01660, entropy= 0.64231, loss=-0.01660
surrogate=-0.00573, entropy= 0.64215, loss=-0.00573
surrogate=-0.01359, entropy= 0.64153, loss=-0.01359
surrogate=-0.03819, entropy= 0.64088, loss=-0.03819
surrogate=-0.03570, entropy= 0.63993, loss=-0.03570
surrogate=-0.01452, entropy= 0.63886, loss=-0.01452
std_min= 0.24318, std_max= 0.35655, std_mean= 0.30308
val lr: [7.889344262295082e-05], policy lr: [9.467213114754097e-05]
Policy Loss: -0.01452, | Entropy Bonus: -0, | Value Loss: 21.946, | Advantage Loss: 1.4497
Time elapsed (s): 1.644552230834961
Agent stdevs: 0.30308005
--------------------------------------------------------------------------------

Step 668
++++++++ Policy training ++++++++++
Current mean reward: 1952.697915 | mean episode length: 533.000000
val_loss= 5.10864
val_loss= 6.69054
val_loss= 4.66113
val_loss= 5.86455
val_loss= 6.44486
val_loss= 4.14535
val_loss= 6.01777
val_loss=10.53706
val_loss= 9.17858
val_loss= 5.00114
adv_loss= 1.09922
adv_loss= 1.31863
adv_loss= 1.11116
adv_loss= 0.96280
adv_loss= 1.02983
adv_loss= 1.98229
adv_loss= 0.84612
adv_loss= 1.03155
adv_loss= 1.37177
adv_loss= 0.58161
surrogate= 0.02440, entropy= 0.63762, loss= 0.02440
surrogate=-0.01156, entropy= 0.63706, loss=-0.01156
surrogate=-0.02556, entropy= 0.63601, loss=-0.02556
surrogate=-0.01160, entropy= 0.63517, loss=-0.01160
surrogate= 0.00240, entropy= 0.63367, loss= 0.00240
surrogate=-0.02150, entropy= 0.63203, loss=-0.02150
surrogate=-0.02932, entropy= 0.63161, loss=-0.02932
surrogate=-0.03424, entropy= 0.63073, loss=-0.03424
surrogate=-0.02683, entropy= 0.62956, loss=-0.02683
surrogate=-0.04646, entropy= 0.62940, loss=-0.04646
std_min= 0.24291, std_max= 0.35549, std_mean= 0.30208
val lr: [7.86372950819672e-05], policy lr: [9.436475409836064e-05]
Policy Loss: -0.046462, | Entropy Bonus: -0, | Value Loss: 5.0011, | Advantage Loss: 0.58161
Time elapsed (s): 1.6589908599853516
Agent stdevs: 0.3020825
--------------------------------------------------------------------------------

Step 669
++++++++ Policy training ++++++++++
Current mean reward: 2296.277071 | mean episode length: 629.000000
val_loss= 7.08586
val_loss= 6.71604
val_loss= 5.89023
val_loss= 6.94818
val_loss= 5.53567
val_loss= 4.60781
val_loss= 4.48411
val_loss= 6.90454
val_loss= 6.69651
val_loss= 5.66035
adv_loss= 0.59385
adv_loss= 0.91436
adv_loss= 1.57079
adv_loss= 0.49472
adv_loss= 1.14249
adv_loss= 1.12055
adv_loss= 0.75024
adv_loss= 0.85164
adv_loss= 1.27724
adv_loss= 0.97814
surrogate= 0.01100, entropy= 0.62927, loss= 0.01100
surrogate= 0.00832, entropy= 0.62977, loss= 0.00832
surrogate=-0.02622, entropy= 0.63005, loss=-0.02622
surrogate=-0.02252, entropy= 0.62980, loss=-0.02252
surrogate=-0.00847, entropy= 0.62988, loss=-0.00847
surrogate=-0.04508, entropy= 0.62984, loss=-0.04508
surrogate=-0.01194, entropy= 0.63063, loss=-0.01194
surrogate= 0.02152, entropy= 0.63145, loss= 0.02152
surrogate=-0.01033, entropy= 0.63208, loss=-0.01033
surrogate=-0.02912, entropy= 0.63244, loss=-0.02912
std_min= 0.24156, std_max= 0.35762, std_mean= 0.30262
val lr: [7.83811475409836e-05], policy lr: [9.405737704918031e-05]
Policy Loss: -0.029125, | Entropy Bonus: -0, | Value Loss: 5.6603, | Advantage Loss: 0.97814
Time elapsed (s): 1.6681413650512695
Agent stdevs: 0.30261865
--------------------------------------------------------------------------------

Step 670
++++++++ Policy training ++++++++++
Current mean reward: 2197.077815 | mean episode length: 623.000000
val_loss=380.35657
val_loss=716.00671
val_loss=780.81805
val_loss=1315.33252
val_loss=2229.54126
val_loss=875.94574
val_loss=849.16980
val_loss=172.11874
val_loss=189.98724
val_loss=271.71600
adv_loss= 3.06610
adv_loss= 1.70832
adv_loss= 2.48037
adv_loss= 3.63378
adv_loss= 3.67803
adv_loss= 1.12853
adv_loss= 1.44186
adv_loss= 1.79453
adv_loss= 1.81330
adv_loss= 2.00296
surrogate=-0.00530, entropy= 0.63290, loss=-0.00530
surrogate=-0.02520, entropy= 0.63382, loss=-0.02520
surrogate=-0.01036, entropy= 0.63406, loss=-0.01036
surrogate=-0.02646, entropy= 0.63506, loss=-0.02646
surrogate=-0.00681, entropy= 0.63571, loss=-0.00681
surrogate=-0.03532, entropy= 0.63648, loss=-0.03532
surrogate=-0.02234, entropy= 0.63760, loss=-0.02234
surrogate=-0.01954, entropy= 0.63860, loss=-0.01954
surrogate=-0.03532, entropy= 0.63915, loss=-0.03532
surrogate=-0.02543, entropy= 0.63960, loss=-0.02543
std_min= 0.24313, std_max= 0.35861, std_mean= 0.30326
val lr: [7.8125e-05], policy lr: [9.374999999999999e-05]
Policy Loss: -0.025434, | Entropy Bonus: -0, | Value Loss: 271.72, | Advantage Loss: 2.003
Time elapsed (s): 1.6837375164031982
Agent stdevs: 0.30326235
--------------------------------------------------------------------------------

Step 671
++++++++ Policy training ++++++++++
Current mean reward: 2017.709229 | mean episode length: 567.333333
val_loss=26.97078
val_loss=48.02191
val_loss=2388.60889
val_loss=71.21221
val_loss=66.23547
val_loss=411.95706
val_loss=35.58451
val_loss=40.45861
val_loss=286.75479
val_loss=358.63794
adv_loss= 6.71066
adv_loss= 6.46377
adv_loss= 9.58213
adv_loss= 6.81660
adv_loss= 4.99691
adv_loss= 5.41098
adv_loss= 2.88683
adv_loss=21.59480
adv_loss= 3.76243
adv_loss= 4.08240
surrogate=-0.01140, entropy= 0.64150, loss=-0.01140
surrogate= 0.00208, entropy= 0.64266, loss= 0.00208
surrogate=-0.03279, entropy= 0.64300, loss=-0.03279
surrogate=-0.03409, entropy= 0.64424, loss=-0.03409
surrogate= 0.01687, entropy= 0.64444, loss= 0.01687
surrogate=-0.01387, entropy= 0.64513, loss=-0.01387
surrogate= 0.00804, entropy= 0.64614, loss= 0.00804
surrogate=-0.01232, entropy= 0.64647, loss=-0.01232
surrogate=-0.00583, entropy= 0.64668, loss=-0.00583
surrogate= 0.02087, entropy= 0.64698, loss= 0.02087
std_min= 0.24396, std_max= 0.35983, std_mean= 0.30401
val lr: [7.78688524590164e-05], policy lr: [9.344262295081966e-05]
Policy Loss: 0.020874, | Entropy Bonus: -0, | Value Loss: 358.64, | Advantage Loss: 4.0824
Time elapsed (s): 1.6616859436035156
Agent stdevs: 0.30400822
--------------------------------------------------------------------------------

Step 672
++++++++ Policy training ++++++++++
Current mean reward: 1977.810013 | mean episode length: 544.666667
val_loss=36.95704
val_loss=30.38874
val_loss=22.56363
val_loss=23.60777
val_loss=22.01185
val_loss=17.29218
val_loss=17.30204
val_loss=17.99846
val_loss=22.74228
val_loss=20.36744
adv_loss= 5.65498
adv_loss= 5.31400
adv_loss= 3.92540
adv_loss= 1.83256
adv_loss= 3.44545
adv_loss= 1.30909
adv_loss= 3.72851
adv_loss= 2.32614
adv_loss= 4.86515
adv_loss= 5.61472
surrogate=-0.00154, entropy= 0.64594, loss=-0.00154
surrogate=-0.01300, entropy= 0.64419, loss=-0.01300
surrogate=-0.00851, entropy= 0.64175, loss=-0.00851
surrogate= 0.03241, entropy= 0.63918, loss= 0.03241
surrogate=-0.00504, entropy= 0.63675, loss=-0.00504
surrogate= 0.02275, entropy= 0.63418, loss= 0.02275
surrogate=-0.02546, entropy= 0.63263, loss=-0.02546
surrogate=-0.00047, entropy= 0.63066, loss=-0.00047
surrogate=-0.01985, entropy= 0.62856, loss=-0.01985
surrogate=-0.01650, entropy= 0.62725, loss=-0.01650
std_min= 0.24179, std_max= 0.35747, std_mean= 0.30205
val lr: [7.76127049180328e-05], policy lr: [9.313524590163934e-05]
Policy Loss: -0.016504, | Entropy Bonus: -0, | Value Loss: 20.367, | Advantage Loss: 5.6147
Time elapsed (s): 1.648651361465454
Agent stdevs: 0.30205318
--------------------------------------------------------------------------------

Step 673
++++++++ Policy training ++++++++++
Current mean reward: 1788.012124 | mean episode length: 487.750000
val_loss=38.49675
val_loss=31.26703
val_loss=29.40477
val_loss=42.03309
val_loss=18.48369
val_loss=13.12424
val_loss=15.08687
val_loss=19.21228
val_loss=20.10801
val_loss=21.15677
adv_loss= 4.09569
adv_loss= 1.11963
adv_loss= 2.91595
adv_loss= 2.41027
adv_loss= 6.94184
adv_loss= 6.86108
adv_loss= 1.13842
adv_loss= 2.79843
adv_loss= 5.87480
adv_loss= 4.13743
surrogate= 0.00946, entropy= 0.62644, loss= 0.00946
surrogate=-0.00814, entropy= 0.62568, loss=-0.00814
surrogate=-0.02187, entropy= 0.62611, loss=-0.02187
surrogate=-0.01403, entropy= 0.62567, loss=-0.01403
surrogate=-0.05293, entropy= 0.62568, loss=-0.05293
surrogate= 0.03436, entropy= 0.62542, loss= 0.03436
surrogate=-0.00038, entropy= 0.62652, loss=-0.00038
surrogate=-0.03296, entropy= 0.62711, loss=-0.03296
surrogate= 0.00455, entropy= 0.62643, loss= 0.00455
surrogate=-0.01202, entropy= 0.62616, loss=-0.01202
std_min= 0.24190, std_max= 0.35903, std_mean= 0.30202
val lr: [7.735655737704919e-05], policy lr: [9.282786885245903e-05]
Policy Loss: -0.012022, | Entropy Bonus: -0, | Value Loss: 21.157, | Advantage Loss: 4.1374
Time elapsed (s): 1.6512207984924316
Agent stdevs: 0.3020163
--------------------------------------------------------------------------------

Step 674
++++++++ Policy training ++++++++++
Current mean reward: 1839.672798 | mean episode length: 511.750000
val_loss=25.26342
val_loss=249.85428
val_loss=980.71472
val_loss=50.69233
val_loss=126.60098
val_loss=1137.21777
val_loss=119.86259
val_loss=148.91618
val_loss=102.92050
val_loss=268.18570
adv_loss= 1.89737
adv_loss=1012.24292
adv_loss=1008.40369
adv_loss= 1.47039
adv_loss= 2.39253
adv_loss= 1.35370
adv_loss= 0.86626
adv_loss= 1.23241
adv_loss= 1.70533
adv_loss= 1.86659
surrogate= 0.01383, entropy= 0.62466, loss= 0.01383
surrogate=-0.03025, entropy= 0.62361, loss=-0.03025
surrogate=-0.01077, entropy= 0.62218, loss=-0.01077
surrogate= 0.00006, entropy= 0.62189, loss= 0.00006
surrogate=-0.00587, entropy= 0.62146, loss=-0.00587
surrogate= 0.01873, entropy= 0.62045, loss= 0.01873
surrogate=-0.01422, entropy= 0.61936, loss=-0.01422
surrogate=-0.01109, entropy= 0.61875, loss=-0.01109
surrogate=-0.02053, entropy= 0.61754, loss=-0.02053
surrogate=-0.01901, entropy= 0.61675, loss=-0.01901
std_min= 0.24133, std_max= 0.35756, std_mean= 0.30103
val lr: [7.710040983606559e-05], policy lr: [9.25204918032787e-05]
Policy Loss: -0.01901, | Entropy Bonus: -0, | Value Loss: 268.19, | Advantage Loss: 1.8666
Time elapsed (s): 1.656397819519043
Agent stdevs: 0.30103338
--------------------------------------------------------------------------------

Step 675
++++++++ Policy training ++++++++++
Current mean reward: 1865.190318 | mean episode length: 521.000000
val_loss=29.38499
val_loss=178.14264
val_loss=43.55114
val_loss=19.84556
val_loss=29.88645
val_loss=24.63156
val_loss=60.09383
val_loss=611.02051
val_loss=21.21903
val_loss=29.37992
adv_loss= 1.02380
adv_loss=812.03680
adv_loss= 2.02450
adv_loss= 2.11706
adv_loss= 1.28910
adv_loss= 1.37931
adv_loss= 5.70166
adv_loss= 2.10805
adv_loss= 3.96919
adv_loss= 5.57394
surrogate= 0.01974, entropy= 0.61544, loss= 0.01974
surrogate= 0.01079, entropy= 0.61416, loss= 0.01079
surrogate= 0.02178, entropy= 0.61339, loss= 0.02178
surrogate=-0.02580, entropy= 0.61167, loss=-0.02580
surrogate=-0.01100, entropy= 0.61035, loss=-0.01100
surrogate=-0.01804, entropy= 0.60988, loss=-0.01804
surrogate=-0.00417, entropy= 0.60970, loss=-0.00417
surrogate=-0.02756, entropy= 0.60888, loss=-0.02756
surrogate=-0.00488, entropy= 0.60810, loss=-0.00488
surrogate=-0.03021, entropy= 0.60788, loss=-0.03021
std_min= 0.23892, std_max= 0.35695, std_mean= 0.30032
val lr: [7.684426229508196e-05], policy lr: [9.221311475409833e-05]
Policy Loss: -0.030214, | Entropy Bonus: -0, | Value Loss: 29.38, | Advantage Loss: 5.5739
Time elapsed (s): 1.6536622047424316
Agent stdevs: 0.30031705
--------------------------------------------------------------------------------

Step 676
++++++++ Policy training ++++++++++
Current mean reward: 3356.623837 | mean episode length: 926.500000
val_loss=40.66183
val_loss=20.09000
val_loss=33.00603
val_loss=15.45366
val_loss=19.08947
val_loss=16.45322
val_loss=14.22351
val_loss=14.90352
val_loss=21.18439
val_loss=16.23223
adv_loss= 1.19067
adv_loss= 1.87832
adv_loss= 1.39733
adv_loss= 0.82150
adv_loss= 2.19900
adv_loss= 4.79268
adv_loss= 1.16648
adv_loss= 1.14953
adv_loss= 2.33556
adv_loss= 2.95194
surrogate= 0.01396, entropy= 0.60799, loss= 0.01396
surrogate=-0.01966, entropy= 0.60816, loss=-0.01966
surrogate= 0.00225, entropy= 0.60827, loss= 0.00225
surrogate=-0.01483, entropy= 0.60796, loss=-0.01483
surrogate=-0.01194, entropy= 0.60907, loss=-0.01194
surrogate=-0.00988, entropy= 0.60873, loss=-0.00988
surrogate=-0.01883, entropy= 0.60918, loss=-0.01883
surrogate=-0.04805, entropy= 0.60981, loss=-0.04805
surrogate=-0.01057, entropy= 0.60992, loss=-0.01057
surrogate=-0.01729, entropy= 0.60976, loss=-0.01729
std_min= 0.23866, std_max= 0.35788, std_mean= 0.30058
val lr: [7.658811475409836e-05], policy lr: [9.190573770491801e-05]
Policy Loss: -0.017292, | Entropy Bonus: -0, | Value Loss: 16.232, | Advantage Loss: 2.9519
Time elapsed (s): 1.6963021755218506
Agent stdevs: 0.30058128
--------------------------------------------------------------------------------

Step 677
++++++++ Policy training ++++++++++
Current mean reward: 2086.459250 | mean episode length: 590.000000
val_loss=978.33276
val_loss=865.56439
val_loss=92.50408
val_loss=1708.50708
val_loss=392.95432
val_loss=717.26166
val_loss=820.33038
val_loss=1082.68152
val_loss=79.32500
val_loss=67.38322
adv_loss= 1.83499
adv_loss= 4.47787
adv_loss= 3.47140
adv_loss= 3.20981
adv_loss= 3.59633
adv_loss=11.86388
adv_loss= 3.18286
adv_loss= 1.72119
adv_loss= 2.39919
adv_loss= 4.34095
surrogate=-0.01741, entropy= 0.60955, loss=-0.01741
surrogate=-0.00246, entropy= 0.61084, loss=-0.00246
surrogate=-0.01224, entropy= 0.61029, loss=-0.01224
surrogate=-0.01344, entropy= 0.61002, loss=-0.01344
surrogate= 0.00529, entropy= 0.60985, loss= 0.00529
surrogate= 0.00959, entropy= 0.61008, loss= 0.00959
surrogate= 0.01053, entropy= 0.61034, loss= 0.01053
surrogate=-0.01377, entropy= 0.61070, loss=-0.01377
surrogate= 0.01898, entropy= 0.61169, loss= 0.01898
surrogate=-0.03272, entropy= 0.61195, loss=-0.03272
std_min= 0.23629, std_max= 0.35999, std_mean= 0.30114
val lr: [7.633196721311475e-05], policy lr: [9.15983606557377e-05]
Policy Loss: -0.032716, | Entropy Bonus: -0, | Value Loss: 67.383, | Advantage Loss: 4.3409
Time elapsed (s): 1.6502773761749268
Agent stdevs: 0.30113554
--------------------------------------------------------------------------------

Step 678
++++++++ Policy training ++++++++++
Current mean reward: 1689.641348 | mean episode length: 457.666667
val_loss=18.80626
val_loss=22.73289
val_loss=23.87498
val_loss=16.94078
val_loss=20.76261
val_loss=14.06709
val_loss=16.42877
val_loss=11.40583
val_loss=16.38300
val_loss= 8.31463
adv_loss= 2.80662
adv_loss= 2.50012
adv_loss= 1.33354
adv_loss= 7.27194
adv_loss= 1.80381
adv_loss= 1.81543
adv_loss= 1.94512
adv_loss= 3.12638
adv_loss= 7.27955
adv_loss= 1.37002
surrogate= 0.01847, entropy= 0.61088, loss= 0.01847
surrogate=-0.01824, entropy= 0.60926, loss=-0.01824
surrogate=-0.01776, entropy= 0.60786, loss=-0.01776
surrogate=-0.02868, entropy= 0.60645, loss=-0.02868
surrogate= 0.00286, entropy= 0.60528, loss= 0.00286
surrogate=-0.03259, entropy= 0.60371, loss=-0.03259
surrogate=-0.00793, entropy= 0.60245, loss=-0.00793
surrogate=-0.04115, entropy= 0.60088, loss=-0.04115
surrogate=-0.02797, entropy= 0.59958, loss=-0.02797
surrogate=-0.02939, entropy= 0.59796, loss=-0.02939
std_min= 0.23581, std_max= 0.35831, std_mean= 0.29968
val lr: [7.607581967213115e-05], policy lr: [9.129098360655737e-05]
Policy Loss: -0.029391, | Entropy Bonus: -0, | Value Loss: 8.3146, | Advantage Loss: 1.37
Time elapsed (s): 1.6621451377868652
Agent stdevs: 0.29967698
--------------------------------------------------------------------------------

Step 679
++++++++ Policy training ++++++++++
Current mean reward: 1656.968080 | mean episode length: 450.250000
val_loss=17.13033
val_loss=21.09171
val_loss=21.98090
val_loss=21.46197
val_loss=18.35486
val_loss=19.60493
val_loss=14.51236
val_loss=17.96393
val_loss=14.63562
val_loss=21.34022
adv_loss= 7.43184
adv_loss= 3.42655
adv_loss= 7.88459
adv_loss= 3.94316
adv_loss= 2.36234
adv_loss= 1.70805
adv_loss= 2.11247
adv_loss= 3.15119
adv_loss= 1.55979
adv_loss= 2.42630
surrogate=-0.00137, entropy= 0.59787, loss=-0.00137
surrogate=-0.01472, entropy= 0.59861, loss=-0.01472
surrogate=-0.00145, entropy= 0.59852, loss=-0.00145
surrogate=-0.01554, entropy= 0.59736, loss=-0.01554
surrogate=-0.02576, entropy= 0.59759, loss=-0.02576
surrogate=-0.02809, entropy= 0.59681, loss=-0.02809
surrogate=-0.00439, entropy= 0.59611, loss=-0.00439
surrogate=-0.02705, entropy= 0.59516, loss=-0.02705
surrogate=-0.01603, entropy= 0.59464, loss=-0.01603
surrogate=-0.02392, entropy= 0.59455, loss=-0.02392
std_min= 0.23538, std_max= 0.35700, std_mean= 0.29930
val lr: [7.581967213114755e-05], policy lr: [9.098360655737704e-05]
Policy Loss: -0.023918, | Entropy Bonus: -0, | Value Loss: 21.34, | Advantage Loss: 2.4263
Time elapsed (s): 1.642930269241333
Agent stdevs: 0.2993013
--------------------------------------------------------------------------------

Step 680
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2131
++++++++ Policy training ++++++++++
Current mean reward: 1653.096947 | mean episode length: 484.000000
val_loss=553.53168
val_loss=533.12909
val_loss=234.42949
val_loss=157.28799
val_loss=796.11896
val_loss=569.28864
val_loss=856.52679
val_loss=2349.47388
val_loss=1487.31104
val_loss=299.40933
adv_loss= 7.87726
adv_loss= 9.71382
adv_loss=17.49990
adv_loss= 5.88957
adv_loss= 5.53210
adv_loss=22.43814
adv_loss= 7.62511
adv_loss= 5.87698
adv_loss= 3.56696
adv_loss= 3.35092
surrogate= 0.00610, entropy= 0.59505, loss= 0.00610
surrogate=-0.02054, entropy= 0.59536, loss=-0.02054
surrogate= 0.00939, entropy= 0.59548, loss= 0.00939
surrogate=-0.00238, entropy= 0.59616, loss=-0.00238
surrogate=-0.03505, entropy= 0.59630, loss=-0.03505
surrogate=-0.01756, entropy= 0.59687, loss=-0.01756
surrogate=-0.03625, entropy= 0.59722, loss=-0.03625
surrogate=-0.00901, entropy= 0.59788, loss=-0.00901
surrogate=-0.02150, entropy= 0.59818, loss=-0.02150
surrogate=-0.03162, entropy= 0.59779, loss=-0.03162
std_min= 0.23480, std_max= 0.35916, std_mean= 0.29980
val lr: [7.556352459016395e-05], policy lr: [9.067622950819672e-05]
Policy Loss: -0.031619, | Entropy Bonus: -0, | Value Loss: 299.41, | Advantage Loss: 3.3509
Time elapsed (s): 1.6522982120513916
Agent stdevs: 0.29980072
--------------------------------------------------------------------------------

Step 681
++++++++ Policy training ++++++++++
Current mean reward: 2197.133527 | mean episode length: 617.333333
val_loss=2046.24670
val_loss=132.21701
val_loss=428.34314
val_loss=1264.09546
val_loss=114.01307
val_loss=139.24890
val_loss=27.17124
val_loss=28.44447
val_loss=42.17229
val_loss=42.64838
adv_loss= 1.86697
adv_loss= 1.48010
adv_loss= 2.32250
adv_loss= 1.61610
adv_loss= 1.19650
adv_loss= 2.00202
adv_loss= 2.29585
adv_loss= 1.70941
adv_loss= 2.90566
adv_loss= 2.45967
surrogate=-0.02870, entropy= 0.59796, loss=-0.02870
surrogate= 0.01270, entropy= 0.59786, loss= 0.01270
surrogate=-0.01804, entropy= 0.59873, loss=-0.01804
surrogate=-0.00769, entropy= 0.59941, loss=-0.00769
surrogate=-0.02858, entropy= 0.59962, loss=-0.02858
surrogate=-0.01263, entropy= 0.60002, loss=-0.01263
surrogate=-0.02062, entropy= 0.59992, loss=-0.02062
surrogate=-0.00571, entropy= 0.60034, loss=-0.00571
surrogate=-0.01076, entropy= 0.60040, loss=-0.01076
surrogate=-0.02741, entropy= 0.60037, loss=-0.02741
std_min= 0.23582, std_max= 0.35983, std_mean= 0.30000
val lr: [7.530737704918033e-05], policy lr: [9.03688524590164e-05]
Policy Loss: -0.027409, | Entropy Bonus: -0, | Value Loss: 42.648, | Advantage Loss: 2.4597
Time elapsed (s): 1.6496429443359375
Agent stdevs: 0.29999542
--------------------------------------------------------------------------------

Step 682
++++++++ Policy training ++++++++++
Current mean reward: 1703.845808 | mean episode length: 461.750000
val_loss=26.63937
val_loss=22.85463
val_loss=22.29089
val_loss=17.03184
val_loss=27.51633
val_loss=25.24490
val_loss=16.46320
val_loss=13.63130
val_loss=10.34839
val_loss=13.62643
adv_loss= 2.14236
adv_loss= 1.72816
adv_loss= 4.58261
adv_loss= 2.85731
adv_loss= 1.37079
adv_loss= 2.57326
adv_loss= 3.19278
adv_loss= 2.29529
adv_loss= 2.29001
adv_loss= 1.77940
surrogate=-0.00593, entropy= 0.60018, loss=-0.00593
surrogate=-0.00369, entropy= 0.59988, loss=-0.00369
surrogate= 0.01011, entropy= 0.59940, loss= 0.01011
surrogate=-0.00026, entropy= 0.59904, loss=-0.00026
surrogate= 0.01210, entropy= 0.59777, loss= 0.01210
surrogate=-0.04441, entropy= 0.59681, loss=-0.04441
surrogate=-0.01168, entropy= 0.59670, loss=-0.01168
surrogate=-0.02032, entropy= 0.59559, loss=-0.02032
surrogate=-0.04114, entropy= 0.59478, loss=-0.04114
surrogate=-0.04197, entropy= 0.59425, loss=-0.04197
std_min= 0.23625, std_max= 0.35770, std_mean= 0.29922
val lr: [7.505122950819673e-05], policy lr: [9.006147540983607e-05]
Policy Loss: -0.041969, | Entropy Bonus: -0, | Value Loss: 13.626, | Advantage Loss: 1.7794
Time elapsed (s): 1.681316614151001
Agent stdevs: 0.29922315
--------------------------------------------------------------------------------

Step 683
++++++++ Policy training ++++++++++
Current mean reward: 1973.018743 | mean episode length: 551.333333
val_loss=100.19708
val_loss=948.34021
val_loss=92.79899
val_loss=283.10791
val_loss=47.95830
val_loss=123.12659
val_loss=1216.00513
val_loss=159.30486
val_loss=102.96514
val_loss=562.10071
adv_loss= 1.90273
adv_loss= 3.16149
adv_loss= 4.66947
adv_loss= 3.09858
adv_loss= 3.00179
adv_loss= 1.62593
adv_loss= 1.91857
adv_loss= 2.99383
adv_loss= 4.19092
adv_loss= 6.40313
surrogate= 0.00965, entropy= 0.59363, loss= 0.00965
surrogate=-0.00209, entropy= 0.59384, loss=-0.00209
surrogate=-0.00019, entropy= 0.59393, loss=-0.00019
surrogate=-0.03526, entropy= 0.59387, loss=-0.03526
surrogate= 0.00820, entropy= 0.59391, loss= 0.00820
surrogate= 0.00462, entropy= 0.59423, loss= 0.00462
surrogate=-0.01392, entropy= 0.59474, loss=-0.01392
surrogate=-0.04303, entropy= 0.59515, loss=-0.04303
surrogate=-0.00359, entropy= 0.59479, loss=-0.00359
surrogate= 0.03732, entropy= 0.59444, loss= 0.03732
std_min= 0.23558, std_max= 0.35765, std_mean= 0.29931
val lr: [7.479508196721313e-05], policy lr: [8.975409836065574e-05]
Policy Loss: 0.037325, | Entropy Bonus: -0, | Value Loss: 562.1, | Advantage Loss: 6.4031
Time elapsed (s): 1.644862174987793
Agent stdevs: 0.29930988
--------------------------------------------------------------------------------

Step 684
++++++++ Policy training ++++++++++
Current mean reward: 1475.978322 | mean episode length: 405.333333
val_loss=23.66142
val_loss=31.21024
val_loss=16.43549
val_loss=13.36357
val_loss=22.43415
val_loss=14.16320
val_loss=13.46550
val_loss=11.67905
val_loss=19.56471
val_loss=24.18227
adv_loss=17.77780
adv_loss= 2.09087
adv_loss=18.82229
adv_loss= 1.39007
adv_loss= 5.98297
adv_loss= 7.54806
adv_loss= 2.11435
adv_loss= 2.23277
adv_loss=16.47857
adv_loss= 6.55033
surrogate=-0.00897, entropy= 0.59458, loss=-0.00897
surrogate= 0.00837, entropy= 0.59430, loss= 0.00837
surrogate=-0.01745, entropy= 0.59391, loss=-0.01745
surrogate=-0.01676, entropy= 0.59351, loss=-0.01676
surrogate=-0.01710, entropy= 0.59333, loss=-0.01710
surrogate=-0.00543, entropy= 0.59323, loss=-0.00543
surrogate=-0.02747, entropy= 0.59241, loss=-0.02747
surrogate=-0.02613, entropy= 0.59166, loss=-0.02613
surrogate=-0.03232, entropy= 0.59104, loss=-0.03232
surrogate=-0.03349, entropy= 0.59057, loss=-0.03349
std_min= 0.23630, std_max= 0.35593, std_mean= 0.29876
val lr: [7.45389344262295e-05], policy lr: [8.944672131147539e-05]
Policy Loss: -0.033492, | Entropy Bonus: -0, | Value Loss: 24.182, | Advantage Loss: 6.5503
Time elapsed (s): 1.6647090911865234
Agent stdevs: 0.29875872
--------------------------------------------------------------------------------

Step 685
++++++++ Policy training ++++++++++
Current mean reward: 1864.367797 | mean episode length: 506.750000
val_loss=22.20919
val_loss=16.17030
val_loss=11.97588
val_loss= 9.88817
val_loss=16.95492
val_loss=10.74577
val_loss= 7.35676
val_loss=11.38756
val_loss= 7.76759
val_loss=19.11150
adv_loss= 1.16261
adv_loss= 1.33172
adv_loss= 1.30584
adv_loss= 2.45604
adv_loss= 1.75530
adv_loss= 5.50384
adv_loss= 1.55947
adv_loss= 0.86653
adv_loss= 1.21954
adv_loss= 0.90954
surrogate=-0.01137, entropy= 0.59072, loss=-0.01137
surrogate=-0.01743, entropy= 0.58995, loss=-0.01743
surrogate=-0.03305, entropy= 0.58955, loss=-0.03305
surrogate=-0.00746, entropy= 0.58868, loss=-0.00746
surrogate=-0.03036, entropy= 0.58784, loss=-0.03036
surrogate=-0.00907, entropy= 0.58721, loss=-0.00907
surrogate=-0.02174, entropy= 0.58600, loss=-0.02174
surrogate=-0.03617, entropy= 0.58511, loss=-0.03617
surrogate=-0.01907, entropy= 0.58424, loss=-0.01907
surrogate=-0.04470, entropy= 0.58322, loss=-0.04470
std_min= 0.23554, std_max= 0.35380, std_mean= 0.29798
val lr: [7.428278688524589e-05], policy lr: [8.913934426229506e-05]
Policy Loss: -0.044704, | Entropy Bonus: -0, | Value Loss: 19.111, | Advantage Loss: 0.90954
Time elapsed (s): 1.6622707843780518
Agent stdevs: 0.29798436
--------------------------------------------------------------------------------

Step 686
++++++++ Policy training ++++++++++
Current mean reward: 2233.998535 | mean episode length: 603.666667
val_loss=20.14225
val_loss=11.61466
val_loss=11.61999
val_loss=14.90620
val_loss= 8.53412
val_loss= 9.83848
val_loss=11.38284
val_loss=18.07026
val_loss=44.15767
val_loss=14.48746
adv_loss= 2.11901
adv_loss=13.24217
adv_loss= 2.96638
adv_loss= 2.50854
adv_loss=12.84721
adv_loss= 5.56223
adv_loss= 2.42433
adv_loss= 4.26313
adv_loss= 1.38746
adv_loss= 1.66340
surrogate= 0.00285, entropy= 0.58298, loss= 0.00285
surrogate=-0.00815, entropy= 0.58318, loss=-0.00815
surrogate= 0.01291, entropy= 0.58209, loss= 0.01291
surrogate=-0.01115, entropy= 0.58165, loss=-0.01115
surrogate=-0.02538, entropy= 0.58044, loss=-0.02538
surrogate=-0.01914, entropy= 0.57975, loss=-0.01914
surrogate=-0.03081, entropy= 0.57844, loss=-0.03081
surrogate=-0.00079, entropy= 0.57769, loss=-0.00079
surrogate= 0.00906, entropy= 0.57628, loss= 0.00906
surrogate=-0.04592, entropy= 0.57511, loss=-0.04592
std_min= 0.23416, std_max= 0.35356, std_mean= 0.29729
val lr: [7.402663934426229e-05], policy lr: [8.883196721311474e-05]
Policy Loss: -0.045922, | Entropy Bonus: -0, | Value Loss: 14.487, | Advantage Loss: 1.6634
Time elapsed (s): 1.6439189910888672
Agent stdevs: 0.29728577
--------------------------------------------------------------------------------

Step 687
++++++++ Policy training ++++++++++
Current mean reward: 2098.187947 | mean episode length: 565.500000
val_loss=13.29048
val_loss=12.34438
val_loss=13.20676
val_loss=10.09584
val_loss=10.08457
val_loss=12.29526
val_loss= 7.65104
val_loss= 9.89759
val_loss=11.57929
val_loss=15.57960
adv_loss= 1.72203
adv_loss= 2.95885
adv_loss= 1.81673
adv_loss= 0.93301
adv_loss= 0.87288
adv_loss= 1.34062
adv_loss= 2.32773
adv_loss= 2.26524
adv_loss= 0.87057
adv_loss= 2.35306
surrogate= 0.01046, entropy= 0.57452, loss= 0.01046
surrogate= 0.00876, entropy= 0.57435, loss= 0.00876
surrogate= 0.02726, entropy= 0.57392, loss= 0.02726
surrogate= 0.00411, entropy= 0.57328, loss= 0.00411
surrogate=-0.01019, entropy= 0.57337, loss=-0.01019
surrogate=-0.01933, entropy= 0.57369, loss=-0.01933
surrogate=-0.01838, entropy= 0.57297, loss=-0.01838
surrogate=-0.01621, entropy= 0.57287, loss=-0.01621
surrogate=-0.02090, entropy= 0.57303, loss=-0.02090
surrogate=-0.01904, entropy= 0.57245, loss=-0.01904
std_min= 0.23400, std_max= 0.35276, std_mean= 0.29699
val lr: [7.377049180327869e-05], policy lr: [8.852459016393441e-05]
Policy Loss: -0.019035, | Entropy Bonus: -0, | Value Loss: 15.58, | Advantage Loss: 2.3531
Time elapsed (s): 1.6635448932647705
Agent stdevs: 0.29699272
--------------------------------------------------------------------------------

Step 688
++++++++ Policy training ++++++++++
Current mean reward: 2182.708967 | mean episode length: 581.666667
val_loss=10.87228
val_loss=15.70857
val_loss=11.18587
val_loss= 8.49648
val_loss=10.31883
val_loss=13.38098
val_loss= 5.44589
val_loss=12.80548
val_loss= 6.94009
val_loss= 7.31191
adv_loss= 6.19642
adv_loss= 5.02204
adv_loss= 0.89934
adv_loss= 2.38253
adv_loss= 1.74817
adv_loss= 1.29685
adv_loss= 2.04653
adv_loss= 1.07395
adv_loss= 1.48993
adv_loss= 1.67551
surrogate= 0.03337, entropy= 0.57184, loss= 0.03337
surrogate=-0.02018, entropy= 0.57083, loss=-0.02018
surrogate= 0.00179, entropy= 0.56952, loss= 0.00179
surrogate=-0.01689, entropy= 0.56802, loss=-0.01689
surrogate=-0.01287, entropy= 0.56693, loss=-0.01287
surrogate=-0.00141, entropy= 0.56650, loss=-0.00141
surrogate=-0.01266, entropy= 0.56534, loss=-0.01266
surrogate=-0.02694, entropy= 0.56400, loss=-0.02694
surrogate=-0.02576, entropy= 0.56275, loss=-0.02576
surrogate=-0.02182, entropy= 0.56176, loss=-0.02182
std_min= 0.23309, std_max= 0.35209, std_mean= 0.29597
val lr: [7.351434426229509e-05], policy lr: [8.821721311475409e-05]
Policy Loss: -0.021822, | Entropy Bonus: -0, | Value Loss: 7.3119, | Advantage Loss: 1.6755
Time elapsed (s): 1.6913158893585205
Agent stdevs: 0.29597205
--------------------------------------------------------------------------------

Step 689
++++++++ Policy training ++++++++++
Current mean reward: 1938.977136 | mean episode length: 521.666667
val_loss= 7.15343
val_loss= 7.49278
val_loss= 3.99292
val_loss= 5.06458
val_loss=14.18349
val_loss= 9.33368
val_loss= 9.05949
val_loss= 7.11456
val_loss= 4.64295
val_loss= 4.91921
adv_loss= 0.93096
adv_loss= 1.12598
adv_loss= 0.78228
adv_loss= 0.55536
adv_loss= 1.07905
adv_loss= 0.49559
adv_loss= 1.68085
adv_loss= 1.23563
adv_loss= 1.56058
adv_loss= 2.16900
surrogate=-0.01972, entropy= 0.56280, loss=-0.01972
surrogate=-0.01442, entropy= 0.56344, loss=-0.01442
surrogate=-0.00216, entropy= 0.56443, loss=-0.00216
surrogate=-0.02154, entropy= 0.56519, loss=-0.02154
surrogate=-0.01592, entropy= 0.56543, loss=-0.01592
surrogate=-0.00102, entropy= 0.56601, loss=-0.00102
surrogate= 0.00548, entropy= 0.56646, loss= 0.00548
surrogate=-0.01811, entropy= 0.56645, loss=-0.01811
surrogate=-0.02126, entropy= 0.56639, loss=-0.02126
surrogate=-0.01700, entropy= 0.56681, loss=-0.01700
std_min= 0.23337, std_max= 0.35295, std_mean= 0.29650
val lr: [7.325819672131148e-05], policy lr: [8.790983606557378e-05]
Policy Loss: -0.017, | Entropy Bonus: -0, | Value Loss: 4.9192, | Advantage Loss: 2.169
Time elapsed (s): 1.653336524963379
Agent stdevs: 0.29650027
--------------------------------------------------------------------------------

Step 690
++++++++ Policy training ++++++++++
Current mean reward: 1958.792054 | mean episode length: 532.000000
val_loss= 6.30456
val_loss= 5.25667
val_loss= 8.08908
val_loss= 9.05531
val_loss= 5.11794
val_loss= 3.88272
val_loss= 7.30396
val_loss= 6.20767
val_loss= 4.78512
val_loss= 6.74402
adv_loss= 1.36327
adv_loss= 0.96342
adv_loss= 2.28274
adv_loss= 0.77184
adv_loss= 0.70653
adv_loss= 4.48716
adv_loss= 0.70446
adv_loss= 1.30040
adv_loss= 2.16441
adv_loss= 1.84936
surrogate= 0.00978, entropy= 0.56635, loss= 0.00978
surrogate=-0.00722, entropy= 0.56534, loss=-0.00722
surrogate=-0.02942, entropy= 0.56508, loss=-0.02942
surrogate=-0.01897, entropy= 0.56498, loss=-0.01897
surrogate= 0.01259, entropy= 0.56439, loss= 0.01259
surrogate=-0.04101, entropy= 0.56317, loss=-0.04101
surrogate=-0.01311, entropy= 0.56326, loss=-0.01311
surrogate=-0.00230, entropy= 0.56211, loss=-0.00230
surrogate=-0.01707, entropy= 0.56205, loss=-0.01707
surrogate=-0.02835, entropy= 0.56208, loss=-0.02835
std_min= 0.23251, std_max= 0.35227, std_mean= 0.29607
val lr: [7.300204918032788e-05], policy lr: [8.760245901639345e-05]
Policy Loss: -0.028352, | Entropy Bonus: -0, | Value Loss: 6.744, | Advantage Loss: 1.8494
Time elapsed (s): 1.6537108421325684
Agent stdevs: 0.29607213
--------------------------------------------------------------------------------

Step 691
++++++++ Policy training ++++++++++
Current mean reward: 1484.994206 | mean episode length: 404.400000
val_loss=57.27032
val_loss=41.23800
val_loss=27.76842
val_loss=32.93892
val_loss= 4.90588
val_loss=11.82415
val_loss= 5.87091
val_loss= 7.73017
val_loss=10.81495
val_loss=12.11723
adv_loss= 1.92242
adv_loss= 1.69043
adv_loss= 6.46210
adv_loss= 1.13152
adv_loss= 4.27979
adv_loss= 2.47933
adv_loss= 0.86386
adv_loss= 1.92574
adv_loss= 2.30168
adv_loss= 1.45498
surrogate=-0.01061, entropy= 0.55998, loss=-0.01061
surrogate=-0.03294, entropy= 0.55731, loss=-0.03294
surrogate= 0.00456, entropy= 0.55555, loss= 0.00456
surrogate= 0.02595, entropy= 0.55380, loss= 0.02595
surrogate=-0.04049, entropy= 0.55210, loss=-0.04049
surrogate= 0.01217, entropy= 0.55116, loss= 0.01217
surrogate=-0.00356, entropy= 0.54915, loss=-0.00356
surrogate=-0.00567, entropy= 0.54857, loss=-0.00567
surrogate=-0.03649, entropy= 0.54749, loss=-0.03649
surrogate=-0.01174, entropy= 0.54621, loss=-0.01174
std_min= 0.23208, std_max= 0.34964, std_mean= 0.29439
val lr: [7.274590163934428e-05], policy lr: [8.729508196721312e-05]
Policy Loss: -0.011742, | Entropy Bonus: -0, | Value Loss: 12.117, | Advantage Loss: 1.455
Time elapsed (s): 1.6202960014343262
Agent stdevs: 0.2943948
--------------------------------------------------------------------------------

Step 692
++++++++ Policy training ++++++++++
Current mean reward: 1852.418688 | mean episode length: 499.250000
val_loss= 7.84552
val_loss= 9.81667
val_loss= 4.20447
val_loss= 4.82721
val_loss= 6.27378
val_loss= 5.91256
val_loss= 5.45596
val_loss= 4.06280
val_loss= 4.25177
val_loss= 3.22227
adv_loss= 2.12708
adv_loss= 1.51646
adv_loss= 0.65424
adv_loss= 2.73384
adv_loss= 1.06742
adv_loss= 1.00868
adv_loss= 1.26615
adv_loss= 0.61685
adv_loss= 1.93274
adv_loss= 1.16937
surrogate= 0.01032, entropy= 0.54653, loss= 0.01032
surrogate=-0.01568, entropy= 0.54712, loss=-0.01568
surrogate= 0.00728, entropy= 0.54690, loss= 0.00728
surrogate=-0.00414, entropy= 0.54690, loss=-0.00414
surrogate=-0.01873, entropy= 0.54709, loss=-0.01873
surrogate=-0.00697, entropy= 0.54684, loss=-0.00697
surrogate= 0.00377, entropy= 0.54732, loss= 0.00377
surrogate=-0.04131, entropy= 0.54708, loss=-0.04131
surrogate=-0.04338, entropy= 0.54762, loss=-0.04338
surrogate=-0.02967, entropy= 0.54773, loss=-0.02967
std_min= 0.23226, std_max= 0.35058, std_mean= 0.29458
val lr: [7.248975409836065e-05], policy lr: [8.698770491803276e-05]
Policy Loss: -0.02967, | Entropy Bonus: -0, | Value Loss: 3.2223, | Advantage Loss: 1.1694
Time elapsed (s): 1.6510629653930664
Agent stdevs: 0.2945783
--------------------------------------------------------------------------------

Step 693
++++++++ Policy training ++++++++++
Current mean reward: 1544.270518 | mean episode length: 414.000000
val_loss= 7.52973
val_loss= 6.69764
val_loss= 5.07516
val_loss= 5.94601
val_loss= 5.16543
val_loss= 5.20370
val_loss= 5.83813
val_loss= 6.23019
val_loss= 6.35146
val_loss= 6.86992
adv_loss= 0.92946
adv_loss= 0.77878
adv_loss= 1.08759
adv_loss= 1.25772
adv_loss= 0.56334
adv_loss= 0.61180
adv_loss= 0.52258
adv_loss= 0.66714
adv_loss= 1.07353
adv_loss= 0.67162
surrogate=-0.00254, entropy= 0.54808, loss=-0.00254
surrogate=-0.00543, entropy= 0.54904, loss=-0.00543
surrogate= 0.00756, entropy= 0.54978, loss= 0.00756
surrogate=-0.00780, entropy= 0.54971, loss=-0.00780
surrogate= 0.00577, entropy= 0.55067, loss= 0.00577
surrogate=-0.01621, entropy= 0.55178, loss=-0.01621
surrogate=-0.00943, entropy= 0.55262, loss=-0.00943
surrogate=-0.01543, entropy= 0.55252, loss=-0.01543
surrogate=-0.02275, entropy= 0.55263, loss=-0.02275
surrogate=-0.02355, entropy= 0.55366, loss=-0.02355
std_min= 0.23233, std_max= 0.35140, std_mean= 0.29520
val lr: [7.223360655737704e-05], policy lr: [8.668032786885243e-05]
Policy Loss: -0.02355, | Entropy Bonus: -0, | Value Loss: 6.8699, | Advantage Loss: 0.67162
Time elapsed (s): 1.6503150463104248
Agent stdevs: 0.29520193
--------------------------------------------------------------------------------

Step 694
++++++++ Policy training ++++++++++
Current mean reward: 1308.893061 | mean episode length: 361.400000
val_loss=23.99774
val_loss=45.61835
val_loss=51.18498
val_loss=45.06973
val_loss=13.10180
val_loss=76.40975
val_loss=23.48621
val_loss=30.61612
val_loss=37.64879
val_loss=61.58251
adv_loss= 1.42434
adv_loss= 0.99613
adv_loss= 4.69799
adv_loss= 1.72813
adv_loss= 1.82697
adv_loss= 8.89151
adv_loss= 2.55615
adv_loss= 1.99483
adv_loss= 5.70431
adv_loss= 1.58820
surrogate= 0.00884, entropy= 0.55429, loss= 0.00884
surrogate= 0.02720, entropy= 0.55609, loss= 0.02720
surrogate= 0.00805, entropy= 0.55676, loss= 0.00805
surrogate=-0.00344, entropy= 0.55795, loss=-0.00344
surrogate=-0.03997, entropy= 0.55935, loss=-0.03997
surrogate=-0.02969, entropy= 0.55935, loss=-0.02969
surrogate=-0.00492, entropy= 0.55960, loss=-0.00492
surrogate=-0.01155, entropy= 0.56049, loss=-0.01155
surrogate=-0.00287, entropy= 0.56054, loss=-0.00287
surrogate=-0.01221, entropy= 0.56184, loss=-0.01221
std_min= 0.23198, std_max= 0.35371, std_mean= 0.29618
val lr: [7.197745901639344e-05], policy lr: [8.637295081967212e-05]
Policy Loss: -0.012208, | Entropy Bonus: -0, | Value Loss: 61.583, | Advantage Loss: 1.5882
Time elapsed (s): 1.6916968822479248
Agent stdevs: 0.29617864
--------------------------------------------------------------------------------

Step 695
++++++++ Policy training ++++++++++
Current mean reward: 1812.039784 | mean episode length: 489.000000
val_loss= 8.92023
val_loss=10.23211
val_loss=15.98934
val_loss=27.40551
val_loss=11.10530
val_loss=10.52988
val_loss=16.69676
val_loss= 7.43257
val_loss= 6.81721
val_loss= 8.88705
adv_loss= 0.42851
adv_loss= 0.86330
adv_loss= 1.52557
adv_loss= 1.11747
adv_loss= 2.77921
adv_loss= 3.45945
adv_loss= 1.03211
adv_loss= 0.94023
adv_loss= 1.07145
adv_loss= 1.57199
surrogate=-0.01364, entropy= 0.56252, loss=-0.01364
surrogate=-0.00724, entropy= 0.56177, loss=-0.00724
surrogate=-0.00445, entropy= 0.56169, loss=-0.00445
surrogate=-0.02494, entropy= 0.56086, loss=-0.02494
surrogate=-0.02383, entropy= 0.56008, loss=-0.02383
surrogate=-0.01640, entropy= 0.55957, loss=-0.01640
surrogate=-0.04216, entropy= 0.55898, loss=-0.04216
surrogate= 0.00274, entropy= 0.55806, loss= 0.00274
surrogate= 0.00857, entropy= 0.55783, loss= 0.00857
surrogate=-0.00924, entropy= 0.55680, loss=-0.00924
std_min= 0.23097, std_max= 0.35149, std_mean= 0.29566
val lr: [7.172131147540984e-05], policy lr: [8.60655737704918e-05]
Policy Loss: -0.0092405, | Entropy Bonus: -0, | Value Loss: 8.887, | Advantage Loss: 1.572
Time elapsed (s): 1.6503546237945557
Agent stdevs: 0.29566392
--------------------------------------------------------------------------------

Step 696
++++++++ Policy training ++++++++++
Current mean reward: 1884.430881 | mean episode length: 506.333333
val_loss=12.84252
val_loss= 5.51711
val_loss= 4.94456
val_loss= 5.39645
val_loss= 6.15812
val_loss=11.21162
val_loss= 7.50945
val_loss= 7.84724
val_loss= 5.11718
val_loss= 5.82452
adv_loss= 2.50167
adv_loss= 1.39134
adv_loss= 0.88668
adv_loss= 0.95173
adv_loss= 0.66675
adv_loss= 1.06331
adv_loss= 1.61907
adv_loss= 1.00521
adv_loss= 1.05113
adv_loss= 1.41099
surrogate=-0.01027, entropy= 0.55539, loss=-0.01027
surrogate= 0.00707, entropy= 0.55348, loss= 0.00707
surrogate=-0.03126, entropy= 0.55171, loss=-0.03126
surrogate=-0.03684, entropy= 0.55155, loss=-0.03684
surrogate=-0.00836, entropy= 0.55027, loss=-0.00836
surrogate=-0.03200, entropy= 0.54927, loss=-0.03200
surrogate= 0.02584, entropy= 0.54897, loss= 0.02584
surrogate=-0.04607, entropy= 0.54819, loss=-0.04607
surrogate=-0.02568, entropy= 0.54713, loss=-0.02568
surrogate=-0.04199, entropy= 0.54730, loss=-0.04199
std_min= 0.23014, std_max= 0.35130, std_mean= 0.29479
val lr: [7.146516393442624e-05], policy lr: [8.575819672131147e-05]
Policy Loss: -0.041989, | Entropy Bonus: -0, | Value Loss: 5.8245, | Advantage Loss: 1.411
Time elapsed (s): 1.6449692249298096
Agent stdevs: 0.29478523
--------------------------------------------------------------------------------

Step 697
++++++++ Policy training ++++++++++
Current mean reward: 2288.591692 | mean episode length: 615.333333
val_loss=11.72193
val_loss= 9.49546
val_loss=14.65664
val_loss= 7.79725
val_loss= 6.98199
val_loss= 8.57364
val_loss= 7.85905
val_loss=11.66215
val_loss=10.86456
val_loss=17.42288
adv_loss= 3.83456
adv_loss= 1.31301
adv_loss= 1.39000
adv_loss= 0.92289
adv_loss= 2.84985
adv_loss= 1.14491
adv_loss= 0.94669
adv_loss= 2.52727
adv_loss= 2.22012
adv_loss= 1.01497
surrogate=-0.00099, entropy= 0.54666, loss=-0.00099
surrogate= 0.00820, entropy= 0.54650, loss= 0.00820
surrogate=-0.01376, entropy= 0.54595, loss=-0.01376
surrogate= 0.00386, entropy= 0.54531, loss= 0.00386
surrogate= 0.02889, entropy= 0.54543, loss= 0.02889
surrogate= 0.01227, entropy= 0.54513, loss= 0.01227
surrogate=-0.00863, entropy= 0.54472, loss=-0.00863
surrogate= 0.01376, entropy= 0.54345, loss= 0.01376
surrogate=-0.01980, entropy= 0.54386, loss=-0.01980
surrogate=-0.02010, entropy= 0.54391, loss=-0.02010
std_min= 0.22956, std_max= 0.34965, std_mean= 0.29442
val lr: [7.120901639344262e-05], policy lr: [8.545081967213114e-05]
Policy Loss: -0.020102, | Entropy Bonus: -0, | Value Loss: 17.423, | Advantage Loss: 1.015
Time elapsed (s): 1.660550594329834
Agent stdevs: 0.29442343
--------------------------------------------------------------------------------

Step 698
++++++++ Policy training ++++++++++
Current mean reward: 1097.856442 | mean episode length: 309.333333
val_loss=1168.15588
val_loss=188.81181
val_loss=944.18005
val_loss=111.62791
val_loss=781.40625
val_loss=129.46280
val_loss=201.29414
val_loss=927.95184
val_loss=267.23120
val_loss=489.44983
adv_loss=14.34454
adv_loss= 3.97612
adv_loss= 4.47887
adv_loss= 8.08380
adv_loss= 9.74451
adv_loss= 7.14839
adv_loss= 3.49097
adv_loss= 8.87066
adv_loss= 5.24019
adv_loss= 4.83501
surrogate= 0.01356, entropy= 0.54401, loss= 0.01356
surrogate=-0.01580, entropy= 0.54390, loss=-0.01580
surrogate= 0.00315, entropy= 0.54428, loss= 0.00315
surrogate=-0.02658, entropy= 0.54461, loss=-0.02658
surrogate=-0.02542, entropy= 0.54537, loss=-0.02542
surrogate=-0.02623, entropy= 0.54594, loss=-0.02623
surrogate=-0.02752, entropy= 0.54664, loss=-0.02752
surrogate=-0.02655, entropy= 0.54635, loss=-0.02655
surrogate=-0.00255, entropy= 0.54620, loss=-0.00255
surrogate= 0.02683, entropy= 0.54644, loss= 0.02683
std_min= 0.22974, std_max= 0.34880, std_mean= 0.29463
val lr: [7.095286885245902e-05], policy lr: [8.514344262295082e-05]
Policy Loss: 0.026832, | Entropy Bonus: -0, | Value Loss: 489.45, | Advantage Loss: 4.835
Time elapsed (s): 1.656665563583374
Agent stdevs: 0.29462942
--------------------------------------------------------------------------------

Step 699
++++++++ Policy training ++++++++++
Current mean reward: 1771.738227 | mean episode length: 476.750000
val_loss=37.79462
val_loss=26.23006
val_loss=10.77162
val_loss=18.76063
val_loss= 7.12395
val_loss=15.67962
val_loss= 6.77191
val_loss=15.57338
val_loss=12.60974
val_loss= 7.33257
adv_loss= 2.51370
adv_loss= 2.74713
adv_loss= 1.34903
adv_loss= 1.03655
adv_loss= 3.25846
adv_loss= 2.61196
adv_loss= 3.03721
adv_loss= 1.89462
adv_loss= 1.76663
adv_loss= 2.21221
surrogate= 0.00608, entropy= 0.54759, loss= 0.00608
surrogate=-0.00696, entropy= 0.54902, loss=-0.00696
surrogate=-0.01614, entropy= 0.55033, loss=-0.01614
surrogate=-0.00917, entropy= 0.55144, loss=-0.00917
surrogate=-0.03109, entropy= 0.55254, loss=-0.03109
surrogate=-0.01218, entropy= 0.55417, loss=-0.01218
surrogate=-0.01972, entropy= 0.55508, loss=-0.01972
surrogate= 0.00591, entropy= 0.55643, loss= 0.00591
surrogate=-0.01261, entropy= 0.55698, loss=-0.01261
surrogate= 0.01658, entropy= 0.55883, loss= 0.01658
std_min= 0.23195, std_max= 0.34605, std_mean= 0.29555
val lr: [7.069672131147542e-05], policy lr: [8.483606557377049e-05]
Policy Loss: 0.016577, | Entropy Bonus: -0, | Value Loss: 7.3326, | Advantage Loss: 2.2122
Time elapsed (s): 1.6870989799499512
Agent stdevs: 0.2955538
--------------------------------------------------------------------------------

Step 700
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1770.9
++++++++ Policy training ++++++++++
Current mean reward: 1615.838352 | mean episode length: 440.000000
val_loss=37.46317
val_loss=18.31098
val_loss=20.34817
val_loss=14.87779
val_loss=20.18584
val_loss=10.34544
val_loss=10.37855
val_loss=13.24738
val_loss=19.44672
val_loss=16.44952
adv_loss= 1.29433
adv_loss= 4.72492
adv_loss= 2.55258
adv_loss= 1.50995
adv_loss= 4.62703
adv_loss= 1.99990
adv_loss= 2.58429
adv_loss= 2.31218
adv_loss= 2.08713
adv_loss= 2.82902
surrogate=-0.00431, entropy= 0.55968, loss=-0.00431
surrogate= 0.00036, entropy= 0.55914, loss= 0.00036
surrogate=-0.02722, entropy= 0.55918, loss=-0.02722
surrogate=-0.03378, entropy= 0.55900, loss=-0.03378
surrogate=-0.00258, entropy= 0.55857, loss=-0.00258
surrogate=-0.01813, entropy= 0.55798, loss=-0.01813
surrogate=-0.03377, entropy= 0.55813, loss=-0.03377
surrogate=-0.00685, entropy= 0.55696, loss=-0.00685
surrogate=-0.03753, entropy= 0.55706, loss=-0.03753
surrogate= 0.01318, entropy= 0.55696, loss= 0.01318
std_min= 0.23242, std_max= 0.34579, std_mean= 0.29530
val lr: [7.044057377049182e-05], policy lr: [8.452868852459017e-05]
Policy Loss: 0.013178, | Entropy Bonus: -0, | Value Loss: 16.45, | Advantage Loss: 2.829
Time elapsed (s): 1.6945257186889648
Agent stdevs: 0.29529783
--------------------------------------------------------------------------------

Step 701
++++++++ Policy training ++++++++++
Current mean reward: 1813.910632 | mean episode length: 486.750000
val_loss= 9.84001
val_loss= 7.94503
val_loss= 6.39318
val_loss= 6.65174
val_loss= 6.03545
val_loss= 5.82393
val_loss= 6.12109
val_loss= 5.59850
val_loss= 5.17769
val_loss= 3.16393
adv_loss= 6.07775
adv_loss= 1.12861
adv_loss= 1.63708
adv_loss= 1.39239
adv_loss= 1.00261
adv_loss= 0.94597
adv_loss= 1.40202
adv_loss= 2.13039
adv_loss= 0.90620
adv_loss= 1.27606
surrogate=-0.02434, entropy= 0.55716, loss=-0.02434
surrogate= 0.00561, entropy= 0.55676, loss= 0.00561
surrogate= 0.00301, entropy= 0.55684, loss= 0.00301
surrogate=-0.02863, entropy= 0.55681, loss=-0.02863
surrogate=-0.00028, entropy= 0.55760, loss=-0.00028
surrogate= 0.01314, entropy= 0.55709, loss= 0.01314
surrogate=-0.02334, entropy= 0.55688, loss=-0.02334
surrogate=-0.01216, entropy= 0.55680, loss=-0.01216
surrogate=-0.01863, entropy= 0.55690, loss=-0.01863
surrogate=-0.04250, entropy= 0.55623, loss=-0.04250
std_min= 0.23204, std_max= 0.34501, std_mean= 0.29523
val lr: [7.018442622950818e-05], policy lr: [8.422131147540981e-05]
Policy Loss: -0.042502, | Entropy Bonus: -0, | Value Loss: 3.1639, | Advantage Loss: 1.2761
Time elapsed (s): 1.6778647899627686
Agent stdevs: 0.29523137
--------------------------------------------------------------------------------

Step 702
++++++++ Policy training ++++++++++
Current mean reward: 2054.922645 | mean episode length: 553.000000
val_loss= 8.03755
val_loss= 8.67520
val_loss=11.58057
val_loss= 5.49652
val_loss= 6.82051
val_loss=28.80941
val_loss= 6.58803
val_loss=10.49989
val_loss=13.65278
val_loss= 9.20926
adv_loss= 2.12645
adv_loss= 1.16430
adv_loss= 1.35356
adv_loss= 2.21235
adv_loss= 0.97255
adv_loss= 1.17109
adv_loss= 1.10449
adv_loss= 3.24856
adv_loss= 2.01666
adv_loss= 1.59969
surrogate= 0.00214, entropy= 0.55699, loss= 0.00214
surrogate=-0.02288, entropy= 0.55799, loss=-0.02288
surrogate= 0.02006, entropy= 0.55922, loss= 0.02006
surrogate=-0.00124, entropy= 0.55991, loss=-0.00124
surrogate=-0.00886, entropy= 0.56081, loss=-0.00886
surrogate=-0.02619, entropy= 0.56149, loss=-0.02619
surrogate=-0.00482, entropy= 0.56212, loss=-0.00482
surrogate=-0.00540, entropy= 0.56277, loss=-0.00540
surrogate=-0.03653, entropy= 0.56363, loss=-0.03653
surrogate=-0.04892, entropy= 0.56353, loss=-0.04892
std_min= 0.23101, std_max= 0.34663, std_mean= 0.29616
val lr: [6.992827868852458e-05], policy lr: [8.391393442622949e-05]
Policy Loss: -0.048918, | Entropy Bonus: -0, | Value Loss: 9.2093, | Advantage Loss: 1.5997
Time elapsed (s): 1.6444041728973389
Agent stdevs: 0.2961609
--------------------------------------------------------------------------------

Step 703
++++++++ Policy training ++++++++++
Current mean reward: 1913.995550 | mean episode length: 511.000000
val_loss= 5.98942
val_loss=16.83038
val_loss= 7.77878
val_loss=11.32169
val_loss= 8.10734
val_loss= 2.73373
val_loss= 4.21905
val_loss= 5.09340
val_loss= 7.05677
val_loss= 5.96628
adv_loss= 1.26898
adv_loss= 1.23924
adv_loss= 1.76567
adv_loss= 0.77609
adv_loss= 0.92629
adv_loss= 1.00826
adv_loss= 0.60280
adv_loss= 1.53642
adv_loss= 1.04208
adv_loss= 3.82322
surrogate= 0.01723, entropy= 0.56265, loss= 0.01723
surrogate= 0.02406, entropy= 0.56124, loss= 0.02406
surrogate=-0.01988, entropy= 0.55878, loss=-0.01988
surrogate= 0.03318, entropy= 0.55725, loss= 0.03318
surrogate=-0.01060, entropy= 0.55521, loss=-0.01060
surrogate=-0.02147, entropy= 0.55346, loss=-0.02147
surrogate=-0.03643, entropy= 0.55200, loss=-0.03643
surrogate=-0.01612, entropy= 0.55082, loss=-0.01612
surrogate=-0.04360, entropy= 0.55016, loss=-0.04360
surrogate=-0.05270, entropy= 0.54898, loss=-0.05270
std_min= 0.23029, std_max= 0.34371, std_mean= 0.29464
val lr: [6.967213114754098e-05], policy lr: [8.360655737704916e-05]
Policy Loss: -0.052699, | Entropy Bonus: -0, | Value Loss: 5.9663, | Advantage Loss: 3.8232
Time elapsed (s): 1.6665432453155518
Agent stdevs: 0.2946372
--------------------------------------------------------------------------------

Step 704
++++++++ Policy training ++++++++++
Current mean reward: 2749.755533 | mean episode length: 744.500000
val_loss= 8.75257
val_loss= 8.13795
val_loss= 5.20321
val_loss= 4.80482
val_loss= 5.28243
val_loss=11.45670
val_loss=12.37683
val_loss= 6.02460
val_loss=13.79113
val_loss=11.67681
adv_loss= 3.17484
adv_loss= 1.04545
adv_loss= 0.53683
adv_loss= 1.95423
adv_loss= 0.63687
adv_loss= 1.10974
adv_loss= 0.87100
adv_loss= 0.46123
adv_loss= 0.53723
adv_loss= 0.64740
surrogate=-0.01813, entropy= 0.54843, loss=-0.01813
surrogate= 0.00906, entropy= 0.54844, loss= 0.00906
surrogate=-0.01020, entropy= 0.54697, loss=-0.01020
surrogate=-0.01758, entropy= 0.54637, loss=-0.01758
surrogate=-0.02745, entropy= 0.54561, loss=-0.02745
surrogate=-0.03665, entropy= 0.54471, loss=-0.03665
surrogate=-0.03490, entropy= 0.54439, loss=-0.03490
surrogate=-0.01704, entropy= 0.54402, loss=-0.01704
surrogate= 0.00656, entropy= 0.54378, loss= 0.00656
surrogate=-0.01377, entropy= 0.54342, loss=-0.01377
std_min= 0.23055, std_max= 0.34372, std_mean= 0.29403
val lr: [6.941598360655738e-05], policy lr: [8.329918032786884e-05]
Policy Loss: -0.013772, | Entropy Bonus: -0, | Value Loss: 11.677, | Advantage Loss: 0.6474
Time elapsed (s): 1.640838861465454
Agent stdevs: 0.2940333
--------------------------------------------------------------------------------

Step 705
++++++++ Policy training ++++++++++
Current mean reward: 2123.742482 | mean episode length: 568.000000
val_loss=10.26756
val_loss= 8.27937
val_loss= 4.16949
val_loss= 6.48056
val_loss= 5.74697
val_loss= 5.23279
val_loss= 5.00902
val_loss= 6.72375
val_loss= 4.24359
val_loss= 3.26042
adv_loss= 0.28042
adv_loss= 0.44896
adv_loss= 1.11228
adv_loss= 1.04076
adv_loss= 0.81007
adv_loss= 0.83902
adv_loss= 0.50424
adv_loss= 0.45560
adv_loss= 0.34036
adv_loss= 0.89330
surrogate= 0.01311, entropy= 0.54368, loss= 0.01311
surrogate=-0.00565, entropy= 0.54307, loss=-0.00565
surrogate=-0.01066, entropy= 0.54295, loss=-0.01066
surrogate= 0.00052, entropy= 0.54303, loss= 0.00052
surrogate= 0.01213, entropy= 0.54377, loss= 0.01213
surrogate=-0.03736, entropy= 0.54271, loss=-0.03736
surrogate=-0.01166, entropy= 0.54206, loss=-0.01166
surrogate=-0.01435, entropy= 0.54277, loss=-0.01435
surrogate=-0.02503, entropy= 0.54205, loss=-0.02503
surrogate=-0.01241, entropy= 0.54150, loss=-0.01241
std_min= 0.22977, std_max= 0.34458, std_mean= 0.29395
val lr: [6.915983606557377e-05], policy lr: [8.299180327868851e-05]
Policy Loss: -0.012413, | Entropy Bonus: -0, | Value Loss: 3.2604, | Advantage Loss: 0.8933
Time elapsed (s): 1.630502700805664
Agent stdevs: 0.2939547
--------------------------------------------------------------------------------

Step 706
++++++++ Policy training ++++++++++
Current mean reward: 1425.714879 | mean episode length: 393.400000
val_loss=13.72848
val_loss=78.41553
val_loss=175.21812
val_loss=163.72916
val_loss=84.95557
val_loss=479.97012
val_loss=94.17117
val_loss=615.55743
val_loss=1346.92542
val_loss=161.26175
adv_loss= 2.08483
adv_loss= 2.62540
adv_loss= 6.06515
adv_loss= 4.00551
adv_loss= 5.20513
adv_loss= 7.27933
adv_loss=12.44765
adv_loss= 1.22016
adv_loss= 4.16612
adv_loss= 6.99437
surrogate=-0.00922, entropy= 0.54086, loss=-0.00922
surrogate=-0.02420, entropy= 0.54136, loss=-0.02420
surrogate=-0.02806, entropy= 0.54158, loss=-0.02806
surrogate=-0.03522, entropy= 0.54227, loss=-0.03522
surrogate= 0.01509, entropy= 0.54248, loss= 0.01509
surrogate=-0.02852, entropy= 0.54320, loss=-0.02852
surrogate=-0.00960, entropy= 0.54319, loss=-0.00960
surrogate=-0.02065, entropy= 0.54404, loss=-0.02065
surrogate=-0.03175, entropy= 0.54395, loss=-0.03175
surrogate=-0.02058, entropy= 0.54416, loss=-0.02058
std_min= 0.22975, std_max= 0.34455, std_mean= 0.29423
val lr: [6.890368852459017e-05], policy lr: [8.26844262295082e-05]
Policy Loss: -0.020578, | Entropy Bonus: -0, | Value Loss: 161.26, | Advantage Loss: 6.9944
Time elapsed (s): 1.6833522319793701
Agent stdevs: 0.2942305
--------------------------------------------------------------------------------

Step 707
++++++++ Policy training ++++++++++
Current mean reward: 1511.128079 | mean episode length: 410.750000
val_loss=25.97586
val_loss=20.22119
val_loss=17.60805
val_loss=12.80983
val_loss=13.42815
val_loss=11.48219
val_loss=10.95792
val_loss=16.26845
val_loss=15.80091
val_loss=14.00778
adv_loss= 2.08385
adv_loss= 1.49327
adv_loss= 1.61570
adv_loss= 3.44905
adv_loss= 2.06845
adv_loss= 1.88816
adv_loss= 1.15476
adv_loss= 2.26700
adv_loss= 1.90091
adv_loss= 2.29924
surrogate= 0.01025, entropy= 0.54351, loss= 0.01025
surrogate=-0.02115, entropy= 0.54408, loss=-0.02115
surrogate=-0.00496, entropy= 0.54291, loss=-0.00496
surrogate=-0.00422, entropy= 0.54326, loss=-0.00422
surrogate=-0.02264, entropy= 0.54270, loss=-0.02264
surrogate= 0.00759, entropy= 0.54266, loss= 0.00759
surrogate=-0.00751, entropy= 0.54201, loss=-0.00751
surrogate= 0.00084, entropy= 0.54184, loss= 0.00084
surrogate=-0.02275, entropy= 0.54145, loss=-0.02275
surrogate=-0.00861, entropy= 0.54093, loss=-0.00861
std_min= 0.22939, std_max= 0.34398, std_mean= 0.29392
val lr: [6.864754098360657e-05], policy lr: [8.237704918032787e-05]
Policy Loss: -0.0086128, | Entropy Bonus: -0, | Value Loss: 14.008, | Advantage Loss: 2.2992
Time elapsed (s): 1.6574833393096924
Agent stdevs: 0.29391813
--------------------------------------------------------------------------------

Step 708
++++++++ Policy training ++++++++++
Current mean reward: 1649.762948 | mean episode length: 450.750000
val_loss=16.99396
val_loss=14.32639
val_loss=13.49578
val_loss= 7.99007
val_loss= 8.55270
val_loss= 7.88240
val_loss=12.58366
val_loss=12.07044
val_loss= 7.04127
val_loss=12.53599
adv_loss= 1.72441
adv_loss= 1.19966
adv_loss= 2.89725
adv_loss= 1.52583
adv_loss= 3.67864
adv_loss= 1.57380
adv_loss= 1.51320
adv_loss= 3.34311
adv_loss= 2.93618
adv_loss= 2.20829
surrogate=-0.00467, entropy= 0.54035, loss=-0.00467
surrogate=-0.00229, entropy= 0.53939, loss=-0.00229
surrogate=-0.00099, entropy= 0.53953, loss=-0.00099
surrogate= 0.00107, entropy= 0.53892, loss= 0.00107
surrogate=-0.00293, entropy= 0.53855, loss=-0.00293
surrogate=-0.02681, entropy= 0.53784, loss=-0.02681
surrogate=-0.01060, entropy= 0.53733, loss=-0.01060
surrogate=-0.02385, entropy= 0.53727, loss=-0.02385
surrogate=-0.00229, entropy= 0.53697, loss=-0.00229
surrogate=-0.01305, entropy= 0.53632, loss=-0.01305
std_min= 0.22851, std_max= 0.34406, std_mean= 0.29355
val lr: [6.839139344262297e-05], policy lr: [8.206967213114755e-05]
Policy Loss: -0.013054, | Entropy Bonus: -0, | Value Loss: 12.536, | Advantage Loss: 2.2083
Time elapsed (s): 1.6286039352416992
Agent stdevs: 0.29355156
--------------------------------------------------------------------------------

Step 709
++++++++ Policy training ++++++++++
Current mean reward: 1661.959431 | mean episode length: 448.250000
val_loss= 6.73973
val_loss= 7.31875
val_loss=11.07511
val_loss= 6.52217
val_loss= 9.07014
val_loss= 9.14857
val_loss=13.60843
val_loss= 7.21997
val_loss=10.04349
val_loss= 5.39448
adv_loss= 3.08334
adv_loss= 0.86105
adv_loss= 4.26195
adv_loss= 1.60568
adv_loss= 2.19574
adv_loss= 1.60336
adv_loss= 2.18911
adv_loss= 1.35734
adv_loss= 1.58737
adv_loss= 1.46466
surrogate=-0.01219, entropy= 0.53556, loss=-0.01219
surrogate=-0.00227, entropy= 0.53551, loss=-0.00227
surrogate=-0.02519, entropy= 0.53516, loss=-0.02519
surrogate= 0.00917, entropy= 0.53506, loss= 0.00917
surrogate=-0.00915, entropy= 0.53522, loss=-0.00915
surrogate= 0.01422, entropy= 0.53534, loss= 0.01422
surrogate=-0.01519, entropy= 0.53441, loss=-0.01519
surrogate=-0.02046, entropy= 0.53381, loss=-0.02046
surrogate=-0.03726, entropy= 0.53367, loss=-0.03726
surrogate=-0.04206, entropy= 0.53388, loss=-0.04206
std_min= 0.22901, std_max= 0.34435, std_mean= 0.29325
val lr: [6.813524590163935e-05], policy lr: [8.176229508196722e-05]
Policy Loss: -0.042064, | Entropy Bonus: -0, | Value Loss: 5.3945, | Advantage Loss: 1.4647
Time elapsed (s): 1.6400132179260254
Agent stdevs: 0.29325265
--------------------------------------------------------------------------------

Step 710
++++++++ Policy training ++++++++++
Current mean reward: 1330.746014 | mean episode length: 360.750000
val_loss= 5.79637
val_loss= 8.35618
val_loss= 6.03469
val_loss=13.71431
val_loss= 9.54118
val_loss= 5.23668
val_loss= 4.37232
val_loss= 4.83839
val_loss= 5.93229
val_loss= 5.24514
adv_loss= 0.67750
adv_loss= 1.93391
adv_loss= 1.14075
adv_loss= 1.98780
adv_loss=17.58613
adv_loss= 3.58935
adv_loss= 1.36045
adv_loss= 2.02529
adv_loss=18.05520
adv_loss= 1.34594
surrogate= 0.01500, entropy= 0.53383, loss= 0.01500
surrogate=-0.02384, entropy= 0.53404, loss=-0.02384
surrogate= 0.00345, entropy= 0.53408, loss= 0.00345
surrogate=-0.02463, entropy= 0.53433, loss=-0.02463
surrogate=-0.04599, entropy= 0.53423, loss=-0.04599
surrogate=-0.01503, entropy= 0.53412, loss=-0.01503
surrogate=-0.00647, entropy= 0.53385, loss=-0.00647
surrogate= 0.00846, entropy= 0.53423, loss= 0.00846
surrogate=-0.03334, entropy= 0.53376, loss=-0.03334
surrogate= 0.00916, entropy= 0.53358, loss= 0.00916
std_min= 0.22858, std_max= 0.34615, std_mean= 0.29334
val lr: [6.787909836065573e-05], policy lr: [8.145491803278687e-05]
Policy Loss: 0.0091562, | Entropy Bonus: -0, | Value Loss: 5.2451, | Advantage Loss: 1.3459
Time elapsed (s): 1.6463863849639893
Agent stdevs: 0.29333928
--------------------------------------------------------------------------------

Step 711
++++++++ Policy training ++++++++++
Current mean reward: 1508.294128 | mean episode length: 407.000000
val_loss= 4.76643
val_loss= 5.22817
val_loss= 8.20378
val_loss= 6.44223
val_loss= 6.24209
val_loss= 6.56572
val_loss= 4.67717
val_loss= 3.85876
val_loss= 6.99417
val_loss= 5.21694
adv_loss= 1.12235
adv_loss= 1.95636
adv_loss= 0.57522
adv_loss= 3.85766
adv_loss= 3.88508
adv_loss= 2.49934
adv_loss= 1.04862
adv_loss= 1.38563
adv_loss= 3.64802
adv_loss= 1.52678
surrogate=-0.00318, entropy= 0.53246, loss=-0.00318
surrogate= 0.03925, entropy= 0.53161, loss= 0.03925
surrogate= 0.01725, entropy= 0.53120, loss= 0.01725
surrogate=-0.01295, entropy= 0.53013, loss=-0.01295
surrogate=-0.02196, entropy= 0.53009, loss=-0.02196
surrogate=-0.03391, entropy= 0.52928, loss=-0.03391
surrogate=-0.01887, entropy= 0.52875, loss=-0.01887
surrogate=-0.00556, entropy= 0.52867, loss=-0.00556
surrogate=-0.03589, entropy= 0.52841, loss=-0.03589
surrogate=-0.03894, entropy= 0.52855, loss=-0.03894
std_min= 0.22928, std_max= 0.34397, std_mean= 0.29266
val lr: [6.762295081967213e-05], policy lr: [8.114754098360655e-05]
Policy Loss: -0.038937, | Entropy Bonus: -0, | Value Loss: 5.2169, | Advantage Loss: 1.5268
Time elapsed (s): 1.6299505233764648
Agent stdevs: 0.29266128
--------------------------------------------------------------------------------

Step 712
++++++++ Policy training ++++++++++
Current mean reward: 1487.781992 | mean episode length: 405.000000
val_loss=17.47338
val_loss=17.41927
val_loss= 9.21598
val_loss= 9.29335
val_loss= 4.12021
val_loss=12.13194
val_loss= 8.12979
val_loss=15.28708
val_loss=13.08230
val_loss=14.73261
adv_loss= 0.95238
adv_loss= 2.00408
adv_loss= 1.75369
adv_loss= 3.67401
adv_loss= 8.81456
adv_loss= 1.23292
adv_loss= 1.08052
adv_loss= 2.24207
adv_loss= 1.03774
adv_loss= 0.84053
surrogate= 0.01916, entropy= 0.52671, loss= 0.01916
surrogate=-0.03725, entropy= 0.52668, loss=-0.03725
surrogate=-0.00197, entropy= 0.52621, loss=-0.00197
surrogate=-0.01572, entropy= 0.52606, loss=-0.01572
surrogate=-0.02836, entropy= 0.52561, loss=-0.02836
surrogate=-0.02419, entropy= 0.52588, loss=-0.02419
surrogate=-0.01324, entropy= 0.52611, loss=-0.01324
surrogate=-0.03131, entropy= 0.52598, loss=-0.03131
surrogate=-0.02285, entropy= 0.52502, loss=-0.02285
surrogate=-0.01746, entropy= 0.52440, loss=-0.01746
std_min= 0.22807, std_max= 0.34415, std_mean= 0.29239
val lr: [6.736680327868853e-05], policy lr: [8.084016393442622e-05]
Policy Loss: -0.017464, | Entropy Bonus: -0, | Value Loss: 14.733, | Advantage Loss: 0.84053
Time elapsed (s): 1.6841657161712646
Agent stdevs: 0.2923902
--------------------------------------------------------------------------------

Step 713
++++++++ Policy training ++++++++++
Current mean reward: 2112.549431 | mean episode length: 574.666667
val_loss=15.45304
val_loss=15.95129
val_loss=10.44369
val_loss=14.31413
val_loss= 4.97180
val_loss= 7.79172
val_loss= 8.30663
val_loss= 6.49408
val_loss=16.61162
val_loss=19.11422
adv_loss= 0.44125
adv_loss= 1.12755
adv_loss= 0.86516
adv_loss= 1.31023
adv_loss= 1.46556
adv_loss= 0.94991
adv_loss= 3.57831
adv_loss= 1.71029
adv_loss= 0.98563
adv_loss= 1.15892
surrogate= 0.01583, entropy= 0.52474, loss= 0.01583
surrogate=-0.01384, entropy= 0.52556, loss=-0.01384
surrogate=-0.01599, entropy= 0.52549, loss=-0.01599
surrogate=-0.00082, entropy= 0.52465, loss=-0.00082
surrogate=-0.02248, entropy= 0.52516, loss=-0.02248
surrogate=-0.03105, entropy= 0.52522, loss=-0.03105
surrogate=-0.03460, entropy= 0.52540, loss=-0.03460
surrogate=-0.01555, entropy= 0.52527, loss=-0.01555
surrogate=-0.00172, entropy= 0.52473, loss=-0.00172
surrogate=-0.01590, entropy= 0.52484, loss=-0.01590
std_min= 0.22839, std_max= 0.34263, std_mean= 0.29234
val lr: [6.711065573770491e-05], policy lr: [8.05327868852459e-05]
Policy Loss: -0.015901, | Entropy Bonus: -0, | Value Loss: 19.114, | Advantage Loss: 1.1589
Time elapsed (s): 1.693917989730835
Agent stdevs: 0.29234123
--------------------------------------------------------------------------------

Step 714
++++++++ Policy training ++++++++++
Current mean reward: 1526.317975 | mean episode length: 412.000000
val_loss=11.79392
val_loss=10.22619
val_loss=10.60274
val_loss=12.89821
val_loss=11.74055
val_loss= 5.56216
val_loss= 8.35158
val_loss= 6.88421
val_loss= 6.90760
val_loss= 8.72112
adv_loss= 0.60256
adv_loss= 0.94362
adv_loss= 0.71340
adv_loss= 3.94960
adv_loss= 0.55826
adv_loss= 1.83436
adv_loss= 1.74801
adv_loss= 1.40681
adv_loss= 1.47860
adv_loss= 1.30428
surrogate= 0.02623, entropy= 0.52519, loss= 0.02623
surrogate=-0.00353, entropy= 0.52529, loss=-0.00353
surrogate=-0.02415, entropy= 0.52486, loss=-0.02415
surrogate=-0.01157, entropy= 0.52326, loss=-0.01157
surrogate=-0.00418, entropy= 0.52290, loss=-0.00418
surrogate=-0.00476, entropy= 0.52216, loss=-0.00476
surrogate= 0.00972, entropy= 0.52141, loss= 0.00972
surrogate= 0.00129, entropy= 0.52164, loss= 0.00129
surrogate=-0.01648, entropy= 0.52117, loss=-0.01648
surrogate=-0.03544, entropy= 0.51989, loss=-0.03544
std_min= 0.22717, std_max= 0.34326, std_mean= 0.29200
val lr: [6.685450819672131e-05], policy lr: [8.022540983606557e-05]
Policy Loss: -0.035444, | Entropy Bonus: -0, | Value Loss: 8.7211, | Advantage Loss: 1.3043
Time elapsed (s): 1.6664495468139648
Agent stdevs: 0.291998
--------------------------------------------------------------------------------

Step 715
++++++++ Policy training ++++++++++
Current mean reward: 1391.339964 | mean episode length: 381.750000
val_loss= 5.72145
val_loss=10.84896
val_loss= 8.87979
val_loss= 5.21796
val_loss= 9.00840
val_loss= 8.79499
val_loss= 6.46121
val_loss=10.23741
val_loss= 7.77608
val_loss= 6.34190
adv_loss= 2.00755
adv_loss= 1.87182
adv_loss= 1.23143
adv_loss= 0.88233
adv_loss= 0.59644
adv_loss= 1.02770
adv_loss= 2.36311
adv_loss= 3.19566
adv_loss= 1.77339
adv_loss= 0.85684
surrogate= 0.00979, entropy= 0.52043, loss= 0.00979
surrogate=-0.02857, entropy= 0.52059, loss=-0.02857
surrogate= 0.00438, entropy= 0.52151, loss= 0.00438
surrogate= 0.00207, entropy= 0.52194, loss= 0.00207
surrogate=-0.01559, entropy= 0.52261, loss=-0.01559
surrogate=-0.03408, entropy= 0.52261, loss=-0.03408
surrogate= 0.00569, entropy= 0.52242, loss= 0.00569
surrogate=-0.00495, entropy= 0.52248, loss=-0.00495
surrogate=-0.01820, entropy= 0.52238, loss=-0.01820
surrogate= 0.00623, entropy= 0.52277, loss= 0.00623
std_min= 0.22720, std_max= 0.34164, std_mean= 0.29224
val lr: [6.659836065573771e-05], policy lr: [7.991803278688524e-05]
Policy Loss: 0.0062296, | Entropy Bonus: -0, | Value Loss: 6.3419, | Advantage Loss: 0.85684
Time elapsed (s): 1.666712999343872
Agent stdevs: 0.29223576
--------------------------------------------------------------------------------

Step 716
++++++++ Policy training ++++++++++
Current mean reward: 2330.875023 | mean episode length: 651.333333
val_loss=192.46948
val_loss=786.49420
val_loss=502.55502
val_loss=1222.25684
val_loss=675.91907
val_loss=105.39294
val_loss=1395.37195
val_loss=21.22785
val_loss=33.09341
val_loss=29.70584
adv_loss= 1.69495
adv_loss= 1.93506
adv_loss= 4.76644
adv_loss= 1.04858
adv_loss= 1.08528
adv_loss= 2.13299
adv_loss= 1.11092
adv_loss= 0.64486
adv_loss= 1.55819
adv_loss= 0.79751
surrogate= 0.01243, entropy= 0.52167, loss= 0.01243
surrogate=-0.01106, entropy= 0.52044, loss=-0.01106
surrogate=-0.01146, entropy= 0.51978, loss=-0.01146
surrogate=-0.02973, entropy= 0.52022, loss=-0.02973
surrogate= 0.01054, entropy= 0.51914, loss= 0.01054
surrogate=-0.01295, entropy= 0.51910, loss=-0.01295
surrogate=-0.00729, entropy= 0.51829, loss=-0.00729
surrogate=-0.02622, entropy= 0.51743, loss=-0.02622
surrogate=-0.01303, entropy= 0.51653, loss=-0.01303
surrogate=-0.00430, entropy= 0.51582, loss=-0.00430
std_min= 0.22666, std_max= 0.33956, std_mean= 0.29152
val lr: [6.634221311475411e-05], policy lr: [7.961065573770492e-05]
Policy Loss: -0.0043005, | Entropy Bonus: -0, | Value Loss: 29.706, | Advantage Loss: 0.79751
Time elapsed (s): 1.6640937328338623
Agent stdevs: 0.29151833
--------------------------------------------------------------------------------

Step 717
++++++++ Policy training ++++++++++
Current mean reward: 3540.116436 | mean episode length: 1000.000000
val_loss=112.51267
val_loss=217.25380
val_loss=1756.40210
val_loss=122.62554
val_loss=586.50916
val_loss=143.84575
val_loss=603.47833
val_loss=998.73462
val_loss=380.20758
val_loss=178.49271
adv_loss=1912.88562
adv_loss= 5.09368
adv_loss= 2.61586
adv_loss= 1.90034
adv_loss= 1.73789
adv_loss= 2.14582
adv_loss= 2.79385
adv_loss= 2.61094
adv_loss=1486.00134
adv_loss=1486.98083
surrogate=-0.00203, entropy= 0.51589, loss=-0.00203
surrogate=-0.00071, entropy= 0.51425, loss=-0.00071
surrogate=-0.02023, entropy= 0.51258, loss=-0.02023
surrogate=-0.02055, entropy= 0.51256, loss=-0.02055
surrogate=-0.04065, entropy= 0.51302, loss=-0.04065
surrogate= 0.00087, entropy= 0.51291, loss= 0.00087
surrogate=-0.02403, entropy= 0.51316, loss=-0.02403
surrogate= 0.01418, entropy= 0.51347, loss= 0.01418
surrogate= 0.00778, entropy= 0.51289, loss= 0.00778
surrogate= 0.01311, entropy= 0.51343, loss= 0.01311
std_min= 0.22753, std_max= 0.33805, std_mean= 0.29112
val lr: [6.60860655737705e-05], policy lr: [7.930327868852459e-05]
Policy Loss: 0.013109, | Entropy Bonus: -0, | Value Loss: 178.49, | Advantage Loss: 1487
Time elapsed (s): 1.6588449478149414
Agent stdevs: 0.29112348
--------------------------------------------------------------------------------

Step 718
++++++++ Policy training ++++++++++
Current mean reward: 1692.821988 | mean episode length: 463.500000
val_loss=39.84237
val_loss=11.18422
val_loss=14.59533
val_loss=24.59606
val_loss=13.53388
val_loss=11.63616
val_loss= 7.80139
val_loss= 6.63128
val_loss= 6.10401
val_loss=15.36050
adv_loss= 1.78773
adv_loss= 4.47312
adv_loss= 3.64743
adv_loss= 1.57898
adv_loss= 1.44578
adv_loss= 1.04291
adv_loss= 1.75050
adv_loss= 2.66426
adv_loss= 3.17675
adv_loss= 2.11841
surrogate=-0.01096, entropy= 0.51339, loss=-0.01096
surrogate= 0.01347, entropy= 0.51368, loss= 0.01347
surrogate=-0.04202, entropy= 0.51430, loss=-0.04202
surrogate=-0.02037, entropy= 0.51497, loss=-0.02037
surrogate=-0.00225, entropy= 0.51389, loss=-0.00225
surrogate= 0.03178, entropy= 0.51349, loss= 0.03178
surrogate=-0.02713, entropy= 0.51333, loss=-0.02713
surrogate=-0.01419, entropy= 0.51415, loss=-0.01419
surrogate=-0.04311, entropy= 0.51380, loss=-0.04311
surrogate=-0.05264, entropy= 0.51458, loss=-0.05264
std_min= 0.22826, std_max= 0.33908, std_mean= 0.29119
val lr: [6.582991803278687e-05], policy lr: [7.899590163934424e-05]
Policy Loss: -0.052642, | Entropy Bonus: -0, | Value Loss: 15.36, | Advantage Loss: 2.1184
Time elapsed (s): 1.7315735816955566
Agent stdevs: 0.2911915
--------------------------------------------------------------------------------

Step 719
++++++++ Policy training ++++++++++
Current mean reward: 2318.302310 | mean episode length: 618.500000
val_loss=29.91993
val_loss= 7.33602
val_loss=10.36704
val_loss= 8.61470
val_loss=10.64831
val_loss=15.31170
val_loss= 7.85198
val_loss=14.02544
val_loss=12.76565
val_loss= 7.13381
adv_loss= 1.04732
adv_loss= 1.34251
adv_loss= 1.07780
adv_loss= 1.86161
adv_loss= 1.07646
adv_loss= 0.96201
adv_loss= 1.01990
adv_loss= 6.11946
adv_loss= 1.81781
adv_loss= 1.53260
surrogate=-0.00937, entropy= 0.51538, loss=-0.00937
surrogate=-0.01081, entropy= 0.51538, loss=-0.01081
surrogate=-0.01428, entropy= 0.51564, loss=-0.01428
surrogate=-0.00658, entropy= 0.51537, loss=-0.00658
surrogate= 0.01316, entropy= 0.51559, loss= 0.01316
surrogate= 0.00125, entropy= 0.51556, loss= 0.00125
surrogate= 0.01802, entropy= 0.51519, loss= 0.01802
surrogate=-0.01753, entropy= 0.51586, loss=-0.01753
surrogate=-0.01910, entropy= 0.51583, loss=-0.01910
surrogate=-0.02469, entropy= 0.51575, loss=-0.02469
std_min= 0.22967, std_max= 0.33709, std_mean= 0.29108
val lr: [6.557377049180327e-05], policy lr: [7.868852459016391e-05]
Policy Loss: -0.024687, | Entropy Bonus: -0, | Value Loss: 7.1338, | Advantage Loss: 1.5326
Time elapsed (s): 1.650986909866333
Agent stdevs: 0.29108298
--------------------------------------------------------------------------------

Step 720
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2254.7
++++++++ Policy training ++++++++++
Current mean reward: 1326.984880 | mean episode length: 359.400000
val_loss=15.23151
val_loss=14.96522
val_loss=24.06278
val_loss=17.31923
val_loss=16.86399
val_loss= 9.31494
val_loss=16.00843
val_loss=15.74181
val_loss= 9.12654
val_loss=12.06972
adv_loss= 3.67091
adv_loss= 2.88058
adv_loss= 1.25987
adv_loss= 1.28265
adv_loss= 5.95057
adv_loss= 2.07416
adv_loss= 3.45174
adv_loss= 1.99816
adv_loss= 1.96896
adv_loss= 2.32203
surrogate= 0.00253, entropy= 0.51421, loss= 0.00253
surrogate=-0.00141, entropy= 0.51291, loss=-0.00141
surrogate= 0.00761, entropy= 0.51227, loss= 0.00761
surrogate=-0.01011, entropy= 0.51073, loss=-0.01011
surrogate=-0.01146, entropy= 0.50971, loss=-0.01146
surrogate= 0.00473, entropy= 0.50793, loss= 0.00473
surrogate=-0.03480, entropy= 0.50673, loss=-0.03480
surrogate=-0.04829, entropy= 0.50533, loss=-0.04829
surrogate=-0.03417, entropy= 0.50425, loss=-0.03417
surrogate=-0.03453, entropy= 0.50308, loss=-0.03453
std_min= 0.22879, std_max= 0.33463, std_mean= 0.28981
val lr: [6.531762295081967e-05], policy lr: [7.838114754098359e-05]
Policy Loss: -0.034528, | Entropy Bonus: -0, | Value Loss: 12.07, | Advantage Loss: 2.322
Time elapsed (s): 1.6431329250335693
Agent stdevs: 0.28981104
--------------------------------------------------------------------------------

Step 721
++++++++ Policy training ++++++++++
Current mean reward: 1838.810210 | mean episode length: 498.000000
val_loss= 6.81847
val_loss=17.85390
val_loss=10.18958
val_loss=21.35389
val_loss=18.93997
val_loss=14.22688
val_loss=11.49478
val_loss=13.89777
val_loss= 9.11822
val_loss=13.14447
adv_loss= 3.12086
adv_loss= 3.28160
adv_loss= 0.75173
adv_loss= 1.87458
adv_loss= 1.25498
adv_loss= 0.80786
adv_loss= 2.60885
adv_loss= 1.36113
adv_loss= 0.54287
adv_loss= 2.05363
surrogate=-0.01138, entropy= 0.50138, loss=-0.01138
surrogate=-0.00350, entropy= 0.50013, loss=-0.00350
surrogate=-0.00817, entropy= 0.49865, loss=-0.00817
surrogate=-0.02711, entropy= 0.49766, loss=-0.02711
surrogate=-0.03943, entropy= 0.49622, loss=-0.03943
surrogate=-0.01359, entropy= 0.49498, loss=-0.01359
surrogate= 0.00531, entropy= 0.49303, loss= 0.00531
surrogate=-0.02960, entropy= 0.49221, loss=-0.02960
surrogate=-0.02202, entropy= 0.49053, loss=-0.02202
surrogate=-0.02507, entropy= 0.48955, loss=-0.02507
std_min= 0.22818, std_max= 0.33415, std_mean= 0.28849
val lr: [6.506147540983606e-05], policy lr: [7.807377049180326e-05]
Policy Loss: -0.025069, | Entropy Bonus: -0, | Value Loss: 13.144, | Advantage Loss: 2.0536
Time elapsed (s): 1.652458667755127
Agent stdevs: 0.28849128
--------------------------------------------------------------------------------

Step 722
++++++++ Policy training ++++++++++
Current mean reward: 1836.005293 | mean episode length: 498.250000
val_loss=11.68592
val_loss= 7.14279
val_loss= 9.85855
val_loss=12.70875
val_loss=14.22630
val_loss= 7.80974
val_loss= 7.26971
val_loss= 7.06037
val_loss= 8.59271
val_loss=10.39524
adv_loss= 1.16287
adv_loss= 2.70291
adv_loss= 1.51540
adv_loss= 4.45729
adv_loss= 2.18468
adv_loss= 0.93378
adv_loss= 0.92551
adv_loss= 2.55376
adv_loss= 4.22247
adv_loss= 7.50783
surrogate=-0.02480, entropy= 0.48831, loss=-0.02480
surrogate=-0.02491, entropy= 0.48727, loss=-0.02491
surrogate=-0.03271, entropy= 0.48617, loss=-0.03271
surrogate=-0.04473, entropy= 0.48528, loss=-0.04473
surrogate=-0.03904, entropy= 0.48438, loss=-0.03904
surrogate=-0.03857, entropy= 0.48289, loss=-0.03857
surrogate=-0.03256, entropy= 0.48226, loss=-0.03256
surrogate=-0.00472, entropy= 0.47985, loss=-0.00472
surrogate=-0.00511, entropy= 0.47873, loss=-0.00511
surrogate=-0.02057, entropy= 0.47797, loss=-0.02057
std_min= 0.22766, std_max= 0.33230, std_mean= 0.28732
val lr: [6.480532786885246e-05], policy lr: [7.776639344262294e-05]
Policy Loss: -0.020572, | Entropy Bonus: -0, | Value Loss: 10.395, | Advantage Loss: 7.5078
Time elapsed (s): 1.6340339183807373
Agent stdevs: 0.28732228
--------------------------------------------------------------------------------

Step 723
++++++++ Policy training ++++++++++
Current mean reward: 2443.721236 | mean episode length: 658.000000
val_loss=16.08140
val_loss= 7.85672
val_loss=15.53700
val_loss= 8.08817
val_loss= 9.59163
val_loss= 7.52750
val_loss= 8.39381
val_loss= 9.27017
val_loss= 6.89385
val_loss= 7.44059
adv_loss= 1.97972
adv_loss= 1.35604
adv_loss= 1.96397
adv_loss= 0.79653
adv_loss= 1.03353
adv_loss= 0.59512
adv_loss= 1.48424
adv_loss= 0.66028
adv_loss= 0.47510
adv_loss= 0.99361
surrogate=-0.01870, entropy= 0.47748, loss=-0.01870
surrogate=-0.00239, entropy= 0.47739, loss=-0.00239
surrogate= 0.00710, entropy= 0.47716, loss= 0.00710
surrogate= 0.02041, entropy= 0.47698, loss= 0.02041
surrogate= 0.00191, entropy= 0.47699, loss= 0.00191
surrogate=-0.00701, entropy= 0.47754, loss=-0.00701
surrogate=-0.01960, entropy= 0.47708, loss=-0.01960
surrogate=-0.01408, entropy= 0.47618, loss=-0.01408
surrogate=-0.03768, entropy= 0.47613, loss=-0.03768
surrogate= 0.03621, entropy= 0.47625, loss= 0.03621
std_min= 0.22721, std_max= 0.33251, std_mean= 0.28721
val lr: [6.454918032786886e-05], policy lr: [7.745901639344263e-05]
Policy Loss: 0.036211, | Entropy Bonus: -0, | Value Loss: 7.4406, | Advantage Loss: 0.99361
Time elapsed (s): 1.6553030014038086
Agent stdevs: 0.28720847
--------------------------------------------------------------------------------

Step 724
++++++++ Policy training ++++++++++
Current mean reward: 1681.546759 | mean episode length: 454.500000
val_loss=12.53937
val_loss=12.17819
val_loss= 8.18126
val_loss= 9.22209
val_loss= 4.91180
val_loss= 6.82935
val_loss= 6.69908
val_loss= 5.92960
val_loss= 6.19785
val_loss= 4.87796
adv_loss= 1.55951
adv_loss= 0.97148
adv_loss= 1.35648
adv_loss= 1.51870
adv_loss= 0.86405
adv_loss= 1.01242
adv_loss= 0.90224
adv_loss= 0.59386
adv_loss= 1.63063
adv_loss= 1.66368
surrogate= 0.00134, entropy= 0.47531, loss= 0.00134
surrogate= 0.00731, entropy= 0.47441, loss= 0.00731
surrogate=-0.00654, entropy= 0.47403, loss=-0.00654
surrogate= 0.00825, entropy= 0.47356, loss= 0.00825
surrogate= 0.00801, entropy= 0.47285, loss= 0.00801
surrogate=-0.01803, entropy= 0.47176, loss=-0.01803
surrogate=-0.02091, entropy= 0.47087, loss=-0.02091
surrogate=-0.01704, entropy= 0.46957, loss=-0.01704
surrogate=-0.02957, entropy= 0.46918, loss=-0.02957
surrogate=-0.02453, entropy= 0.46893, loss=-0.02453
std_min= 0.22618, std_max= 0.33232, std_mean= 0.28658
val lr: [6.429303278688526e-05], policy lr: [7.71516393442623e-05]
Policy Loss: -0.024529, | Entropy Bonus: -0, | Value Loss: 4.878, | Advantage Loss: 1.6637
Time elapsed (s): 1.6693646907806396
Agent stdevs: 0.28657934
--------------------------------------------------------------------------------

Step 725
++++++++ Policy training ++++++++++
Current mean reward: 1296.508729 | mean episode length: 350.200000
val_loss=13.12089
val_loss= 6.36294
val_loss=13.12353
val_loss=10.09840
val_loss= 6.89214
val_loss= 6.23377
val_loss= 6.52100
val_loss= 6.49150
val_loss= 8.95368
val_loss= 8.60354
adv_loss= 1.13009
adv_loss= 3.08370
adv_loss= 1.51803
adv_loss= 0.74483
adv_loss= 0.96486
adv_loss= 0.82952
adv_loss= 0.83973
adv_loss= 4.30212
adv_loss= 2.57221
adv_loss= 2.93997
surrogate=-0.02218, entropy= 0.46863, loss=-0.02218
surrogate= 0.00382, entropy= 0.46823, loss= 0.00382
surrogate=-0.00822, entropy= 0.46798, loss=-0.00822
surrogate=-0.00958, entropy= 0.46775, loss=-0.00958
surrogate=-0.00946, entropy= 0.46769, loss=-0.00946
surrogate=-0.00342, entropy= 0.46704, loss=-0.00342
surrogate=-0.03293, entropy= 0.46703, loss=-0.03293
surrogate=-0.00118, entropy= 0.46731, loss=-0.00118
surrogate=-0.03845, entropy= 0.46713, loss=-0.03845
surrogate=-0.02411, entropy= 0.46690, loss=-0.02411
std_min= 0.22603, std_max= 0.33135, std_mean= 0.28636
val lr: [6.403688524590164e-05], policy lr: [7.684426229508197e-05]
Policy Loss: -0.024108, | Entropy Bonus: -0, | Value Loss: 8.6035, | Advantage Loss: 2.94
Time elapsed (s): 1.6555604934692383
Agent stdevs: 0.2863599
--------------------------------------------------------------------------------

Step 726
++++++++ Policy training ++++++++++
Current mean reward: 1560.748580 | mean episode length: 426.750000
val_loss=10.54967
val_loss= 6.81967
val_loss= 9.17095
val_loss= 7.56464
val_loss= 4.14675
val_loss= 8.23441
val_loss= 7.32527
val_loss=11.31170
val_loss= 6.39459
val_loss= 4.90375
adv_loss= 2.76767
adv_loss= 1.00687
adv_loss= 0.63312
adv_loss= 1.24330
adv_loss= 1.20537
adv_loss= 0.54872
adv_loss= 0.85217
adv_loss= 1.41827
adv_loss= 0.58768
adv_loss= 1.54149
surrogate=-0.00929, entropy= 0.46588, loss=-0.00929
surrogate=-0.03195, entropy= 0.46619, loss=-0.03195
surrogate=-0.00338, entropy= 0.46631, loss=-0.00338
surrogate=-0.01919, entropy= 0.46619, loss=-0.01919
surrogate=-0.02126, entropy= 0.46550, loss=-0.02126
surrogate=-0.01495, entropy= 0.46599, loss=-0.01495
surrogate=-0.02368, entropy= 0.46539, loss=-0.02368
surrogate=-0.04202, entropy= 0.46510, loss=-0.04202
surrogate=-0.02237, entropy= 0.46520, loss=-0.02237
surrogate=-0.00291, entropy= 0.46533, loss=-0.00291
std_min= 0.22727, std_max= 0.33014, std_mean= 0.28604
val lr: [6.378073770491804e-05], policy lr: [7.653688524590165e-05]
Policy Loss: -0.0029067, | Entropy Bonus: -0, | Value Loss: 4.9037, | Advantage Loss: 1.5415
Time elapsed (s): 1.6430962085723877
Agent stdevs: 0.2860382
--------------------------------------------------------------------------------

Step 727
++++++++ Policy training ++++++++++
Current mean reward: 1642.748722 | mean episode length: 446.666667
val_loss= 9.62535
val_loss= 8.14142
val_loss= 6.26414
val_loss=10.08581
val_loss= 3.89105
val_loss= 6.83596
val_loss= 5.75107
val_loss= 5.39980
val_loss= 5.96842
val_loss= 5.97473
adv_loss= 1.01071
adv_loss= 0.90206
adv_loss= 1.54725
adv_loss= 0.58314
adv_loss= 0.90544
adv_loss= 0.61192
adv_loss= 0.63325
adv_loss= 1.01096
adv_loss= 1.57153
adv_loss= 0.92845
surrogate=-0.00540, entropy= 0.46510, loss=-0.00540
surrogate=-0.00863, entropy= 0.46332, loss=-0.00863
surrogate=-0.00396, entropy= 0.46193, loss=-0.00396
surrogate=-0.00900, entropy= 0.46110, loss=-0.00900
surrogate=-0.01589, entropy= 0.45997, loss=-0.01589
surrogate=-0.00715, entropy= 0.46023, loss=-0.00715
surrogate=-0.03072, entropy= 0.45981, loss=-0.03072
surrogate= 0.00587, entropy= 0.45905, loss= 0.00587
surrogate=-0.03080, entropy= 0.45832, loss=-0.03080
surrogate= 0.00229, entropy= 0.45726, loss= 0.00229
std_min= 0.22648, std_max= 0.32970, std_mean= 0.28530
val lr: [6.352459016393442e-05], policy lr: [7.62295081967213e-05]
Policy Loss: 0.0022909, | Entropy Bonus: -0, | Value Loss: 5.9747, | Advantage Loss: 0.92845
Time elapsed (s): 1.6380515098571777
Agent stdevs: 0.28529742
--------------------------------------------------------------------------------

Step 728
++++++++ Policy training ++++++++++
Current mean reward: 1790.642171 | mean episode length: 489.000000
val_loss= 5.68860
val_loss= 8.52459
val_loss= 8.78909
val_loss= 8.03549
val_loss= 6.80910
val_loss= 6.24063
val_loss= 9.20101
val_loss= 7.51726
val_loss= 6.90214
val_loss= 6.31951
adv_loss= 5.66634
adv_loss= 1.94028
adv_loss= 1.44317
adv_loss= 1.05145
adv_loss= 1.02484
adv_loss= 1.72527
adv_loss= 2.20636
adv_loss= 1.27014
adv_loss= 0.96429
adv_loss= 1.23574
surrogate= 0.01870, entropy= 0.45689, loss= 0.01870
surrogate= 0.00168, entropy= 0.45561, loss= 0.00168
surrogate=-0.01371, entropy= 0.45445, loss=-0.01371
surrogate=-0.01704, entropy= 0.45387, loss=-0.01704
surrogate=-0.00680, entropy= 0.45287, loss=-0.00680
surrogate=-0.01230, entropy= 0.45267, loss=-0.01230
surrogate=-0.01733, entropy= 0.45170, loss=-0.01733
surrogate=-0.01031, entropy= 0.45128, loss=-0.01031
surrogate=-0.02720, entropy= 0.45034, loss=-0.02720
surrogate=-0.01357, entropy= 0.44936, loss=-0.01357
std_min= 0.22499, std_max= 0.33008, std_mean= 0.28469
val lr: [6.326844262295082e-05], policy lr: [7.592213114754097e-05]
Policy Loss: -0.013566, | Entropy Bonus: -0, | Value Loss: 6.3195, | Advantage Loss: 1.2357
Time elapsed (s): 1.6305603981018066
Agent stdevs: 0.28468704
--------------------------------------------------------------------------------

Step 729
++++++++ Policy training ++++++++++
Current mean reward: 1078.497361 | mean episode length: 296.000000
val_loss=12.87997
val_loss=10.34875
val_loss= 4.58965
val_loss= 7.03017
val_loss= 9.39652
val_loss= 5.63831
val_loss= 8.20892
val_loss=13.14979
val_loss= 7.20343
val_loss= 6.79262
adv_loss= 1.48937
adv_loss= 1.39475
adv_loss= 0.97916
adv_loss= 1.04069
adv_loss= 3.42442
adv_loss= 2.38410
adv_loss= 0.77580
adv_loss= 2.19082
adv_loss= 3.17953
adv_loss= 1.82679
surrogate=-0.01769, entropy= 0.44952, loss=-0.01769
surrogate= 0.00698, entropy= 0.44907, loss= 0.00698
surrogate= 0.01407, entropy= 0.44948, loss= 0.01407
surrogate= 0.01041, entropy= 0.44842, loss= 0.01041
surrogate=-0.01772, entropy= 0.44872, loss=-0.01772
surrogate=-0.01382, entropy= 0.44887, loss=-0.01382
surrogate=-0.02935, entropy= 0.44884, loss=-0.02935
surrogate=-0.01161, entropy= 0.44853, loss=-0.01161
surrogate=-0.03026, entropy= 0.44821, loss=-0.03026
surrogate=-0.00056, entropy= 0.44841, loss=-0.00056
std_min= 0.22577, std_max= 0.32904, std_mean= 0.28448
val lr: [6.30122950819672e-05], policy lr: [7.561475409836064e-05]
Policy Loss: -0.0005648, | Entropy Bonus: -0, | Value Loss: 6.7926, | Advantage Loss: 1.8268
Time elapsed (s): 1.6455883979797363
Agent stdevs: 0.2844751
--------------------------------------------------------------------------------

Step 730
++++++++ Policy training ++++++++++
Current mean reward: 2165.396294 | mean episode length: 610.000000
val_loss=426.93228
val_loss=50.71035
val_loss=87.41930
val_loss=175.93071
val_loss=579.22351
val_loss=1084.56775
val_loss=1614.31604
val_loss=277.88461
val_loss=124.44704
val_loss=49.48531
adv_loss= 2.24035
adv_loss= 1.47311
adv_loss= 2.66121
adv_loss= 1.98904
adv_loss= 2.12573
adv_loss= 6.21297
adv_loss= 3.00094
adv_loss= 3.26331
adv_loss= 2.50545
adv_loss= 2.35566
surrogate= 0.03840, entropy= 0.44805, loss= 0.03840
surrogate=-0.01007, entropy= 0.44677, loss=-0.01007
surrogate= 0.02579, entropy= 0.44637, loss= 0.02579
surrogate=-0.02807, entropy= 0.44555, loss=-0.02807
surrogate= 0.00040, entropy= 0.44477, loss= 0.00040
surrogate=-0.00097, entropy= 0.44431, loss=-0.00097
surrogate=-0.03243, entropy= 0.44376, loss=-0.03243
surrogate=-0.01678, entropy= 0.44362, loss=-0.01678
surrogate=-0.02128, entropy= 0.44339, loss=-0.02128
surrogate=-0.02198, entropy= 0.44260, loss=-0.02198
std_min= 0.22581, std_max= 0.32916, std_mean= 0.28390
val lr: [6.27561475409836e-05], policy lr: [7.530737704918032e-05]
Policy Loss: -0.021981, | Entropy Bonus: -0, | Value Loss: 49.485, | Advantage Loss: 2.3557
Time elapsed (s): 1.6615006923675537
Agent stdevs: 0.28389814
--------------------------------------------------------------------------------

Step 731
++++++++ Policy training ++++++++++
Current mean reward: 2242.641484 | mean episode length: 635.000000
val_loss=123.43877
val_loss=62.24603
val_loss=88.62330
val_loss=526.84894
val_loss=324.42816
val_loss=1102.86340
val_loss=62.46644
val_loss=187.47421
val_loss=109.14520
val_loss=99.85369
adv_loss= 1.47360
adv_loss= 3.94272
adv_loss= 2.76410
adv_loss= 2.99479
adv_loss= 2.30316
adv_loss= 2.19062
adv_loss= 2.75423
adv_loss= 3.06397
adv_loss= 2.50978
adv_loss= 1.32323
surrogate=-0.00660, entropy= 0.44327, loss=-0.00660
surrogate= 0.02726, entropy= 0.44461, loss= 0.02726
surrogate=-0.01258, entropy= 0.44544, loss=-0.01258
surrogate=-0.04158, entropy= 0.44612, loss=-0.04158
surrogate=-0.01698, entropy= 0.44729, loss=-0.01698
surrogate=-0.02331, entropy= 0.44813, loss=-0.02331
surrogate=-0.02076, entropy= 0.44957, loss=-0.02076
surrogate=-0.01534, entropy= 0.45041, loss=-0.01534
surrogate=-0.02738, entropy= 0.45095, loss=-0.02738
surrogate= 0.00010, entropy= 0.45110, loss= 0.00010
std_min= 0.22573, std_max= 0.33100, std_mean= 0.28481
val lr: [6.25e-05], policy lr: [7.5e-05]
Policy Loss: 9.9868e-05, | Entropy Bonus: -0, | Value Loss: 99.854, | Advantage Loss: 1.3232
Time elapsed (s): 1.6777238845825195
Agent stdevs: 0.28481194
--------------------------------------------------------------------------------

Step 732
++++++++ Policy training ++++++++++
Current mean reward: 2398.314155 | mean episode length: 677.500000
val_loss=214.11835
val_loss=1297.17981
val_loss=652.96558
val_loss=769.86163
val_loss=46.01037
val_loss=33.33893
val_loss=33.18838
val_loss=317.84555
val_loss=320.95639
val_loss=60.24790
adv_loss= 3.95674
adv_loss= 2.41569
adv_loss= 1.78775
adv_loss= 2.17646
adv_loss= 2.80281
adv_loss= 2.71353
adv_loss= 2.91108
adv_loss= 2.55533
adv_loss= 2.49274
adv_loss= 3.26432
surrogate= 0.02437, entropy= 0.45157, loss= 0.02437
surrogate=-0.01861, entropy= 0.45095, loss=-0.01861
surrogate= 0.02566, entropy= 0.45066, loss= 0.02566
surrogate=-0.00130, entropy= 0.45074, loss=-0.00130
surrogate= 0.00190, entropy= 0.45058, loss= 0.00190
surrogate=-0.01611, entropy= 0.45021, loss=-0.01611
surrogate=-0.00367, entropy= 0.44990, loss=-0.00367
surrogate=-0.00949, entropy= 0.44913, loss=-0.00949
surrogate= 0.01873, entropy= 0.44888, loss= 0.01873
surrogate=-0.02431, entropy= 0.44848, loss=-0.02431
std_min= 0.22574, std_max= 0.32921, std_mean= 0.28449
val lr: [6.22438524590164e-05], policy lr: [7.469262295081967e-05]
Policy Loss: -0.024314, | Entropy Bonus: -0, | Value Loss: 60.248, | Advantage Loss: 3.2643
Time elapsed (s): 1.6553449630737305
Agent stdevs: 0.28448656
--------------------------------------------------------------------------------

Step 733
++++++++ Policy training ++++++++++
Current mean reward: 3486.407045 | mean episode length: 1000.000000
val_loss=396.49316
val_loss=1866.20557
val_loss=160.17004
val_loss=286.86954
val_loss=382.16870
val_loss=851.82098
val_loss=470.82132
val_loss=161.37738
val_loss=922.11224
val_loss=338.70111
adv_loss= 1.40106
adv_loss= 2.53893
adv_loss= 6.12898
adv_loss= 3.47606
adv_loss= 4.11611
adv_loss= 1.61673
adv_loss=1735.53296
adv_loss= 2.21147
adv_loss= 2.30824
adv_loss= 1.62381
surrogate= 0.02593, entropy= 0.44666, loss= 0.02593
surrogate= 0.02256, entropy= 0.44496, loss= 0.02256
surrogate= 0.01264, entropy= 0.44462, loss= 0.01264
surrogate= 0.00434, entropy= 0.44372, loss= 0.00434
surrogate=-0.00882, entropy= 0.44221, loss=-0.00882
surrogate=-0.00007, entropy= 0.44127, loss=-0.00007
surrogate=-0.01362, entropy= 0.44040, loss=-0.01362
surrogate=-0.01937, entropy= 0.43959, loss=-0.01937
surrogate= 0.00582, entropy= 0.43835, loss= 0.00582
surrogate= 0.02056, entropy= 0.43708, loss= 0.02056
std_min= 0.22511, std_max= 0.32749, std_mean= 0.28337
val lr: [6.19877049180328e-05], policy lr: [7.438524590163934e-05]
Policy Loss: 0.020561, | Entropy Bonus: -0, | Value Loss: 338.7, | Advantage Loss: 1.6238
Time elapsed (s): 1.6433947086334229
Agent stdevs: 0.28337076
--------------------------------------------------------------------------------

Step 734
++++++++ Policy training ++++++++++
Current mean reward: 2157.106102 | mean episode length: 591.000000
val_loss=42.29686
val_loss=31.69168
val_loss=23.86673
val_loss=29.26371
val_loss=21.60080
val_loss=24.14453
val_loss=23.56483
val_loss=21.78674
val_loss=21.61115
val_loss=28.23063
adv_loss= 1.50406
adv_loss= 2.00191
adv_loss= 3.87428
adv_loss= 3.60422
adv_loss= 3.11234
adv_loss= 8.22847
adv_loss= 2.23716
adv_loss= 3.46221
adv_loss= 3.27088
adv_loss= 1.74127
surrogate=-0.00120, entropy= 0.43736, loss=-0.00120
surrogate=-0.01030, entropy= 0.43710, loss=-0.01030
surrogate= 0.00862, entropy= 0.43716, loss= 0.00862
surrogate= 0.00670, entropy= 0.43603, loss= 0.00670
surrogate=-0.01057, entropy= 0.43646, loss=-0.01057
surrogate=-0.01477, entropy= 0.43684, loss=-0.01477
surrogate=-0.00897, entropy= 0.43714, loss=-0.00897
surrogate= 0.00030, entropy= 0.43692, loss= 0.00030
surrogate=-0.04119, entropy= 0.43695, loss=-0.04119
surrogate=-0.00892, entropy= 0.43650, loss=-0.00892
std_min= 0.22610, std_max= 0.32577, std_mean= 0.28316
val lr: [6.173155737704919e-05], policy lr: [7.407786885245902e-05]
Policy Loss: -0.0089218, | Entropy Bonus: -0, | Value Loss: 28.231, | Advantage Loss: 1.7413
Time elapsed (s): 1.6642553806304932
Agent stdevs: 0.28315768
--------------------------------------------------------------------------------

Step 735
++++++++ Policy training ++++++++++
Current mean reward: 3467.767191 | mean episode length: 1000.000000
val_loss=146.36005
val_loss=1199.85864
val_loss=1253.68640
val_loss=238.84700
val_loss=649.22375
val_loss=942.88232
val_loss=139.26784
val_loss=3040.23315
val_loss=644.24274
val_loss=613.69855
adv_loss= 6.47782
adv_loss= 7.57632
adv_loss= 4.22881
adv_loss= 4.99653
adv_loss=1444.40393
adv_loss= 4.50722
adv_loss= 1.70316
adv_loss= 2.67475
adv_loss= 7.68446
adv_loss= 4.12588
surrogate=-0.00246, entropy= 0.43661, loss=-0.00246
surrogate= 0.00444, entropy= 0.43696, loss= 0.00444
surrogate=-0.00464, entropy= 0.43724, loss=-0.00464
surrogate=-0.00193, entropy= 0.43733, loss=-0.00193
surrogate=-0.02826, entropy= 0.43772, loss=-0.02826
surrogate=-0.00556, entropy= 0.43763, loss=-0.00556
surrogate=-0.01709, entropy= 0.43773, loss=-0.01709
surrogate=-0.01215, entropy= 0.43809, loss=-0.01215
surrogate=-0.00282, entropy= 0.43817, loss=-0.00282
surrogate=-0.02292, entropy= 0.43842, loss=-0.02292
std_min= 0.22665, std_max= 0.32579, std_mean= 0.28330
val lr: [6.147540983606559e-05], policy lr: [7.377049180327869e-05]
Policy Loss: -0.022924, | Entropy Bonus: -0, | Value Loss: 613.7, | Advantage Loss: 4.1259
Time elapsed (s): 1.6659305095672607
Agent stdevs: 0.283296
--------------------------------------------------------------------------------

Step 736
++++++++ Policy training ++++++++++
Current mean reward: 2232.491094 | mean episode length: 615.333333
val_loss=44.82932
val_loss=31.52122
val_loss=23.07732
val_loss=31.62554
val_loss=18.46994
val_loss=151.34171
val_loss=28.92336
val_loss=20.12569
val_loss=19.72070
val_loss=17.11492
adv_loss= 3.01241
adv_loss= 3.50914
adv_loss= 5.34564
adv_loss= 3.92237
adv_loss= 3.04130
adv_loss= 2.91141
adv_loss= 1.98517
adv_loss= 2.53672
adv_loss= 7.68056
adv_loss= 6.14821
surrogate=-0.00806, entropy= 0.43900, loss=-0.00806
surrogate=-0.01271, entropy= 0.43898, loss=-0.01271
surrogate=-0.00482, entropy= 0.43822, loss=-0.00482
surrogate=-0.02292, entropy= 0.43802, loss=-0.02292
surrogate=-0.01964, entropy= 0.43733, loss=-0.01964
surrogate=-0.00850, entropy= 0.43698, loss=-0.00850
surrogate= 0.00748, entropy= 0.43632, loss= 0.00748
surrogate=-0.02853, entropy= 0.43589, loss=-0.02853
surrogate=-0.03166, entropy= 0.43550, loss=-0.03166
surrogate=-0.02235, entropy= 0.43522, loss=-0.02235
std_min= 0.22588, std_max= 0.32467, std_mean= 0.28302
val lr: [6.121926229508196e-05], policy lr: [7.346311475409834e-05]
Policy Loss: -0.02235, | Entropy Bonus: -0, | Value Loss: 17.115, | Advantage Loss: 6.1482
Time elapsed (s): 1.6615009307861328
Agent stdevs: 0.28302488
--------------------------------------------------------------------------------

Step 737
++++++++ Policy training ++++++++++
Current mean reward: 2118.488739 | mean episode length: 578.333333
val_loss=16.37310
val_loss=21.19710
val_loss=14.26259
val_loss=18.50406
val_loss=17.08910
val_loss=18.81051
val_loss=19.44771
val_loss=12.27181
val_loss= 7.69562
val_loss=15.89207
adv_loss= 2.18188
adv_loss= 1.09321
adv_loss= 1.51366
adv_loss= 3.00551
adv_loss= 1.47457
adv_loss= 1.66678
adv_loss= 2.12960
adv_loss= 1.95815
adv_loss= 2.21708
adv_loss= 3.56691
surrogate= 0.01674, entropy= 0.43497, loss= 0.01674
surrogate= 0.00157, entropy= 0.43413, loss= 0.00157
surrogate=-0.01614, entropy= 0.43369, loss=-0.01614
surrogate=-0.01538, entropy= 0.43316, loss=-0.01538
surrogate=-0.01735, entropy= 0.43160, loss=-0.01735
surrogate=-0.01741, entropy= 0.43134, loss=-0.01741
surrogate=-0.01480, entropy= 0.43085, loss=-0.01480
surrogate=-0.00893, entropy= 0.42968, loss=-0.00893
surrogate=-0.01788, entropy= 0.42925, loss=-0.01788
surrogate= 0.00891, entropy= 0.42839, loss= 0.00891
std_min= 0.22546, std_max= 0.32425, std_mean= 0.28238
val lr: [6.096311475409835e-05], policy lr: [7.315573770491801e-05]
Policy Loss: 0.0089058, | Entropy Bonus: -0, | Value Loss: 15.892, | Advantage Loss: 3.5669
Time elapsed (s): 1.6956408023834229
Agent stdevs: 0.28237653
--------------------------------------------------------------------------------

Step 738
++++++++ Policy training ++++++++++
Current mean reward: 3314.946655 | mean episode length: 919.500000
val_loss=46.76996
val_loss=31.58479
val_loss=79.21979
val_loss=57.28696
val_loss=539.35968
val_loss=48.74218
val_loss=60.90137
val_loss=290.51996
val_loss=34.72972
val_loss=219.25362
adv_loss= 1.77186
adv_loss= 1.32877
adv_loss= 1.31322
adv_loss= 2.22376
adv_loss= 1.06227
adv_loss= 2.16618
adv_loss= 4.31307
adv_loss= 2.46633
adv_loss= 1.02780
adv_loss= 0.77949
surrogate= 0.01086, entropy= 0.42853, loss= 0.01086
surrogate=-0.02267, entropy= 0.42870, loss=-0.02267
surrogate= 0.01133, entropy= 0.42920, loss= 0.01133
surrogate=-0.00607, entropy= 0.42965, loss=-0.00607
surrogate= 0.01631, entropy= 0.42984, loss= 0.01631
surrogate=-0.01547, entropy= 0.42965, loss=-0.01547
surrogate=-0.02815, entropy= 0.43018, loss=-0.02815
surrogate=-0.02776, entropy= 0.43018, loss=-0.02776
surrogate=-0.00918, entropy= 0.43105, loss=-0.00918
surrogate= 0.00254, entropy= 0.43103, loss= 0.00254
std_min= 0.22398, std_max= 0.32631, std_mean= 0.28286
val lr: [6.070696721311475e-05], policy lr: [7.284836065573769e-05]
Policy Loss: 0.0025373, | Entropy Bonus: -0, | Value Loss: 219.25, | Advantage Loss: 0.77949
Time elapsed (s): 1.6565399169921875
Agent stdevs: 0.28286484
--------------------------------------------------------------------------------

Step 739
++++++++ Policy training ++++++++++
Current mean reward: 2305.825749 | mean episode length: 650.000000
val_loss=380.44922
val_loss=52.12068
val_loss=180.45816
val_loss=59.96125
val_loss=89.74468
val_loss=69.94612
val_loss=545.84271
val_loss=1058.91479
val_loss=495.58084
val_loss=354.76788
adv_loss= 3.97158
adv_loss= 2.43767
adv_loss= 1.94604
adv_loss= 3.25625
adv_loss= 3.78226
adv_loss= 2.40133
adv_loss= 5.92714
adv_loss= 1.39668
adv_loss= 1.48504
adv_loss=12.90821
surrogate=-0.01845, entropy= 0.43294, loss=-0.01845
surrogate=-0.01818, entropy= 0.43374, loss=-0.01818
surrogate=-0.01671, entropy= 0.43400, loss=-0.01671
surrogate= 0.00603, entropy= 0.43516, loss= 0.00603
surrogate=-0.01033, entropy= 0.43563, loss=-0.01033
surrogate=-0.01524, entropy= 0.43654, loss=-0.01524
surrogate=-0.01958, entropy= 0.43693, loss=-0.01958
surrogate= 0.00420, entropy= 0.43740, loss= 0.00420
surrogate= 0.00279, entropy= 0.43769, loss= 0.00279
surrogate=-0.02937, entropy= 0.43802, loss=-0.02937
std_min= 0.22391, std_max= 0.32745, std_mean= 0.28360
val lr: [6.0450819672131146e-05], policy lr: [7.254098360655736e-05]
Policy Loss: -0.029366, | Entropy Bonus: -0, | Value Loss: 354.77, | Advantage Loss: 12.908
Time elapsed (s): 1.6594092845916748
Agent stdevs: 0.2835978
--------------------------------------------------------------------------------

Step 740
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2687.9
++++++++ Policy training ++++++++++
Current mean reward: 2368.225912 | mean episode length: 675.500000
val_loss=625.70892
val_loss=36.36229
val_loss=43.55210
val_loss=359.55991
val_loss=102.02328
val_loss=27.89099
val_loss=57.96999
val_loss=36.84148
val_loss=109.46354
val_loss=46.63423
adv_loss= 2.77924
adv_loss= 2.14708
adv_loss= 1.87627
adv_loss= 1.31763
adv_loss= 1.39913
adv_loss= 4.04952
adv_loss= 1.17426
adv_loss= 1.84654
adv_loss= 2.21800
adv_loss= 1.92048
surrogate= 0.00162, entropy= 0.43822, loss= 0.00162
surrogate=-0.01757, entropy= 0.43928, loss=-0.01757
surrogate= 0.03012, entropy= 0.43965, loss= 0.03012
surrogate=-0.01670, entropy= 0.44108, loss=-0.01670
surrogate=-0.02114, entropy= 0.44077, loss=-0.02114
surrogate= 0.01093, entropy= 0.44139, loss= 0.01093
surrogate= 0.01050, entropy= 0.44173, loss= 0.01050
surrogate=-0.01922, entropy= 0.44240, loss=-0.01922
surrogate=-0.01477, entropy= 0.44248, loss=-0.01477
surrogate=-0.00849, entropy= 0.44277, loss=-0.00849
std_min= 0.22424, std_max= 0.32694, std_mean= 0.28402
val lr: [6.0194672131147546e-05], policy lr: [7.223360655737705e-05]
Policy Loss: -0.0084874, | Entropy Bonus: -0, | Value Loss: 46.634, | Advantage Loss: 1.9205
Time elapsed (s): 1.6678340435028076
Agent stdevs: 0.28402266
--------------------------------------------------------------------------------

Step 741
++++++++ Policy training ++++++++++
Current mean reward: 2305.241334 | mean episode length: 645.333333
val_loss=1078.81042
val_loss=369.79675
val_loss=266.69385
val_loss=68.30252
val_loss=63.96740
val_loss=1278.89771
val_loss=127.70996
val_loss=69.12180
val_loss=1117.38782
val_loss=656.10101
adv_loss= 2.66738
adv_loss= 2.29520
adv_loss= 1.63466
adv_loss= 1.55196
adv_loss= 2.33566
adv_loss= 2.97036
adv_loss= 8.11078
adv_loss= 2.41963
adv_loss= 2.59502
adv_loss= 2.66244
surrogate=-0.01662, entropy= 0.44268, loss=-0.01662
surrogate= 0.00939, entropy= 0.44315, loss= 0.00939
surrogate=-0.03210, entropy= 0.44288, loss=-0.03210
surrogate=-0.02159, entropy= 0.44229, loss=-0.02159
surrogate= 0.01620, entropy= 0.44195, loss= 0.01620
surrogate=-0.02564, entropy= 0.44131, loss=-0.02564
surrogate= 0.01113, entropy= 0.44123, loss= 0.01113
surrogate=-0.01034, entropy= 0.44145, loss=-0.01034
surrogate=-0.01708, entropy= 0.44065, loss=-0.01708
surrogate=-0.02652, entropy= 0.44068, loss=-0.02652
std_min= 0.22400, std_max= 0.32646, std_mean= 0.28383
val lr: [5.993852459016394e-05], policy lr: [7.192622950819672e-05]
Policy Loss: -0.026516, | Entropy Bonus: -0, | Value Loss: 656.1, | Advantage Loss: 2.6624
Time elapsed (s): 1.6571366786956787
Agent stdevs: 0.28382504
--------------------------------------------------------------------------------

Step 742
++++++++ Policy training ++++++++++
Current mean reward: 3147.574220 | mean episode length: 881.500000
val_loss=183.81657
val_loss=243.09186
val_loss=21.12060
val_loss=1431.97864
val_loss=80.72580
val_loss=24.93208
val_loss=21.09451
val_loss=1277.08789
val_loss=159.59596
val_loss=183.93710
adv_loss= 1.79819
adv_loss= 1.13232
adv_loss= 2.36700
adv_loss= 2.46599
adv_loss= 0.82713
adv_loss= 1.35704
adv_loss= 1.73963
adv_loss= 2.42371
adv_loss= 3.56324
adv_loss= 1.18076
surrogate=-0.02668, entropy= 0.44103, loss=-0.02668
surrogate=-0.00262, entropy= 0.44229, loss=-0.00262
surrogate=-0.00997, entropy= 0.44308, loss=-0.00997
surrogate= 0.01326, entropy= 0.44387, loss= 0.01326
surrogate=-0.00499, entropy= 0.44464, loss=-0.00499
surrogate= 0.00255, entropy= 0.44580, loss= 0.00255
surrogate=-0.01602, entropy= 0.44640, loss=-0.01602
surrogate=-0.03051, entropy= 0.44646, loss=-0.03051
surrogate= 0.01928, entropy= 0.44701, loss= 0.01928
surrogate=-0.03499, entropy= 0.44745, loss=-0.03499
std_min= 0.22466, std_max= 0.32597, std_mean= 0.28442
val lr: [5.968237704918034e-05], policy lr: [7.16188524590164e-05]
Policy Loss: -0.034987, | Entropy Bonus: -0, | Value Loss: 183.94, | Advantage Loss: 1.1808
Time elapsed (s): 1.6767518520355225
Agent stdevs: 0.28442308
--------------------------------------------------------------------------------

Step 743
++++++++ Policy training ++++++++++
Current mean reward: 2021.439194 | mean episode length: 565.666667
val_loss=276.23572
val_loss=52.89973
val_loss=126.48414
val_loss=433.80652
val_loss=300.34164
val_loss=79.50662
val_loss=18.71908
val_loss=593.11377
val_loss=129.21434
val_loss=30.35480
adv_loss= 2.87843
adv_loss= 2.57575
adv_loss= 1.60144
adv_loss= 2.10932
adv_loss= 1.73189
adv_loss= 1.16245
adv_loss= 3.16819
adv_loss= 0.78439
adv_loss= 2.51483
adv_loss= 2.07594
surrogate= 0.02033, entropy= 0.44730, loss= 0.02033
surrogate=-0.02106, entropy= 0.44687, loss=-0.02106
surrogate= 0.01323, entropy= 0.44672, loss= 0.01323
surrogate= 0.04020, entropy= 0.44670, loss= 0.04020
surrogate= 0.02301, entropy= 0.44675, loss= 0.02301
surrogate=-0.01474, entropy= 0.44689, loss=-0.01474
surrogate=-0.01096, entropy= 0.44594, loss=-0.01096
surrogate=-0.00643, entropy= 0.44648, loss=-0.00643
surrogate=-0.02239, entropy= 0.44668, loss=-0.02239
surrogate= 0.01301, entropy= 0.44654, loss= 0.01301
std_min= 0.22392, std_max= 0.32666, std_mean= 0.28443
val lr: [5.942622950819673e-05], policy lr: [7.131147540983607e-05]
Policy Loss: 0.013007, | Entropy Bonus: -0, | Value Loss: 30.355, | Advantage Loss: 2.0759
Time elapsed (s): 1.6511321067810059
Agent stdevs: 0.28442886
--------------------------------------------------------------------------------

Step 744
++++++++ Policy training ++++++++++
Current mean reward: 2240.443225 | mean episode length: 606.000000
val_loss=18.84002
val_loss=18.22322
val_loss=17.48042
val_loss=14.65410
val_loss= 9.09801
val_loss= 9.81104
val_loss=15.19136
val_loss=11.21613
val_loss= 5.27138
val_loss= 8.10926
adv_loss= 1.57741
adv_loss= 2.01799
adv_loss= 3.42495
adv_loss= 2.45935
adv_loss= 2.54846
adv_loss= 1.13340
adv_loss= 2.15499
adv_loss= 1.98912
adv_loss= 4.49291
adv_loss= 2.19695
surrogate= 0.00040, entropy= 0.44670, loss= 0.00040
surrogate= 0.00196, entropy= 0.44614, loss= 0.00196
surrogate=-0.02864, entropy= 0.44633, loss=-0.02864
surrogate=-0.01031, entropy= 0.44655, loss=-0.01031
surrogate=-0.00686, entropy= 0.44632, loss=-0.00686
surrogate=-0.02702, entropy= 0.44676, loss=-0.02702
surrogate=-0.01703, entropy= 0.44641, loss=-0.01703
surrogate=-0.04276, entropy= 0.44637, loss=-0.04276
surrogate=-0.04105, entropy= 0.44607, loss=-0.04105
surrogate=-0.01144, entropy= 0.44590, loss=-0.01144
std_min= 0.22323, std_max= 0.32796, std_mean= 0.28448
val lr: [5.917008196721313e-05], policy lr: [7.100409836065575e-05]
Policy Loss: -0.011445, | Entropy Bonus: -0, | Value Loss: 8.1093, | Advantage Loss: 2.1969
Time elapsed (s): 1.6471071243286133
Agent stdevs: 0.28448078
--------------------------------------------------------------------------------

Step 745
++++++++ Policy training ++++++++++
Current mean reward: 3576.875244 | mean episode length: 998.000000
val_loss=1677.45740
val_loss=38.96065
val_loss=70.19624
val_loss=1024.74377
val_loss=292.04480
val_loss=16.42650
val_loss=1690.92480
val_loss=907.46088
val_loss=793.85785
val_loss=1674.10901
adv_loss= 1.69527
adv_loss= 1.51112
adv_loss= 0.79319
adv_loss= 0.70600
adv_loss= 2.78137
adv_loss= 1.61506
adv_loss= 0.76440
adv_loss= 1.43729
adv_loss= 1.33318
adv_loss= 1.08286
surrogate=-0.01195, entropy= 0.44570, loss=-0.01195
surrogate= 0.01535, entropy= 0.44576, loss= 0.01535
surrogate=-0.00455, entropy= 0.44575, loss=-0.00455
surrogate=-0.01328, entropy= 0.44574, loss=-0.01328
surrogate=-0.01702, entropy= 0.44534, loss=-0.01702
surrogate=-0.01018, entropy= 0.44514, loss=-0.01018
surrogate=-0.00087, entropy= 0.44540, loss=-0.00087
surrogate=-0.01608, entropy= 0.44548, loss=-0.01608
surrogate=-0.01542, entropy= 0.44485, loss=-0.01542
surrogate=-0.02522, entropy= 0.44469, loss=-0.02522
std_min= 0.22379, std_max= 0.32806, std_mean= 0.28430
val lr: [5.89139344262295e-05], policy lr: [7.06967213114754e-05]
Policy Loss: -0.025218, | Entropy Bonus: -0, | Value Loss: 1674.1, | Advantage Loss: 1.0829
Time elapsed (s): 1.6529157161712646
Agent stdevs: 0.28429785
--------------------------------------------------------------------------------

Step 746
++++++++ Policy training ++++++++++
Current mean reward: 2058.718759 | mean episode length: 576.000000
val_loss=231.90703
val_loss=2020.91919
val_loss=598.57336
val_loss=199.28021
val_loss=198.90733
val_loss=31.18706
val_loss=441.34784
val_loss=581.41693
val_loss=1702.64697
val_loss=35.31866
adv_loss= 1.55025
adv_loss= 2.17255
adv_loss= 1.64813
adv_loss= 0.95759
adv_loss= 2.64533
adv_loss= 2.99911
adv_loss= 3.04867
adv_loss= 4.34447
adv_loss= 1.76858
adv_loss= 1.46124
surrogate=-0.00167, entropy= 0.44465, loss=-0.00167
surrogate= 0.02882, entropy= 0.44362, loss= 0.02882
surrogate=-0.00274, entropy= 0.44367, loss=-0.00274
surrogate=-0.00538, entropy= 0.44379, loss=-0.00538
surrogate=-0.01328, entropy= 0.44337, loss=-0.01328
surrogate=-0.04051, entropy= 0.44303, loss=-0.04051
surrogate=-0.01803, entropy= 0.44342, loss=-0.01803
surrogate= 0.01262, entropy= 0.44284, loss= 0.01262
surrogate=-0.02992, entropy= 0.44193, loss=-0.02992
surrogate= 0.00999, entropy= 0.44213, loss= 0.00999
std_min= 0.22317, std_max= 0.32779, std_mean= 0.28410
val lr: [5.86577868852459e-05], policy lr: [7.038934426229507e-05]
Policy Loss: 0.00999, | Entropy Bonus: -0, | Value Loss: 35.319, | Advantage Loss: 1.4612
Time elapsed (s): 1.6553280353546143
Agent stdevs: 0.28410295
--------------------------------------------------------------------------------

Step 747
++++++++ Policy training ++++++++++
Current mean reward: 2605.069587 | mean episode length: 732.000000
val_loss=150.33179
val_loss=41.11016
val_loss=79.95804
val_loss=277.70108
val_loss=635.79755
val_loss=79.05833
val_loss=699.70685
val_loss=1479.19849
val_loss=562.50433
val_loss=174.59772
adv_loss= 3.67568
adv_loss= 3.04471
adv_loss= 3.62044
adv_loss= 2.71409
adv_loss= 1.68862
adv_loss= 2.08752
adv_loss= 1.67296
adv_loss= 1.84303
adv_loss=12.63569
adv_loss= 2.11109
surrogate=-0.00690, entropy= 0.44143, loss=-0.00690
surrogate=-0.02293, entropy= 0.44076, loss=-0.02293
surrogate=-0.01243, entropy= 0.43985, loss=-0.01243
surrogate=-0.03182, entropy= 0.43980, loss=-0.03182
surrogate= 0.02054, entropy= 0.43907, loss= 0.02054
surrogate=-0.00961, entropy= 0.43844, loss=-0.00961
surrogate=-0.00334, entropy= 0.43766, loss=-0.00334
surrogate=-0.01241, entropy= 0.43727, loss=-0.01241
surrogate=-0.02708, entropy= 0.43673, loss=-0.02708
surrogate= 0.00605, entropy= 0.43600, loss= 0.00605
std_min= 0.22317, std_max= 0.32691, std_mean= 0.28346
val lr: [5.840163934426229e-05], policy lr: [7.008196721311474e-05]
Policy Loss: 0.0060507, | Entropy Bonus: -0, | Value Loss: 174.6, | Advantage Loss: 2.1111
Time elapsed (s): 1.6559638977050781
Agent stdevs: 0.28346324
--------------------------------------------------------------------------------

Step 748
++++++++ Policy training ++++++++++
Current mean reward: 1548.700194 | mean episode length: 420.000000
val_loss=15.48574
val_loss= 8.46731
val_loss=22.69878
val_loss= 9.94712
val_loss= 7.91174
val_loss= 8.85080
val_loss=10.04871
val_loss=13.42348
val_loss= 5.39134
val_loss=13.49754
adv_loss= 1.57704
adv_loss= 2.63866
adv_loss= 1.34323
adv_loss= 1.62719
adv_loss= 2.12625
adv_loss= 1.67374
adv_loss= 3.08662
adv_loss= 1.67325
adv_loss= 0.92064
adv_loss= 1.59755
surrogate=-0.01225, entropy= 0.43561, loss=-0.01225
surrogate=-0.00007, entropy= 0.43437, loss=-0.00007
surrogate=-0.02860, entropy= 0.43340, loss=-0.02860
surrogate= 0.00595, entropy= 0.43335, loss= 0.00595
surrogate= 0.00679, entropy= 0.43291, loss= 0.00679
surrogate=-0.02358, entropy= 0.43228, loss=-0.02358
surrogate=-0.01957, entropy= 0.43142, loss=-0.01957
surrogate= 0.00216, entropy= 0.43132, loss= 0.00216
surrogate= 0.00070, entropy= 0.43094, loss= 0.00070
surrogate=-0.01417, entropy= 0.43019, loss=-0.01417
std_min= 0.22357, std_max= 0.32597, std_mean= 0.28281
val lr: [5.814549180327869e-05], policy lr: [6.977459016393442e-05]
Policy Loss: -0.014172, | Entropy Bonus: -0, | Value Loss: 13.498, | Advantage Loss: 1.5976
Time elapsed (s): 1.6701276302337646
Agent stdevs: 0.2828121
--------------------------------------------------------------------------------

Step 749
++++++++ Policy training ++++++++++
Current mean reward: 1835.208892 | mean episode length: 499.750000
val_loss=14.42290
val_loss=13.53969
val_loss=10.14032
val_loss=16.50580
val_loss=14.91620
val_loss=10.67035
val_loss=13.16747
val_loss=18.74750
val_loss= 7.64171
val_loss= 5.67008
adv_loss= 1.52445
adv_loss= 1.61683
adv_loss= 1.07760
adv_loss= 2.67087
adv_loss= 1.62329
adv_loss= 2.55318
adv_loss= 5.27297
adv_loss= 0.83540
adv_loss= 4.43852
adv_loss= 6.65870
surrogate= 0.01730, entropy= 0.43107, loss= 0.01730
surrogate=-0.00446, entropy= 0.43138, loss=-0.00446
surrogate=-0.01053, entropy= 0.43156, loss=-0.01053
surrogate=-0.02680, entropy= 0.43200, loss=-0.02680
surrogate=-0.02120, entropy= 0.43227, loss=-0.02120
surrogate=-0.00756, entropy= 0.43309, loss=-0.00756
surrogate=-0.01074, entropy= 0.43340, loss=-0.01074
surrogate=-0.01016, entropy= 0.43374, loss=-0.01016
surrogate=-0.01741, entropy= 0.43441, loss=-0.01741
surrogate=-0.00284, entropy= 0.43490, loss=-0.00284
std_min= 0.22505, std_max= 0.32666, std_mean= 0.28314
val lr: [5.7889344262295084e-05], policy lr: [6.946721311475409e-05]
Policy Loss: -0.00284, | Entropy Bonus: -0, | Value Loss: 5.6701, | Advantage Loss: 6.6587
Time elapsed (s): 1.6605134010314941
Agent stdevs: 0.28314236
--------------------------------------------------------------------------------

Step 750
++++++++ Policy training ++++++++++
Current mean reward: 2421.300612 | mean episode length: 659.000000
val_loss=17.66428
val_loss=17.77443
val_loss=17.41178
val_loss= 7.57561
val_loss=16.20042
val_loss= 9.77166
val_loss= 8.05707
val_loss= 8.14910
val_loss= 8.52921
val_loss=10.22105
adv_loss= 1.84966
adv_loss= 1.85803
adv_loss= 0.84548
adv_loss= 1.17004
adv_loss= 1.40984
adv_loss= 2.19933
adv_loss= 3.27021
adv_loss= 2.93438
adv_loss= 0.62628
adv_loss= 1.81735
surrogate= 0.03497, entropy= 0.43508, loss= 0.03497
surrogate=-0.04241, entropy= 0.43510, loss=-0.04241
surrogate=-0.01571, entropy= 0.43496, loss=-0.01571
surrogate=-0.01271, entropy= 0.43514, loss=-0.01271
surrogate=-0.01987, entropy= 0.43460, loss=-0.01987
surrogate=-0.03203, entropy= 0.43452, loss=-0.03203
surrogate= 0.00564, entropy= 0.43376, loss= 0.00564
surrogate=-0.02259, entropy= 0.43350, loss=-0.02259
surrogate=-0.00432, entropy= 0.43348, loss=-0.00432
surrogate=-0.01010, entropy= 0.43359, loss=-0.01010
std_min= 0.22552, std_max= 0.32577, std_mean= 0.28293
val lr: [5.7633196721311484e-05], policy lr: [6.915983606557377e-05]
Policy Loss: -0.010097, | Entropy Bonus: -0, | Value Loss: 10.221, | Advantage Loss: 1.8173
Time elapsed (s): 1.6421842575073242
Agent stdevs: 0.28293058
--------------------------------------------------------------------------------

Step 751
++++++++ Policy training ++++++++++
Current mean reward: 3336.620097 | mean episode length: 928.000000
val_loss=757.38647
val_loss=113.63639
val_loss=417.04602
val_loss=644.49921
val_loss=46.44802
val_loss=489.49985
val_loss=43.08966
val_loss=507.41788
val_loss=511.88477
val_loss=179.53276
adv_loss= 3.42094
adv_loss= 2.38240
adv_loss= 2.15516
adv_loss= 1.21856
adv_loss= 7.29642
adv_loss= 4.13572
adv_loss= 1.76163
adv_loss= 2.44528
adv_loss= 5.43363
adv_loss= 1.17363
surrogate= 0.05061, entropy= 0.43372, loss= 0.05061
surrogate=-0.02879, entropy= 0.43407, loss=-0.02879
surrogate=-0.01856, entropy= 0.43417, loss=-0.01856
surrogate= 0.00225, entropy= 0.43426, loss= 0.00225
surrogate= 0.00873, entropy= 0.43402, loss= 0.00873
surrogate=-0.00081, entropy= 0.43436, loss=-0.00081
surrogate=-0.01436, entropy= 0.43447, loss=-0.01436
surrogate= 0.00794, entropy= 0.43466, loss= 0.00794
surrogate=-0.01698, entropy= 0.43401, loss=-0.01698
surrogate=-0.03090, entropy= 0.43401, loss=-0.03090
std_min= 0.22484, std_max= 0.32589, std_mean= 0.28305
val lr: [5.737704918032788e-05], policy lr: [6.885245901639344e-05]
Policy Loss: -0.030899, | Entropy Bonus: -0, | Value Loss: 179.53, | Advantage Loss: 1.1736
Time elapsed (s): 1.6521599292755127
Agent stdevs: 0.28304994
--------------------------------------------------------------------------------

Step 752
++++++++ Policy training ++++++++++
Current mean reward: 2138.869004 | mean episode length: 594.333333
val_loss=804.05957
val_loss=33.32049
val_loss=40.94194
val_loss=330.35440
val_loss=812.14612
val_loss=79.19042
val_loss=871.82465
val_loss=137.45062
val_loss=240.01442
val_loss=65.36290
adv_loss= 2.32398
adv_loss= 0.86426
adv_loss= 1.29239
adv_loss=1608.99316
adv_loss= 1.58790
adv_loss= 1.09253
adv_loss= 1.53301
adv_loss= 2.21461
adv_loss= 1.50424
adv_loss= 1.54613
surrogate= 0.02213, entropy= 0.43341, loss= 0.02213
surrogate=-0.02080, entropy= 0.43175, loss=-0.02080
surrogate=-0.02412, entropy= 0.43090, loss=-0.02412
surrogate=-0.03052, entropy= 0.43044, loss=-0.03052
surrogate= 0.00946, entropy= 0.43076, loss= 0.00946
surrogate=-0.00979, entropy= 0.43066, loss=-0.00979
surrogate=-0.01775, entropy= 0.43059, loss=-0.01775
surrogate=-0.02200, entropy= 0.43078, loss=-0.02200
surrogate=-0.01266, entropy= 0.43032, loss=-0.01266
surrogate=-0.02860, entropy= 0.42996, loss=-0.02860
std_min= 0.22569, std_max= 0.32505, std_mean= 0.28253
val lr: [5.712090163934428e-05], policy lr: [6.854508196721312e-05]
Policy Loss: -0.028605, | Entropy Bonus: -0, | Value Loss: 65.363, | Advantage Loss: 1.5461
Time elapsed (s): 1.6505529880523682
Agent stdevs: 0.28253293
--------------------------------------------------------------------------------

Step 753
++++++++ Policy training ++++++++++
Current mean reward: 3203.600100 | mean episode length: 902.500000
val_loss=1003.82202
val_loss=876.85364
val_loss=2135.45215
val_loss=42.25858
val_loss=185.30957
val_loss=57.59300
val_loss=452.72101
val_loss=620.96338
val_loss=37.84629
val_loss=1107.72058
adv_loss= 0.97165
adv_loss= 1.83895
adv_loss= 1.35611
adv_loss= 1.47473
adv_loss= 7.71743
adv_loss= 2.54992
adv_loss= 1.42097
adv_loss= 0.98825
adv_loss= 2.53383
adv_loss= 1.30422
surrogate= 0.01018, entropy= 0.42985, loss= 0.01018
surrogate=-0.00312, entropy= 0.42948, loss=-0.00312
surrogate=-0.00273, entropy= 0.42936, loss=-0.00273
surrogate= 0.01222, entropy= 0.42977, loss= 0.01222
surrogate=-0.00421, entropy= 0.42969, loss=-0.00421
surrogate= 0.00874, entropy= 0.42991, loss= 0.00874
surrogate=-0.02569, entropy= 0.42966, loss=-0.02569
surrogate=-0.01683, entropy= 0.42918, loss=-0.01683
surrogate= 0.03809, entropy= 0.42861, loss= 0.03809
surrogate= 0.00902, entropy= 0.42844, loss= 0.00902
std_min= 0.22623, std_max= 0.32408, std_mean= 0.28230
val lr: [5.686475409836064e-05], policy lr: [6.823770491803276e-05]
Policy Loss: 0.0090207, | Entropy Bonus: -0, | Value Loss: 1107.7, | Advantage Loss: 1.3042
Time elapsed (s): 1.6647419929504395
Agent stdevs: 0.28229734
--------------------------------------------------------------------------------

Step 754
++++++++ Policy training ++++++++++
Current mean reward: 1903.494663 | mean episode length: 531.666667
val_loss=102.37286
val_loss=111.10322
val_loss=311.42441
val_loss=694.02936
val_loss=74.41600
val_loss=32.08488
val_loss=56.03496
val_loss=571.06732
val_loss=553.17767
val_loss=106.96291
adv_loss= 1.64406
adv_loss= 1.57198
adv_loss= 1.48964
adv_loss= 2.05790
adv_loss= 1.35343
adv_loss= 0.97391
adv_loss= 1.29520
adv_loss= 1.06600
adv_loss= 1.70931
adv_loss= 1.60688
surrogate=-0.00279, entropy= 0.42737, loss=-0.00279
surrogate=-0.00238, entropy= 0.42667, loss=-0.00238
surrogate= 0.00221, entropy= 0.42621, loss= 0.00221
surrogate=-0.00471, entropy= 0.42551, loss=-0.00471
surrogate=-0.00870, entropy= 0.42536, loss=-0.00870
surrogate=-0.02057, entropy= 0.42496, loss=-0.02057
surrogate=-0.00476, entropy= 0.42469, loss=-0.00476
surrogate=-0.00531, entropy= 0.42464, loss=-0.00531
surrogate=-0.03181, entropy= 0.42438, loss=-0.03181
surrogate=-0.02621, entropy= 0.42365, loss=-0.02621
std_min= 0.22541, std_max= 0.32319, std_mean= 0.28188
val lr: [5.660860655737704e-05], policy lr: [6.793032786885244e-05]
Policy Loss: -0.02621, | Entropy Bonus: -0, | Value Loss: 106.96, | Advantage Loss: 1.6069
Time elapsed (s): 1.6691880226135254
Agent stdevs: 0.281883
--------------------------------------------------------------------------------

Step 755
++++++++ Policy training ++++++++++
Current mean reward: 2269.364004 | mean episode length: 634.666667
val_loss=103.52120
val_loss=42.64278
val_loss=688.26019
val_loss=32.96288
val_loss=102.20360
val_loss=48.24382
val_loss=1343.50146
val_loss=1456.72998
val_loss=55.29008
val_loss=240.73376
adv_loss= 2.28344
adv_loss= 3.50498
adv_loss= 1.67125
adv_loss= 2.21808
adv_loss= 2.77039
adv_loss= 2.17177
adv_loss= 2.84458
adv_loss= 2.38138
adv_loss= 2.99442
adv_loss= 4.44566
surrogate=-0.02293, entropy= 0.42360, loss=-0.02293
surrogate= 0.00152, entropy= 0.42401, loss= 0.00152
surrogate=-0.01464, entropy= 0.42465, loss=-0.01464
surrogate= 0.01746, entropy= 0.42518, loss= 0.01746
surrogate= 0.01922, entropy= 0.42539, loss= 0.01922
surrogate=-0.01336, entropy= 0.42561, loss=-0.01336
surrogate=-0.02048, entropy= 0.42617, loss=-0.02048
surrogate=-0.02426, entropy= 0.42665, loss=-0.02426
surrogate=-0.02427, entropy= 0.42737, loss=-0.02427
surrogate= 0.00059, entropy= 0.42733, loss= 0.00059
std_min= 0.22583, std_max= 0.32307, std_mean= 0.28220
val lr: [5.6352459016393436e-05], policy lr: [6.762295081967211e-05]
Policy Loss: 0.00058695, | Entropy Bonus: -0, | Value Loss: 240.73, | Advantage Loss: 4.4457
Time elapsed (s): 1.672196388244629
Agent stdevs: 0.28220487
--------------------------------------------------------------------------------

Step 756
++++++++ Policy training ++++++++++
Current mean reward: 3249.755958 | mean episode length: 912.500000
val_loss=149.19771
val_loss=39.05374
val_loss=789.74188
val_loss=23.01547
val_loss=23.77700
val_loss=3852.23193
val_loss=24.80157
val_loss=48.18005
val_loss=897.71497
val_loss=25.37505
adv_loss= 3.42955
adv_loss= 2.61217
adv_loss= 1.07459
adv_loss= 1.48629
adv_loss= 1.44453
adv_loss= 1.80691
adv_loss= 1.82790
adv_loss= 1.35745
adv_loss= 2.27671
adv_loss= 2.18298
surrogate=-0.01673, entropy= 0.42658, loss=-0.01673
surrogate= 0.01858, entropy= 0.42676, loss= 0.01858
surrogate= 0.00617, entropy= 0.42544, loss= 0.00617
surrogate=-0.01023, entropy= 0.42492, loss=-0.01023
surrogate= 0.01005, entropy= 0.42514, loss= 0.01005
surrogate=-0.00635, entropy= 0.42473, loss=-0.00635
surrogate=-0.02368, entropy= 0.42395, loss=-0.02368
surrogate=-0.01692, entropy= 0.42389, loss=-0.01692
surrogate=-0.01004, entropy= 0.42324, loss=-0.01004
surrogate= 0.02337, entropy= 0.42209, loss= 0.02337
std_min= 0.22547, std_max= 0.32213, std_mean= 0.28170
val lr: [5.6096311475409836e-05], policy lr: [6.731557377049179e-05]
Policy Loss: 0.023375, | Entropy Bonus: -0, | Value Loss: 25.375, | Advantage Loss: 2.183
Time elapsed (s): 1.6773476600646973
Agent stdevs: 0.2816965
--------------------------------------------------------------------------------

Step 757
++++++++ Policy training ++++++++++
Current mean reward: 2100.580805 | mean episode length: 589.333333
val_loss=343.10803
val_loss=97.17661
val_loss=775.06390
val_loss=38.94113
val_loss=573.30872
val_loss=669.59216
val_loss=636.42578
val_loss=45.80734
val_loss=29.46052
val_loss=451.03711
adv_loss= 0.99008
adv_loss= 0.63693
adv_loss= 2.47753
adv_loss= 1.41190
adv_loss= 3.16605
adv_loss= 1.83662
adv_loss= 1.67070
adv_loss= 2.06720
adv_loss= 4.12394
adv_loss= 3.64128
surrogate= 0.01253, entropy= 0.42154, loss= 0.01253
surrogate=-0.02260, entropy= 0.42174, loss=-0.02260
surrogate= 0.00730, entropy= 0.42129, loss= 0.00730
surrogate= 0.00859, entropy= 0.42095, loss= 0.00859
surrogate=-0.00689, entropy= 0.42104, loss=-0.00689
surrogate=-0.00587, entropy= 0.42148, loss=-0.00587
surrogate=-0.00007, entropy= 0.42136, loss=-0.00007
surrogate=-0.00777, entropy= 0.42156, loss=-0.00777
surrogate= 0.00797, entropy= 0.42146, loss= 0.00797
surrogate=-0.02050, entropy= 0.42180, loss=-0.02050
std_min= 0.22618, std_max= 0.32054, std_mean= 0.28155
val lr: [5.584016393442623e-05], policy lr: [6.700819672131147e-05]
Policy Loss: -0.020503, | Entropy Bonus: -0, | Value Loss: 451.04, | Advantage Loss: 3.6413
Time elapsed (s): 1.6888096332550049
Agent stdevs: 0.28155324
--------------------------------------------------------------------------------

Step 758
++++++++ Policy training ++++++++++
Current mean reward: 3390.317158 | mean episode length: 928.500000
val_loss=19.26224
val_loss=19.67903
val_loss=17.86521
val_loss=11.43842
val_loss=14.52845
val_loss= 6.15617
val_loss=21.99361
val_loss=13.23880
val_loss=12.02816
val_loss=19.38532
adv_loss= 1.19674
adv_loss= 1.13615
adv_loss= 1.85730
adv_loss= 0.79970
adv_loss= 1.29537
adv_loss= 1.18512
adv_loss= 2.44853
adv_loss= 2.90242
adv_loss= 1.25087
adv_loss= 1.30716
surrogate=-0.01418, entropy= 0.42143, loss=-0.01418
surrogate=-0.01154, entropy= 0.42080, loss=-0.01154
surrogate= 0.01789, entropy= 0.42012, loss= 0.01789
surrogate=-0.01534, entropy= 0.41971, loss=-0.01534
surrogate= 0.00097, entropy= 0.41929, loss= 0.00097
surrogate=-0.00923, entropy= 0.41814, loss=-0.00923
surrogate= 0.00248, entropy= 0.41690, loss= 0.00248
surrogate=-0.00411, entropy= 0.41631, loss=-0.00411
surrogate= 0.00482, entropy= 0.41562, loss= 0.00482
surrogate=-0.04411, entropy= 0.41469, loss=-0.04411
std_min= 0.22525, std_max= 0.31980, std_mean= 0.28093
val lr: [5.558401639344263e-05], policy lr: [6.670081967213115e-05]
Policy Loss: -0.044112, | Entropy Bonus: -0, | Value Loss: 19.385, | Advantage Loss: 1.3072
Time elapsed (s): 1.6781911849975586
Agent stdevs: 0.28092885
--------------------------------------------------------------------------------

Step 759
++++++++ Policy training ++++++++++
Current mean reward: 2101.414923 | mean episode length: 570.000000
val_loss=13.08669
val_loss=17.10785
val_loss=19.46403
val_loss=13.66116
val_loss=15.11640
val_loss=15.05231
val_loss=10.96828
val_loss=12.09493
val_loss= 8.95423
val_loss=14.91079
adv_loss= 0.87644
adv_loss= 0.85575
adv_loss= 1.81413
adv_loss= 1.18415
adv_loss= 1.87495
adv_loss= 1.45119
adv_loss= 1.10770
adv_loss= 3.08116
adv_loss= 1.79658
adv_loss= 1.78414
surrogate= 0.00894, entropy= 0.41417, loss= 0.00894
surrogate=-0.01042, entropy= 0.41425, loss=-0.01042
surrogate=-0.00954, entropy= 0.41464, loss=-0.00954
surrogate= 0.00607, entropy= 0.41487, loss= 0.00607
surrogate= 0.01634, entropy= 0.41448, loss= 0.01634
surrogate=-0.02423, entropy= 0.41452, loss=-0.02423
surrogate= 0.00489, entropy= 0.41464, loss= 0.00489
surrogate=-0.02651, entropy= 0.41422, loss=-0.02651
surrogate=-0.01201, entropy= 0.41403, loss=-0.01201
surrogate= 0.00043, entropy= 0.41369, loss= 0.00043
std_min= 0.22525, std_max= 0.31923, std_mean= 0.28082
val lr: [5.532786885245902e-05], policy lr: [6.639344262295082e-05]
Policy Loss: 0.00042882, | Entropy Bonus: -0, | Value Loss: 14.911, | Advantage Loss: 1.7841
Time elapsed (s): 1.6544039249420166
Agent stdevs: 0.2808189
--------------------------------------------------------------------------------

Step 760
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2622.3
++++++++ Policy training ++++++++++
Current mean reward: 2336.622056 | mean episode length: 650.000000
val_loss=841.27954
val_loss=59.99830
val_loss=96.01078
val_loss=24.08610
val_loss=433.68411
val_loss=18.34373
val_loss=32.08726
val_loss=513.38843
val_loss=23.79086
val_loss=69.25801
adv_loss= 1.14514
adv_loss= 2.04205
adv_loss=1291.62573
adv_loss= 1.57935
adv_loss= 1.14124
adv_loss= 2.21048
adv_loss= 1.54512
adv_loss= 1.56052
adv_loss= 1.41350
adv_loss= 4.04146
surrogate= 0.02146, entropy= 0.41311, loss= 0.02146
surrogate=-0.00712, entropy= 0.41321, loss=-0.00712
surrogate=-0.01948, entropy= 0.41273, loss=-0.01948
surrogate= 0.00320, entropy= 0.41197, loss= 0.00320
surrogate=-0.00018, entropy= 0.41242, loss=-0.00018
surrogate=-0.00729, entropy= 0.41222, loss=-0.00729
surrogate= 0.00379, entropy= 0.41170, loss= 0.00379
surrogate= 0.00709, entropy= 0.41206, loss= 0.00709
surrogate= 0.00725, entropy= 0.41157, loss= 0.00725
surrogate=-0.02026, entropy= 0.41152, loss=-0.02026
std_min= 0.22447, std_max= 0.31848, std_mean= 0.28067
val lr: [5.507172131147542e-05], policy lr: [6.60860655737705e-05]
Policy Loss: -0.020259, | Entropy Bonus: -0, | Value Loss: 69.258, | Advantage Loss: 4.0415
Time elapsed (s): 1.6576919555664062
Agent stdevs: 0.28067115
--------------------------------------------------------------------------------

Step 761
++++++++ Policy training ++++++++++
Current mean reward: 1628.745579 | mean episode length: 446.000000
val_loss=23.45341
val_loss=19.62452
val_loss=13.39211
val_loss=10.25727
val_loss=11.96952
val_loss=15.54668
val_loss= 6.16360
val_loss=14.30229
val_loss=10.09605
val_loss= 9.92971
adv_loss= 1.63279
adv_loss= 1.57273
adv_loss= 1.15344
adv_loss= 4.08599
adv_loss= 0.97135
adv_loss= 1.73200
adv_loss= 2.30794
adv_loss= 0.86184
adv_loss= 2.74121
adv_loss= 1.41976
surrogate=-0.01441, entropy= 0.41133, loss=-0.01441
surrogate= 0.01158, entropy= 0.41138, loss= 0.01158
surrogate=-0.00498, entropy= 0.41121, loss=-0.00498
surrogate=-0.00465, entropy= 0.41114, loss=-0.00465
surrogate=-0.01211, entropy= 0.41105, loss=-0.01211
surrogate=-0.00045, entropy= 0.41102, loss=-0.00045
surrogate=-0.01864, entropy= 0.41058, loss=-0.01864
surrogate=-0.02339, entropy= 0.41015, loss=-0.02339
surrogate=-0.02119, entropy= 0.41032, loss=-0.02119
surrogate=-0.03160, entropy= 0.41034, loss=-0.03160
std_min= 0.22425, std_max= 0.31939, std_mean= 0.28060
val lr: [5.4815573770491815e-05], policy lr: [6.577868852459017e-05]
Policy Loss: -0.031595, | Entropy Bonus: -0, | Value Loss: 9.9297, | Advantage Loss: 1.4198
Time elapsed (s): 1.6662840843200684
Agent stdevs: 0.28059945
--------------------------------------------------------------------------------

Step 762
++++++++ Policy training ++++++++++
Current mean reward: 1482.737374 | mean episode length: 407.800000
val_loss=14.66642
val_loss=35.45653
val_loss=15.65822
val_loss=16.34612
val_loss= 9.37619
val_loss=15.24720
val_loss=12.42045
val_loss=14.70157
val_loss=16.14334
val_loss=27.76011
adv_loss= 1.62178
adv_loss=11.16682
adv_loss= 2.63368
adv_loss= 3.38084
adv_loss= 9.04132
adv_loss=10.35964
adv_loss= 4.19330
adv_loss=15.39904
adv_loss=10.50658
adv_loss= 3.68218
surrogate=-0.00292, entropy= 0.40960, loss=-0.00292
surrogate=-0.01402, entropy= 0.40883, loss=-0.01402
surrogate=-0.01771, entropy= 0.40757, loss=-0.01771
surrogate=-0.01102, entropy= 0.40701, loss=-0.01102
surrogate=-0.02018, entropy= 0.40515, loss=-0.02018
surrogate=-0.05288, entropy= 0.40422, loss=-0.05288
surrogate=-0.00076, entropy= 0.40317, loss=-0.00076
surrogate=-0.00935, entropy= 0.40180, loss=-0.00935
surrogate=-0.01973, entropy= 0.40109, loss=-0.01973
surrogate=-0.03578, entropy= 0.40012, loss=-0.03578
std_min= 0.22452, std_max= 0.31859, std_mean= 0.27953
val lr: [5.455942622950819e-05], policy lr: [6.547131147540982e-05]
Policy Loss: -0.035781, | Entropy Bonus: -0, | Value Loss: 27.76, | Advantage Loss: 3.6822
Time elapsed (s): 1.6455469131469727
Agent stdevs: 0.27953422
--------------------------------------------------------------------------------

Step 763
++++++++ Policy training ++++++++++
Current mean reward: 1280.951488 | mean episode length: 352.750000
val_loss=12.88601
val_loss=10.27510
val_loss= 9.21866
val_loss= 7.66086
val_loss=11.00310
val_loss= 7.56737
val_loss=10.19402
val_loss= 8.72044
val_loss=12.05042
val_loss= 5.67287
adv_loss= 3.35728
adv_loss= 1.51507
adv_loss= 2.36275
adv_loss= 2.18465
adv_loss= 1.41100
adv_loss= 2.43347
adv_loss= 1.10012
adv_loss= 1.48328
adv_loss= 1.76673
adv_loss= 1.04159
surrogate= 0.02531, entropy= 0.40010, loss= 0.02531
surrogate= 0.00897, entropy= 0.40100, loss= 0.00897
surrogate=-0.02424, entropy= 0.40137, loss=-0.02424
surrogate=-0.00673, entropy= 0.40164, loss=-0.00673
surrogate= 0.01423, entropy= 0.40219, loss= 0.01423
surrogate=-0.01367, entropy= 0.40246, loss=-0.01367
surrogate=-0.01759, entropy= 0.40293, loss=-0.01759
surrogate=-0.02255, entropy= 0.40250, loss=-0.02255
surrogate=-0.00249, entropy= 0.40275, loss=-0.00249
surrogate=-0.03701, entropy= 0.40243, loss=-0.03701
std_min= 0.22502, std_max= 0.31856, std_mean= 0.27972
val lr: [5.430327868852458e-05], policy lr: [6.51639344262295e-05]
Policy Loss: -0.037012, | Entropy Bonus: -0, | Value Loss: 5.6729, | Advantage Loss: 1.0416
Time elapsed (s): 1.6499638557434082
Agent stdevs: 0.27971536
--------------------------------------------------------------------------------

Step 764
++++++++ Policy training ++++++++++
Current mean reward: 1118.333528 | mean episode length: 308.750000
val_loss=13.83924
val_loss=14.46307
val_loss=24.27459
val_loss=12.63369
val_loss= 6.08649
val_loss= 8.93265
val_loss= 8.89259
val_loss= 9.66001
val_loss=12.09258
val_loss=11.35927
adv_loss= 1.58818
adv_loss= 1.38879
adv_loss= 1.28985
adv_loss= 1.01202
adv_loss= 3.26106
adv_loss= 1.97873
adv_loss= 2.91575
adv_loss= 0.50834
adv_loss= 1.67530
adv_loss= 0.73780
surrogate= 0.00719, entropy= 0.40104, loss= 0.00719
surrogate=-0.00195, entropy= 0.40014, loss=-0.00195
surrogate=-0.02944, entropy= 0.39890, loss=-0.02944
surrogate=-0.01960, entropy= 0.39800, loss=-0.01960
surrogate=-0.01539, entropy= 0.39652, loss=-0.01539
surrogate= 0.01918, entropy= 0.39565, loss= 0.01918
surrogate=-0.01555, entropy= 0.39467, loss=-0.01555
surrogate=-0.02079, entropy= 0.39375, loss=-0.02079
surrogate= 0.00388, entropy= 0.39343, loss= 0.00388
surrogate=-0.02480, entropy= 0.39273, loss=-0.02480
std_min= 0.22389, std_max= 0.31745, std_mean= 0.27885
val lr: [5.404713114754098e-05], policy lr: [6.485655737704917e-05]
Policy Loss: -0.024795, | Entropy Bonus: -0, | Value Loss: 11.359, | Advantage Loss: 0.7378
Time elapsed (s): 1.6619651317596436
Agent stdevs: 0.27884763
--------------------------------------------------------------------------------

Step 765
++++++++ Policy training ++++++++++
Current mean reward: 1613.982282 | mean episode length: 435.250000
val_loss= 4.78038
val_loss= 4.31806
val_loss= 4.24847
val_loss= 3.51975
val_loss= 5.54092
val_loss= 6.94138
val_loss= 3.64111
val_loss= 5.01901
val_loss= 5.79713
val_loss= 4.37416
adv_loss= 0.79609
adv_loss= 0.54239
adv_loss= 0.92356
adv_loss= 0.80462
adv_loss= 0.91291
adv_loss= 1.70697
adv_loss= 1.53579
adv_loss= 1.09783
adv_loss= 0.65851
adv_loss= 1.42768
surrogate=-0.02568, entropy= 0.39127, loss=-0.02568
surrogate=-0.00486, entropy= 0.38979, loss=-0.00486
surrogate= 0.00209, entropy= 0.38828, loss= 0.00209
surrogate=-0.04721, entropy= 0.38737, loss=-0.04721
surrogate=-0.03611, entropy= 0.38592, loss=-0.03611
surrogate=-0.00946, entropy= 0.38444, loss=-0.00946
surrogate=-0.00454, entropy= 0.38374, loss=-0.00454
surrogate=-0.02062, entropy= 0.38276, loss=-0.02062
surrogate= 0.00714, entropy= 0.38104, loss= 0.00714
surrogate=-0.02950, entropy= 0.38055, loss=-0.02950
std_min= 0.22275, std_max= 0.31677, std_mean= 0.27776
val lr: [5.3790983606557374e-05], policy lr: [6.454918032786884e-05]
Policy Loss: -0.029503, | Entropy Bonus: -0, | Value Loss: 4.3742, | Advantage Loss: 1.4277
Time elapsed (s): 1.6520204544067383
Agent stdevs: 0.277756
--------------------------------------------------------------------------------

Step 766
++++++++ Policy training ++++++++++
Current mean reward: 1377.733171 | mean episode length: 383.200000
val_loss=777.77283
val_loss=43.15959
val_loss=539.72791
val_loss=465.68774
val_loss=64.41238
val_loss=65.42628
val_loss=16.36912
val_loss=56.62056
val_loss=412.29120
val_loss=171.11203
adv_loss= 9.17805
adv_loss= 4.26747
adv_loss= 3.69675
adv_loss= 6.48041
adv_loss= 4.14266
adv_loss= 3.24901
adv_loss= 4.62984
adv_loss= 2.70181
adv_loss= 4.32271
adv_loss= 3.98256
surrogate=-0.01875, entropy= 0.37961, loss=-0.01875
surrogate= 0.01364, entropy= 0.37931, loss= 0.01364
surrogate=-0.01966, entropy= 0.37909, loss=-0.01966
surrogate=-0.01787, entropy= 0.37881, loss=-0.01787
surrogate= 0.02306, entropy= 0.37828, loss= 0.02306
surrogate=-0.01716, entropy= 0.37881, loss=-0.01716
surrogate=-0.02954, entropy= 0.37896, loss=-0.02954
surrogate=-0.00696, entropy= 0.37869, loss=-0.00696
surrogate=-0.00260, entropy= 0.37887, loss=-0.00260
surrogate=-0.00651, entropy= 0.37902, loss=-0.00651
std_min= 0.22279, std_max= 0.31726, std_mean= 0.27762
val lr: [5.3534836065573773e-05], policy lr: [6.424180327868852e-05]
Policy Loss: -0.0065078, | Entropy Bonus: -0, | Value Loss: 171.11, | Advantage Loss: 3.9826
Time elapsed (s): 1.6842737197875977
Agent stdevs: 0.27762
--------------------------------------------------------------------------------

Step 767
++++++++ Policy training ++++++++++
Current mean reward: 1394.182873 | mean episode length: 386.200000
val_loss=46.06947
val_loss=36.04182
val_loss=37.02705
val_loss=24.00930
val_loss=27.54128
val_loss=14.74245
val_loss=13.99017
val_loss=13.03900
val_loss= 9.34721
val_loss=16.52912
adv_loss= 1.55832
adv_loss= 6.03522
adv_loss= 1.22650
adv_loss= 2.89391
adv_loss= 1.62288
adv_loss= 1.75578
adv_loss= 1.14266
adv_loss= 1.63026
adv_loss= 1.54751
adv_loss= 2.58390
surrogate= 0.02422, entropy= 0.37798, loss= 0.02422
surrogate=-0.01229, entropy= 0.37699, loss=-0.01229
surrogate=-0.00148, entropy= 0.37648, loss=-0.00148
surrogate=-0.01366, entropy= 0.37565, loss=-0.01366
surrogate= 0.00286, entropy= 0.37531, loss= 0.00286
surrogate=-0.00628, entropy= 0.37454, loss=-0.00628
surrogate=-0.04728, entropy= 0.37363, loss=-0.04728
surrogate=-0.02626, entropy= 0.37271, loss=-0.02626
surrogate=-0.03837, entropy= 0.37215, loss=-0.03837
surrogate=-0.02582, entropy= 0.37104, loss=-0.02582
std_min= 0.22151, std_max= 0.31644, std_mean= 0.27695
val lr: [5.3278688524590167e-05], policy lr: [6.393442622950819e-05]
Policy Loss: -0.025819, | Entropy Bonus: -0, | Value Loss: 16.529, | Advantage Loss: 2.5839
Time elapsed (s): 1.676938772201538
Agent stdevs: 0.27694985
--------------------------------------------------------------------------------

Step 768
++++++++ Policy training ++++++++++
Current mean reward: 1828.248348 | mean episode length: 497.250000
val_loss=18.77659
val_loss=14.08099
val_loss= 9.62147
val_loss= 6.21990
val_loss= 7.88270
val_loss= 8.26336
val_loss= 8.95368
val_loss=10.13417
val_loss= 6.70671
val_loss= 8.51212
adv_loss= 2.07542
adv_loss= 1.65512
adv_loss= 1.47792
adv_loss= 1.75355
adv_loss= 0.97896
adv_loss= 2.57564
adv_loss= 1.16613
adv_loss= 1.69770
adv_loss= 1.94859
adv_loss= 1.29408
surrogate=-0.00449, entropy= 0.36926, loss=-0.00449
surrogate=-0.00364, entropy= 0.36852, loss=-0.00364
surrogate=-0.02512, entropy= 0.36756, loss=-0.02512
surrogate=-0.02031, entropy= 0.36654, loss=-0.02031
surrogate=-0.01053, entropy= 0.36551, loss=-0.01053
surrogate=-0.02683, entropy= 0.36431, loss=-0.02683
surrogate=-0.01574, entropy= 0.36382, loss=-0.01574
surrogate= 0.00612, entropy= 0.36311, loss= 0.00612
surrogate=-0.01523, entropy= 0.36181, loss=-0.01523
surrogate=-0.04118, entropy= 0.36143, loss=-0.04118
std_min= 0.22102, std_max= 0.31572, std_mean= 0.27605
val lr: [5.3022540983606566e-05], policy lr: [6.362704918032787e-05]
Policy Loss: -0.041178, | Entropy Bonus: -0, | Value Loss: 8.5121, | Advantage Loss: 1.2941
Time elapsed (s): 1.649564266204834
Agent stdevs: 0.2760537
--------------------------------------------------------------------------------

Step 769
++++++++ Policy training ++++++++++
Current mean reward: 2282.287877 | mean episode length: 636.666667
val_loss=26.22147
val_loss=146.00665
val_loss=134.67140
val_loss=106.77537
val_loss=941.74652
val_loss=96.96731
val_loss=45.77401
val_loss=83.96972
val_loss=80.15939
val_loss=117.47953
adv_loss= 1.98187
adv_loss=12.63182
adv_loss= 2.14753
adv_loss= 5.02618
adv_loss= 2.92467
adv_loss= 2.43995
adv_loss= 2.46188
adv_loss= 2.00570
adv_loss= 4.60191
adv_loss=12.81806
surrogate= 0.00332, entropy= 0.36163, loss= 0.00332
surrogate=-0.01334, entropy= 0.36244, loss=-0.01334
surrogate=-0.03994, entropy= 0.36184, loss=-0.03994
surrogate=-0.02192, entropy= 0.36218, loss=-0.02192
surrogate=-0.01352, entropy= 0.36222, loss=-0.01352
surrogate=-0.01928, entropy= 0.36293, loss=-0.01928
surrogate=-0.02641, entropy= 0.36300, loss=-0.02641
surrogate=-0.01631, entropy= 0.36280, loss=-0.01631
surrogate=-0.01223, entropy= 0.36312, loss=-0.01223
surrogate=-0.03502, entropy= 0.36315, loss=-0.03502
std_min= 0.22146, std_max= 0.31578, std_mean= 0.27618
val lr: [5.276639344262296e-05], policy lr: [6.331967213114755e-05]
Policy Loss: -0.035018, | Entropy Bonus: -0, | Value Loss: 117.48, | Advantage Loss: 12.818
Time elapsed (s): 1.636321783065796
Agent stdevs: 0.276179
--------------------------------------------------------------------------------

Step 770
++++++++ Policy training ++++++++++
Current mean reward: 1193.286684 | mean episode length: 328.200000
val_loss=13.42834
val_loss=21.63466
val_loss=24.47610
val_loss=15.67929
val_loss=11.15626
val_loss=16.05480
val_loss=15.57161
val_loss= 8.05447
val_loss=10.66377
val_loss= 8.37725
adv_loss= 1.39015
adv_loss= 1.41331
adv_loss= 2.18540
adv_loss= 1.48596
adv_loss= 2.30106
adv_loss= 2.65801
adv_loss= 2.36280
adv_loss= 1.33566
adv_loss= 2.79187
adv_loss= 0.94419
surrogate= 0.00521, entropy= 0.36274, loss= 0.00521
surrogate=-0.00917, entropy= 0.36184, loss=-0.00917
surrogate=-0.00427, entropy= 0.36170, loss=-0.00427
surrogate=-0.02291, entropy= 0.36095, loss=-0.02291
surrogate=-0.01873, entropy= 0.36020, loss=-0.01873
surrogate=-0.01624, entropy= 0.35942, loss=-0.01624
surrogate=-0.01631, entropy= 0.35926, loss=-0.01631
surrogate= 0.01017, entropy= 0.35857, loss= 0.01017
surrogate=-0.02346, entropy= 0.35846, loss=-0.02346
surrogate=-0.02421, entropy= 0.35770, loss=-0.02421
std_min= 0.22044, std_max= 0.31724, std_mean= 0.27579
val lr: [5.251024590163936e-05], policy lr: [6.301229508196723e-05]
Policy Loss: -0.02421, | Entropy Bonus: -0, | Value Loss: 8.3773, | Advantage Loss: 0.94419
Time elapsed (s): 1.6632378101348877
Agent stdevs: 0.275794
--------------------------------------------------------------------------------

Step 771
++++++++ Policy training ++++++++++
Current mean reward: 1868.192843 | mean episode length: 503.250000
val_loss=15.15454
val_loss= 9.66281
val_loss= 8.65438
val_loss= 4.63851
val_loss= 7.26386
val_loss= 3.75366
val_loss= 3.69358
val_loss= 8.30493
val_loss= 6.96744
val_loss= 8.68052
adv_loss= 1.06686
adv_loss= 2.04802
adv_loss= 1.29861
adv_loss= 0.90337
adv_loss= 0.81544
adv_loss= 1.12771
adv_loss= 1.05377
adv_loss= 1.62119
adv_loss= 0.72843
adv_loss= 1.32553
surrogate= 0.01271, entropy= 0.35821, loss= 0.01271
surrogate=-0.00947, entropy= 0.35818, loss=-0.00947
surrogate=-0.00728, entropy= 0.35885, loss=-0.00728
surrogate=-0.00453, entropy= 0.35931, loss=-0.00453
surrogate=-0.03413, entropy= 0.35935, loss=-0.03413
surrogate=-0.01540, entropy= 0.35954, loss=-0.01540
surrogate=-0.01132, entropy= 0.35950, loss=-0.01132
surrogate=-0.00135, entropy= 0.35940, loss=-0.00135
surrogate=-0.03227, entropy= 0.35981, loss=-0.03227
surrogate= 0.01607, entropy= 0.35974, loss= 0.01607
std_min= 0.22047, std_max= 0.31812, std_mean= 0.27602
val lr: [5.2254098360655725e-05], policy lr: [6.270491803278686e-05]
Policy Loss: 0.016073, | Entropy Bonus: -0, | Value Loss: 8.6805, | Advantage Loss: 1.3255
Time elapsed (s): 1.6536579132080078
Agent stdevs: 0.27601779
--------------------------------------------------------------------------------

Step 772
++++++++ Policy training ++++++++++
Current mean reward: 1415.334685 | mean episode length: 385.000000
val_loss=11.87822
val_loss=12.25712
val_loss=14.49778
val_loss= 7.08912
val_loss=14.32900
val_loss=10.71630
val_loss= 9.73629
val_loss= 8.22411
val_loss=17.56070
val_loss=10.68270
adv_loss= 5.50140
adv_loss= 1.59322
adv_loss= 4.82550
adv_loss= 1.00653
adv_loss= 4.23287
adv_loss= 1.24884
adv_loss= 1.95649
adv_loss= 4.48071
adv_loss= 2.43871
adv_loss= 1.17111
surrogate= 0.01309, entropy= 0.35883, loss= 0.01309
surrogate=-0.01095, entropy= 0.35748, loss=-0.01095
surrogate=-0.01154, entropy= 0.35630, loss=-0.01154
surrogate=-0.04693, entropy= 0.35490, loss=-0.04693
surrogate=-0.01631, entropy= 0.35390, loss=-0.01631
surrogate=-0.01570, entropy= 0.35221, loss=-0.01570
surrogate=-0.01319, entropy= 0.35094, loss=-0.01319
surrogate= 0.01474, entropy= 0.34968, loss= 0.01474
surrogate=-0.01825, entropy= 0.34852, loss=-0.01825
surrogate=-0.02594, entropy= 0.34842, loss=-0.02594
std_min= 0.21934, std_max= 0.31768, std_mean= 0.27503
val lr: [5.1997950819672125e-05], policy lr: [6.239754098360654e-05]
Policy Loss: -0.025943, | Entropy Bonus: -0, | Value Loss: 10.683, | Advantage Loss: 1.1711
Time elapsed (s): 1.669870138168335
Agent stdevs: 0.275027
--------------------------------------------------------------------------------

Step 773
++++++++ Policy training ++++++++++
Current mean reward: 2316.776284 | mean episode length: 645.666667
val_loss=19.14176
val_loss=560.01501
val_loss=84.72247
val_loss=53.06710
val_loss=926.74182
val_loss=88.35477
val_loss=30.91667
val_loss=33.11639
val_loss=518.39044
val_loss=22.75476
adv_loss= 2.49351
adv_loss= 1.66114
adv_loss= 2.20580
adv_loss= 1.46382
adv_loss= 3.36660
adv_loss= 1.54969
adv_loss= 2.24328
adv_loss= 1.26162
adv_loss= 2.02156
adv_loss= 3.22346
surrogate=-0.01535, entropy= 0.34875, loss=-0.01535
surrogate=-0.01026, entropy= 0.34967, loss=-0.01026
surrogate=-0.00510, entropy= 0.34935, loss=-0.00510
surrogate=-0.03328, entropy= 0.34945, loss=-0.03328
surrogate=-0.00783, entropy= 0.35016, loss=-0.00783
surrogate=-0.00914, entropy= 0.34995, loss=-0.00914
surrogate= 0.00322, entropy= 0.35062, loss= 0.00322
surrogate=-0.03460, entropy= 0.35099, loss=-0.03460
surrogate=-0.02417, entropy= 0.35140, loss=-0.02417
surrogate=-0.01934, entropy= 0.35171, loss=-0.01934
std_min= 0.21955, std_max= 0.31721, std_mean= 0.27531
val lr: [5.174180327868852e-05], policy lr: [6.209016393442621e-05]
Policy Loss: -0.019338, | Entropy Bonus: -0, | Value Loss: 22.755, | Advantage Loss: 3.2235
Time elapsed (s): 1.6739764213562012
Agent stdevs: 0.27531114
--------------------------------------------------------------------------------

Step 774
++++++++ Policy training ++++++++++
Current mean reward: 2153.091926 | mean episode length: 585.000000
val_loss=24.05066
val_loss=26.53329
val_loss=21.62173
val_loss=11.63489
val_loss=20.28700
val_loss=14.67057
val_loss=13.08850
val_loss=13.90781
val_loss=14.44051
val_loss= 9.17604
adv_loss= 2.05998
adv_loss= 1.05033
adv_loss= 1.77014
adv_loss= 4.43113
adv_loss= 1.16360
adv_loss= 1.40779
adv_loss= 1.76895
adv_loss= 2.19038
adv_loss= 1.06717
adv_loss= 1.15150
surrogate= 0.00670, entropy= 0.35228, loss= 0.00670
surrogate= 0.00953, entropy= 0.35316, loss= 0.00953
surrogate= 0.00703, entropy= 0.35373, loss= 0.00703
surrogate=-0.00728, entropy= 0.35474, loss=-0.00728
surrogate=-0.01640, entropy= 0.35495, loss=-0.01640
surrogate=-0.00733, entropy= 0.35600, loss=-0.00733
surrogate=-0.00559, entropy= 0.35684, loss=-0.00559
surrogate=-0.00535, entropy= 0.35779, loss=-0.00535
surrogate=-0.03121, entropy= 0.35847, loss=-0.03121
surrogate=-0.03183, entropy= 0.35908, loss=-0.03183
std_min= 0.21920, std_max= 0.31822, std_mean= 0.27609
val lr: [5.148565573770492e-05], policy lr: [6.17827868852459e-05]
Policy Loss: -0.031831, | Entropy Bonus: -0, | Value Loss: 9.176, | Advantage Loss: 1.1515
Time elapsed (s): 1.647890329360962
Agent stdevs: 0.27609214
--------------------------------------------------------------------------------

Step 775
++++++++ Policy training ++++++++++
Current mean reward: 1636.363793 | mean episode length: 440.250000
val_loss=19.20306
val_loss=10.37105
val_loss=12.17871
val_loss= 6.64013
val_loss= 4.51707
val_loss= 8.82140
val_loss=10.29638
val_loss= 7.69125
val_loss=10.14265
val_loss=14.26498
adv_loss= 2.86725
adv_loss= 1.69864
adv_loss= 0.52579
adv_loss= 1.42757
adv_loss= 0.88396
adv_loss= 1.88401
adv_loss= 0.52894
adv_loss= 1.34886
adv_loss= 1.50353
adv_loss= 0.72914
surrogate=-0.00837, entropy= 0.35948, loss=-0.00837
surrogate= 0.00116, entropy= 0.35952, loss= 0.00116
surrogate=-0.00769, entropy= 0.35927, loss=-0.00769
surrogate=-0.02032, entropy= 0.35879, loss=-0.02032
surrogate= 0.01550, entropy= 0.35899, loss= 0.01550
surrogate=-0.00544, entropy= 0.35878, loss=-0.00544
surrogate=-0.02608, entropy= 0.35879, loss=-0.02608
surrogate=-0.01833, entropy= 0.35847, loss=-0.01833
surrogate= 0.02159, entropy= 0.35805, loss= 0.02159
surrogate= 0.00945, entropy= 0.35790, loss= 0.00945
std_min= 0.21929, std_max= 0.31836, std_mean= 0.27597
val lr: [5.122950819672131e-05], policy lr: [6.147540983606557e-05]
Policy Loss: 0.0094462, | Entropy Bonus: -0, | Value Loss: 14.265, | Advantage Loss: 0.72914
Time elapsed (s): 1.6567237377166748
Agent stdevs: 0.2759692
--------------------------------------------------------------------------------

Step 776
++++++++ Policy training ++++++++++
Current mean reward: 1231.928573 | mean episode length: 333.666667
val_loss= 7.25695
val_loss= 6.12289
val_loss= 7.70400
val_loss= 7.94907
val_loss= 6.20440
val_loss= 6.42366
val_loss= 6.27101
val_loss= 8.29519
val_loss= 3.77005
val_loss= 5.89507
adv_loss= 1.41738
adv_loss= 3.14754
adv_loss= 2.15143
adv_loss= 1.71799
adv_loss= 0.99908
adv_loss= 3.56602
adv_loss= 3.42445
adv_loss= 2.28769
adv_loss= 7.26277
adv_loss= 6.79228
surrogate=-0.00582, entropy= 0.35736, loss=-0.00582
surrogate=-0.01835, entropy= 0.35630, loss=-0.01835
surrogate= 0.00279, entropy= 0.35464, loss= 0.00279
surrogate=-0.00545, entropy= 0.35344, loss=-0.00545
surrogate=-0.02203, entropy= 0.35250, loss=-0.02203
surrogate=-0.01212, entropy= 0.35162, loss=-0.01212
surrogate=-0.02061, entropy= 0.35047, loss=-0.02061
surrogate=-0.03175, entropy= 0.34914, loss=-0.03175
surrogate=-0.04254, entropy= 0.34846, loss=-0.04254
surrogate=-0.03468, entropy= 0.34727, loss=-0.03468
std_min= 0.21891, std_max= 0.31731, std_mean= 0.27495
val lr: [5.097336065573771e-05], policy lr: [6.116803278688525e-05]
Policy Loss: -0.034678, | Entropy Bonus: -0, | Value Loss: 5.8951, | Advantage Loss: 6.7923
Time elapsed (s): 1.647904634475708
Agent stdevs: 0.27495176
--------------------------------------------------------------------------------

Step 777
++++++++ Policy training ++++++++++
Current mean reward: 1474.105044 | mean episode length: 397.400000
val_loss= 6.14062
val_loss= 6.81464
val_loss= 5.11870
val_loss= 3.18601
val_loss= 8.91497
val_loss= 5.72873
val_loss= 9.48440
val_loss=10.47831
val_loss= 5.84576
val_loss= 5.55595
adv_loss= 1.35141
adv_loss= 1.11918
adv_loss= 0.86331
adv_loss= 3.97933
adv_loss= 1.08357
adv_loss= 1.25980
adv_loss= 0.71032
adv_loss= 1.24834
adv_loss= 1.15347
adv_loss= 1.59892
surrogate= 0.00878, entropy= 0.34751, loss= 0.00878
surrogate=-0.00380, entropy= 0.34756, loss=-0.00380
surrogate=-0.00399, entropy= 0.34767, loss=-0.00399
surrogate= 0.00307, entropy= 0.34807, loss= 0.00307
surrogate=-0.00249, entropy= 0.34827, loss=-0.00249
surrogate=-0.03337, entropy= 0.34804, loss=-0.03337
surrogate= 0.00301, entropy= 0.34776, loss= 0.00301
surrogate=-0.02473, entropy= 0.34855, loss=-0.02473
surrogate=-0.01742, entropy= 0.34890, loss=-0.01742
surrogate=-0.00765, entropy= 0.34874, loss=-0.00765
std_min= 0.21884, std_max= 0.31728, std_mean= 0.27510
val lr: [5.0717213114754104e-05], policy lr: [6.086065573770492e-05]
Policy Loss: -0.0076498, | Entropy Bonus: -0, | Value Loss: 5.5559, | Advantage Loss: 1.5989
Time elapsed (s): 1.6601178646087646
Agent stdevs: 0.27510127
--------------------------------------------------------------------------------

Step 778
++++++++ Policy training ++++++++++
Current mean reward: 2410.033498 | mean episode length: 651.666667
val_loss=10.74802
val_loss= 9.73766
val_loss= 6.07758
val_loss= 9.98307
val_loss= 7.41006
val_loss= 9.84105
val_loss= 8.07127
val_loss= 7.61529
val_loss=12.91821
val_loss= 9.04829
adv_loss= 1.03353
adv_loss= 0.84589
adv_loss= 3.10655
adv_loss= 1.19141
adv_loss= 0.81792
adv_loss= 0.86803
adv_loss= 5.11406
adv_loss= 1.34118
adv_loss= 2.90674
adv_loss= 1.25809
surrogate=-0.00874, entropy= 0.34925, loss=-0.00874
surrogate= 0.01091, entropy= 0.34989, loss= 0.01091
surrogate= 0.00060, entropy= 0.35002, loss= 0.00060
surrogate=-0.01881, entropy= 0.34995, loss=-0.01881
surrogate=-0.02296, entropy= 0.35004, loss=-0.02296
surrogate=-0.02996, entropy= 0.35060, loss=-0.02996
surrogate=-0.03139, entropy= 0.35039, loss=-0.03139
surrogate=-0.03116, entropy= 0.35094, loss=-0.03116
surrogate= 0.00176, entropy= 0.35123, loss= 0.00176
surrogate=-0.02789, entropy= 0.35143, loss=-0.02789
std_min= 0.21949, std_max= 0.31768, std_mean= 0.27530
val lr: [5.0461065573770504e-05], policy lr: [6.05532786885246e-05]
Policy Loss: -0.027886, | Entropy Bonus: -0, | Value Loss: 9.0483, | Advantage Loss: 1.2581
Time elapsed (s): 1.6474320888519287
Agent stdevs: 0.27530453
--------------------------------------------------------------------------------

Step 779
++++++++ Policy training ++++++++++
Current mean reward: 1734.253127 | mean episode length: 469.250000
val_loss= 5.56141
val_loss= 6.91763
val_loss= 6.75979
val_loss=10.16319
val_loss=10.57085
val_loss= 7.06110
val_loss= 7.86542
val_loss= 8.93188
val_loss= 7.90464
val_loss=12.25625
adv_loss= 0.89752
adv_loss= 0.46706
adv_loss= 1.90363
adv_loss= 0.89342
adv_loss= 1.92937
adv_loss= 1.30542
adv_loss= 0.67170
adv_loss= 0.44287
adv_loss= 2.48969
adv_loss= 0.85900
surrogate=-0.01499, entropy= 0.35098, loss=-0.01499
surrogate=-0.01184, entropy= 0.35061, loss=-0.01184
surrogate=-0.03329, entropy= 0.35053, loss=-0.03329
surrogate=-0.00492, entropy= 0.35020, loss=-0.00492
surrogate=-0.02210, entropy= 0.34941, loss=-0.02210
surrogate=-0.01921, entropy= 0.34930, loss=-0.01921
surrogate=-0.01435, entropy= 0.34867, loss=-0.01435
surrogate=-0.05142, entropy= 0.34841, loss=-0.05142
surrogate=-0.02764, entropy= 0.34762, loss=-0.02764
surrogate=-0.02665, entropy= 0.34763, loss=-0.02665
std_min= 0.21967, std_max= 0.31635, std_mean= 0.27488
val lr: [5.020491803278687e-05], policy lr: [6.024590163934424e-05]
Policy Loss: -0.026647, | Entropy Bonus: -0, | Value Loss: 12.256, | Advantage Loss: 0.859
Time elapsed (s): 1.6824333667755127
Agent stdevs: 0.2748771
--------------------------------------------------------------------------------

Step 780
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1697.3
++++++++ Policy training ++++++++++
Current mean reward: 2039.632755 | mean episode length: 559.333333
val_loss= 9.83722
val_loss=13.39357
val_loss=20.12032
val_loss=12.79027
val_loss=11.11792
val_loss=14.15590
val_loss=10.15476
val_loss=19.72951
val_loss=17.61620
val_loss=12.55326
adv_loss= 1.72224
adv_loss= 1.31114
adv_loss= 1.12005
adv_loss= 1.05422
adv_loss= 0.98213
adv_loss= 0.75389
adv_loss= 1.23534
adv_loss= 1.31842
adv_loss= 0.40574
adv_loss= 2.12134
surrogate=-0.00281, entropy= 0.34757, loss=-0.00281
surrogate=-0.03493, entropy= 0.34711, loss=-0.03493
surrogate=-0.01891, entropy= 0.34643, loss=-0.01891
surrogate=-0.00623, entropy= 0.34643, loss=-0.00623
surrogate=-0.03451, entropy= 0.34622, loss=-0.03451
surrogate= 0.00489, entropy= 0.34534, loss= 0.00489
surrogate=-0.01782, entropy= 0.34514, loss=-0.01782
surrogate=-0.02896, entropy= 0.34440, loss=-0.02896
surrogate=-0.02058, entropy= 0.34378, loss=-0.02058
surrogate=-0.01996, entropy= 0.34297, loss=-0.01996
std_min= 0.21812, std_max= 0.31678, std_mean= 0.27461
val lr: [4.994877049180327e-05], policy lr: [5.993852459016392e-05]
Policy Loss: -0.019965, | Entropy Bonus: -0, | Value Loss: 12.553, | Advantage Loss: 2.1213
Time elapsed (s): 1.640453815460205
Agent stdevs: 0.2746111
--------------------------------------------------------------------------------

Step 781
++++++++ Policy training ++++++++++
Current mean reward: 2281.454737 | mean episode length: 622.333333
val_loss=114.25183
val_loss=28.32845
val_loss=100.93176
val_loss=18.34949
val_loss=10.80356
val_loss=10.59022
val_loss=70.04695
val_loss=23.10942
val_loss=14.07802
val_loss=16.38067
adv_loss= 3.08400
adv_loss= 2.36354
adv_loss= 1.83390
adv_loss= 1.31552
adv_loss= 2.31926
adv_loss= 1.25095
adv_loss= 0.56805
adv_loss= 1.55662
adv_loss=80.19805
adv_loss= 1.03603
surrogate=-0.01013, entropy= 0.34312, loss=-0.01013
surrogate= 0.00222, entropy= 0.34299, loss= 0.00222
surrogate=-0.01059, entropy= 0.34312, loss=-0.01059
surrogate=-0.01565, entropy= 0.34365, loss=-0.01565
surrogate=-0.01329, entropy= 0.34350, loss=-0.01329
surrogate=-0.02215, entropy= 0.34317, loss=-0.02215
surrogate=-0.01787, entropy= 0.34303, loss=-0.01787
surrogate=-0.01204, entropy= 0.34310, loss=-0.01204
surrogate=-0.01077, entropy= 0.34296, loss=-0.01077
surrogate= 0.00313, entropy= 0.34295, loss= 0.00313
std_min= 0.21751, std_max= 0.31828, std_mean= 0.27472
val lr: [4.969262295081966e-05], policy lr: [5.963114754098359e-05]
Policy Loss: 0.0031337, | Entropy Bonus: -0, | Value Loss: 16.381, | Advantage Loss: 1.036
Time elapsed (s): 1.642928123474121
Agent stdevs: 0.2747154
--------------------------------------------------------------------------------

Step 782
++++++++ Policy training ++++++++++
Current mean reward: 1828.831748 | mean episode length: 495.250000
val_loss=13.53012
val_loss=13.50776
val_loss= 6.49754
val_loss= 6.48225
val_loss= 8.76726
val_loss= 8.12995
val_loss= 6.02466
val_loss= 4.12561
val_loss= 4.27155
val_loss= 6.73870
adv_loss= 2.91552
adv_loss= 1.11026
adv_loss= 1.38611
adv_loss= 0.89703
adv_loss= 2.60687
adv_loss= 2.40202
adv_loss= 0.96324
adv_loss= 2.23297
adv_loss= 1.87870
adv_loss= 0.68738
surrogate=-0.01813, entropy= 0.34219, loss=-0.01813
surrogate= 0.00666, entropy= 0.34218, loss= 0.00666
surrogate= 0.00462, entropy= 0.34156, loss= 0.00462
surrogate=-0.01690, entropy= 0.34154, loss=-0.01690
surrogate=-0.03001, entropy= 0.34192, loss=-0.03001
surrogate=-0.01084, entropy= 0.34140, loss=-0.01084
surrogate=-0.00479, entropy= 0.34086, loss=-0.00479
surrogate=-0.03949, entropy= 0.34048, loss=-0.03949
surrogate=-0.00522, entropy= 0.34003, loss=-0.00522
surrogate=-0.00488, entropy= 0.33929, loss=-0.00488
std_min= 0.21713, std_max= 0.31742, std_mean= 0.27438
val lr: [4.943647540983606e-05], policy lr: [5.932377049180327e-05]
Policy Loss: -0.0048792, | Entropy Bonus: -0, | Value Loss: 6.7387, | Advantage Loss: 0.68738
Time elapsed (s): 1.6441612243652344
Agent stdevs: 0.2743796
--------------------------------------------------------------------------------

Step 783
++++++++ Policy training ++++++++++
Current mean reward: 2447.125546 | mean episode length: 660.000000
val_loss=13.65196
val_loss= 9.79344
val_loss= 5.23990
val_loss= 4.49766
val_loss= 6.24919
val_loss= 7.00289
val_loss= 7.97634
val_loss= 6.17864
val_loss= 6.56270
val_loss= 7.72246
adv_loss= 0.64366
adv_loss= 0.29749
adv_loss= 0.58572
adv_loss= 0.36215
adv_loss= 0.78390
adv_loss= 0.86255
adv_loss= 0.43090
adv_loss= 0.53590
adv_loss= 0.43077
adv_loss= 1.15078
surrogate= 0.01352, entropy= 0.33953, loss= 0.01352
surrogate= 0.00072, entropy= 0.33927, loss= 0.00072
surrogate= 0.00181, entropy= 0.33929, loss= 0.00181
surrogate=-0.00221, entropy= 0.33936, loss=-0.00221
surrogate=-0.01258, entropy= 0.33960, loss=-0.01258
surrogate=-0.01183, entropy= 0.33955, loss=-0.01183
surrogate=-0.01315, entropy= 0.33938, loss=-0.01315
surrogate=-0.04552, entropy= 0.33925, loss=-0.04552
surrogate=-0.03476, entropy= 0.33931, loss=-0.03476
surrogate= 0.00301, entropy= 0.33905, loss= 0.00301
std_min= 0.21717, std_max= 0.31712, std_mean= 0.27434
val lr: [4.9180327868852456e-05], policy lr: [5.901639344262294e-05]
Policy Loss: 0.0030071, | Entropy Bonus: -0, | Value Loss: 7.7225, | Advantage Loss: 1.1508
Time elapsed (s): 1.6525084972381592
Agent stdevs: 0.27434415
--------------------------------------------------------------------------------

Step 784
++++++++ Policy training ++++++++++
Current mean reward: 1504.846384 | mean episode length: 408.000000
val_loss= 9.71339
val_loss= 9.92784
val_loss=10.59752
val_loss=10.01632
val_loss= 6.84246
val_loss= 8.91845
val_loss= 7.15285
val_loss= 3.37991
val_loss= 9.70323
val_loss= 3.85986
adv_loss= 1.28908
adv_loss= 0.75772
adv_loss= 2.32924
adv_loss= 0.86686
adv_loss= 1.34289
adv_loss= 0.67232
adv_loss= 1.73902
adv_loss= 0.76738
adv_loss= 1.10062
adv_loss= 0.85947
surrogate=-0.00219, entropy= 0.33878, loss=-0.00219
surrogate= 0.00719, entropy= 0.33884, loss= 0.00719
surrogate=-0.02793, entropy= 0.33846, loss=-0.02793
surrogate=-0.01664, entropy= 0.33804, loss=-0.01664
surrogate=-0.00206, entropy= 0.33776, loss=-0.00206
surrogate=-0.02662, entropy= 0.33738, loss=-0.02662
surrogate= 0.00366, entropy= 0.33778, loss= 0.00366
surrogate=-0.04496, entropy= 0.33787, loss=-0.04496
surrogate=-0.00670, entropy= 0.33758, loss=-0.00670
surrogate=-0.02336, entropy= 0.33746, loss=-0.02336
std_min= 0.21740, std_max= 0.31688, std_mean= 0.27416
val lr: [4.8924180327868856e-05], policy lr: [5.870901639344262e-05]
Policy Loss: -0.023358, | Entropy Bonus: -0, | Value Loss: 3.8599, | Advantage Loss: 0.85947
Time elapsed (s): 1.6580908298492432
Agent stdevs: 0.2741573
--------------------------------------------------------------------------------

Step 785
++++++++ Policy training ++++++++++
Current mean reward: 2419.481056 | mean episode length: 669.666667
val_loss=17.68491
val_loss=18.91242
val_loss=90.06805
val_loss=30.77170
val_loss=1435.30750
val_loss=60.62429
val_loss=310.07764
val_loss=555.80762
val_loss=893.94080
val_loss=120.14560
adv_loss= 1.41985
adv_loss= 1.31439
adv_loss= 2.66335
adv_loss= 1.57946
adv_loss= 0.74817
adv_loss= 0.78304
adv_loss= 2.55785
adv_loss= 1.60688
adv_loss= 1.90845
adv_loss= 1.17924
surrogate= 0.02319, entropy= 0.33740, loss= 0.02319
surrogate=-0.00040, entropy= 0.33709, loss=-0.00040
surrogate=-0.01089, entropy= 0.33800, loss=-0.01089
surrogate= 0.01860, entropy= 0.33792, loss= 0.01860
surrogate=-0.02454, entropy= 0.33806, loss=-0.02454
surrogate=-0.01539, entropy= 0.33744, loss=-0.01539
surrogate= 0.00860, entropy= 0.33706, loss= 0.00860
surrogate=-0.03215, entropy= 0.33713, loss=-0.03215
surrogate=-0.02249, entropy= 0.33661, loss=-0.02249
surrogate=-0.02415, entropy= 0.33671, loss=-0.02415
std_min= 0.21725, std_max= 0.31730, std_mean= 0.27411
val lr: [4.866803278688525e-05], policy lr: [5.84016393442623e-05]
Policy Loss: -0.02415, | Entropy Bonus: -0, | Value Loss: 120.15, | Advantage Loss: 1.1792
Time elapsed (s): 1.6731297969818115
Agent stdevs: 0.27411422
--------------------------------------------------------------------------------

Step 786
++++++++ Policy training ++++++++++
Current mean reward: 1827.507156 | mean episode length: 488.000000
val_loss=16.37387
val_loss=10.35587
val_loss= 6.84974
val_loss= 8.65777
val_loss= 5.82429
val_loss= 7.43499
val_loss= 6.47618
val_loss= 7.61255
val_loss= 8.86896
val_loss= 3.91516
adv_loss= 3.07506
adv_loss= 1.00137
adv_loss= 0.62060
adv_loss= 1.63262
adv_loss= 0.96037
adv_loss= 1.16516
adv_loss= 1.26992
adv_loss= 1.00732
adv_loss= 0.66959
adv_loss= 3.13419
surrogate=-0.01644, entropy= 0.33673, loss=-0.01644
surrogate= 0.02248, entropy= 0.33654, loss= 0.02248
surrogate=-0.02011, entropy= 0.33617, loss=-0.02011
surrogate= 0.00856, entropy= 0.33592, loss= 0.00856
surrogate=-0.00358, entropy= 0.33573, loss=-0.00358
surrogate=-0.01269, entropy= 0.33519, loss=-0.01269
surrogate=-0.02833, entropy= 0.33522, loss=-0.02833
surrogate=-0.00280, entropy= 0.33431, loss=-0.00280
surrogate= 0.01353, entropy= 0.33448, loss= 0.01353
surrogate= 0.00302, entropy= 0.33465, loss= 0.00302
std_min= 0.21657, std_max= 0.31821, std_mean= 0.27402
val lr: [4.841188524590165e-05], policy lr: [5.809426229508197e-05]
Policy Loss: 0.0030243, | Entropy Bonus: -0, | Value Loss: 3.9152, | Advantage Loss: 3.1342
Time elapsed (s): 1.6369047164916992
Agent stdevs: 0.274021
--------------------------------------------------------------------------------

Step 787
++++++++ Policy training ++++++++++
Current mean reward: 2253.815258 | mean episode length: 605.666667
val_loss= 9.26103
val_loss=13.66396
val_loss= 7.42098
val_loss= 8.17006
val_loss= 8.50219
val_loss= 8.65820
val_loss= 6.20294
val_loss= 4.06205
val_loss= 5.32623
val_loss= 5.70389
adv_loss= 1.18409
adv_loss= 0.63516
adv_loss= 0.49301
adv_loss= 0.95194
adv_loss= 1.12509
adv_loss= 1.09698
adv_loss= 0.95646
adv_loss= 1.02890
adv_loss= 1.08055
adv_loss= 0.94395
surrogate= 0.00979, entropy= 0.33447, loss= 0.00979
surrogate=-0.00523, entropy= 0.33263, loss=-0.00523
surrogate=-0.00150, entropy= 0.33247, loss=-0.00150
surrogate= 0.02253, entropy= 0.33126, loss= 0.02253
surrogate=-0.03439, entropy= 0.33080, loss=-0.03439
surrogate= 0.00866, entropy= 0.32951, loss= 0.00866
surrogate=-0.00787, entropy= 0.32878, loss=-0.00787
surrogate=-0.00179, entropy= 0.32830, loss=-0.00179
surrogate=-0.01591, entropy= 0.32689, loss=-0.01591
surrogate= 0.00889, entropy= 0.32555, loss= 0.00889
std_min= 0.21573, std_max= 0.31807, std_mean= 0.27324
val lr: [4.815573770491804e-05], policy lr: [5.778688524590165e-05]
Policy Loss: 0.0088947, | Entropy Bonus: -0, | Value Loss: 5.7039, | Advantage Loss: 0.94395
Time elapsed (s): 1.6435351371765137
Agent stdevs: 0.27323887
--------------------------------------------------------------------------------

Step 788
++++++++ Policy training ++++++++++
Current mean reward: 2390.678632 | mean episode length: 665.500000
val_loss= 9.09294
val_loss=12.33930
val_loss=562.10358
val_loss=39.18483
val_loss=1195.16504
val_loss=91.99120
val_loss=35.42216
val_loss=33.09745
val_loss=39.78807
val_loss=58.03618
adv_loss=1948.27979
adv_loss= 1.47309
adv_loss= 0.89371
adv_loss= 0.58208
adv_loss= 1.44537
adv_loss= 0.98512
adv_loss= 1.33160
adv_loss= 0.60809
adv_loss= 0.77083
adv_loss= 1.18127
surrogate= 0.00179, entropy= 0.32584, loss= 0.00179
surrogate=-0.02013, entropy= 0.32584, loss=-0.02013
surrogate= 0.06007, entropy= 0.32624, loss= 0.06007
surrogate=-0.01939, entropy= 0.32629, loss=-0.01939
surrogate=-0.02771, entropy= 0.32637, loss=-0.02771
surrogate= 0.00575, entropy= 0.32652, loss= 0.00575
surrogate=-0.02507, entropy= 0.32685, loss=-0.02507
surrogate=-0.01018, entropy= 0.32704, loss=-0.01018
surrogate= 0.00463, entropy= 0.32674, loss= 0.00463
surrogate=-0.02774, entropy= 0.32714, loss=-0.02774
std_min= 0.21551, std_max= 0.31863, std_mean= 0.27343
val lr: [4.7899590163934415e-05], policy lr: [5.7479508196721294e-05]
Policy Loss: -0.027741, | Entropy Bonus: -0, | Value Loss: 58.036, | Advantage Loss: 1.1813
Time elapsed (s): 1.6317105293273926
Agent stdevs: 0.2734331
--------------------------------------------------------------------------------

Step 789
++++++++ Policy training ++++++++++
Current mean reward: 2150.800027 | mean episode length: 597.333333
val_loss=28.29218
val_loss=100.55721
val_loss=202.82727
val_loss=17.99355
val_loss=1455.86511
val_loss=22.73845
val_loss=1567.15161
val_loss=999.60406
val_loss=20.63962
val_loss=949.03906
adv_loss= 3.29733
adv_loss= 1.58682
adv_loss= 0.57960
adv_loss= 2.91684
adv_loss= 2.22775
adv_loss= 1.01421
adv_loss= 0.41827
adv_loss= 0.88420
adv_loss= 1.35923
adv_loss= 4.36552
surrogate=-0.00102, entropy= 0.32694, loss=-0.00102
surrogate=-0.00178, entropy= 0.32737, loss=-0.00178
surrogate=-0.00379, entropy= 0.32758, loss=-0.00379
surrogate= 0.01597, entropy= 0.32786, loss= 0.01597
surrogate=-0.03037, entropy= 0.32828, loss=-0.03037
surrogate=-0.02554, entropy= 0.32867, loss=-0.02554
surrogate=-0.01094, entropy= 0.32899, loss=-0.01094
surrogate=-0.02855, entropy= 0.32899, loss=-0.02855
surrogate=-0.03082, entropy= 0.32964, loss=-0.03082
surrogate=-0.02830, entropy= 0.32999, loss=-0.02830
std_min= 0.21606, std_max= 0.31872, std_mean= 0.27365
val lr: [4.764344262295081e-05], policy lr: [5.717213114754097e-05]
Policy Loss: -0.028295, | Entropy Bonus: -0, | Value Loss: 949.04, | Advantage Loss: 4.3655
Time elapsed (s): 1.6406867504119873
Agent stdevs: 0.2736455
--------------------------------------------------------------------------------

Step 790
++++++++ Policy training ++++++++++
Current mean reward: 1777.235384 | mean episode length: 474.000000
val_loss=13.31408
val_loss=10.61138
val_loss=12.50462
val_loss= 5.18320
val_loss= 6.18855
val_loss= 6.76392
val_loss= 4.39696
val_loss= 6.33720
val_loss= 6.49122
val_loss= 4.69640
adv_loss= 0.57815
adv_loss= 0.57101
adv_loss= 0.68163
adv_loss= 0.65156
adv_loss= 1.20157
adv_loss= 1.04711
adv_loss= 2.52318
adv_loss= 1.04065
adv_loss= 1.36965
adv_loss= 1.80300
surrogate= 0.01962, entropy= 0.32914, loss= 0.01962
surrogate=-0.01202, entropy= 0.32876, loss=-0.01202
surrogate=-0.00876, entropy= 0.32845, loss=-0.00876
surrogate=-0.01381, entropy= 0.32816, loss=-0.01381
surrogate=-0.01856, entropy= 0.32792, loss=-0.01856
surrogate=-0.02575, entropy= 0.32808, loss=-0.02575
surrogate=-0.02171, entropy= 0.32802, loss=-0.02171
surrogate=-0.02238, entropy= 0.32755, loss=-0.02238
surrogate=-0.00974, entropy= 0.32728, loss=-0.00974
surrogate= 0.00074, entropy= 0.32706, loss= 0.00074
std_min= 0.21633, std_max= 0.31715, std_mean= 0.27329
val lr: [4.738729508196721e-05], policy lr: [5.686475409836064e-05]
Policy Loss: 0.00074207, | Entropy Bonus: -0, | Value Loss: 4.6964, | Advantage Loss: 1.803
Time elapsed (s): 1.663102626800537
Agent stdevs: 0.27328658
--------------------------------------------------------------------------------

Step 791
++++++++ Policy training ++++++++++
Current mean reward: 2291.142233 | mean episode length: 614.666667
val_loss= 5.90696
val_loss= 8.31375
val_loss= 5.15809
val_loss= 3.55313
val_loss= 5.14352
val_loss= 5.58110
val_loss= 4.20101
val_loss= 3.26196
val_loss= 3.15982
val_loss= 3.97073
adv_loss= 1.75053
adv_loss= 0.78096
adv_loss= 0.46123
adv_loss= 1.83358
adv_loss= 1.85574
adv_loss= 0.60336
adv_loss= 0.89259
adv_loss= 0.61012
adv_loss= 0.81910
adv_loss= 1.38673
surrogate=-0.01184, entropy= 0.32710, loss=-0.01184
surrogate=-0.00457, entropy= 0.32712, loss=-0.00457
surrogate=-0.02669, entropy= 0.32694, loss=-0.02669
surrogate=-0.00371, entropy= 0.32697, loss=-0.00371
surrogate=-0.00540, entropy= 0.32657, loss=-0.00540
surrogate= 0.00394, entropy= 0.32670, loss= 0.00394
surrogate= 0.00613, entropy= 0.32624, loss= 0.00613
surrogate=-0.01533, entropy= 0.32580, loss=-0.01533
surrogate=-0.00363, entropy= 0.32551, loss=-0.00363
surrogate=-0.00657, entropy= 0.32518, loss=-0.00657
std_min= 0.21548, std_max= 0.31781, std_mean= 0.27322
val lr: [4.71311475409836e-05], policy lr: [5.655737704918032e-05]
Policy Loss: -0.0065654, | Entropy Bonus: -0, | Value Loss: 3.9707, | Advantage Loss: 1.3867
Time elapsed (s): 1.6847612857818604
Agent stdevs: 0.27322158
--------------------------------------------------------------------------------

Step 792
++++++++ Policy training ++++++++++
Current mean reward: 2492.914597 | mean episode length: 705.000000
val_loss=363.42352
val_loss=33.71437
val_loss=1543.41968
val_loss=797.20453
val_loss=38.89156
val_loss=154.74962
val_loss=1181.28540
val_loss=479.63553
val_loss=1011.30054
val_loss=54.01051
adv_loss= 0.58936
adv_loss= 1.57117
adv_loss= 1.31238
adv_loss= 1.52487
adv_loss= 2.64852
adv_loss= 0.82661
adv_loss= 1.56386
adv_loss= 2.48257
adv_loss= 1.34070
adv_loss= 0.79504
surrogate=-0.00902, entropy= 0.32521, loss=-0.00902
surrogate=-0.01507, entropy= 0.32549, loss=-0.01507
surrogate=-0.02585, entropy= 0.32553, loss=-0.02585
surrogate=-0.00511, entropy= 0.32563, loss=-0.00511
surrogate=-0.00466, entropy= 0.32593, loss=-0.00466
surrogate= 0.03450, entropy= 0.32658, loss= 0.03450
surrogate= 0.00231, entropy= 0.32631, loss= 0.00231
surrogate=-0.01223, entropy= 0.32659, loss=-0.01223
surrogate=-0.02554, entropy= 0.32721, loss=-0.02554
surrogate=-0.02010, entropy= 0.32812, loss=-0.02010
std_min= 0.21584, std_max= 0.31824, std_mean= 0.27348
val lr: [4.6875e-05], policy lr: [5.625e-05]
Policy Loss: -0.020097, | Entropy Bonus: -0, | Value Loss: 54.011, | Advantage Loss: 0.79504
Time elapsed (s): 1.6526801586151123
Agent stdevs: 0.2734821
--------------------------------------------------------------------------------

Step 793
++++++++ Policy training ++++++++++
Current mean reward: 2108.756898 | mean episode length: 569.333333
val_loss=20.77541
val_loss=16.11935
val_loss=16.06362
val_loss=13.62125
val_loss= 7.41896
val_loss= 8.49794
val_loss=10.51062
val_loss=12.37822
val_loss= 7.40206
val_loss= 7.64102
adv_loss= 0.68375
adv_loss= 1.43358
adv_loss= 0.50520
adv_loss= 0.72230
adv_loss= 0.77931
adv_loss= 0.96212
adv_loss= 0.74254
adv_loss= 0.73997
adv_loss= 0.68689
adv_loss= 0.94968
surrogate= 0.00314, entropy= 0.32796, loss= 0.00314
surrogate=-0.00269, entropy= 0.32752, loss=-0.00269
surrogate= 0.03062, entropy= 0.32728, loss= 0.03062
surrogate= 0.00257, entropy= 0.32662, loss= 0.00257
surrogate=-0.01642, entropy= 0.32635, loss=-0.01642
surrogate=-0.02147, entropy= 0.32597, loss=-0.02147
surrogate=-0.01668, entropy= 0.32593, loss=-0.01668
surrogate=-0.01713, entropy= 0.32534, loss=-0.01713
surrogate=-0.01109, entropy= 0.32511, loss=-0.01109
surrogate= 0.00254, entropy= 0.32485, loss= 0.00254
std_min= 0.21598, std_max= 0.31822, std_mean= 0.27315
val lr: [4.66188524590164e-05], policy lr: [5.594262295081967e-05]
Policy Loss: 0.0025439, | Entropy Bonus: -0, | Value Loss: 7.641, | Advantage Loss: 0.94968
Time elapsed (s): 1.6471338272094727
Agent stdevs: 0.2731484
--------------------------------------------------------------------------------

Step 794
++++++++ Policy training ++++++++++
Current mean reward: 3527.054876 | mean episode length: 1000.000000
val_loss=289.53229
val_loss=50.18570
val_loss=2445.26562
val_loss=380.63358
val_loss=108.16798
val_loss=1447.22070
val_loss=114.26448
val_loss=323.65164
val_loss=299.16913
val_loss=91.32613
adv_loss= 2.03611
adv_loss= 3.34063
adv_loss= 1.23279
adv_loss= 1.39235
adv_loss= 1.95807
adv_loss= 0.87637
adv_loss= 1.44221
adv_loss= 4.38510
adv_loss=1974.99023
adv_loss= 1.64838
surrogate=-0.01530, entropy= 0.32550, loss=-0.01530
surrogate=-0.00576, entropy= 0.32611, loss=-0.00576
surrogate=-0.01969, entropy= 0.32676, loss=-0.01969
surrogate= 0.01295, entropy= 0.32768, loss= 0.01295
surrogate= 0.00248, entropy= 0.32801, loss= 0.00248
surrogate=-0.02692, entropy= 0.32858, loss=-0.02692
surrogate=-0.02997, entropy= 0.32914, loss=-0.02997
surrogate=-0.00610, entropy= 0.32960, loss=-0.00610
surrogate=-0.00872, entropy= 0.33022, loss=-0.00872
surrogate=-0.02338, entropy= 0.33051, loss=-0.02338
std_min= 0.21667, std_max= 0.31919, std_mean= 0.27365
val lr: [4.6362704918032794e-05], policy lr: [5.563524590163935e-05]
Policy Loss: -0.02338, | Entropy Bonus: -0, | Value Loss: 91.326, | Advantage Loss: 1.6484
Time elapsed (s): 1.6299734115600586
Agent stdevs: 0.2736523
--------------------------------------------------------------------------------

Step 795
++++++++ Policy training ++++++++++
Current mean reward: 1666.902550 | mean episode length: 451.666667
val_loss=22.06229
val_loss=23.01518
val_loss=22.41454
val_loss=31.78826
val_loss=12.96400
val_loss=13.25899
val_loss=11.86266
val_loss=21.51800
val_loss=10.53170
val_loss=12.75347
adv_loss= 2.47202
adv_loss= 1.26505
adv_loss= 2.60575
adv_loss= 1.64647
adv_loss= 1.06860
adv_loss= 1.35183
adv_loss= 3.24561
adv_loss= 2.29300
adv_loss= 2.10963
adv_loss= 0.62331
surrogate=-0.01416, entropy= 0.33029, loss=-0.01416
surrogate=-0.00762, entropy= 0.32948, loss=-0.00762
surrogate=-0.01206, entropy= 0.32963, loss=-0.01206
surrogate=-0.03924, entropy= 0.32923, loss=-0.03924
surrogate=-0.02506, entropy= 0.32915, loss=-0.02506
surrogate= 0.00942, entropy= 0.32926, loss= 0.00942
surrogate=-0.01748, entropy= 0.32878, loss=-0.01748
surrogate=-0.04516, entropy= 0.32865, loss=-0.04516
surrogate=-0.02641, entropy= 0.32793, loss=-0.02641
surrogate=-0.01599, entropy= 0.32770, loss=-0.01599
std_min= 0.21575, std_max= 0.31941, std_mean= 0.27349
val lr: [4.6106557377049194e-05], policy lr: [5.532786885245902e-05]
Policy Loss: -0.01599, | Entropy Bonus: -0, | Value Loss: 12.753, | Advantage Loss: 0.62331
Time elapsed (s): 1.6514015197753906
Agent stdevs: 0.2734867
--------------------------------------------------------------------------------

Step 796
++++++++ Policy training ++++++++++
Current mean reward: 3534.941970 | mean episode length: 1000.000000
val_loss=756.05469
val_loss=2353.45288
val_loss=610.38928
val_loss=875.56543
val_loss=2599.69214
val_loss=662.82825
val_loss=2176.66797
val_loss=2587.31128
val_loss=1063.58691
val_loss=932.88409
adv_loss= 1.27709
adv_loss= 0.70948
adv_loss= 0.77011
adv_loss=1866.86475
adv_loss= 0.88756
adv_loss= 0.88530
adv_loss= 0.58483
adv_loss= 1.41120
adv_loss= 0.70453
adv_loss= 1.23880
surrogate= 0.03963, entropy= 0.32668, loss= 0.03963
surrogate=-0.03082, entropy= 0.32639, loss=-0.03082
surrogate=-0.01288, entropy= 0.32630, loss=-0.01288
surrogate=-0.00119, entropy= 0.32663, loss=-0.00119
surrogate=-0.01570, entropy= 0.32637, loss=-0.01570
surrogate= 0.00113, entropy= 0.32575, loss= 0.00113
surrogate=-0.01908, entropy= 0.32504, loss=-0.01908
surrogate=-0.01281, entropy= 0.32478, loss=-0.01281
surrogate=-0.01384, entropy= 0.32481, loss=-0.01384
surrogate=-0.01340, entropy= 0.32408, loss=-0.01340
std_min= 0.21562, std_max= 0.31916, std_mean= 0.27315
val lr: [4.585040983606559e-05], policy lr: [5.5020491803278696e-05]
Policy Loss: -0.013396, | Entropy Bonus: -0, | Value Loss: 932.88, | Advantage Loss: 1.2388
Time elapsed (s): 1.6454102993011475
Agent stdevs: 0.27314845
--------------------------------------------------------------------------------

Step 797
++++++++ Policy training ++++++++++
Current mean reward: 1588.322136 | mean episode length: 428.750000
val_loss=11.34440
val_loss=14.01712
val_loss=12.31955
val_loss=12.36291
val_loss=12.04011
val_loss=17.90733
val_loss= 7.87712
val_loss=15.52624
val_loss=11.16809
val_loss= 7.27423
adv_loss= 1.35740
adv_loss= 3.32699
adv_loss= 3.65607
adv_loss= 0.76233
adv_loss= 0.94189
adv_loss= 3.11213
adv_loss= 1.59629
adv_loss= 1.48349
adv_loss= 1.30945
adv_loss= 0.86969
surrogate= 0.02316, entropy= 0.32325, loss= 0.02316
surrogate=-0.01734, entropy= 0.32271, loss=-0.01734
surrogate=-0.00562, entropy= 0.32195, loss=-0.00562
surrogate=-0.01228, entropy= 0.32147, loss=-0.01228
surrogate=-0.03184, entropy= 0.32070, loss=-0.03184
surrogate=-0.02711, entropy= 0.32024, loss=-0.02711
surrogate=-0.01530, entropy= 0.31974, loss=-0.01530
surrogate=-0.01466, entropy= 0.31894, loss=-0.01466
surrogate=-0.00929, entropy= 0.31826, loss=-0.00929
surrogate=-0.01726, entropy= 0.31739, loss=-0.01726
std_min= 0.21620, std_max= 0.31889, std_mean= 0.27245
val lr: [4.559426229508196e-05], policy lr: [5.4713114754098344e-05]
Policy Loss: -0.017264, | Entropy Bonus: -0, | Value Loss: 7.2742, | Advantage Loss: 0.86969
Time elapsed (s): 1.6748015880584717
Agent stdevs: 0.27244547
--------------------------------------------------------------------------------

Step 798
++++++++ Policy training ++++++++++
Current mean reward: 2662.713580 | mean episode length: 744.500000
val_loss=375.13770
val_loss=601.30518
val_loss=23.08695
val_loss=324.51862
val_loss=17.39434
val_loss=56.62454
val_loss=881.50958
val_loss=34.44408
val_loss=42.60500
val_loss=20.54200
adv_loss= 1.06326
adv_loss= 0.78478
adv_loss= 1.24149
adv_loss= 1.54620
adv_loss= 2.50104
adv_loss= 0.72054
adv_loss= 0.97427
adv_loss= 1.41712
adv_loss= 2.87103
adv_loss= 2.61166
surrogate= 0.00996, entropy= 0.31643, loss= 0.00996
surrogate=-0.00088, entropy= 0.31472, loss=-0.00088
surrogate=-0.02682, entropy= 0.31436, loss=-0.02682
surrogate= 0.01241, entropy= 0.31393, loss= 0.01241
surrogate=-0.00526, entropy= 0.31380, loss=-0.00526
surrogate= 0.00159, entropy= 0.31292, loss= 0.00159
surrogate= 0.05553, entropy= 0.31222, loss= 0.05553
surrogate=-0.00377, entropy= 0.31206, loss=-0.00377
surrogate=-0.00713, entropy= 0.31120, loss=-0.00713
surrogate=-0.01292, entropy= 0.31125, loss=-0.01292
std_min= 0.21565, std_max= 0.31853, std_mean= 0.27191
val lr: [4.533811475409835e-05], policy lr: [5.440573770491802e-05]
Policy Loss: -0.012917, | Entropy Bonus: -0, | Value Loss: 20.542, | Advantage Loss: 2.6117
Time elapsed (s): 1.6358776092529297
Agent stdevs: 0.27191332
--------------------------------------------------------------------------------

Step 799
++++++++ Policy training ++++++++++
Current mean reward: 2293.804858 | mean episode length: 634.333333
val_loss=91.37623
val_loss=26.09628
val_loss=85.75135
val_loss=1371.31201
val_loss=646.57330
val_loss=1404.34131
val_loss=1349.13525
val_loss=160.46031
val_loss=26.44142
val_loss=28.69909
adv_loss= 1.47840
adv_loss= 0.64753
adv_loss= 0.96198
adv_loss= 1.49125
adv_loss= 1.27455
adv_loss= 1.11031
adv_loss= 0.91848
adv_loss= 1.23067
adv_loss= 0.53028
adv_loss= 0.86767
surrogate= 0.00675, entropy= 0.31107, loss= 0.00675
surrogate=-0.00264, entropy= 0.31106, loss=-0.00264
surrogate= 0.01305, entropy= 0.31116, loss= 0.01305
surrogate=-0.01180, entropy= 0.31100, loss=-0.01180
surrogate=-0.00579, entropy= 0.31120, loss=-0.00579
surrogate=-0.00386, entropy= 0.31113, loss=-0.00386
surrogate=-0.02435, entropy= 0.31075, loss=-0.02435
surrogate= 0.01798, entropy= 0.31029, loss= 0.01798
surrogate=-0.00039, entropy= 0.31004, loss=-0.00039
surrogate=-0.00225, entropy= 0.30985, loss=-0.00225
std_min= 0.21544, std_max= 0.31783, std_mean= 0.27177
val lr: [4.508196721311475e-05], policy lr: [5.409836065573769e-05]
Policy Loss: -0.0022524, | Entropy Bonus: -0, | Value Loss: 28.699, | Advantage Loss: 0.86767
Time elapsed (s): 1.6253552436828613
Agent stdevs: 0.27177402
--------------------------------------------------------------------------------

Step 800
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2349.3
++++++++ Policy training ++++++++++
Current mean reward: 3613.632723 | mean episode length: 1000.000000
val_loss=767.16437
val_loss=31.08290
val_loss=1511.57947
val_loss=1076.46326
val_loss=235.61221
val_loss=70.29066
val_loss=1623.94641
val_loss=486.44946
val_loss=342.21063
val_loss=36.17744
adv_loss= 2.55829
adv_loss= 1.82992
adv_loss= 1.20857
adv_loss= 2.63522
adv_loss= 5.13921
adv_loss= 3.17103
adv_loss= 0.43890
adv_loss=1713.51672
adv_loss= 2.37322
adv_loss= 1.46722
surrogate= 0.00914, entropy= 0.30928, loss= 0.00914
surrogate= 0.00522, entropy= 0.30887, loss= 0.00522
surrogate=-0.02350, entropy= 0.30962, loss=-0.02350
surrogate=-0.01103, entropy= 0.31031, loss=-0.01103
surrogate=-0.01016, entropy= 0.31042, loss=-0.01016
surrogate= 0.02666, entropy= 0.31056, loss= 0.02666
surrogate=-0.02922, entropy= 0.31052, loss=-0.02922
surrogate=-0.00519, entropy= 0.31074, loss=-0.00519
surrogate=-0.01492, entropy= 0.31145, loss=-0.01492
surrogate=-0.02551, entropy= 0.31207, loss=-0.02551
std_min= 0.21570, std_max= 0.31764, std_mean= 0.27195
val lr: [4.4825819672131146e-05], policy lr: [5.379098360655737e-05]
Policy Loss: -0.025513, | Entropy Bonus: -0, | Value Loss: 36.177, | Advantage Loss: 1.4672
Time elapsed (s): 1.6405088901519775
Agent stdevs: 0.27195063
--------------------------------------------------------------------------------

Step 801
++++++++ Policy training ++++++++++
Current mean reward: 2206.236643 | mean episode length: 608.333333
val_loss=33.54707
val_loss=104.87315
val_loss=277.01346
val_loss=45.33293
val_loss=137.55235
val_loss=2263.88037
val_loss=555.16986
val_loss=409.85898
val_loss=15.97338
val_loss=299.89999
adv_loss= 1.91688
adv_loss=1008.07440
adv_loss= 2.06468
adv_loss= 1.73737
adv_loss= 1.87630
adv_loss= 2.39739
adv_loss= 3.81041
adv_loss= 2.40876
adv_loss= 2.51044
adv_loss= 2.11295
surrogate=-0.01204, entropy= 0.31250, loss=-0.01204
surrogate=-0.01263, entropy= 0.31261, loss=-0.01263
surrogate=-0.02490, entropy= 0.31252, loss=-0.02490
surrogate=-0.01233, entropy= 0.31328, loss=-0.01233
surrogate=-0.02757, entropy= 0.31298, loss=-0.02757
surrogate=-0.02216, entropy= 0.31299, loss=-0.02216
surrogate= 0.00071, entropy= 0.31316, loss= 0.00071
surrogate=-0.01519, entropy= 0.31330, loss=-0.01519
surrogate=-0.02164, entropy= 0.31341, loss=-0.02164
surrogate=-0.03051, entropy= 0.31350, loss=-0.03051
std_min= 0.21558, std_max= 0.31833, std_mean= 0.27212
val lr: [4.4569672131147546e-05], policy lr: [5.348360655737705e-05]
Policy Loss: -0.030506, | Entropy Bonus: -0, | Value Loss: 299.9, | Advantage Loss: 2.1129
Time elapsed (s): 1.654296875
Agent stdevs: 0.27212316
--------------------------------------------------------------------------------

Step 802
++++++++ Policy training ++++++++++
Current mean reward: 2471.056937 | mean episode length: 673.000000
val_loss=13.70371
val_loss=18.80934
val_loss=10.00476
val_loss=14.53227
val_loss=11.39868
val_loss= 8.76994
val_loss= 8.25017
val_loss=10.26312
val_loss= 7.45280
val_loss= 8.72898
adv_loss= 0.71783
adv_loss= 1.95353
adv_loss= 2.92110
adv_loss= 1.04215
adv_loss= 1.35431
adv_loss= 1.58441
adv_loss= 1.04854
adv_loss= 1.90779
adv_loss= 2.00000
adv_loss= 0.71282
surrogate= 0.02192, entropy= 0.31341, loss= 0.02192
surrogate= 0.04423, entropy= 0.31385, loss= 0.04423
surrogate=-0.02070, entropy= 0.31390, loss=-0.02070
surrogate=-0.01940, entropy= 0.31393, loss=-0.01940
surrogate=-0.04306, entropy= 0.31373, loss=-0.04306
surrogate=-0.00772, entropy= 0.31385, loss=-0.00772
surrogate=-0.01479, entropy= 0.31352, loss=-0.01479
surrogate= 0.00052, entropy= 0.31296, loss= 0.00052
surrogate=-0.02283, entropy= 0.31270, loss=-0.02283
surrogate=-0.00379, entropy= 0.31261, loss=-0.00379
std_min= 0.21586, std_max= 0.31686, std_mean= 0.27195
val lr: [4.431352459016394e-05], policy lr: [5.317622950819672e-05]
Policy Loss: -0.0037942, | Entropy Bonus: -0, | Value Loss: 8.729, | Advantage Loss: 0.71282
Time elapsed (s): 1.6367483139038086
Agent stdevs: 0.2719527
--------------------------------------------------------------------------------

Step 803
++++++++ Policy training ++++++++++
Current mean reward: 3428.107809 | mean episode length: 946.500000
val_loss=390.19894
val_loss=11.40231
val_loss=1380.31238
val_loss=141.31660
val_loss=1185.88232
val_loss=1036.26904
val_loss=714.69019
val_loss=95.74466
val_loss=614.44031
val_loss=22.16103
adv_loss= 0.92700
adv_loss= 1.56418
adv_loss= 0.86401
adv_loss= 0.93227
adv_loss= 0.93109
adv_loss= 0.89179
adv_loss= 0.90279
adv_loss= 1.34723
adv_loss= 0.67422
adv_loss= 0.55332
surrogate=-0.01095, entropy= 0.31171, loss=-0.01095
surrogate= 0.00605, entropy= 0.31162, loss= 0.00605
surrogate=-0.00279, entropy= 0.31176, loss=-0.00279
surrogate=-0.00568, entropy= 0.31132, loss=-0.00568
surrogate=-0.01943, entropy= 0.31085, loss=-0.01943
surrogate=-0.01890, entropy= 0.31117, loss=-0.01890
surrogate= 0.00505, entropy= 0.31142, loss= 0.00505
surrogate=-0.01647, entropy= 0.31132, loss=-0.01647
surrogate=-0.02710, entropy= 0.31056, loss=-0.02710
surrogate= 0.02215, entropy= 0.31003, loss= 0.02215
std_min= 0.21577, std_max= 0.31668, std_mean= 0.27171
val lr: [4.405737704918034e-05], policy lr: [5.28688524590164e-05]
Policy Loss: 0.022152, | Entropy Bonus: -0, | Value Loss: 22.161, | Advantage Loss: 0.55332
Time elapsed (s): 1.694032907485962
Agent stdevs: 0.27171472
--------------------------------------------------------------------------------

Step 804
++++++++ Policy training ++++++++++
Current mean reward: 2725.918827 | mean episode length: 760.500000
val_loss=113.64971
val_loss=579.09814
val_loss=1247.32312
val_loss=102.09526
val_loss=953.35699
val_loss=1061.84973
val_loss=366.66193
val_loss=184.13957
val_loss=113.50642
val_loss=29.21881
adv_loss= 1.61004
adv_loss= 0.55750
adv_loss= 0.78002
adv_loss= 2.24573
adv_loss= 1.28733
adv_loss= 1.97186
adv_loss= 1.49265
adv_loss= 0.73118
adv_loss= 2.97456
adv_loss= 1.21145
surrogate=-0.00733, entropy= 0.30969, loss=-0.00733
surrogate= 0.00903, entropy= 0.30932, loss= 0.00903
surrogate=-0.01767, entropy= 0.30942, loss=-0.01767
surrogate=-0.00649, entropy= 0.30900, loss=-0.00649
surrogate=-0.01155, entropy= 0.30948, loss=-0.01155
surrogate=-0.00770, entropy= 0.30979, loss=-0.00770
surrogate=-0.00517, entropy= 0.30950, loss=-0.00517
surrogate=-0.00001, entropy= 0.30944, loss=-0.00001
surrogate=-0.01509, entropy= 0.30977, loss=-0.01509
surrogate= 0.02754, entropy= 0.30933, loss= 0.02754
std_min= 0.21599, std_max= 0.31677, std_mean= 0.27163
val lr: [4.380122950819673e-05], policy lr: [5.256147540983607e-05]
Policy Loss: 0.027543, | Entropy Bonus: -0, | Value Loss: 29.219, | Advantage Loss: 1.2114
Time elapsed (s): 1.6585607528686523
Agent stdevs: 0.2716305
--------------------------------------------------------------------------------

Step 805
++++++++ Policy training ++++++++++
Current mean reward: 2036.810362 | mean episode length: 553.333333
val_loss=17.73007
val_loss=11.61082
val_loss=14.40445
val_loss=10.87732
val_loss= 6.01894
val_loss= 8.62074
val_loss=12.82411
val_loss= 5.27122
val_loss=10.61380
val_loss=10.90004
adv_loss= 3.01691
adv_loss= 0.66687
adv_loss= 1.59869
adv_loss= 1.47244
adv_loss= 1.39878
adv_loss= 1.02128
adv_loss= 4.08125
adv_loss= 0.77648
adv_loss= 0.75728
adv_loss= 2.24192
surrogate=-0.00076, entropy= 0.30907, loss=-0.00076
surrogate=-0.00047, entropy= 0.30849, loss=-0.00047
surrogate= 0.01824, entropy= 0.30750, loss= 0.01824
surrogate=-0.01078, entropy= 0.30668, loss=-0.01078
surrogate= 0.00161, entropy= 0.30611, loss= 0.00161
surrogate= 0.00668, entropy= 0.30505, loss= 0.00668
surrogate=-0.00599, entropy= 0.30414, loss=-0.00599
surrogate=-0.03317, entropy= 0.30326, loss=-0.03317
surrogate=-0.00557, entropy= 0.30211, loss=-0.00557
surrogate= 0.01180, entropy= 0.30208, loss= 0.01180
std_min= 0.21543, std_max= 0.31579, std_mean= 0.27097
val lr: [4.354508196721313e-05], policy lr: [5.225409836065575e-05]
Policy Loss: 0.011804, | Entropy Bonus: -0, | Value Loss: 10.9, | Advantage Loss: 2.2419
Time elapsed (s): 1.6251270771026611
Agent stdevs: 0.27096632
--------------------------------------------------------------------------------

Step 806
++++++++ Policy training ++++++++++
Current mean reward: 2439.202233 | mean episode length: 683.000000
val_loss=864.36853
val_loss=27.42346
val_loss=12.89339
val_loss=35.43598
val_loss=19.14570
val_loss=1003.59717
val_loss=813.21613
val_loss=1285.58044
val_loss=32.82156
val_loss=22.88567
adv_loss= 1.64913
adv_loss= 0.79215
adv_loss= 0.48109
adv_loss= 1.00559
adv_loss= 1.21445
adv_loss= 1.80959
adv_loss= 4.09143
adv_loss= 0.83359
adv_loss= 1.95009
adv_loss= 1.45405
surrogate=-0.01863, entropy= 0.30146, loss=-0.01863
surrogate= 0.00261, entropy= 0.30153, loss= 0.00261
surrogate= 0.00621, entropy= 0.30113, loss= 0.00621
surrogate= 0.00803, entropy= 0.30053, loss= 0.00803
surrogate= 0.01072, entropy= 0.29984, loss= 0.01072
surrogate=-0.00077, entropy= 0.29985, loss=-0.00077
surrogate=-0.02026, entropy= 0.29956, loss=-0.02026
surrogate=-0.01277, entropy= 0.29928, loss=-0.01277
surrogate= 0.00996, entropy= 0.29923, loss= 0.00996
surrogate=-0.00549, entropy= 0.29933, loss=-0.00549
std_min= 0.21555, std_max= 0.31513, std_mean= 0.27068
val lr: [4.32889344262295e-05], policy lr: [5.194672131147539e-05]
Policy Loss: -0.0054907, | Entropy Bonus: -0, | Value Loss: 22.886, | Advantage Loss: 1.4541
Time elapsed (s): 1.6406238079071045
Agent stdevs: 0.27067688
--------------------------------------------------------------------------------

Step 807
++++++++ Policy training ++++++++++
Current mean reward: 3523.165322 | mean episode length: 1000.000000
val_loss=70.84288
val_loss=1953.31299
val_loss=803.41095
val_loss=92.30920
val_loss=239.26682
val_loss=501.47568
val_loss=1355.24487
val_loss=79.71435
val_loss=419.86829
val_loss=840.15155
adv_loss= 4.28164
adv_loss= 1.55880
adv_loss= 3.48992
adv_loss= 5.85450
adv_loss= 2.69234
adv_loss= 6.85758
adv_loss= 2.06228
adv_loss= 1.97961
adv_loss= 2.13044
adv_loss= 1.64130
surrogate= 0.00788, entropy= 0.29913, loss= 0.00788
surrogate=-0.01040, entropy= 0.29913, loss=-0.01040
surrogate=-0.01799, entropy= 0.29897, loss=-0.01799
surrogate=-0.01758, entropy= 0.29876, loss=-0.01758
surrogate=-0.00874, entropy= 0.29893, loss=-0.00874
surrogate=-0.01278, entropy= 0.29879, loss=-0.01278
surrogate=-0.00944, entropy= 0.29872, loss=-0.00944
surrogate= 0.02744, entropy= 0.29864, loss= 0.02744
surrogate=-0.00275, entropy= 0.29847, loss=-0.00275
surrogate=-0.01631, entropy= 0.29797, loss=-0.01631
std_min= 0.21535, std_max= 0.31417, std_mean= 0.27054
val lr: [4.30327868852459e-05], policy lr: [5.163934426229507e-05]
Policy Loss: -0.016308, | Entropy Bonus: -0, | Value Loss: 840.15, | Advantage Loss: 1.6413
Time elapsed (s): 1.6590771675109863
Agent stdevs: 0.27053857
--------------------------------------------------------------------------------

Step 808
++++++++ Policy training ++++++++++
Current mean reward: 1888.253778 | mean episode length: 515.666667
val_loss=32.35527
val_loss=16.98693
val_loss=24.34945
val_loss=12.75849
val_loss=11.13059
val_loss=12.23591
val_loss=19.19523
val_loss=16.07126
val_loss=11.35809
val_loss= 7.73518
adv_loss= 4.57415
adv_loss= 1.52109
adv_loss= 1.86316
adv_loss= 2.12000
adv_loss= 2.19591
adv_loss= 2.35165
adv_loss= 1.94934
adv_loss= 1.98464
adv_loss= 3.46944
adv_loss= 2.87655
surrogate= 0.00409, entropy= 0.29828, loss= 0.00409
surrogate= 0.01960, entropy= 0.29781, loss= 0.01960
surrogate=-0.01800, entropy= 0.29772, loss=-0.01800
surrogate= 0.00597, entropy= 0.29714, loss= 0.00597
surrogate=-0.03161, entropy= 0.29688, loss=-0.03161
surrogate=-0.01412, entropy= 0.29626, loss=-0.01412
surrogate=-0.02030, entropy= 0.29576, loss=-0.02030
surrogate=-0.02102, entropy= 0.29474, loss=-0.02102
surrogate=-0.03007, entropy= 0.29441, loss=-0.03007
surrogate=-0.01222, entropy= 0.29358, loss=-0.01222
std_min= 0.21462, std_max= 0.31427, std_mean= 0.27020
val lr: [4.277663934426229e-05], policy lr: [5.133196721311474e-05]
Policy Loss: -0.012223, | Entropy Bonus: -0, | Value Loss: 7.7352, | Advantage Loss: 2.8766
Time elapsed (s): 1.647007703781128
Agent stdevs: 0.27020252
--------------------------------------------------------------------------------

Step 809
++++++++ Policy training ++++++++++
Current mean reward: 3496.692431 | mean episode length: 1000.000000
val_loss=554.89343
val_loss=185.61206
val_loss=161.61951
val_loss=101.00948
val_loss=1185.31421
val_loss=279.99979
val_loss=1616.35498
val_loss=1875.10522
val_loss=257.82587
val_loss=1595.01636
adv_loss= 7.65000
adv_loss= 4.18391
adv_loss= 3.08554
adv_loss= 1.47426
adv_loss= 1.97594
adv_loss=1671.73376
adv_loss=1731.49915
adv_loss=1671.55481
adv_loss= 4.85799
adv_loss= 4.90349
surrogate= 0.03348, entropy= 0.29356, loss= 0.03348
surrogate=-0.00592, entropy= 0.29369, loss=-0.00592
surrogate= 0.00609, entropy= 0.29432, loss= 0.00609
surrogate=-0.01440, entropy= 0.29452, loss=-0.01440
surrogate=-0.02146, entropy= 0.29472, loss=-0.02146
surrogate=-0.00083, entropy= 0.29491, loss=-0.00083
surrogate= 0.00515, entropy= 0.29496, loss= 0.00515
surrogate=-0.01448, entropy= 0.29508, loss=-0.01448
surrogate=-0.00261, entropy= 0.29523, loss=-0.00261
surrogate=-0.00016, entropy= 0.29557, loss=-0.00016
std_min= 0.21496, std_max= 0.31425, std_mean= 0.27036
val lr: [4.252049180327869e-05], policy lr: [5.102459016393442e-05]
Policy Loss: -0.00015586, | Entropy Bonus: -0, | Value Loss: 1595, | Advantage Loss: 4.9035
Time elapsed (s): 1.6927833557128906
Agent stdevs: 0.27035603
--------------------------------------------------------------------------------

Step 810
++++++++ Policy training ++++++++++
Current mean reward: 2881.856990 | mean episode length: 777.500000
val_loss=84.24596
val_loss=20.85161
val_loss=22.41902
val_loss=19.69387
val_loss=12.29729
val_loss=14.37553
val_loss=17.47563
val_loss=15.05010
val_loss=12.75201
val_loss=11.60203
adv_loss= 2.65667
adv_loss= 1.31452
adv_loss= 2.63139
adv_loss= 2.14285
adv_loss= 5.55264
adv_loss= 3.36105
adv_loss= 4.38449
adv_loss= 4.81064
adv_loss= 3.96584
adv_loss= 3.42659
surrogate= 0.00217, entropy= 0.29551, loss= 0.00217
surrogate= 0.01387, entropy= 0.29475, loss= 0.01387
surrogate=-0.00865, entropy= 0.29396, loss=-0.00865
surrogate= 0.00668, entropy= 0.29330, loss= 0.00668
surrogate=-0.01147, entropy= 0.29224, loss=-0.01147
surrogate=-0.01396, entropy= 0.29184, loss=-0.01396
surrogate=-0.01320, entropy= 0.29121, loss=-0.01320
surrogate=-0.03084, entropy= 0.29072, loss=-0.03084
surrogate=-0.02547, entropy= 0.28996, loss=-0.02547
surrogate=-0.00390, entropy= 0.28931, loss=-0.00390
std_min= 0.21481, std_max= 0.31341, std_mean= 0.26975
val lr: [4.2264344262295084e-05], policy lr: [5.07172131147541e-05]
Policy Loss: -0.0039018, | Entropy Bonus: -0, | Value Loss: 11.602, | Advantage Loss: 3.4266
Time elapsed (s): 1.6680285930633545
Agent stdevs: 0.26975003
--------------------------------------------------------------------------------

Step 811
++++++++ Policy training ++++++++++
Current mean reward: 3508.895523 | mean episode length: 1000.000000
val_loss=729.02881
val_loss=315.44302
val_loss=778.48730
val_loss=88.40508
val_loss=1256.86023
val_loss=1731.78674
val_loss=1999.40015
val_loss=552.13324
val_loss=2694.16772
val_loss=1452.23206
adv_loss= 3.86988
adv_loss= 1.35035
adv_loss= 1.24116
adv_loss= 2.14476
adv_loss= 1.73472
adv_loss= 1.63891
adv_loss= 1.38986
adv_loss= 1.20614
adv_loss= 1.04905
adv_loss=1669.47266
surrogate=-0.01731, entropy= 0.28874, loss=-0.01731
surrogate=-0.00758, entropy= 0.28898, loss=-0.00758
surrogate=-0.03082, entropy= 0.28940, loss=-0.03082
surrogate=-0.02041, entropy= 0.28975, loss=-0.02041
surrogate=-0.02226, entropy= 0.28959, loss=-0.02226
surrogate=-0.01378, entropy= 0.29013, loss=-0.01378
surrogate= 0.00020, entropy= 0.29047, loss= 0.00020
surrogate= 0.01115, entropy= 0.29053, loss= 0.01115
surrogate= 0.01114, entropy= 0.29080, loss= 0.01114
surrogate= 0.00511, entropy= 0.29125, loss= 0.00511
std_min= 0.21530, std_max= 0.31373, std_mean= 0.26990
val lr: [4.2008196721311484e-05], policy lr: [5.040983606557377e-05]
Policy Loss: 0.0051079, | Entropy Bonus: -0, | Value Loss: 1452.2, | Advantage Loss: 1669.5
Time elapsed (s): 1.6344068050384521
Agent stdevs: 0.26989624
--------------------------------------------------------------------------------

Step 812
++++++++ Policy training ++++++++++
Current mean reward: 2292.940061 | mean episode length: 636.333333
val_loss=316.39526
val_loss=376.61526
val_loss=1224.94861
val_loss=518.31335
val_loss=517.21515
val_loss=27.21392
val_loss=251.12296
val_loss=43.33980
val_loss=1173.54602
val_loss=29.01491
adv_loss= 2.47176
adv_loss= 1.98873
adv_loss= 2.43726
adv_loss= 2.08230
adv_loss=1853.36426
adv_loss= 2.57768
adv_loss= 1.70965
adv_loss= 1.92502
adv_loss= 2.27872
adv_loss= 1.29554
surrogate= 0.01822, entropy= 0.29125, loss= 0.01822
surrogate=-0.01484, entropy= 0.29154, loss=-0.01484
surrogate=-0.02432, entropy= 0.29159, loss=-0.02432
surrogate= 0.00154, entropy= 0.29139, loss= 0.00154
surrogate= 0.00064, entropy= 0.29212, loss= 0.00064
surrogate=-0.02370, entropy= 0.29226, loss=-0.02370
surrogate=-0.03691, entropy= 0.29286, loss=-0.03691
surrogate=-0.01275, entropy= 0.29291, loss=-0.01275
surrogate=-0.03622, entropy= 0.29267, loss=-0.03622
surrogate=-0.00663, entropy= 0.29295, loss=-0.00663
std_min= 0.21541, std_max= 0.31373, std_mean= 0.27004
val lr: [4.1752049180327877e-05], policy lr: [5.0102459016393447e-05]
Policy Loss: -0.0066269, | Entropy Bonus: -0, | Value Loss: 29.015, | Advantage Loss: 1.2955
Time elapsed (s): 1.6382761001586914
Agent stdevs: 0.27004388
--------------------------------------------------------------------------------

Step 813
++++++++ Policy training ++++++++++
Current mean reward: 2849.568415 | mean episode length: 792.500000
val_loss=22.58597
val_loss=65.19849
val_loss=1523.81335
val_loss=23.34595
val_loss=1211.99463
val_loss=77.41010
val_loss=1166.60242
val_loss=392.99496
val_loss=244.85051
val_loss=31.74456
adv_loss= 2.60739
adv_loss= 1.37307
adv_loss= 1.92680
adv_loss=1695.38660
adv_loss= 1.54252
adv_loss= 2.78789
adv_loss= 3.48923
adv_loss= 1.35476
adv_loss= 0.82312
adv_loss= 1.88782
surrogate=-0.00206, entropy= 0.29377, loss=-0.00206
surrogate=-0.01734, entropy= 0.29412, loss=-0.01734
surrogate=-0.01018, entropy= 0.29443, loss=-0.01018
surrogate= 0.01396, entropy= 0.29470, loss= 0.01396
surrogate=-0.00108, entropy= 0.29508, loss=-0.00108
surrogate= 0.01445, entropy= 0.29512, loss= 0.01445
surrogate=-0.00213, entropy= 0.29560, loss=-0.00213
surrogate=-0.00256, entropy= 0.29562, loss=-0.00256
surrogate=-0.01921, entropy= 0.29581, loss=-0.01921
surrogate=-0.01809, entropy= 0.29609, loss=-0.01809
std_min= 0.21532, std_max= 0.31392, std_mean= 0.27036
val lr: [4.1495901639344276e-05], policy lr: [4.979508196721312e-05]
Policy Loss: -0.018087, | Entropy Bonus: -0, | Value Loss: 31.745, | Advantage Loss: 1.8878
Time elapsed (s): 1.6497242450714111
Agent stdevs: 0.27035603
--------------------------------------------------------------------------------

Step 814
++++++++ Policy training ++++++++++
Current mean reward: 2082.507601 | mean episode length: 586.333333
val_loss=282.64209
val_loss=49.12740
val_loss=137.93805
val_loss=885.87781
val_loss=52.17958
val_loss=728.46069
val_loss=1142.04700
val_loss=201.36664
val_loss=29.33001
val_loss=37.30370
adv_loss= 1.95267
adv_loss= 1.30470
adv_loss= 4.72573
adv_loss=1751.46497
adv_loss= 2.18902
adv_loss= 2.84253
adv_loss= 2.24783
adv_loss= 1.46093
adv_loss= 2.14394
adv_loss= 2.69962
surrogate=-0.01727, entropy= 0.29632, loss=-0.01727
surrogate=-0.00025, entropy= 0.29643, loss=-0.00025
surrogate=-0.01605, entropy= 0.29655, loss=-0.01605
surrogate=-0.01505, entropy= 0.29647, loss=-0.01505
surrogate= 0.00039, entropy= 0.29625, loss= 0.00039
surrogate=-0.01316, entropy= 0.29589, loss=-0.01316
surrogate=-0.00726, entropy= 0.29565, loss=-0.00726
surrogate= 0.00033, entropy= 0.29586, loss= 0.00033
surrogate= 0.00398, entropy= 0.29575, loss= 0.00398
surrogate=-0.02542, entropy= 0.29595, loss=-0.02542
std_min= 0.21499, std_max= 0.31423, std_mean= 0.27039
val lr: [4.123975409836064e-05], policy lr: [4.948770491803277e-05]
Policy Loss: -0.025416, | Entropy Bonus: -0, | Value Loss: 37.304, | Advantage Loss: 2.6996
Time elapsed (s): 1.6634657382965088
Agent stdevs: 0.27038586
--------------------------------------------------------------------------------

Step 815
++++++++ Policy training ++++++++++
Current mean reward: 3508.082458 | mean episode length: 1000.000000
val_loss=913.55341
val_loss=84.36079
val_loss=834.84589
val_loss=819.29944
val_loss=1268.42883
val_loss=872.26978
val_loss=715.73218
val_loss=505.44489
val_loss=1566.93103
val_loss=913.74286
adv_loss= 0.62794
adv_loss= 2.70984
adv_loss= 2.83059
adv_loss=1652.15063
adv_loss= 1.55500
adv_loss= 2.83877
adv_loss=1651.15063
adv_loss= 1.18037
adv_loss= 1.77998
adv_loss= 4.18937
surrogate=-0.00744, entropy= 0.29561, loss=-0.00744
surrogate= 0.01091, entropy= 0.29530, loss= 0.01091
surrogate=-0.01018, entropy= 0.29434, loss=-0.01018
surrogate= 0.00102, entropy= 0.29361, loss= 0.00102
surrogate= 0.00615, entropy= 0.29311, loss= 0.00615
surrogate=-0.03381, entropy= 0.29262, loss=-0.03381
surrogate=-0.00976, entropy= 0.29219, loss=-0.00976
surrogate=-0.00850, entropy= 0.29180, loss=-0.00850
surrogate=-0.00549, entropy= 0.29118, loss=-0.00549
surrogate=-0.00810, entropy= 0.29060, loss=-0.00810
std_min= 0.21530, std_max= 0.31278, std_mean= 0.26980
val lr: [4.098360655737704e-05], policy lr: [4.918032786885244e-05]
Policy Loss: -0.0081047, | Entropy Bonus: -0, | Value Loss: 913.74, | Advantage Loss: 4.1894
Time elapsed (s): 1.6905837059020996
Agent stdevs: 0.26979876
--------------------------------------------------------------------------------

Step 816
++++++++ Policy training ++++++++++
Current mean reward: 2165.846824 | mean episode length: 589.000000
val_loss=23.60934
val_loss=22.45072
val_loss=13.90072
val_loss=14.62102
val_loss=13.63915
val_loss=19.09252
val_loss=13.86384
val_loss= 8.05803
val_loss=18.78161
val_loss=13.54281
adv_loss= 2.35456
adv_loss= 1.32406
adv_loss= 2.31618
adv_loss= 3.01501
adv_loss= 1.16074
adv_loss= 0.99362
adv_loss= 1.14147
adv_loss= 1.60417
adv_loss= 1.48113
adv_loss= 1.83577
surrogate= 0.00677, entropy= 0.29024, loss= 0.00677
surrogate= 0.02161, entropy= 0.28998, loss= 0.02161
surrogate=-0.00769, entropy= 0.28983, loss=-0.00769
surrogate=-0.00131, entropy= 0.28991, loss=-0.00131
surrogate=-0.01043, entropy= 0.28992, loss=-0.01043
surrogate= 0.01265, entropy= 0.29013, loss= 0.01265
surrogate=-0.01776, entropy= 0.28964, loss=-0.01776
surrogate=-0.00338, entropy= 0.28924, loss=-0.00338
surrogate=-0.01955, entropy= 0.28924, loss=-0.01955
surrogate=-0.03351, entropy= 0.28936, loss=-0.03351
std_min= 0.21569, std_max= 0.31169, std_mean= 0.26961
val lr: [4.0727459016393435e-05], policy lr: [4.887295081967212e-05]
Policy Loss: -0.033514, | Entropy Bonus: -0, | Value Loss: 13.543, | Advantage Loss: 1.8358
Time elapsed (s): 1.674142599105835
Agent stdevs: 0.26961198
--------------------------------------------------------------------------------

Step 817
++++++++ Policy training ++++++++++
Current mean reward: 2436.668506 | mean episode length: 669.666667
val_loss=1084.08423
val_loss=21.44088
val_loss=684.11304
val_loss=18.71705
val_loss=18.20648
val_loss=18.64248
val_loss=432.97995
val_loss=33.81430
val_loss=41.95905
val_loss=1031.63623
adv_loss= 1.21048
adv_loss= 2.98873
adv_loss= 0.93509
adv_loss= 1.30903
adv_loss= 0.89077
adv_loss= 0.97352
adv_loss= 1.09559
adv_loss= 1.18058
adv_loss= 0.84464
adv_loss= 1.21598
surrogate= 0.00223, entropy= 0.28967, loss= 0.00223
surrogate= 0.00086, entropy= 0.28940, loss= 0.00086
surrogate=-0.00592, entropy= 0.28890, loss=-0.00592
surrogate=-0.01452, entropy= 0.28885, loss=-0.01452
surrogate=-0.01350, entropy= 0.28936, loss=-0.01350
surrogate=-0.01121, entropy= 0.28944, loss=-0.01121
surrogate= 0.00646, entropy= 0.29000, loss= 0.00646
surrogate=-0.01043, entropy= 0.28947, loss=-0.01043
surrogate=-0.01208, entropy= 0.28933, loss=-0.01208
surrogate=-0.00583, entropy= 0.28900, loss=-0.00583
std_min= 0.21586, std_max= 0.31125, std_mean= 0.26954
val lr: [4.0471311475409835e-05], policy lr: [4.856557377049179e-05]
Policy Loss: -0.0058338, | Entropy Bonus: -0, | Value Loss: 1031.6, | Advantage Loss: 1.216
Time elapsed (s): 1.649261236190796
Agent stdevs: 0.2695438
--------------------------------------------------------------------------------

Step 818
++++++++ Policy training ++++++++++
Current mean reward: 2470.639533 | mean episode length: 665.666667
val_loss=17.04072
val_loss=12.15881
val_loss=15.43836
val_loss=11.74267
val_loss= 5.86797
val_loss=11.90615
val_loss= 8.86199
val_loss=10.66967
val_loss= 8.65458
val_loss= 9.24232
adv_loss= 0.73262
adv_loss= 1.36506
adv_loss= 0.99964
adv_loss= 1.76285
adv_loss= 1.27434
adv_loss= 1.01639
adv_loss= 1.16521
adv_loss= 0.56575
adv_loss= 1.96184
adv_loss= 0.91980
surrogate= 0.00134, entropy= 0.28950, loss= 0.00134
surrogate=-0.00164, entropy= 0.28898, loss=-0.00164
surrogate=-0.01299, entropy= 0.28942, loss=-0.01299
surrogate=-0.01102, entropy= 0.28938, loss=-0.01102
surrogate= 0.00571, entropy= 0.28973, loss= 0.00571
surrogate= 0.00591, entropy= 0.28985, loss= 0.00591
surrogate=-0.02661, entropy= 0.29000, loss=-0.02661
surrogate=-0.00715, entropy= 0.28992, loss=-0.00715
surrogate=-0.01530, entropy= 0.28970, loss=-0.01530
surrogate=-0.03585, entropy= 0.28957, loss=-0.03585
std_min= 0.21560, std_max= 0.31103, std_mean= 0.26962
val lr: [4.021516393442623e-05], policy lr: [4.825819672131147e-05]
Policy Loss: -0.035854, | Entropy Bonus: -0, | Value Loss: 9.2423, | Advantage Loss: 0.9198
Time elapsed (s): 1.6410894393920898
Agent stdevs: 0.2696179
--------------------------------------------------------------------------------

Step 819
++++++++ Policy training ++++++++++
Current mean reward: 2354.906980 | mean episode length: 660.000000
val_loss=92.70662
val_loss=1447.59167
val_loss=90.27089
val_loss=1551.88428
val_loss=29.98698
val_loss=41.68861
val_loss=128.38466
val_loss=1887.93457
val_loss=18.29662
val_loss=1078.69763
adv_loss= 3.20376
adv_loss= 2.59895
adv_loss= 4.30945
adv_loss= 0.68840
adv_loss= 2.84800
adv_loss=1784.21069
adv_loss= 2.02882
adv_loss= 1.50457
adv_loss= 0.89147
adv_loss= 1.30016
surrogate=-0.03261, entropy= 0.28995, loss=-0.03261
surrogate= 0.00580, entropy= 0.28954, loss= 0.00580
surrogate=-0.03908, entropy= 0.28973, loss=-0.03908
surrogate=-0.01775, entropy= 0.28987, loss=-0.01775
surrogate=-0.01101, entropy= 0.29044, loss=-0.01101
surrogate=-0.01955, entropy= 0.29063, loss=-0.01955
surrogate= 0.03540, entropy= 0.29099, loss= 0.03540
surrogate= 0.00024, entropy= 0.29088, loss= 0.00024
surrogate= 0.01833, entropy= 0.29120, loss= 0.01833
surrogate=-0.01547, entropy= 0.29091, loss=-0.01547
std_min= 0.21584, std_max= 0.31088, std_mean= 0.26971
val lr: [3.995901639344263e-05], policy lr: [4.795081967213115e-05]
Policy Loss: -0.015465, | Entropy Bonus: -0, | Value Loss: 1078.7, | Advantage Loss: 1.3002
Time elapsed (s): 1.656172275543213
Agent stdevs: 0.26971355
--------------------------------------------------------------------------------

Step 820
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2587.2
++++++++ Policy training ++++++++++
Current mean reward: 1878.859855 | mean episode length: 509.333333
val_loss=18.90642
val_loss=11.08092
val_loss=12.52577
val_loss= 8.22272
val_loss= 9.20080
val_loss=11.34715
val_loss=18.44032
val_loss=16.61584
val_loss= 6.70017
val_loss= 5.26160
adv_loss= 1.58496
adv_loss= 1.09816
adv_loss= 4.96063
adv_loss= 3.02534
adv_loss= 0.92075
adv_loss= 4.12636
adv_loss= 4.17699
adv_loss= 1.84721
adv_loss= 1.33449
adv_loss= 1.15997
surrogate=-0.01762, entropy= 0.29083, loss=-0.01762
surrogate= 0.00042, entropy= 0.29169, loss= 0.00042
surrogate=-0.01010, entropy= 0.29232, loss=-0.01010
surrogate=-0.00911, entropy= 0.29321, loss=-0.00911
surrogate= 0.00702, entropy= 0.29358, loss= 0.00702
surrogate=-0.01313, entropy= 0.29454, loss=-0.01313
surrogate=-0.01468, entropy= 0.29533, loss=-0.01468
surrogate=-0.01450, entropy= 0.29600, loss=-0.01450
surrogate=-0.00008, entropy= 0.29636, loss=-0.00008
surrogate=-0.02861, entropy= 0.29685, loss=-0.02861
std_min= 0.21674, std_max= 0.31101, std_mean= 0.27019
val lr: [3.970286885245902e-05], policy lr: [4.764344262295082e-05]
Policy Loss: -0.028614, | Entropy Bonus: -0, | Value Loss: 5.2616, | Advantage Loss: 1.16
Time elapsed (s): 1.6274738311767578
Agent stdevs: 0.27019063
--------------------------------------------------------------------------------

Step 821
++++++++ Policy training ++++++++++
Current mean reward: 1407.848093 | mean episode length: 387.400000
val_loss=13.22599
val_loss=15.72694
val_loss= 7.74446
val_loss= 7.06744
val_loss=10.75845
val_loss= 7.10035
val_loss=10.21342
val_loss= 4.89013
val_loss= 8.84578
val_loss= 7.54366
adv_loss= 1.68120
adv_loss= 2.20705
adv_loss= 1.81830
adv_loss= 3.12185
adv_loss= 1.26525
adv_loss= 1.58181
adv_loss= 1.90709
adv_loss= 0.76076
adv_loss= 1.13407
adv_loss= 1.18835
surrogate=-0.01999, entropy= 0.29742, loss=-0.01999
surrogate=-0.00995, entropy= 0.29726, loss=-0.00995
surrogate=-0.02538, entropy= 0.29753, loss=-0.02538
surrogate=-0.02832, entropy= 0.29752, loss=-0.02832
surrogate=-0.02325, entropy= 0.29738, loss=-0.02325
surrogate= 0.01353, entropy= 0.29727, loss= 0.01353
surrogate=-0.00123, entropy= 0.29663, loss=-0.00123
surrogate=-0.05514, entropy= 0.29662, loss=-0.05514
surrogate=-0.02391, entropy= 0.29676, loss=-0.02391
surrogate= 0.03306, entropy= 0.29675, loss= 0.03306
std_min= 0.21604, std_max= 0.31179, std_mean= 0.27027
val lr: [3.944672131147542e-05], policy lr: [4.7336065573770496e-05]
Policy Loss: 0.033058, | Entropy Bonus: -0, | Value Loss: 7.5437, | Advantage Loss: 1.1883
Time elapsed (s): 1.6985888481140137
Agent stdevs: 0.27027288
--------------------------------------------------------------------------------

Step 822
++++++++ Policy training ++++++++++
Current mean reward: 2314.964392 | mean episode length: 651.500000
val_loss=25.77806
val_loss=618.83917
val_loss=24.25828
val_loss=1007.76428
val_loss=107.71046
val_loss=946.98676
val_loss=1661.58508
val_loss=2763.75879
val_loss=177.79082
val_loss=1730.30920
adv_loss= 0.93639
adv_loss=1940.02612
adv_loss= 0.90355
adv_loss= 1.12523
adv_loss= 1.14737
adv_loss= 0.88429
adv_loss= 1.26684
adv_loss= 0.91154
adv_loss= 1.17915
adv_loss= 3.40935
surrogate= 0.00056, entropy= 0.29691, loss= 0.00056
surrogate= 0.01572, entropy= 0.29768, loss= 0.01572
surrogate=-0.01561, entropy= 0.29827, loss=-0.01561
surrogate=-0.03564, entropy= 0.29850, loss=-0.03564
surrogate=-0.03128, entropy= 0.29853, loss=-0.03128
surrogate=-0.01077, entropy= 0.29911, loss=-0.01077
surrogate=-0.01226, entropy= 0.29971, loss=-0.01226
surrogate=-0.02956, entropy= 0.30000, loss=-0.02956
surrogate=-0.02237, entropy= 0.30016, loss=-0.02237
surrogate=-0.03189, entropy= 0.30078, loss=-0.03189
std_min= 0.21636, std_max= 0.31177, std_mean= 0.27062
val lr: [3.9190573770491814e-05], policy lr: [4.702868852459018e-05]
Policy Loss: -0.031887, | Entropy Bonus: -0, | Value Loss: 1730.3, | Advantage Loss: 3.4093
Time elapsed (s): 1.6732122898101807
Agent stdevs: 0.27062142
--------------------------------------------------------------------------------

Step 823
++++++++ Policy training ++++++++++
Current mean reward: 2868.135998 | mean episode length: 792.500000
val_loss=1394.96143
val_loss=963.20673
val_loss=15.10925
val_loss=27.96526
val_loss=297.25394
val_loss=731.72778
val_loss=449.29971
val_loss=35.64870
val_loss=35.27036
val_loss=182.31068
adv_loss= 1.11045
adv_loss= 1.09281
adv_loss= 1.21072
adv_loss= 1.18587
adv_loss= 0.92654
adv_loss= 0.90392
adv_loss= 0.67076
adv_loss= 0.99503
adv_loss= 1.14364
adv_loss= 1.07943
surrogate= 0.02508, entropy= 0.30067, loss= 0.02508
surrogate=-0.00943, entropy= 0.30085, loss=-0.00943
surrogate=-0.03059, entropy= 0.30092, loss=-0.03059
surrogate=-0.00968, entropy= 0.30051, loss=-0.00968
surrogate= 0.00377, entropy= 0.30039, loss= 0.00377
surrogate=-0.02505, entropy= 0.30053, loss=-0.02505
surrogate=-0.00140, entropy= 0.30074, loss=-0.00140
surrogate=-0.01385, entropy= 0.30098, loss=-0.01385
surrogate=-0.01648, entropy= 0.30060, loss=-0.01648
surrogate= 0.03001, entropy= 0.30029, loss= 0.03001
std_min= 0.21636, std_max= 0.31105, std_mean= 0.27055
val lr: [3.893442622950819e-05], policy lr: [4.672131147540982e-05]
Policy Loss: 0.030012, | Entropy Bonus: -0, | Value Loss: 182.31, | Advantage Loss: 1.0794
Time elapsed (s): 1.6422946453094482
Agent stdevs: 0.27055362
--------------------------------------------------------------------------------

Step 824
++++++++ Policy training ++++++++++
Current mean reward: 2273.830032 | mean episode length: 644.000000
val_loss=13.64226
val_loss=47.96292
val_loss=1109.95129
val_loss=55.15250
val_loss=415.85083
val_loss=404.64752
val_loss=574.76117
val_loss=463.30463
val_loss=364.13071
val_loss=93.17715
adv_loss= 4.87307
adv_loss= 1.51070
adv_loss= 1.35708
adv_loss= 1.20774
adv_loss= 1.42446
adv_loss= 0.78099
adv_loss= 2.03246
adv_loss= 3.85965
adv_loss= 1.41745
adv_loss= 2.61948
surrogate= 0.00086, entropy= 0.29967, loss= 0.00086
surrogate= 0.01356, entropy= 0.30020, loss= 0.01356
surrogate=-0.03101, entropy= 0.30047, loss=-0.03101
surrogate=-0.01022, entropy= 0.30066, loss=-0.01022
surrogate=-0.02421, entropy= 0.30061, loss=-0.02421
surrogate=-0.01837, entropy= 0.30079, loss=-0.01837
surrogate=-0.02543, entropy= 0.30106, loss=-0.02543
surrogate=-0.02463, entropy= 0.30123, loss=-0.02463
surrogate=-0.04962, entropy= 0.30081, loss=-0.04962
surrogate= 0.01756, entropy= 0.30047, loss= 0.01756
std_min= 0.21659, std_max= 0.31071, std_mean= 0.27054
val lr: [3.867827868852458e-05], policy lr: [4.641393442622949e-05]
Policy Loss: 0.017564, | Entropy Bonus: -0, | Value Loss: 93.177, | Advantage Loss: 2.6195
Time elapsed (s): 1.6324348449707031
Agent stdevs: 0.27053714
--------------------------------------------------------------------------------

Step 825
++++++++ Policy training ++++++++++
Current mean reward: 1788.382025 | mean episode length: 483.750000
val_loss=29.34880
val_loss=11.05594
val_loss=11.09529
val_loss= 9.53949
val_loss=12.21887
val_loss= 5.79620
val_loss=12.75930
val_loss= 8.95511
val_loss= 7.01585
val_loss= 7.46545
adv_loss= 2.22171
adv_loss= 1.99374
adv_loss= 2.56260
adv_loss= 4.10361
adv_loss= 2.91953
adv_loss= 1.07517
adv_loss= 2.53148
adv_loss= 2.46688
adv_loss= 3.58754
adv_loss= 0.98309
surrogate=-0.00727, entropy= 0.30020, loss=-0.00727
surrogate=-0.00534, entropy= 0.29954, loss=-0.00534
surrogate=-0.01199, entropy= 0.29907, loss=-0.01199
surrogate=-0.01540, entropy= 0.29841, loss=-0.01540
surrogate=-0.01052, entropy= 0.29745, loss=-0.01052
surrogate=-0.02407, entropy= 0.29664, loss=-0.02407
surrogate=-0.00796, entropy= 0.29576, loss=-0.00796
surrogate= 0.00035, entropy= 0.29482, loss= 0.00035
surrogate=-0.01145, entropy= 0.29424, loss=-0.01145
surrogate=-0.04208, entropy= 0.29365, loss=-0.04208
std_min= 0.21635, std_max= 0.30989, std_mean= 0.26989
val lr: [3.842213114754098e-05], policy lr: [4.610655737704917e-05]
Policy Loss: -0.042085, | Entropy Bonus: -0, | Value Loss: 7.4654, | Advantage Loss: 0.98309
Time elapsed (s): 1.6506271362304688
Agent stdevs: 0.2698885
--------------------------------------------------------------------------------

Step 826
++++++++ Policy training ++++++++++
Current mean reward: 2270.126281 | mean episode length: 626.000000
val_loss=404.36484
val_loss=217.56348
val_loss=2392.99609
val_loss=2372.57056
val_loss=103.55311
val_loss=319.42676
val_loss=1035.54651
val_loss=111.73153
val_loss=55.40390
val_loss=275.15082
adv_loss= 1.44225
adv_loss= 0.94687
adv_loss= 0.93783
adv_loss= 5.26743
adv_loss= 0.85589
adv_loss= 0.86488
adv_loss= 2.23498
adv_loss= 1.15713
adv_loss= 0.69786
adv_loss= 0.81591
surrogate=-0.00452, entropy= 0.29368, loss=-0.00452
surrogate=-0.00772, entropy= 0.29424, loss=-0.00772
surrogate= 0.01818, entropy= 0.29483, loss= 0.01818
surrogate= 0.01081, entropy= 0.29515, loss= 0.01081
surrogate= 0.00716, entropy= 0.29613, loss= 0.00716
surrogate=-0.02741, entropy= 0.29670, loss=-0.02741
surrogate= 0.00745, entropy= 0.29729, loss= 0.00745
surrogate= 0.03969, entropy= 0.29779, loss= 0.03969
surrogate=-0.01866, entropy= 0.29843, loss=-0.01866
surrogate=-0.01352, entropy= 0.29891, loss=-0.01352
std_min= 0.21669, std_max= 0.31104, std_mean= 0.27039
val lr: [3.816598360655737e-05], policy lr: [4.579918032786885e-05]
Policy Loss: -0.013519, | Entropy Bonus: -0, | Value Loss: 275.15, | Advantage Loss: 0.81591
Time elapsed (s): 1.635000228881836
Agent stdevs: 0.2703879
--------------------------------------------------------------------------------

Step 827
++++++++ Policy training ++++++++++
Current mean reward: 1748.052416 | mean episode length: 480.500000
val_loss=1222.40381
val_loss=31.74419
val_loss=56.19068
val_loss=56.69910
val_loss=1284.60339
val_loss=73.66311
val_loss=1195.51624
val_loss=58.44800
val_loss=321.40445
val_loss=60.14816
adv_loss= 3.26771
adv_loss= 2.67289
adv_loss= 2.57088
adv_loss= 2.56349
adv_loss= 1.59572
adv_loss= 1.54886
adv_loss= 5.32553
adv_loss= 2.36521
adv_loss= 2.47278
adv_loss= 2.53715
surrogate= 0.00121, entropy= 0.29876, loss= 0.00121
surrogate=-0.00004, entropy= 0.29859, loss=-0.00004
surrogate=-0.00663, entropy= 0.29908, loss=-0.00663
surrogate=-0.00066, entropy= 0.29945, loss=-0.00066
surrogate=-0.01559, entropy= 0.29956, loss=-0.01559
surrogate=-0.02358, entropy= 0.29944, loss=-0.02358
surrogate= 0.01010, entropy= 0.29960, loss= 0.01010
surrogate=-0.02567, entropy= 0.30040, loss=-0.02567
surrogate=-0.02582, entropy= 0.30029, loss=-0.02582
surrogate= 0.00544, entropy= 0.29986, loss= 0.00544
std_min= 0.21634, std_max= 0.31142, std_mean= 0.27052
val lr: [3.790983606557377e-05], policy lr: [4.549180327868852e-05]
Policy Loss: 0.0054356, | Entropy Bonus: -0, | Value Loss: 60.148, | Advantage Loss: 2.5371
Time elapsed (s): 1.6850826740264893
Agent stdevs: 0.27052245
--------------------------------------------------------------------------------

Step 828
++++++++ Policy training ++++++++++
Current mean reward: 1815.509337 | mean episode length: 498.750000
val_loss=15.13943
val_loss=19.40537
val_loss=2281.51196
val_loss=375.62158
val_loss=18.46063
val_loss=388.23846
val_loss=71.31565
val_loss=50.57172
val_loss=2230.66895
val_loss=1322.61804
adv_loss= 1.59371
adv_loss= 1.17229
adv_loss= 1.46802
adv_loss= 2.63768
adv_loss= 0.77184
adv_loss= 1.35949
adv_loss= 1.83797
adv_loss= 3.25111
adv_loss= 3.18117
adv_loss= 1.16184
surrogate=-0.02123, entropy= 0.29856, loss=-0.02123
surrogate=-0.02552, entropy= 0.29810, loss=-0.02552
surrogate=-0.00964, entropy= 0.29731, loss=-0.00964
surrogate=-0.02743, entropy= 0.29726, loss=-0.02743
surrogate=-0.01123, entropy= 0.29680, loss=-0.01123
surrogate=-0.01563, entropy= 0.29629, loss=-0.01563
surrogate=-0.02263, entropy= 0.29602, loss=-0.02263
surrogate= 0.00118, entropy= 0.29545, loss= 0.00118
surrogate= 0.03052, entropy= 0.29529, loss= 0.03052
surrogate=-0.02351, entropy= 0.29503, loss=-0.02351
std_min= 0.21592, std_max= 0.31059, std_mean= 0.27008
val lr: [3.7653688524590166e-05], policy lr: [4.51844262295082e-05]
Policy Loss: -0.023512, | Entropy Bonus: -0, | Value Loss: 1322.6, | Advantage Loss: 1.1618
Time elapsed (s): 1.6583929061889648
Agent stdevs: 0.2700846
--------------------------------------------------------------------------------

Step 829
++++++++ Policy training ++++++++++
Current mean reward: 2401.175664 | mean episode length: 661.666667
val_loss=249.79460
val_loss=28.40272
val_loss=1115.27515
val_loss=109.38674
val_loss=45.90265
val_loss=117.05520
val_loss=437.74094
val_loss=68.04012
val_loss=124.57211
val_loss=114.89338
adv_loss= 4.62862
adv_loss= 1.14834
adv_loss= 1.93550
adv_loss= 1.63893
adv_loss= 1.16970
adv_loss= 1.78073
adv_loss= 3.92869
adv_loss= 1.90063
adv_loss= 1.14451
adv_loss= 1.58487
surrogate= 0.00425, entropy= 0.29456, loss= 0.00425
surrogate=-0.03591, entropy= 0.29538, loss=-0.03591
surrogate= 0.00252, entropy= 0.29545, loss= 0.00252
surrogate=-0.00898, entropy= 0.29544, loss=-0.00898
surrogate=-0.01247, entropy= 0.29559, loss=-0.01247
surrogate=-0.00259, entropy= 0.29568, loss=-0.00259
surrogate= 0.00925, entropy= 0.29628, loss= 0.00925
surrogate=-0.01121, entropy= 0.29683, loss=-0.01121
surrogate= 0.01480, entropy= 0.29730, loss= 0.01480
surrogate=-0.03148, entropy= 0.29780, loss=-0.03148
std_min= 0.21621, std_max= 0.31120, std_mean= 0.27034
val lr: [3.7397540983606566e-05], policy lr: [4.487704918032787e-05]
Policy Loss: -0.031482, | Entropy Bonus: -0, | Value Loss: 114.89, | Advantage Loss: 1.5849
Time elapsed (s): 1.659048318862915
Agent stdevs: 0.27033707
--------------------------------------------------------------------------------

Step 830
++++++++ Policy training ++++++++++
Current mean reward: 1871.013319 | mean episode length: 505.000000
val_loss=18.36238
val_loss=11.41545
val_loss= 8.12041
val_loss= 9.39332
val_loss= 9.90884
val_loss=10.14175
val_loss= 7.63492
val_loss= 7.08019
val_loss= 6.65060
val_loss= 8.82178
adv_loss= 1.64585
adv_loss= 4.83307
adv_loss= 0.95453
adv_loss= 1.75911
adv_loss= 0.82906
adv_loss= 1.02496
adv_loss= 1.35497
adv_loss= 1.21321
adv_loss= 1.97455
adv_loss= 0.86662
surrogate=-0.00315, entropy= 0.29771, loss=-0.00315
surrogate= 0.01936, entropy= 0.29735, loss= 0.01936
surrogate= 0.01106, entropy= 0.29713, loss= 0.01106
surrogate= 0.00198, entropy= 0.29729, loss= 0.00198
surrogate= 0.00966, entropy= 0.29680, loss= 0.00966
surrogate=-0.00534, entropy= 0.29621, loss=-0.00534
surrogate=-0.01943, entropy= 0.29619, loss=-0.01943
surrogate= 0.00257, entropy= 0.29618, loss= 0.00257
surrogate=-0.04468, entropy= 0.29630, loss=-0.04468
surrogate=-0.03619, entropy= 0.29607, loss=-0.03619
std_min= 0.21652, std_max= 0.31073, std_mean= 0.27013
val lr: [3.714139344262296e-05], policy lr: [4.456967213114755e-05]
Policy Loss: -0.03619, | Entropy Bonus: -0, | Value Loss: 8.8218, | Advantage Loss: 0.86662
Time elapsed (s): 1.6561572551727295
Agent stdevs: 0.27012601
--------------------------------------------------------------------------------

Step 831
++++++++ Policy training ++++++++++
Current mean reward: 2374.056506 | mean episode length: 652.666667
val_loss=31.03541
val_loss=331.87082
val_loss=138.16010
val_loss=532.97137
val_loss=890.05756
val_loss=695.81317
val_loss=41.44019
val_loss=90.10359
val_loss=863.67468
val_loss=32.55735
adv_loss= 2.16566
adv_loss= 1.49339
adv_loss= 1.53636
adv_loss= 1.44250
adv_loss=1605.09290
adv_loss= 1.40433
adv_loss= 1.64596
adv_loss= 0.89526
adv_loss= 1.91690
adv_loss= 0.83320
surrogate= 0.00052, entropy= 0.29635, loss= 0.00052
surrogate= 0.01719, entropy= 0.29548, loss= 0.01719
surrogate=-0.01765, entropy= 0.29547, loss=-0.01765
surrogate= 0.00591, entropy= 0.29507, loss= 0.00591
surrogate=-0.01954, entropy= 0.29521, loss=-0.01954
surrogate=-0.02143, entropy= 0.29523, loss=-0.02143
surrogate=-0.00746, entropy= 0.29510, loss=-0.00746
surrogate=-0.01222, entropy= 0.29532, loss=-0.01222
surrogate=-0.01993, entropy= 0.29533, loss=-0.01993
surrogate=-0.03422, entropy= 0.29544, loss=-0.03422
std_min= 0.21703, std_max= 0.31040, std_mean= 0.27001
val lr: [3.688524590163936e-05], policy lr: [4.426229508196723e-05]
Policy Loss: -0.034224, | Entropy Bonus: -0, | Value Loss: 32.557, | Advantage Loss: 0.8332
Time elapsed (s): 1.6377394199371338
Agent stdevs: 0.2700063
--------------------------------------------------------------------------------

Step 832
++++++++ Policy training ++++++++++
Current mean reward: 3537.473790 | mean episode length: 1000.000000
val_loss=34.14451
val_loss=156.07069
val_loss=52.66914
val_loss=1288.35303
val_loss=808.30798
val_loss=82.54045
val_loss=1617.36523
val_loss=434.77173
val_loss=95.33604
val_loss=441.73276
adv_loss= 7.92852
adv_loss= 2.32745
adv_loss= 7.39126
adv_loss= 5.15089
adv_loss= 2.49267
adv_loss= 3.53788
adv_loss= 1.60279
adv_loss= 2.85710
adv_loss=1178.73108
adv_loss= 4.66856
surrogate= 0.03322, entropy= 0.29514, loss= 0.03322
surrogate=-0.01588, entropy= 0.29512, loss=-0.01588
surrogate= 0.01617, entropy= 0.29526, loss= 0.01617
surrogate= 0.00948, entropy= 0.29515, loss= 0.00948
surrogate=-0.00383, entropy= 0.29460, loss=-0.00383
surrogate=-0.01445, entropy= 0.29441, loss=-0.01445
surrogate=-0.00904, entropy= 0.29378, loss=-0.00904
surrogate=-0.00051, entropy= 0.29397, loss=-0.00051
surrogate= 0.00285, entropy= 0.29387, loss= 0.00285
surrogate=-0.01718, entropy= 0.29346, loss=-0.01718
std_min= 0.21739, std_max= 0.31076, std_mean= 0.26980
val lr: [3.6629098360655725e-05], policy lr: [4.395491803278687e-05]
Policy Loss: -0.017178, | Entropy Bonus: -0, | Value Loss: 441.73, | Advantage Loss: 4.6686
Time elapsed (s): 1.6310272216796875
Agent stdevs: 0.26979628
--------------------------------------------------------------------------------

Step 833
++++++++ Policy training ++++++++++
Current mean reward: 1947.901513 | mean episode length: 533.000000
val_loss=26.77429
val_loss=33.42511
val_loss=14.63015
val_loss=15.17866
val_loss=20.87129
val_loss=13.02759
val_loss=10.35248
val_loss=24.02169
val_loss=12.87166
val_loss=16.72654
adv_loss= 3.01740
adv_loss= 4.92911
adv_loss= 2.36943
adv_loss= 3.91735
adv_loss= 3.85718
adv_loss= 4.91586
adv_loss= 1.71652
adv_loss= 1.06307
adv_loss= 1.57049
adv_loss= 1.95321
surrogate= 0.00070, entropy= 0.29365, loss= 0.00070
surrogate=-0.01079, entropy= 0.29368, loss=-0.01079
surrogate=-0.02151, entropy= 0.29416, loss=-0.02151
surrogate=-0.01514, entropy= 0.29459, loss=-0.01514
surrogate=-0.01497, entropy= 0.29450, loss=-0.01497
surrogate=-0.02220, entropy= 0.29431, loss=-0.02220
surrogate=-0.00567, entropy= 0.29458, loss=-0.00567
surrogate=-0.02245, entropy= 0.29449, loss=-0.02245
surrogate=-0.00249, entropy= 0.29458, loss=-0.00249
surrogate=-0.00978, entropy= 0.29453, loss=-0.00978
std_min= 0.21765, std_max= 0.31122, std_mean= 0.26989
val lr: [3.6372950819672125e-05], policy lr: [4.364754098360654e-05]
Policy Loss: -0.0097772, | Entropy Bonus: -0, | Value Loss: 16.727, | Advantage Loss: 1.9532
Time elapsed (s): 1.6771230697631836
Agent stdevs: 0.26988605
--------------------------------------------------------------------------------

Step 834
++++++++ Policy training ++++++++++
Current mean reward: 2500.595332 | mean episode length: 673.333333
val_loss=16.52965
val_loss= 8.09012
val_loss=11.75405
val_loss= 7.03910
val_loss= 9.18985
val_loss=10.81095
val_loss=11.61401
val_loss= 7.33371
val_loss= 7.80153
val_loss= 5.43449
adv_loss= 1.90516
adv_loss= 1.81234
adv_loss= 3.89949
adv_loss= 1.71473
adv_loss= 5.49951
adv_loss= 2.15458
adv_loss= 0.72405
adv_loss= 1.62357
adv_loss= 1.58557
adv_loss= 1.34422
surrogate=-0.00550, entropy= 0.29394, loss=-0.00550
surrogate= 0.00316, entropy= 0.29319, loss= 0.00316
surrogate= 0.01796, entropy= 0.29257, loss= 0.01796
surrogate=-0.00303, entropy= 0.29162, loss=-0.00303
surrogate=-0.00563, entropy= 0.29103, loss=-0.00563
surrogate=-0.02045, entropy= 0.29029, loss=-0.02045
surrogate= 0.01312, entropy= 0.28956, loss= 0.01312
surrogate=-0.00412, entropy= 0.28860, loss=-0.00412
surrogate=-0.00987, entropy= 0.28817, loss=-0.00987
surrogate=-0.04284, entropy= 0.28747, loss=-0.04284
std_min= 0.21658, std_max= 0.31089, std_mean= 0.26932
val lr: [3.611680327868852e-05], policy lr: [4.3340163934426216e-05]
Policy Loss: -0.042844, | Entropy Bonus: -0, | Value Loss: 5.4345, | Advantage Loss: 1.3442
Time elapsed (s): 1.646629810333252
Agent stdevs: 0.26931602
--------------------------------------------------------------------------------

Step 835
++++++++ Policy training ++++++++++
Current mean reward: 2652.945949 | mean episode length: 720.000000
val_loss=11.74613
val_loss= 8.40066
val_loss= 8.27021
val_loss= 7.08089
val_loss= 9.03041
val_loss= 6.40776
val_loss= 7.72398
val_loss= 6.43477
val_loss= 5.26024
val_loss= 5.70065
adv_loss= 1.83412
adv_loss= 2.03212
adv_loss= 1.19295
adv_loss= 2.73709
adv_loss= 1.64901
adv_loss= 2.20939
adv_loss= 1.31316
adv_loss= 1.44204
adv_loss= 0.77178
adv_loss= 1.15120
surrogate=-0.00207, entropy= 0.28678, loss=-0.00207
surrogate=-0.00406, entropy= 0.28554, loss=-0.00406
surrogate=-0.00134, entropy= 0.28426, loss=-0.00134
surrogate=-0.02090, entropy= 0.28311, loss=-0.02090
surrogate=-0.01977, entropy= 0.28168, loss=-0.01977
surrogate=-0.02936, entropy= 0.28036, loss=-0.02936
surrogate=-0.00728, entropy= 0.27919, loss=-0.00728
surrogate=-0.03216, entropy= 0.27829, loss=-0.03216
surrogate=-0.04745, entropy= 0.27724, loss=-0.04745
surrogate=-0.04593, entropy= 0.27625, loss=-0.04593
std_min= 0.21561, std_max= 0.31032, std_mean= 0.26835
val lr: [3.586065573770492e-05], policy lr: [4.30327868852459e-05]
Policy Loss: -0.045927, | Entropy Bonus: -0, | Value Loss: 5.7007, | Advantage Loss: 1.1512
Time elapsed (s): 1.6287987232208252
Agent stdevs: 0.26834896
--------------------------------------------------------------------------------

Step 836
++++++++ Policy training ++++++++++
Current mean reward: 2020.370789 | mean episode length: 563.666667
val_loss=14.19700
val_loss=61.30807
val_loss=36.01655
val_loss=1715.85864
val_loss=1235.95227
val_loss=358.54068
val_loss=37.80460
val_loss=781.45880
val_loss=933.66534
val_loss=105.06175
adv_loss=1899.43884
adv_loss= 1.55003
adv_loss= 0.60812
adv_loss= 4.46926
adv_loss= 0.89442
adv_loss= 1.54280
adv_loss= 1.19780
adv_loss= 1.10155
adv_loss= 1.19411
adv_loss= 1.78348
surrogate=-0.01498, entropy= 0.27616, loss=-0.01498
surrogate= 0.00049, entropy= 0.27636, loss= 0.00049
surrogate=-0.01858, entropy= 0.27655, loss=-0.01858
surrogate=-0.01990, entropy= 0.27649, loss=-0.01990
surrogate=-0.03095, entropy= 0.27652, loss=-0.03095
surrogate=-0.01725, entropy= 0.27588, loss=-0.01725
surrogate=-0.00182, entropy= 0.27571, loss=-0.00182
surrogate= 0.00724, entropy= 0.27554, loss= 0.00724
surrogate=-0.02597, entropy= 0.27540, loss=-0.02597
surrogate=-0.00171, entropy= 0.27554, loss=-0.00171
std_min= 0.21549, std_max= 0.31042, std_mean= 0.26830
val lr: [3.560450819672131e-05], policy lr: [4.272540983606557e-05]
Policy Loss: -0.001707, | Entropy Bonus: -0, | Value Loss: 105.06, | Advantage Loss: 1.7835
Time elapsed (s): 1.6325266361236572
Agent stdevs: 0.2682982
--------------------------------------------------------------------------------

Step 837
++++++++ Policy training ++++++++++
Current mean reward: 2006.096731 | mean episode length: 564.333333
val_loss=1014.84369
val_loss=44.16025
val_loss=801.37164
val_loss=65.08316
val_loss=340.06604
val_loss=204.34444
val_loss=167.19055
val_loss=263.73077
val_loss=1636.99683
val_loss=744.40747
adv_loss= 2.65122
adv_loss= 2.81054
adv_loss= 2.30627
adv_loss= 2.34581
adv_loss= 2.23668
adv_loss= 2.57972
adv_loss= 1.87550
adv_loss= 2.93558
adv_loss= 3.99388
adv_loss= 2.30233
surrogate=-0.00203, entropy= 0.27562, loss=-0.00203
surrogate=-0.00476, entropy= 0.27563, loss=-0.00476
surrogate=-0.00024, entropy= 0.27564, loss=-0.00024
surrogate=-0.01436, entropy= 0.27611, loss=-0.01436
surrogate= 0.00854, entropy= 0.27609, loss= 0.00854
surrogate=-0.00712, entropy= 0.27639, loss=-0.00712
surrogate=-0.00748, entropy= 0.27680, loss=-0.00748
surrogate=-0.01325, entropy= 0.27716, loss=-0.01325
surrogate=-0.02600, entropy= 0.27779, loss=-0.02600
surrogate=-0.01024, entropy= 0.27898, loss=-0.01024
std_min= 0.21588, std_max= 0.31050, std_mean= 0.26859
val lr: [3.534836065573771e-05], policy lr: [4.2418032786885246e-05]
Policy Loss: -0.010238, | Entropy Bonus: -0, | Value Loss: 744.41, | Advantage Loss: 2.3023
Time elapsed (s): 1.636113166809082
Agent stdevs: 0.26858523
--------------------------------------------------------------------------------

Step 838
++++++++ Policy training ++++++++++
Current mean reward: 3565.434852 | mean episode length: 1000.000000
val_loss=1448.60718
val_loss=134.22711
val_loss=1342.24072
val_loss=484.60132
val_loss=289.82465
val_loss=41.66418
val_loss=2086.77856
val_loss=566.68958
val_loss=34.49065
val_loss=2583.88867
adv_loss=1866.47363
adv_loss= 0.81417
adv_loss= 0.67786
adv_loss= 1.15199
adv_loss= 1.24160
adv_loss= 1.49113
adv_loss= 0.86500
adv_loss= 0.95047
adv_loss= 2.10355
adv_loss= 1.06446
surrogate= 0.00147, entropy= 0.27890, loss= 0.00147
surrogate=-0.01222, entropy= 0.27873, loss=-0.01222
surrogate=-0.00665, entropy= 0.27837, loss=-0.00665
surrogate=-0.00698, entropy= 0.27817, loss=-0.00698
surrogate=-0.01269, entropy= 0.27781, loss=-0.01269
surrogate=-0.02459, entropy= 0.27791, loss=-0.02459
surrogate=-0.02905, entropy= 0.27754, loss=-0.02905
surrogate=-0.02787, entropy= 0.27706, loss=-0.02787
surrogate=-0.02750, entropy= 0.27708, loss=-0.02750
surrogate= 0.00615, entropy= 0.27677, loss= 0.00615
std_min= 0.21566, std_max= 0.30949, std_mean= 0.26836
val lr: [3.5092213114754104e-05], policy lr: [4.211065573770492e-05]
Policy Loss: 0.0061513, | Entropy Bonus: -0, | Value Loss: 2583.9, | Advantage Loss: 1.0645
Time elapsed (s): 1.6550536155700684
Agent stdevs: 0.26836395
--------------------------------------------------------------------------------

Step 839
++++++++ Policy training ++++++++++
Current mean reward: 1994.003690 | mean episode length: 537.500000
val_loss=17.54651
val_loss=14.53896
val_loss= 7.46914
val_loss= 7.08513
val_loss=10.76641
val_loss=11.78086
val_loss= 4.13731
val_loss=10.66082
val_loss= 5.88348
val_loss= 8.87305
adv_loss= 2.27887
adv_loss= 5.48649
adv_loss= 1.47522
adv_loss= 1.97252
adv_loss= 1.11609
adv_loss= 2.11197
adv_loss= 4.12890
adv_loss= 0.67698
adv_loss= 2.39769
adv_loss= 1.79319
surrogate=-0.00018, entropy= 0.27668, loss=-0.00018
surrogate=-0.00897, entropy= 0.27641, loss=-0.00897
surrogate= 0.00224, entropy= 0.27583, loss= 0.00224
surrogate=-0.01385, entropy= 0.27524, loss=-0.01385
surrogate=-0.00252, entropy= 0.27487, loss=-0.00252
surrogate=-0.01245, entropy= 0.27461, loss=-0.01245
surrogate=-0.02783, entropy= 0.27416, loss=-0.02783
surrogate=-0.00772, entropy= 0.27420, loss=-0.00772
surrogate=-0.03517, entropy= 0.27396, loss=-0.03517
surrogate=-0.01114, entropy= 0.27379, loss=-0.01114
std_min= 0.21627, std_max= 0.30916, std_mean= 0.26802
val lr: [3.4836065573770504e-05], policy lr: [4.18032786885246e-05]
Policy Loss: -0.011145, | Entropy Bonus: -0, | Value Loss: 8.873, | Advantage Loss: 1.7932
Time elapsed (s): 1.673696517944336
Agent stdevs: 0.26801583
--------------------------------------------------------------------------------

Step 840
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2447.8
++++++++ Policy training ++++++++++
Current mean reward: 2749.764861 | mean episode length: 771.500000
val_loss=15.00119
val_loss=1362.86841
val_loss=119.63414
val_loss=157.95596
val_loss=931.81464
val_loss=42.66053
val_loss=1556.09106
val_loss=71.58982
val_loss=100.60909
val_loss=54.66428
adv_loss= 1.49453
adv_loss= 0.74483
adv_loss= 0.67369
adv_loss= 0.96543
adv_loss= 1.33106
adv_loss= 2.97560
adv_loss= 5.37895
adv_loss= 1.02765
adv_loss= 0.61725
adv_loss= 1.77147
surrogate=-0.00497, entropy= 0.27381, loss=-0.00497
surrogate=-0.00500, entropy= 0.27414, loss=-0.00500
surrogate=-0.02168, entropy= 0.27431, loss=-0.02168
surrogate=-0.00988, entropy= 0.27470, loss=-0.00988
surrogate=-0.01033, entropy= 0.27471, loss=-0.01033
surrogate=-0.00301, entropy= 0.27482, loss=-0.00301
surrogate=-0.02648, entropy= 0.27489, loss=-0.02648
surrogate=-0.01267, entropy= 0.27508, loss=-0.01267
surrogate= 0.01236, entropy= 0.27531, loss= 0.01236
surrogate=-0.01598, entropy= 0.27536, loss=-0.01598
std_min= 0.21626, std_max= 0.31001, std_mean= 0.26819
val lr: [3.457991803278687e-05], policy lr: [4.149590163934424e-05]
Policy Loss: -0.015978, | Entropy Bonus: -0, | Value Loss: 54.664, | Advantage Loss: 1.7715
Time elapsed (s): 1.6543715000152588
Agent stdevs: 0.26819393
--------------------------------------------------------------------------------

Step 841
++++++++ Policy training ++++++++++
Current mean reward: 3094.038718 | mean episode length: 835.500000
val_loss=24.13516
val_loss=14.91790
val_loss=13.42207
val_loss= 9.48030
val_loss= 9.29167
val_loss= 8.71819
val_loss=15.96774
val_loss=10.09455
val_loss=11.33435
val_loss=11.44497
adv_loss= 5.07229
adv_loss= 1.40352
adv_loss= 1.45700
adv_loss= 2.03724
adv_loss= 2.42641
adv_loss= 1.02541
adv_loss= 1.77200
adv_loss= 1.61995
adv_loss= 1.08678
adv_loss= 2.36373
surrogate= 0.00766, entropy= 0.27517, loss= 0.00766
surrogate= 0.00440, entropy= 0.27436, loss= 0.00440
surrogate=-0.00245, entropy= 0.27340, loss=-0.00245
surrogate=-0.00167, entropy= 0.27298, loss=-0.00167
surrogate=-0.02404, entropy= 0.27203, loss=-0.02404
surrogate=-0.00113, entropy= 0.27118, loss=-0.00113
surrogate= 0.00428, entropy= 0.27054, loss= 0.00428
surrogate= 0.02823, entropy= 0.26981, loss= 0.02823
surrogate= 0.00822, entropy= 0.26926, loss= 0.00822
surrogate=-0.01500, entropy= 0.26790, loss=-0.01500
std_min= 0.21559, std_max= 0.30921, std_mean= 0.26754
val lr: [3.432377049180327e-05], policy lr: [4.118852459016392e-05]
Policy Loss: -0.015004, | Entropy Bonus: -0, | Value Loss: 11.445, | Advantage Loss: 2.3637
Time elapsed (s): 1.6341381072998047
Agent stdevs: 0.26753718
--------------------------------------------------------------------------------

Step 842
++++++++ Policy training ++++++++++
Current mean reward: 3485.658262 | mean episode length: 1000.000000
val_loss=2353.85620
val_loss=1381.95544
val_loss=1404.22498
val_loss=495.00296
val_loss=825.83356
val_loss=78.44273
val_loss=418.55994
val_loss=1319.92188
val_loss=161.25397
val_loss=72.45957
adv_loss= 1.63826
adv_loss= 1.40440
adv_loss= 1.67458
adv_loss= 1.72545
adv_loss= 1.12543
adv_loss= 3.40502
adv_loss= 1.81186
adv_loss= 1.89288
adv_loss= 1.50473
adv_loss= 1.72124
surrogate=-0.00881, entropy= 0.26814, loss=-0.00881
surrogate= 0.00054, entropy= 0.26864, loss= 0.00054
surrogate=-0.01096, entropy= 0.26922, loss=-0.01096
surrogate=-0.00065, entropy= 0.26940, loss=-0.00065
surrogate=-0.00192, entropy= 0.26993, loss=-0.00192
surrogate=-0.02604, entropy= 0.27010, loss=-0.02604
surrogate=-0.00727, entropy= 0.27055, loss=-0.00727
surrogate=-0.02341, entropy= 0.27114, loss=-0.02341
surrogate=-0.01825, entropy= 0.27140, loss=-0.01825
surrogate=-0.03461, entropy= 0.27206, loss=-0.03461
std_min= 0.21609, std_max= 0.31000, std_mean= 0.26790
val lr: [3.406762295081966e-05], policy lr: [4.088114754098359e-05]
Policy Loss: -0.034606, | Entropy Bonus: -0, | Value Loss: 72.46, | Advantage Loss: 1.7212
Time elapsed (s): 1.634803056716919
Agent stdevs: 0.2679032
--------------------------------------------------------------------------------

Step 843
++++++++ Policy training ++++++++++
Current mean reward: 2990.039294 | mean episode length: 837.000000
val_loss=1690.69727
val_loss=1239.09180
val_loss=1045.78625
val_loss=997.65100
val_loss=155.11829
val_loss=31.26210
val_loss=359.23999
val_loss=682.14911
val_loss=156.60283
val_loss=108.44137
adv_loss= 6.30913
adv_loss= 1.14255
adv_loss= 2.91524
adv_loss= 4.85693
adv_loss= 2.72645
adv_loss= 1.71721
adv_loss= 3.02506
adv_loss= 1.97239
adv_loss= 1.80166
adv_loss= 1.59192
surrogate=-0.01559, entropy= 0.27208, loss=-0.01559
surrogate=-0.01157, entropy= 0.27188, loss=-0.01157
surrogate=-0.00001, entropy= 0.27190, loss=-0.00001
surrogate= 0.02571, entropy= 0.27221, loss= 0.02571
surrogate=-0.00442, entropy= 0.27206, loss=-0.00442
surrogate=-0.02315, entropy= 0.27241, loss=-0.02315
surrogate=-0.01537, entropy= 0.27257, loss=-0.01537
surrogate=-0.01532, entropy= 0.27250, loss=-0.01532
surrogate= 0.00853, entropy= 0.27247, loss= 0.00853
surrogate=-0.01230, entropy= 0.27259, loss=-0.01230
std_min= 0.21575, std_max= 0.30980, std_mean= 0.26798
val lr: [3.381147540983606e-05], policy lr: [4.057377049180327e-05]
Policy Loss: -0.0123, | Entropy Bonus: -0, | Value Loss: 108.44, | Advantage Loss: 1.5919
Time elapsed (s): 1.6406986713409424
Agent stdevs: 0.26797694
--------------------------------------------------------------------------------

Step 844
++++++++ Policy training ++++++++++
Current mean reward: 2413.955781 | mean episode length: 673.333333
val_loss=122.96202
val_loss=120.05741
val_loss=29.05922
val_loss=196.95074
val_loss=86.50425
val_loss=130.87398
val_loss=2357.42334
val_loss=184.57843
val_loss=51.71412
val_loss=520.79718
adv_loss= 2.09766
adv_loss= 2.97350
adv_loss= 1.88942
adv_loss= 2.11873
adv_loss= 1.75700
adv_loss= 2.73536
adv_loss= 2.83778
adv_loss= 4.12663
adv_loss= 1.87917
adv_loss= 4.02446
surrogate=-0.00676, entropy= 0.27191, loss=-0.00676
surrogate= 0.01894, entropy= 0.27133, loss= 0.01894
surrogate=-0.01323, entropy= 0.27092, loss=-0.01323
surrogate=-0.02382, entropy= 0.27088, loss=-0.02382
surrogate= 0.00092, entropy= 0.27041, loss= 0.00092
surrogate=-0.01094, entropy= 0.27035, loss=-0.01094
surrogate=-0.01214, entropy= 0.27034, loss=-0.01214
surrogate=-0.00327, entropy= 0.27013, loss=-0.00327
surrogate=-0.01248, entropy= 0.26973, loss=-0.01248
surrogate=-0.01400, entropy= 0.26941, loss=-0.01400
std_min= 0.21570, std_max= 0.30993, std_mean= 0.26769
val lr: [3.3555327868852456e-05], policy lr: [4.026639344262295e-05]
Policy Loss: -0.014, | Entropy Bonus: -0, | Value Loss: 520.8, | Advantage Loss: 4.0245
Time elapsed (s): 1.658085584640503
Agent stdevs: 0.267691
--------------------------------------------------------------------------------

Step 845
++++++++ Policy training ++++++++++
Current mean reward: 3495.505742 | mean episode length: 1000.000000
val_loss=480.75681
val_loss=425.12180
val_loss=1127.53174
val_loss=1928.36133
val_loss=3900.91870
val_loss=3992.16870
val_loss=1027.09351
val_loss=102.72784
val_loss=1884.01611
val_loss=242.67595
adv_loss= 1.98046
adv_loss= 1.80849
adv_loss= 1.02453
adv_loss= 1.27901
adv_loss= 1.77242
adv_loss= 1.91271
adv_loss= 2.00660
adv_loss= 1.66828
adv_loss= 0.87479
adv_loss= 2.00336
surrogate=-0.01091, entropy= 0.26890, loss=-0.01091
surrogate=-0.01392, entropy= 0.26850, loss=-0.01392
surrogate=-0.00612, entropy= 0.26856, loss=-0.00612
surrogate= 0.01427, entropy= 0.26816, loss= 0.01427
surrogate= 0.00784, entropy= 0.26746, loss= 0.00784
surrogate=-0.01399, entropy= 0.26782, loss=-0.01399
surrogate=-0.02268, entropy= 0.26701, loss=-0.02268
surrogate=-0.02317, entropy= 0.26681, loss=-0.02317
surrogate=-0.00615, entropy= 0.26661, loss=-0.00615
surrogate=-0.01447, entropy= 0.26643, loss=-0.01447
std_min= 0.21538, std_max= 0.30981, std_mean= 0.26744
val lr: [3.3299180327868856e-05], policy lr: [3.995901639344262e-05]
Policy Loss: -0.014472, | Entropy Bonus: -0, | Value Loss: 242.68, | Advantage Loss: 2.0034
Time elapsed (s): 1.6807799339294434
Agent stdevs: 0.26744282
--------------------------------------------------------------------------------

Step 846
++++++++ Policy training ++++++++++
Current mean reward: 3476.097931 | mean episode length: 1000.000000
val_loss=1216.67944
val_loss=439.36575
val_loss=1037.76221
val_loss=948.71552
val_loss=431.60541
val_loss=238.20854
val_loss=931.68829
val_loss=2156.01904
val_loss=68.17237
val_loss=3613.62744
adv_loss= 3.85879
adv_loss= 1.80131
adv_loss= 2.79655
adv_loss= 1.70351
adv_loss= 1.73730
adv_loss= 2.80406
adv_loss= 1.77241
adv_loss= 1.56427
adv_loss= 3.64282
adv_loss= 1.45382
surrogate= 0.00932, entropy= 0.26551, loss= 0.00932
surrogate=-0.03788, entropy= 0.26506, loss=-0.03788
surrogate= 0.01795, entropy= 0.26490, loss= 0.01795
surrogate=-0.00714, entropy= 0.26465, loss=-0.00714
surrogate=-0.01513, entropy= 0.26406, loss=-0.01513
surrogate=-0.01440, entropy= 0.26407, loss=-0.01440
surrogate=-0.02087, entropy= 0.26386, loss=-0.02087
surrogate=-0.02872, entropy= 0.26367, loss=-0.02872
surrogate=-0.01740, entropy= 0.26324, loss=-0.01740
surrogate= 0.04482, entropy= 0.26272, loss= 0.04482
std_min= 0.21491, std_max= 0.30949, std_mean= 0.26714
val lr: [3.304303278688525e-05], policy lr: [3.9651639344262296e-05]
Policy Loss: 0.044816, | Entropy Bonus: -0, | Value Loss: 3613.6, | Advantage Loss: 1.4538
Time elapsed (s): 1.6674747467041016
Agent stdevs: 0.2671356
--------------------------------------------------------------------------------

Step 847
++++++++ Policy training ++++++++++
Current mean reward: 2402.629456 | mean episode length: 676.000000
val_loss=34.69436
val_loss=169.55209
val_loss=61.78798
val_loss=23.63563
val_loss=72.71243
val_loss=654.56702
val_loss=567.61963
val_loss=44.30489
val_loss=148.75401
val_loss=56.17472
adv_loss= 1.85711
adv_loss= 2.30956
adv_loss= 3.47087
adv_loss= 2.70304
adv_loss= 1.93344
adv_loss= 1.18854
adv_loss= 2.74379
adv_loss= 3.66984
adv_loss= 1.65927
adv_loss= 1.33284
surrogate= 0.00729, entropy= 0.26257, loss= 0.00729
surrogate= 0.00003, entropy= 0.26208, loss= 0.00003
surrogate= 0.00524, entropy= 0.26187, loss= 0.00524
surrogate= 0.01013, entropy= 0.26216, loss= 0.01013
surrogate= 0.00707, entropy= 0.26196, loss= 0.00707
surrogate=-0.00439, entropy= 0.26190, loss=-0.00439
surrogate=-0.00537, entropy= 0.26221, loss=-0.00537
surrogate=-0.00718, entropy= 0.26270, loss=-0.00718
surrogate= 0.00391, entropy= 0.26269, loss= 0.00391
surrogate=-0.02724, entropy= 0.26248, loss=-0.02724
std_min= 0.21489, std_max= 0.30952, std_mean= 0.26712
val lr: [3.278688524590165e-05], policy lr: [3.934426229508198e-05]
Policy Loss: -0.027242, | Entropy Bonus: -0, | Value Loss: 56.175, | Advantage Loss: 1.3328
Time elapsed (s): 1.673135757446289
Agent stdevs: 0.26711535
--------------------------------------------------------------------------------

Step 848
++++++++ Policy training ++++++++++
Current mean reward: 2273.637526 | mean episode length: 632.333333
val_loss=378.12418
val_loss=677.47748
val_loss=2038.28870
val_loss=91.86964
val_loss=150.59097
val_loss=35.19816
val_loss=2122.43774
val_loss=2275.59985
val_loss=44.04486
val_loss=28.34826
adv_loss= 2.31441
adv_loss= 1.58675
adv_loss= 1.27787
adv_loss= 1.37724
adv_loss= 7.25579
adv_loss= 1.90471
adv_loss= 7.42078
adv_loss= 4.93085
adv_loss= 2.11898
adv_loss= 1.43565
surrogate=-0.01039, entropy= 0.26250, loss=-0.01039
surrogate=-0.01031, entropy= 0.26230, loss=-0.01031
surrogate=-0.00358, entropy= 0.26249, loss=-0.00358
surrogate= 0.01158, entropy= 0.26253, loss= 0.01158
surrogate= 0.01032, entropy= 0.26278, loss= 0.01032
surrogate=-0.01477, entropy= 0.26280, loss=-0.01477
surrogate=-0.01090, entropy= 0.26280, loss=-0.01090
surrogate=-0.02523, entropy= 0.26276, loss=-0.02523
surrogate=-0.01589, entropy= 0.26282, loss=-0.01589
surrogate=-0.01448, entropy= 0.26254, loss=-0.01448
std_min= 0.21505, std_max= 0.31010, std_mean= 0.26713
val lr: [3.253073770491804e-05], policy lr: [3.903688524590165e-05]
Policy Loss: -0.014482, | Entropy Bonus: -0, | Value Loss: 28.348, | Advantage Loss: 1.4356
Time elapsed (s): 1.6342382431030273
Agent stdevs: 0.26712522
--------------------------------------------------------------------------------

Step 849
++++++++ Policy training ++++++++++
Current mean reward: 2089.652436 | mean episode length: 577.666667
val_loss=44.54038
val_loss=31.95263
val_loss=34.24360
val_loss=1410.14160
val_loss=285.07950
val_loss=1103.31470
val_loss=161.08623
val_loss=25.14760
val_loss=294.47601
val_loss=365.55438
adv_loss= 1.55520
adv_loss= 0.77723
adv_loss= 1.68513
adv_loss= 1.41289
adv_loss= 2.08054
adv_loss= 2.28583
adv_loss= 2.49447
adv_loss= 0.80237
adv_loss= 2.09780
adv_loss= 0.95757
surrogate= 0.01999, entropy= 0.26233, loss= 0.01999
surrogate=-0.01099, entropy= 0.26193, loss=-0.01099
surrogate=-0.02585, entropy= 0.26233, loss=-0.02585
surrogate=-0.04232, entropy= 0.26250, loss=-0.04232
surrogate=-0.01102, entropy= 0.26246, loss=-0.01102
surrogate=-0.00447, entropy= 0.26263, loss=-0.00447
surrogate=-0.01662, entropy= 0.26244, loss=-0.01662
surrogate=-0.00719, entropy= 0.26238, loss=-0.00719
surrogate=-0.02497, entropy= 0.26224, loss=-0.02497
surrogate=-0.02476, entropy= 0.26214, loss=-0.02476
std_min= 0.21506, std_max= 0.30996, std_mean= 0.26708
val lr: [3.2274590163934415e-05], policy lr: [3.872950819672129e-05]
Policy Loss: -0.024761, | Entropy Bonus: -0, | Value Loss: 365.55, | Advantage Loss: 0.95757
Time elapsed (s): 1.6303246021270752
Agent stdevs: 0.26708397
--------------------------------------------------------------------------------

Step 850
++++++++ Policy training ++++++++++
Current mean reward: 3502.698025 | mean episode length: 1000.000000
val_loss=58.95699
val_loss=707.20148
val_loss=2350.85278
val_loss=78.91753
val_loss=1698.41931
val_loss=860.44287
val_loss=1169.86353
val_loss=1515.99207
val_loss=1811.32983
val_loss=1262.52722
adv_loss=1744.19397
adv_loss= 3.18646
adv_loss= 0.97846
adv_loss= 1.19416
adv_loss= 2.53045
adv_loss= 1.89251
adv_loss= 3.42193
adv_loss= 1.61130
adv_loss= 1.20683
adv_loss= 2.10619
surrogate=-0.00285, entropy= 0.26225, loss=-0.00285
surrogate=-0.01010, entropy= 0.26207, loss=-0.01010
surrogate=-0.00405, entropy= 0.26231, loss=-0.00405
surrogate= 0.00197, entropy= 0.26235, loss= 0.00197
surrogate= 0.01052, entropy= 0.26223, loss= 0.01052
surrogate=-0.01291, entropy= 0.26242, loss=-0.01291
surrogate= 0.00192, entropy= 0.26232, loss= 0.00192
surrogate=-0.03968, entropy= 0.26270, loss=-0.03968
surrogate=-0.00574, entropy= 0.26273, loss=-0.00574
surrogate= 0.01149, entropy= 0.26279, loss= 0.01149
std_min= 0.21452, std_max= 0.31068, std_mean= 0.26722
val lr: [3.201844262295081e-05], policy lr: [3.842213114754097e-05]
Policy Loss: 0.01149, | Entropy Bonus: -0, | Value Loss: 1262.5, | Advantage Loss: 2.1062
Time elapsed (s): 1.6416435241699219
Agent stdevs: 0.26722243
--------------------------------------------------------------------------------

Step 851
++++++++ Policy training ++++++++++
Current mean reward: 2324.734702 | mean episode length: 651.000000
val_loss=31.70387
val_loss=976.11328
val_loss=47.14256
val_loss=93.02204
val_loss=55.48441
val_loss=35.58588
val_loss=241.30695
val_loss=59.52855
val_loss=2490.92822
val_loss=86.29385
adv_loss= 1.21868
adv_loss= 1.81447
adv_loss= 3.41689
adv_loss=1798.78760
adv_loss= 1.08180
adv_loss= 1.95775
adv_loss= 2.31504
adv_loss= 2.02053
adv_loss= 1.54119
adv_loss= 2.62386
surrogate=-0.00079, entropy= 0.26292, loss=-0.00079
surrogate=-0.02171, entropy= 0.26315, loss=-0.02171
surrogate=-0.01555, entropy= 0.26325, loss=-0.01555
surrogate=-0.02574, entropy= 0.26348, loss=-0.02574
surrogate=-0.02265, entropy= 0.26328, loss=-0.02265
surrogate= 0.00165, entropy= 0.26357, loss= 0.00165
surrogate=-0.03170, entropy= 0.26369, loss=-0.03170
surrogate=-0.03173, entropy= 0.26403, loss=-0.03173
surrogate=-0.00535, entropy= 0.26417, loss=-0.00535
surrogate= 0.00567, entropy= 0.26428, loss= 0.00567
std_min= 0.21462, std_max= 0.31060, std_mean= 0.26735
val lr: [3.176229508196721e-05], policy lr: [3.811475409836065e-05]
Policy Loss: 0.0056692, | Entropy Bonus: -0, | Value Loss: 86.294, | Advantage Loss: 2.6239
Time elapsed (s): 1.6570417881011963
Agent stdevs: 0.2673457
--------------------------------------------------------------------------------

Step 852
++++++++ Policy training ++++++++++
Current mean reward: 2194.245786 | mean episode length: 612.333333
val_loss=37.87020
val_loss=1527.36938
val_loss=288.56476
val_loss=201.34695
val_loss=26.25254
val_loss=30.09451
val_loss=335.61258
val_loss=31.93503
val_loss=23.64275
val_loss=947.34778
adv_loss= 0.99397
adv_loss= 1.20633
adv_loss= 0.99115
adv_loss= 1.52407
adv_loss= 0.78918
adv_loss= 1.90065
adv_loss= 2.77995
adv_loss= 1.20050
adv_loss= 1.09835
adv_loss= 0.99028
surrogate= 0.00208, entropy= 0.26444, loss= 0.00208
surrogate=-0.00642, entropy= 0.26482, loss=-0.00642
surrogate= 0.00457, entropy= 0.26487, loss= 0.00457
surrogate=-0.01011, entropy= 0.26496, loss=-0.01011
surrogate=-0.00500, entropy= 0.26541, loss=-0.00500
surrogate= 0.00339, entropy= 0.26584, loss= 0.00339
surrogate=-0.02732, entropy= 0.26603, loss=-0.02732
surrogate= 0.01825, entropy= 0.26599, loss= 0.01825
surrogate=-0.01399, entropy= 0.26643, loss=-0.01399
surrogate= 0.00493, entropy= 0.26675, loss= 0.00493
std_min= 0.21537, std_max= 0.31041, std_mean= 0.26749
val lr: [3.15061475409836e-05], policy lr: [3.780737704918032e-05]
Policy Loss: 0.0049303, | Entropy Bonus: -0, | Value Loss: 947.35, | Advantage Loss: 0.99028
Time elapsed (s): 1.651714563369751
Agent stdevs: 0.2674949
--------------------------------------------------------------------------------

Step 853
++++++++ Policy training ++++++++++
Current mean reward: 2227.138766 | mean episode length: 601.500000
val_loss=21.79501
val_loss=17.27126
val_loss=11.53964
val_loss=10.21634
val_loss=13.13834
val_loss= 9.95499
val_loss=11.13656
val_loss= 7.17677
val_loss= 8.91541
val_loss=14.94647
adv_loss= 0.78589
adv_loss= 1.78584
adv_loss= 1.16561
adv_loss= 1.97926
adv_loss= 0.90846
adv_loss= 2.03909
adv_loss= 1.45362
adv_loss= 0.78825
adv_loss= 1.16454
adv_loss= 1.68457
surrogate= 0.01458, entropy= 0.26622, loss= 0.01458
surrogate=-0.00294, entropy= 0.26553, loss=-0.00294
surrogate= 0.00295, entropy= 0.26516, loss= 0.00295
surrogate=-0.00558, entropy= 0.26435, loss=-0.00558
surrogate=-0.01078, entropy= 0.26402, loss=-0.01078
surrogate=-0.03135, entropy= 0.26344, loss=-0.03135
surrogate=-0.01701, entropy= 0.26317, loss=-0.01701
surrogate=-0.03462, entropy= 0.26246, loss=-0.03462
surrogate=-0.01408, entropy= 0.26199, loss=-0.01408
surrogate=-0.02333, entropy= 0.26162, loss=-0.02333
std_min= 0.21517, std_max= 0.30929, std_mean= 0.26700
val lr: [3.125e-05], policy lr: [3.75e-05]
Policy Loss: -0.023332, | Entropy Bonus: -0, | Value Loss: 14.946, | Advantage Loss: 1.6846
Time elapsed (s): 1.6376662254333496
Agent stdevs: 0.2670002
--------------------------------------------------------------------------------

Step 854
++++++++ Policy training ++++++++++
Current mean reward: 2344.475197 | mean episode length: 649.666667
val_loss=607.67413
val_loss=285.62595
val_loss=16.88772
val_loss=27.60747
val_loss=21.92909
val_loss=1238.60889
val_loss=156.58276
val_loss=20.43792
val_loss=198.40225
val_loss=40.51673
adv_loss= 1.00955
adv_loss= 0.86903
adv_loss=1765.43445
adv_loss= 1.05528
adv_loss= 1.49248
adv_loss= 1.85478
adv_loss= 0.87333
adv_loss= 1.74006
adv_loss= 3.31222
adv_loss= 1.35719
surrogate= 0.05141, entropy= 0.26120, loss= 0.05141
surrogate=-0.00192, entropy= 0.26097, loss=-0.00192
surrogate= 0.00535, entropy= 0.26087, loss= 0.00535
surrogate=-0.00788, entropy= 0.26118, loss=-0.00788
surrogate=-0.02256, entropy= 0.26151, loss=-0.02256
surrogate= 0.00077, entropy= 0.26189, loss= 0.00077
surrogate=-0.01602, entropy= 0.26172, loss=-0.01602
surrogate= 0.02011, entropy= 0.26178, loss= 0.02011
surrogate=-0.01791, entropy= 0.26160, loss=-0.01791
surrogate=-0.00496, entropy= 0.26190, loss=-0.00496
std_min= 0.21504, std_max= 0.31012, std_mean= 0.26707
val lr: [3.09938524590164e-05], policy lr: [3.719262295081967e-05]
Policy Loss: -0.0049601, | Entropy Bonus: -0, | Value Loss: 40.517, | Advantage Loss: 1.3572
Time elapsed (s): 1.6347434520721436
Agent stdevs: 0.26706728
--------------------------------------------------------------------------------

Step 855
++++++++ Policy training ++++++++++
Current mean reward: 2353.928365 | mean episode length: 638.000000
val_loss=18.80979
val_loss=13.70414
val_loss=11.48006
val_loss=13.09576
val_loss= 7.79797
val_loss= 8.30197
val_loss=10.23507
val_loss= 5.96873
val_loss= 7.35331
val_loss= 8.15643
adv_loss= 0.99549
adv_loss= 0.73742
adv_loss= 0.87134
adv_loss= 1.32398
adv_loss= 0.85746
adv_loss= 1.11428
adv_loss= 1.69252
adv_loss= 0.41209
adv_loss= 1.09449
adv_loss= 0.85616
surrogate= 0.00219, entropy= 0.26168, loss= 0.00219
surrogate=-0.00730, entropy= 0.26146, loss=-0.00730
surrogate=-0.00028, entropy= 0.26142, loss=-0.00028
surrogate=-0.03353, entropy= 0.26126, loss=-0.03353
surrogate=-0.00141, entropy= 0.26138, loss=-0.00141
surrogate=-0.01725, entropy= 0.26105, loss=-0.01725
surrogate=-0.02775, entropy= 0.26123, loss=-0.02775
surrogate= 0.00052, entropy= 0.26156, loss= 0.00052
surrogate=-0.03328, entropy= 0.26161, loss=-0.03328
surrogate=-0.00075, entropy= 0.26192, loss=-0.00075
std_min= 0.21490, std_max= 0.31026, std_mean= 0.26709
val lr: [3.0737704918032794e-05], policy lr: [3.6885245901639346e-05]
Policy Loss: -0.00075444, | Entropy Bonus: -0, | Value Loss: 8.1564, | Advantage Loss: 0.85616
Time elapsed (s): 1.6361327171325684
Agent stdevs: 0.26708767
--------------------------------------------------------------------------------

Step 856
++++++++ Policy training ++++++++++
Current mean reward: 1386.618642 | mean episode length: 382.000000
val_loss=13.54406
val_loss= 8.08841
val_loss= 8.91718
val_loss= 9.33497
val_loss= 8.11877
val_loss= 9.91452
val_loss= 6.86883
val_loss= 8.25584
val_loss= 6.22291
val_loss= 5.07591
adv_loss= 1.24298
adv_loss= 1.08919
adv_loss= 1.84504
adv_loss= 0.89714
adv_loss= 2.27654
adv_loss= 0.93885
adv_loss= 0.96293
adv_loss= 0.94389
adv_loss= 1.83792
adv_loss= 2.54769
surrogate=-0.00494, entropy= 0.26167, loss=-0.00494
surrogate= 0.00611, entropy= 0.26170, loss= 0.00611
surrogate=-0.00877, entropy= 0.26164, loss=-0.00877
surrogate=-0.00324, entropy= 0.26140, loss=-0.00324
surrogate=-0.00646, entropy= 0.26147, loss=-0.00646
surrogate=-0.00361, entropy= 0.26168, loss=-0.00361
surrogate=-0.02601, entropy= 0.26175, loss=-0.02601
surrogate=-0.02052, entropy= 0.26194, loss=-0.02052
surrogate= 0.01885, entropy= 0.26195, loss= 0.01885
surrogate=-0.00876, entropy= 0.26220, loss=-0.00876
std_min= 0.21541, std_max= 0.30950, std_mean= 0.26704
val lr: [3.048155737704919e-05], policy lr: [3.657786885245903e-05]
Policy Loss: -0.0087646, | Entropy Bonus: -0, | Value Loss: 5.0759, | Advantage Loss: 2.5477
Time elapsed (s): 1.662224531173706
Agent stdevs: 0.2670376
--------------------------------------------------------------------------------

Step 857
++++++++ Policy training ++++++++++
Current mean reward: 1594.328576 | mean episode length: 436.000000
val_loss=17.37637
val_loss=13.74795
val_loss=11.53346
val_loss= 9.41996
val_loss=10.04280
val_loss=14.84417
val_loss= 6.62862
val_loss= 7.31978
val_loss= 8.67261
val_loss= 5.94506
adv_loss= 1.36661
adv_loss= 1.72135
adv_loss= 1.21629
adv_loss= 1.49255
adv_loss= 1.70427
adv_loss= 1.43925
adv_loss= 4.42340
adv_loss= 3.27348
adv_loss= 0.86590
adv_loss= 2.56733
surrogate=-0.01579, entropy= 0.26216, loss=-0.01579
surrogate=-0.00181, entropy= 0.26231, loss=-0.00181
surrogate= 0.02051, entropy= 0.26210, loss= 0.02051
surrogate=-0.01798, entropy= 0.26239, loss=-0.01798
surrogate=-0.02301, entropy= 0.26226, loss=-0.02301
surrogate=-0.01610, entropy= 0.26218, loss=-0.01610
surrogate= 0.00443, entropy= 0.26184, loss= 0.00443
surrogate=-0.01047, entropy= 0.26184, loss=-0.01047
surrogate=-0.00230, entropy= 0.26141, loss=-0.00230
surrogate=-0.01975, entropy= 0.26130, loss=-0.01975
std_min= 0.21535, std_max= 0.30970, std_mean= 0.26697
val lr: [3.0225409836065587e-05], policy lr: [3.62704918032787e-05]
Policy Loss: -0.019752, | Entropy Bonus: -0, | Value Loss: 5.9451, | Advantage Loss: 2.5673
Time elapsed (s): 1.6513566970825195
Agent stdevs: 0.26696834
--------------------------------------------------------------------------------

Step 858
++++++++ Policy training ++++++++++
Current mean reward: 1755.106716 | mean episode length: 478.500000
val_loss=22.10389
val_loss=17.39538
val_loss=18.19228
val_loss=12.80832
val_loss=12.13957
val_loss=14.93823
val_loss=13.47016
val_loss= 8.01011
val_loss=13.96291
val_loss= 8.96744
adv_loss= 1.04008
adv_loss= 1.17270
adv_loss= 0.65833
adv_loss= 1.46620
adv_loss= 5.58873
adv_loss= 5.70000
adv_loss= 2.00194
adv_loss= 9.71101
adv_loss= 1.41808
adv_loss= 1.43949
surrogate= 0.00060, entropy= 0.26172, loss= 0.00060
surrogate= 0.03332, entropy= 0.26212, loss= 0.03332
surrogate=-0.01562, entropy= 0.26268, loss=-0.01562
surrogate=-0.01063, entropy= 0.26294, loss=-0.01063
surrogate=-0.01479, entropy= 0.26300, loss=-0.01479
surrogate=-0.01896, entropy= 0.26301, loss=-0.01896
surrogate= 0.02900, entropy= 0.26289, loss= 0.02900
surrogate=-0.02098, entropy= 0.26324, loss=-0.02098
surrogate=-0.00822, entropy= 0.26325, loss=-0.00822
surrogate=-0.01901, entropy= 0.26344, loss=-0.01901
std_min= 0.21542, std_max= 0.30980, std_mean= 0.26716
val lr: [2.9969262295081956e-05], policy lr: [3.596311475409834e-05]
Policy Loss: -0.019011, | Entropy Bonus: -0, | Value Loss: 8.9674, | Advantage Loss: 1.4395
Time elapsed (s): 1.667008399963379
Agent stdevs: 0.26716292
--------------------------------------------------------------------------------

Step 859
++++++++ Policy training ++++++++++
Current mean reward: 1556.931637 | mean episode length: 423.250000
val_loss= 9.25719
val_loss= 9.41575
val_loss= 8.68054
val_loss= 7.30075
val_loss= 8.46260
val_loss= 7.52048
val_loss= 7.64875
val_loss=10.42287
val_loss= 5.77774
val_loss= 7.46215
adv_loss= 1.45928
adv_loss= 2.62466
adv_loss= 1.72398
adv_loss= 0.58661
adv_loss= 0.95107
adv_loss= 3.57405
adv_loss= 0.87262
adv_loss= 1.13959
adv_loss= 0.94404
adv_loss= 1.32298
surrogate=-0.00621, entropy= 0.26373, loss=-0.00621
surrogate= 0.01675, entropy= 0.26353, loss= 0.01675
surrogate=-0.00357, entropy= 0.26336, loss=-0.00357
surrogate=-0.01596, entropy= 0.26289, loss=-0.01596
surrogate=-0.00342, entropy= 0.26304, loss=-0.00342
surrogate=-0.02108, entropy= 0.26280, loss=-0.02108
surrogate=-0.02136, entropy= 0.26280, loss=-0.02136
surrogate=-0.01592, entropy= 0.26245, loss=-0.01592
surrogate= 0.00190, entropy= 0.26228, loss= 0.00190
surrogate=-0.00980, entropy= 0.26202, loss=-0.00980
std_min= 0.21536, std_max= 0.31064, std_mean= 0.26707
val lr: [2.9713114754098352e-05], policy lr: [3.5655737704918016e-05]
Policy Loss: -0.0097987, | Entropy Bonus: -0, | Value Loss: 7.4622, | Advantage Loss: 1.323
Time elapsed (s): 1.6386384963989258
Agent stdevs: 0.26706827
--------------------------------------------------------------------------------

Step 860
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1729.4
++++++++ Policy training ++++++++++
Current mean reward: 1622.841707 | mean episode length: 436.000000
val_loss=14.40009
val_loss= 9.22732
val_loss=16.37423
val_loss= 6.14362
val_loss=11.42083
val_loss= 8.92528
val_loss= 5.95316
val_loss= 9.01178
val_loss= 9.13072
val_loss= 5.74728
adv_loss= 0.77673
adv_loss= 1.78875
adv_loss= 1.91446
adv_loss= 1.20448
adv_loss= 0.49069
adv_loss= 2.02234
adv_loss= 1.13029
adv_loss= 1.70329
adv_loss= 0.83921
adv_loss= 1.33256
surrogate=-0.02635, entropy= 0.26213, loss=-0.02635
surrogate=-0.02853, entropy= 0.26189, loss=-0.02853
surrogate=-0.02174, entropy= 0.26183, loss=-0.02174
surrogate=-0.02573, entropy= 0.26149, loss=-0.02573
surrogate=-0.02266, entropy= 0.26153, loss=-0.02266
surrogate= 0.02822, entropy= 0.26144, loss= 0.02822
surrogate=-0.00657, entropy= 0.26142, loss=-0.00657
surrogate=-0.01119, entropy= 0.26144, loss=-0.01119
surrogate=-0.03485, entropy= 0.26158, loss=-0.03485
surrogate=-0.03039, entropy= 0.26144, loss=-0.03039
std_min= 0.21520, std_max= 0.31075, std_mean= 0.26703
val lr: [2.945696721311475e-05], policy lr: [3.53483606557377e-05]
Policy Loss: -0.030387, | Entropy Bonus: -0, | Value Loss: 5.7473, | Advantage Loss: 1.3326
Time elapsed (s): 1.644648790359497
Agent stdevs: 0.26703408
--------------------------------------------------------------------------------

Step 861
++++++++ Policy training ++++++++++
Current mean reward: 1421.381113 | mean episode length: 384.000000
val_loss= 7.29767
val_loss= 7.23741
val_loss= 4.92014
val_loss= 5.93983
val_loss= 4.69530
val_loss= 7.12490
val_loss= 4.30145
val_loss= 5.69512
val_loss= 8.65797
val_loss= 6.53232
adv_loss= 0.88429
adv_loss= 2.02111
adv_loss= 4.19143
adv_loss= 1.84518
adv_loss= 1.32218
adv_loss= 1.14557
adv_loss= 4.88094
adv_loss= 1.18161
adv_loss= 1.24002
adv_loss= 1.13705
surrogate= 0.01022, entropy= 0.26154, loss= 0.01022
surrogate=-0.00951, entropy= 0.26152, loss=-0.00951
surrogate= 0.00503, entropy= 0.26142, loss= 0.00503
surrogate=-0.02183, entropy= 0.26136, loss=-0.02183
surrogate=-0.02920, entropy= 0.26105, loss=-0.02920
surrogate= 0.00657, entropy= 0.26078, loss= 0.00657
surrogate= 0.00386, entropy= 0.26068, loss= 0.00386
surrogate=-0.00374, entropy= 0.26042, loss=-0.00374
surrogate=-0.01508, entropy= 0.26022, loss=-0.01508
surrogate=-0.02031, entropy= 0.25999, loss=-0.02031
std_min= 0.21471, std_max= 0.31086, std_mean= 0.26695
val lr: [2.9200819672131145e-05], policy lr: [3.504098360655737e-05]
Policy Loss: -0.020309, | Entropy Bonus: -0, | Value Loss: 6.5323, | Advantage Loss: 1.1371
Time elapsed (s): 1.6338417530059814
Agent stdevs: 0.2669505
--------------------------------------------------------------------------------

Step 862
++++++++ Policy training ++++++++++
Current mean reward: 1729.169110 | mean episode length: 469.500000
val_loss= 6.91514
val_loss= 8.19117
val_loss= 4.59938
val_loss= 3.49110
val_loss= 5.58627
val_loss= 7.69032
val_loss= 5.59427
val_loss= 7.45526
val_loss= 3.79177
val_loss= 4.01744
adv_loss= 1.30115
adv_loss= 1.01609
adv_loss= 1.64769
adv_loss= 2.69107
adv_loss= 1.37327
adv_loss= 0.45723
adv_loss= 1.10697
adv_loss= 0.65908
adv_loss= 0.98484
adv_loss= 1.03187
surrogate= 0.00394, entropy= 0.25969, loss= 0.00394
surrogate=-0.02600, entropy= 0.25950, loss=-0.02600
surrogate=-0.00248, entropy= 0.25894, loss=-0.00248
surrogate=-0.01022, entropy= 0.25843, loss=-0.01022
surrogate=-0.02974, entropy= 0.25789, loss=-0.02974
surrogate=-0.00181, entropy= 0.25718, loss=-0.00181
surrogate=-0.02921, entropy= 0.25661, loss=-0.02921
surrogate=-0.01853, entropy= 0.25587, loss=-0.01853
surrogate=-0.01876, entropy= 0.25554, loss=-0.01876
surrogate=-0.04402, entropy= 0.25514, loss=-0.04402
std_min= 0.21474, std_max= 0.31021, std_mean= 0.26648
val lr: [2.8944672131147542e-05], policy lr: [3.4733606557377046e-05]
Policy Loss: -0.044024, | Entropy Bonus: -0, | Value Loss: 4.0174, | Advantage Loss: 1.0319
Time elapsed (s): 1.671248197555542
Agent stdevs: 0.26647934
--------------------------------------------------------------------------------

Step 863
++++++++ Policy training ++++++++++
Current mean reward: 1150.080328 | mean episode length: 318.000000
val_loss=166.92146
val_loss=2871.17920
val_loss=55.22448
val_loss=1264.89832
val_loss=35.41758
val_loss=237.56973
val_loss=637.10089
val_loss=19.06201
val_loss=742.93561
val_loss=1733.80750
adv_loss= 1.21369
adv_loss= 0.92292
adv_loss= 4.02590
adv_loss= 1.09302
adv_loss= 1.00082
adv_loss= 0.74801
adv_loss= 1.57905
adv_loss= 1.03334
adv_loss= 2.12950
adv_loss= 1.60255
surrogate=-0.02204, entropy= 0.25542, loss=-0.02204
surrogate=-0.01489, entropy= 0.25572, loss=-0.01489
surrogate=-0.04153, entropy= 0.25610, loss=-0.04153
surrogate=-0.01364, entropy= 0.25653, loss=-0.01364
surrogate= 0.01622, entropy= 0.25706, loss= 0.01622
surrogate=-0.02401, entropy= 0.25743, loss=-0.02401
surrogate=-0.01378, entropy= 0.25789, loss=-0.01378
surrogate=-0.00241, entropy= 0.25821, loss=-0.00241
surrogate=-0.00751, entropy= 0.25852, loss=-0.00751
surrogate= 0.00349, entropy= 0.25886, loss= 0.00349
std_min= 0.21490, std_max= 0.31019, std_mean= 0.26681
val lr: [2.868852459016394e-05], policy lr: [3.442622950819672e-05]
Policy Loss: 0.0034859, | Entropy Bonus: -0, | Value Loss: 1733.8, | Advantage Loss: 1.6025
Time elapsed (s): 1.6884260177612305
Agent stdevs: 0.2668067
--------------------------------------------------------------------------------

Step 864
++++++++ Policy training ++++++++++
Current mean reward: 2249.117967 | mean episode length: 610.333333
val_loss=15.73189
val_loss= 7.23422
val_loss= 6.81936
val_loss=16.18380
val_loss=18.26049
val_loss= 5.97543
val_loss= 9.71176
val_loss= 4.77335
val_loss=14.28836
val_loss=10.38125
adv_loss= 1.44986
adv_loss= 0.64241
adv_loss= 1.32115
adv_loss= 2.22682
adv_loss= 1.88021
adv_loss= 1.84260
adv_loss= 0.69232
adv_loss= 0.61790
adv_loss= 1.96864
adv_loss= 1.19845
surrogate= 0.01195, entropy= 0.25896, loss= 0.01195
surrogate=-0.00418, entropy= 0.25883, loss=-0.00418
surrogate= 0.02977, entropy= 0.25899, loss= 0.02977
surrogate=-0.01310, entropy= 0.25873, loss=-0.01310
surrogate= 0.03147, entropy= 0.25849, loss= 0.03147
surrogate=-0.02472, entropy= 0.25835, loss=-0.02472
surrogate=-0.01162, entropy= 0.25854, loss=-0.01162
surrogate=-0.02724, entropy= 0.25866, loss=-0.02724
surrogate=-0.00836, entropy= 0.25830, loss=-0.00836
surrogate=-0.02340, entropy= 0.25817, loss=-0.02340
std_min= 0.21494, std_max= 0.31020, std_mean= 0.26674
val lr: [2.8432377049180335e-05], policy lr: [3.41188524590164e-05]
Policy Loss: -0.023401, | Entropy Bonus: -0, | Value Loss: 10.381, | Advantage Loss: 1.1985
Time elapsed (s): 1.678079605102539
Agent stdevs: 0.2667378
--------------------------------------------------------------------------------

Step 865
++++++++ Policy training ++++++++++
Current mean reward: 2032.258153 | mean episode length: 564.666667
val_loss=93.16485
val_loss=453.51685
val_loss=12.75701
val_loss=16.60886
val_loss=58.97403
val_loss=23.84583
val_loss=1054.32825
val_loss=225.33485
val_loss=624.54767
val_loss=52.76491
adv_loss= 0.89404
adv_loss= 1.28646
adv_loss= 0.66286
adv_loss= 0.96898
adv_loss= 1.73580
adv_loss= 1.45483
adv_loss= 1.31868
adv_loss= 1.31141
adv_loss= 1.03370
adv_loss= 1.03089
surrogate= 0.05758, entropy= 0.25812, loss= 0.05758
surrogate=-0.00810, entropy= 0.25777, loss=-0.00810
surrogate=-0.00386, entropy= 0.25753, loss=-0.00386
surrogate=-0.01455, entropy= 0.25753, loss=-0.01455
surrogate=-0.01869, entropy= 0.25729, loss=-0.01869
surrogate=-0.00799, entropy= 0.25727, loss=-0.00799
surrogate= 0.00187, entropy= 0.25701, loss= 0.00187
surrogate= 0.01921, entropy= 0.25715, loss= 0.01921
surrogate=-0.02324, entropy= 0.25700, loss=-0.02324
surrogate= 0.00038, entropy= 0.25684, loss= 0.00038
std_min= 0.21456, std_max= 0.31064, std_mean= 0.26667
val lr: [2.817622950819673e-05], policy lr: [3.3811475409836076e-05]
Policy Loss: 0.00038072, | Entropy Bonus: -0, | Value Loss: 52.765, | Advantage Loss: 1.0309
Time elapsed (s): 1.6631085872650146
Agent stdevs: 0.26667058
--------------------------------------------------------------------------------

Step 866
++++++++ Policy training ++++++++++
Current mean reward: 1844.489690 | mean episode length: 500.000000
val_loss=12.43303
val_loss= 8.98784
val_loss= 7.45728
val_loss= 5.98322
val_loss= 5.55980
val_loss= 3.61332
val_loss= 7.30504
val_loss= 4.51765
val_loss= 5.32470
val_loss= 5.39978
adv_loss= 0.49526
adv_loss= 0.99873
adv_loss= 0.65084
adv_loss= 1.98699
adv_loss= 1.91909
adv_loss= 1.35883
adv_loss= 1.42684
adv_loss= 1.17845
adv_loss= 1.02219
adv_loss= 1.55294
surrogate= 0.00534, entropy= 0.25694, loss= 0.00534
surrogate=-0.02685, entropy= 0.25739, loss=-0.02685
surrogate=-0.00851, entropy= 0.25758, loss=-0.00851
surrogate=-0.00900, entropy= 0.25779, loss=-0.00900
surrogate=-0.01471, entropy= 0.25786, loss=-0.01471
surrogate=-0.00863, entropy= 0.25790, loss=-0.00863
surrogate=-0.01706, entropy= 0.25802, loss=-0.01706
surrogate=-0.01499, entropy= 0.25824, loss=-0.01499
surrogate=-0.02298, entropy= 0.25831, loss=-0.02298
surrogate=-0.01421, entropy= 0.25839, loss=-0.01421
std_min= 0.21459, std_max= 0.31042, std_mean= 0.26680
val lr: [2.7920081967213128e-05], policy lr: [3.350409836065575e-05]
Policy Loss: -0.01421, | Entropy Bonus: -0, | Value Loss: 5.3998, | Advantage Loss: 1.5529
Time elapsed (s): 1.684516191482544
Agent stdevs: 0.26679912
--------------------------------------------------------------------------------

Step 867
++++++++ Policy training ++++++++++
Current mean reward: 2317.142787 | mean episode length: 645.666667
val_loss= 9.17336
val_loss=1412.30078
val_loss=129.62567
val_loss=448.70145
val_loss=26.96257
val_loss=21.92184
val_loss=1301.70361
val_loss=22.74778
val_loss=850.32825
val_loss=25.21703
adv_loss= 0.63507
adv_loss=1872.06079
adv_loss= 1.32267
adv_loss= 0.56311
adv_loss= 0.84419
adv_loss= 0.55761
adv_loss= 1.74694
adv_loss= 1.12639
adv_loss= 0.95766
adv_loss= 0.79261
surrogate=-0.00844, entropy= 0.25851, loss=-0.00844
surrogate= 0.00625, entropy= 0.25856, loss= 0.00625
surrogate=-0.00032, entropy= 0.25865, loss=-0.00032
surrogate=-0.00423, entropy= 0.25840, loss=-0.00423
surrogate=-0.03132, entropy= 0.25828, loss=-0.03132
surrogate=-0.02445, entropy= 0.25803, loss=-0.02445
surrogate=-0.00691, entropy= 0.25791, loss=-0.00691
surrogate=-0.01228, entropy= 0.25788, loss=-0.01228
surrogate=-0.02105, entropy= 0.25773, loss=-0.02105
surrogate=-0.01347, entropy= 0.25783, loss=-0.01347
std_min= 0.21458, std_max= 0.31034, std_mean= 0.26675
val lr: [2.7663934426229497e-05], policy lr: [3.319672131147539e-05]
Policy Loss: -0.013473, | Entropy Bonus: -0, | Value Loss: 25.217, | Advantage Loss: 0.79261
Time elapsed (s): 1.6996426582336426
Agent stdevs: 0.2667458
--------------------------------------------------------------------------------

Step 868
++++++++ Policy training ++++++++++
Current mean reward: 1439.913507 | mean episode length: 391.000000
val_loss=16.22528
val_loss=10.93038
val_loss=10.54016
val_loss= 6.75257
val_loss= 8.21944
val_loss= 6.02363
val_loss= 5.81104
val_loss= 8.34835
val_loss= 6.15149
val_loss= 7.56294
adv_loss= 1.34788
adv_loss= 1.37118
adv_loss= 1.41728
adv_loss= 2.95015
adv_loss= 1.41508
adv_loss= 2.21192
adv_loss= 1.99065
adv_loss= 2.13332
adv_loss= 0.80580
adv_loss= 0.74473
surrogate= 0.00130, entropy= 0.25744, loss= 0.00130
surrogate=-0.00512, entropy= 0.25699, loss=-0.00512
surrogate=-0.00944, entropy= 0.25664, loss=-0.00944
surrogate=-0.00379, entropy= 0.25633, loss=-0.00379
surrogate=-0.00898, entropy= 0.25588, loss=-0.00898
surrogate= 0.00991, entropy= 0.25541, loss= 0.00991
surrogate= 0.00357, entropy= 0.25490, loss= 0.00357
surrogate=-0.00888, entropy= 0.25454, loss=-0.00888
surrogate=-0.00708, entropy= 0.25417, loss=-0.00708
surrogate=-0.02755, entropy= 0.25388, loss=-0.02755
std_min= 0.21435, std_max= 0.31008, std_mean= 0.26640
val lr: [2.7407786885245894e-05], policy lr: [3.288934426229507e-05]
Policy Loss: -0.027554, | Entropy Bonus: -0, | Value Loss: 7.5629, | Advantage Loss: 0.74473
Time elapsed (s): 1.6798903942108154
Agent stdevs: 0.26639658
--------------------------------------------------------------------------------

Step 869
++++++++ Policy training ++++++++++
Current mean reward: 1459.291607 | mean episode length: 396.400000
val_loss= 9.45496
val_loss= 7.74881
val_loss= 7.03702
val_loss=10.31485
val_loss= 6.55477
val_loss= 6.96068
val_loss= 8.20751
val_loss= 8.01429
val_loss= 9.13251
val_loss= 5.76873
adv_loss= 1.62221
adv_loss= 1.63659
adv_loss= 2.14774
adv_loss= 1.53335
adv_loss= 0.67239
adv_loss= 0.75499
adv_loss= 2.19020
adv_loss= 0.72047
adv_loss= 2.34556
adv_loss= 1.54206
surrogate= 0.00365, entropy= 0.25384, loss= 0.00365
surrogate= 0.00092, entropy= 0.25413, loss= 0.00092
surrogate=-0.00868, entropy= 0.25404, loss=-0.00868
surrogate=-0.01270, entropy= 0.25389, loss=-0.01270
surrogate=-0.01667, entropy= 0.25413, loss=-0.01667
surrogate=-0.00336, entropy= 0.25399, loss=-0.00336
surrogate=-0.01674, entropy= 0.25380, loss=-0.01674
surrogate= 0.00380, entropy= 0.25389, loss= 0.00380
surrogate=-0.02758, entropy= 0.25364, loss=-0.02758
surrogate=-0.03309, entropy= 0.25364, loss=-0.03309
std_min= 0.21420, std_max= 0.31038, std_mean= 0.26640
val lr: [2.715163934426229e-05], policy lr: [3.258196721311475e-05]
Policy Loss: -0.033093, | Entropy Bonus: -0, | Value Loss: 5.7687, | Advantage Loss: 1.5421
Time elapsed (s): 1.6877858638763428
Agent stdevs: 0.26640016
--------------------------------------------------------------------------------

Step 870
++++++++ Policy training ++++++++++
Current mean reward: 1659.065186 | mean episode length: 448.250000
val_loss=12.46322
val_loss= 6.17815
val_loss= 6.73601
val_loss=13.33756
val_loss= 7.62943
val_loss= 6.60597
val_loss= 5.42679
val_loss= 6.40050
val_loss= 8.61059
val_loss= 6.25510
adv_loss= 1.28665
adv_loss= 1.45195
adv_loss= 1.03440
adv_loss= 0.97477
adv_loss= 0.61139
adv_loss= 1.37732
adv_loss= 1.27153
adv_loss= 0.69415
adv_loss= 1.31461
adv_loss= 0.97441
surrogate=-0.00099, entropy= 0.25375, loss=-0.00099
surrogate=-0.00072, entropy= 0.25309, loss=-0.00072
surrogate=-0.02641, entropy= 0.25270, loss=-0.02641
surrogate=-0.02411, entropy= 0.25263, loss=-0.02411
surrogate=-0.01793, entropy= 0.25233, loss=-0.01793
surrogate=-0.00518, entropy= 0.25202, loss=-0.00518
surrogate=-0.00487, entropy= 0.25161, loss=-0.00487
surrogate=-0.01936, entropy= 0.25149, loss=-0.01936
surrogate=-0.02818, entropy= 0.25105, loss=-0.02818
surrogate=-0.01594, entropy= 0.25090, loss=-0.01594
std_min= 0.21411, std_max= 0.30944, std_mean= 0.26612
val lr: [2.6895491803278687e-05], policy lr: [3.227459016393442e-05]
Policy Loss: -0.015942, | Entropy Bonus: -0, | Value Loss: 6.2551, | Advantage Loss: 0.97441
Time elapsed (s): 1.677783727645874
Agent stdevs: 0.26611936
--------------------------------------------------------------------------------

Step 871
++++++++ Policy training ++++++++++
Current mean reward: 1457.433609 | mean episode length: 395.000000
val_loss= 9.76548
val_loss= 7.52846
val_loss= 8.79819
val_loss= 8.30195
val_loss=11.67688
val_loss= 5.78409
val_loss= 8.20549
val_loss= 6.08843
val_loss= 5.76918
val_loss= 6.69740
adv_loss= 2.06490
adv_loss= 0.77660
adv_loss= 0.95973
adv_loss= 0.48234
adv_loss= 1.64785
adv_loss= 1.10066
adv_loss= 1.28472
adv_loss= 0.94036
adv_loss= 0.88885
adv_loss= 1.63078
surrogate=-0.00508, entropy= 0.25052, loss=-0.00508
surrogate=-0.03009, entropy= 0.25037, loss=-0.03009
surrogate=-0.00419, entropy= 0.24991, loss=-0.00419
surrogate=-0.00795, entropy= 0.24976, loss=-0.00795
surrogate=-0.01150, entropy= 0.24957, loss=-0.01150
surrogate=-0.01107, entropy= 0.24932, loss=-0.01107
surrogate=-0.03228, entropy= 0.24898, loss=-0.03228
surrogate=-0.00092, entropy= 0.24857, loss=-0.00092
surrogate=-0.02117, entropy= 0.24814, loss=-0.02117
surrogate=-0.02744, entropy= 0.24784, loss=-0.02744
std_min= 0.21352, std_max= 0.30940, std_mean= 0.26589
val lr: [2.6639344262295083e-05], policy lr: [3.1967213114754096e-05]
Policy Loss: -0.027437, | Entropy Bonus: -0, | Value Loss: 6.6974, | Advantage Loss: 1.6308
Time elapsed (s): 1.6479084491729736
Agent stdevs: 0.2658948
--------------------------------------------------------------------------------

Step 872
++++++++ Policy training ++++++++++
Current mean reward: 1816.634728 | mean episode length: 491.250000
val_loss= 5.43627
val_loss= 7.95325
val_loss= 5.93650
val_loss= 5.13354
val_loss= 8.06184
val_loss= 3.93877
val_loss= 6.64085
val_loss= 5.63854
val_loss= 4.70429
val_loss= 4.36706
adv_loss= 2.06315
adv_loss= 1.47130
adv_loss= 0.95674
adv_loss= 0.82238
adv_loss= 1.07173
adv_loss= 1.65918
adv_loss= 1.08924
adv_loss= 0.68626
adv_loss= 1.95357
adv_loss= 1.47193
surrogate= 0.00353, entropy= 0.24759, loss= 0.00353
surrogate= 0.00697, entropy= 0.24735, loss= 0.00697
surrogate=-0.00642, entropy= 0.24739, loss=-0.00642
surrogate=-0.02438, entropy= 0.24710, loss=-0.02438
surrogate= 0.00092, entropy= 0.24678, loss= 0.00092
surrogate=-0.01312, entropy= 0.24654, loss=-0.01312
surrogate=-0.02588, entropy= 0.24627, loss=-0.02588
surrogate=-0.00725, entropy= 0.24592, loss=-0.00725
surrogate=-0.02003, entropy= 0.24594, loss=-0.02003
surrogate= 0.00119, entropy= 0.24569, loss= 0.00119
std_min= 0.21365, std_max= 0.30908, std_mean= 0.26568
val lr: [2.638319672131148e-05], policy lr: [3.165983606557378e-05]
Policy Loss: 0.0011867, | Entropy Bonus: -0, | Value Loss: 4.3671, | Advantage Loss: 1.4719
Time elapsed (s): 1.6435585021972656
Agent stdevs: 0.2656761
--------------------------------------------------------------------------------

Step 873
++++++++ Policy training ++++++++++
Current mean reward: 1527.273789 | mean episode length: 410.750000
val_loss=14.54499
val_loss= 5.12260
val_loss= 6.71445
val_loss= 6.69977
val_loss= 5.54797
val_loss= 9.95905
val_loss= 6.66123
val_loss= 5.40139
val_loss= 5.65734
val_loss= 3.63425
adv_loss= 1.04066
adv_loss= 0.50129
adv_loss= 1.81148
adv_loss= 0.86755
adv_loss= 1.64685
adv_loss= 0.64138
adv_loss= 0.91471
adv_loss= 0.58100
adv_loss= 1.04370
adv_loss= 0.65273
surrogate= 0.00441, entropy= 0.24604, loss= 0.00441
surrogate= 0.01431, entropy= 0.24607, loss= 0.01431
surrogate=-0.01441, entropy= 0.24599, loss=-0.01441
surrogate=-0.03441, entropy= 0.24585, loss=-0.03441
surrogate=-0.01410, entropy= 0.24575, loss=-0.01410
surrogate=-0.03269, entropy= 0.24554, loss=-0.03269
surrogate=-0.00884, entropy= 0.24534, loss=-0.00884
surrogate=-0.04459, entropy= 0.24513, loss=-0.04459
surrogate=-0.02323, entropy= 0.24533, loss=-0.02323
surrogate=-0.03658, entropy= 0.24513, loss=-0.03658
std_min= 0.21349, std_max= 0.30908, std_mean= 0.26564
val lr: [2.6127049180327876e-05], policy lr: [3.135245901639345e-05]
Policy Loss: -0.036582, | Entropy Bonus: -0, | Value Loss: 3.6342, | Advantage Loss: 0.65273
Time elapsed (s): 1.6466114521026611
Agent stdevs: 0.26563764
--------------------------------------------------------------------------------

Step 874
++++++++ Policy training ++++++++++
Current mean reward: 2207.564273 | mean episode length: 593.666667
val_loss= 4.49685
val_loss= 5.85285
val_loss= 3.36317
val_loss= 4.33058
val_loss= 6.43117
val_loss= 4.02469
val_loss= 5.28317
val_loss= 4.21300
val_loss= 3.31717
val_loss= 3.14950
adv_loss= 0.95480
adv_loss= 1.31330
adv_loss= 1.36608
adv_loss= 1.46963
adv_loss= 0.73638
adv_loss= 0.98760
adv_loss= 0.86664
adv_loss= 1.21950
adv_loss= 1.48734
adv_loss= 1.85258
surrogate=-0.00751, entropy= 0.24501, loss=-0.00751
surrogate=-0.00704, entropy= 0.24462, loss=-0.00704
surrogate= 0.00032, entropy= 0.24439, loss= 0.00032
surrogate=-0.02269, entropy= 0.24407, loss=-0.02269
surrogate=-0.00918, entropy= 0.24367, loss=-0.00918
surrogate= 0.00651, entropy= 0.24332, loss= 0.00651
surrogate= 0.00258, entropy= 0.24310, loss= 0.00258
surrogate= 0.00317, entropy= 0.24263, loss= 0.00317
surrogate=-0.01675, entropy= 0.24244, loss=-0.01675
surrogate=-0.01477, entropy= 0.24233, loss=-0.01477
std_min= 0.21323, std_max= 0.30844, std_mean= 0.26538
val lr: [2.5870901639344273e-05], policy lr: [3.1045081967213126e-05]
Policy Loss: -0.014775, | Entropy Bonus: -0, | Value Loss: 3.1495, | Advantage Loss: 1.8526
Time elapsed (s): 1.653810739517212
Agent stdevs: 0.26538137
--------------------------------------------------------------------------------

Step 875
++++++++ Policy training ++++++++++
Current mean reward: 1964.494605 | mean episode length: 531.000000
val_loss= 5.70318
val_loss= 7.82633
val_loss= 6.20954
val_loss= 4.82561
val_loss= 6.36195
val_loss= 4.61316
val_loss= 6.09312
val_loss= 4.79709
val_loss= 6.29358
val_loss= 5.46845
adv_loss= 0.86646
adv_loss= 0.81437
adv_loss= 0.49421
adv_loss= 1.12539
adv_loss= 0.40765
adv_loss= 1.50163
adv_loss= 1.35003
adv_loss= 1.23686
adv_loss= 1.30366
adv_loss= 1.10408
surrogate= 0.01799, entropy= 0.24211, loss= 0.01799
surrogate=-0.01140, entropy= 0.24238, loss=-0.01140
surrogate=-0.00071, entropy= 0.24241, loss=-0.00071
surrogate=-0.01088, entropy= 0.24255, loss=-0.01088
surrogate=-0.01941, entropy= 0.24261, loss=-0.01941
surrogate=-0.00824, entropy= 0.24254, loss=-0.00824
surrogate=-0.04204, entropy= 0.24237, loss=-0.04204
surrogate=-0.00427, entropy= 0.24255, loss=-0.00427
surrogate=-0.01672, entropy= 0.24271, loss=-0.01672
surrogate=-0.02346, entropy= 0.24267, loss=-0.02346
std_min= 0.21337, std_max= 0.30776, std_mean= 0.26538
val lr: [2.5614754098360642e-05], policy lr: [3.0737704918032767e-05]
Policy Loss: -0.023459, | Entropy Bonus: -0, | Value Loss: 5.4685, | Advantage Loss: 1.1041
Time elapsed (s): 1.655519962310791
Agent stdevs: 0.26537663
--------------------------------------------------------------------------------

Step 876
++++++++ Policy training ++++++++++
Current mean reward: 1384.772643 | mean episode length: 372.000000
val_loss= 6.35636
val_loss= 3.19987
val_loss= 3.88200
val_loss= 3.96381
val_loss= 2.56533
val_loss= 3.68498
val_loss= 3.79516
val_loss= 3.32866
val_loss= 5.53797
val_loss= 2.45298
adv_loss= 0.77909
adv_loss= 1.64674
adv_loss= 0.95672
adv_loss= 1.37496
adv_loss= 1.34314
adv_loss= 1.19949
adv_loss= 1.09100
adv_loss= 0.39434
adv_loss= 1.08179
adv_loss= 1.02588
surrogate= 0.00263, entropy= 0.24280, loss= 0.00263
surrogate= 0.00108, entropy= 0.24284, loss= 0.00108
surrogate=-0.01234, entropy= 0.24270, loss=-0.01234
surrogate= 0.00212, entropy= 0.24289, loss= 0.00212
surrogate=-0.00878, entropy= 0.24288, loss=-0.00878
surrogate=-0.00660, entropy= 0.24305, loss=-0.00660
surrogate=-0.00788, entropy= 0.24330, loss=-0.00788
surrogate=-0.00313, entropy= 0.24315, loss=-0.00313
surrogate=-0.00832, entropy= 0.24301, loss=-0.00832
surrogate=-0.01846, entropy= 0.24291, loss=-0.01846
std_min= 0.21311, std_max= 0.30817, std_mean= 0.26544
val lr: [2.535860655737704e-05], policy lr: [3.0430327868852444e-05]
Policy Loss: -0.018465, | Entropy Bonus: -0, | Value Loss: 2.453, | Advantage Loss: 1.0259
Time elapsed (s): 1.683971643447876
Agent stdevs: 0.26543903
--------------------------------------------------------------------------------

Step 877
++++++++ Policy training ++++++++++
Current mean reward: 1685.031104 | mean episode length: 451.750000
val_loss= 3.25472
val_loss= 5.29770
val_loss= 4.56882
val_loss= 4.47977
val_loss= 2.91387
val_loss= 4.30463
val_loss= 5.33546
val_loss= 3.76852
val_loss= 3.58236
val_loss= 4.69340
adv_loss= 1.85683
adv_loss= 0.71460
adv_loss= 1.26729
adv_loss= 0.74620
adv_loss= 0.95802
adv_loss= 0.98951
adv_loss= 0.84501
adv_loss= 0.53288
adv_loss= 0.95462
adv_loss= 0.83926
surrogate= 0.00703, entropy= 0.24354, loss= 0.00703
surrogate= 0.01213, entropy= 0.24424, loss= 0.01213
surrogate=-0.00783, entropy= 0.24512, loss=-0.00783
surrogate=-0.01225, entropy= 0.24563, loss=-0.01225
surrogate=-0.00554, entropy= 0.24623, loss=-0.00554
surrogate=-0.00582, entropy= 0.24683, loss=-0.00582
surrogate=-0.01446, entropy= 0.24759, loss=-0.01446
surrogate= 0.00417, entropy= 0.24839, loss= 0.00417
surrogate=-0.01737, entropy= 0.24865, loss=-0.01737
surrogate=-0.00783, entropy= 0.24919, loss=-0.00783
std_min= 0.21329, std_max= 0.30922, std_mean= 0.26604
val lr: [2.5102459016393435e-05], policy lr: [3.012295081967212e-05]
Policy Loss: -0.0078343, | Entropy Bonus: -0, | Value Loss: 4.6934, | Advantage Loss: 0.83926
Time elapsed (s): 1.657667875289917
Agent stdevs: 0.2660369
--------------------------------------------------------------------------------

Step 878
++++++++ Policy training ++++++++++
Current mean reward: 2109.406229 | mean episode length: 570.333333
val_loss= 3.93486
val_loss= 4.64333
val_loss= 3.89130
val_loss= 4.42535
val_loss= 4.68375
val_loss= 3.22332
val_loss= 4.03278
val_loss= 6.13650
val_loss= 4.58679
val_loss= 3.68487
adv_loss= 0.49394
adv_loss= 0.57714
adv_loss= 0.71464
adv_loss= 0.64612
adv_loss= 1.14696
adv_loss= 0.74522
adv_loss= 0.88431
adv_loss= 0.85760
adv_loss= 0.81743
adv_loss= 0.91956
surrogate=-0.00047, entropy= 0.24910, loss=-0.00047
surrogate=-0.01746, entropy= 0.24884, loss=-0.01746
surrogate=-0.00920, entropy= 0.24817, loss=-0.00920
surrogate=-0.00649, entropy= 0.24782, loss=-0.00649
surrogate=-0.02131, entropy= 0.24738, loss=-0.02131
surrogate=-0.01575, entropy= 0.24707, loss=-0.01575
surrogate= 0.00335, entropy= 0.24649, loss= 0.00335
surrogate=-0.01737, entropy= 0.24589, loss=-0.01737
surrogate=-0.01794, entropy= 0.24547, loss=-0.01794
surrogate=-0.01323, entropy= 0.24510, loss=-0.01323
std_min= 0.21302, std_max= 0.30873, std_mean= 0.26567
val lr: [2.484631147540983e-05], policy lr: [2.9815573770491797e-05]
Policy Loss: -0.013234, | Entropy Bonus: -0, | Value Loss: 3.6849, | Advantage Loss: 0.91956
Time elapsed (s): 1.6409828662872314
Agent stdevs: 0.26566672
--------------------------------------------------------------------------------

Step 879
++++++++ Policy training ++++++++++
Current mean reward: 2359.616701 | mean episode length: 651.666667
val_loss=234.77248
val_loss=239.65852
val_loss=388.06363
val_loss=16.95315
val_loss=1019.52692
val_loss=654.61121
val_loss=17.47126
val_loss=730.43024
val_loss=96.47844
val_loss=17.25820
adv_loss= 1.62003
adv_loss= 0.47475
adv_loss= 0.63231
adv_loss= 0.70388
adv_loss= 0.86817
adv_loss= 0.63542
adv_loss= 1.64457
adv_loss= 0.67688
adv_loss= 0.74614
adv_loss= 1.02350
surrogate= 0.00384, entropy= 0.24468, loss= 0.00384
surrogate=-0.00539, entropy= 0.24395, loss=-0.00539
surrogate= 0.00142, entropy= 0.24365, loss= 0.00142
surrogate=-0.00566, entropy= 0.24379, loss=-0.00566
surrogate= 0.00418, entropy= 0.24350, loss= 0.00418
surrogate=-0.00362, entropy= 0.24341, loss=-0.00362
surrogate=-0.01480, entropy= 0.24315, loss=-0.01480
surrogate=-0.00149, entropy= 0.24288, loss=-0.00149
surrogate=-0.01760, entropy= 0.24283, loss=-0.01760
surrogate=-0.02521, entropy= 0.24257, loss=-0.02521
std_min= 0.21297, std_max= 0.30828, std_mean= 0.26542
val lr: [2.4590163934426228e-05], policy lr: [2.950819672131147e-05]
Policy Loss: -0.02521, | Entropy Bonus: -0, | Value Loss: 17.258, | Advantage Loss: 1.0235
Time elapsed (s): 1.653381586074829
Agent stdevs: 0.26542234
--------------------------------------------------------------------------------

Step 880
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1900.7
++++++++ Policy training ++++++++++
Current mean reward: 1374.409528 | mean episode length: 372.000000
val_loss=11.97017
val_loss= 8.05808
val_loss= 7.46988
val_loss= 4.25612
val_loss= 8.94712
val_loss= 6.36379
val_loss= 7.40769
val_loss=10.95879
val_loss= 4.42962
val_loss= 4.50337
adv_loss= 0.76668
adv_loss= 5.24892
adv_loss= 1.08910
adv_loss= 1.85968
adv_loss= 1.48127
adv_loss= 0.72567
adv_loss= 1.57084
adv_loss= 2.42033
adv_loss= 0.51815
adv_loss= 0.58068
surrogate=-0.00469, entropy= 0.24267, loss=-0.00469
surrogate= 0.00132, entropy= 0.24324, loss= 0.00132
surrogate= 0.01387, entropy= 0.24344, loss= 0.01387
surrogate= 0.00265, entropy= 0.24379, loss= 0.00265
surrogate=-0.01126, entropy= 0.24423, loss=-0.01126
surrogate=-0.01018, entropy= 0.24446, loss=-0.01018
surrogate=-0.01162, entropy= 0.24471, loss=-0.01162
surrogate=-0.02947, entropy= 0.24526, loss=-0.02947
surrogate=-0.03172, entropy= 0.24571, loss=-0.03172
surrogate=-0.02297, entropy= 0.24604, loss=-0.02297
std_min= 0.21292, std_max= 0.30906, std_mean= 0.26578
val lr: [2.4334016393442625e-05], policy lr: [2.920081967213115e-05]
Policy Loss: -0.022968, | Entropy Bonus: -0, | Value Loss: 4.5034, | Advantage Loss: 0.58068
Time elapsed (s): 1.6491611003875732
Agent stdevs: 0.2657773
--------------------------------------------------------------------------------

Step 881
++++++++ Policy training ++++++++++
Current mean reward: 1352.817290 | mean episode length: 366.600000
val_loss= 9.10593
val_loss= 5.42282
val_loss= 5.82471
val_loss= 7.57801
val_loss= 5.12749
val_loss= 4.22600
val_loss=16.70118
val_loss=11.61127
val_loss= 4.55261
val_loss= 8.30833
adv_loss= 0.72319
adv_loss= 2.08653
adv_loss= 0.91151
adv_loss= 0.65335
adv_loss= 0.94444
adv_loss= 7.15328
adv_loss= 2.01288
adv_loss= 0.70712
adv_loss= 2.01113
adv_loss= 0.44557
surrogate=-0.01131, entropy= 0.24575, loss=-0.01131
surrogate= 0.00963, entropy= 0.24567, loss= 0.00963
surrogate=-0.00431, entropy= 0.24533, loss=-0.00431
surrogate=-0.01041, entropy= 0.24485, loss=-0.01041
surrogate=-0.00827, entropy= 0.24465, loss=-0.00827
surrogate=-0.04304, entropy= 0.24467, loss=-0.04304
surrogate= 0.00389, entropy= 0.24407, loss= 0.00389
surrogate=-0.00694, entropy= 0.24392, loss=-0.00694
surrogate=-0.00141, entropy= 0.24372, loss=-0.00141
surrogate=-0.01709, entropy= 0.24360, loss=-0.01709
std_min= 0.21273, std_max= 0.30892, std_mean= 0.26557
val lr: [2.407786885245902e-05], policy lr: [2.8893442622950823e-05]
Policy Loss: -0.01709, | Entropy Bonus: -0, | Value Loss: 8.3083, | Advantage Loss: 0.44557
Time elapsed (s): 1.6574211120605469
Agent stdevs: 0.26556507
--------------------------------------------------------------------------------

Step 882
++++++++ Policy training ++++++++++
Current mean reward: 2086.638048 | mean episode length: 564.000000
val_loss= 8.61472
val_loss= 6.68798
val_loss= 8.78054
val_loss= 8.61434
val_loss= 7.98272
val_loss= 5.21936
val_loss= 5.72175
val_loss= 7.33535
val_loss= 4.75526
val_loss= 5.41171
adv_loss= 1.05155
adv_loss= 1.60193
adv_loss= 0.43145
adv_loss= 1.56363
adv_loss= 0.49240
adv_loss= 0.71762
adv_loss= 2.71756
adv_loss= 0.88494
adv_loss= 1.39308
adv_loss= 0.71192
surrogate=-0.00567, entropy= 0.24350, loss=-0.00567
surrogate= 0.00762, entropy= 0.24295, loss= 0.00762
surrogate=-0.00866, entropy= 0.24254, loss=-0.00866
surrogate=-0.01270, entropy= 0.24190, loss=-0.01270
surrogate=-0.03245, entropy= 0.24161, loss=-0.03245
surrogate= 0.00949, entropy= 0.24119, loss= 0.00949
surrogate=-0.03228, entropy= 0.24084, loss=-0.03228
surrogate=-0.01816, entropy= 0.24042, loss=-0.01816
surrogate=-0.02534, entropy= 0.24019, loss=-0.02534
surrogate=-0.02848, entropy= 0.23963, loss=-0.02848
std_min= 0.21250, std_max= 0.30872, std_mean= 0.26522
val lr: [2.382172131147542e-05], policy lr: [2.85860655737705e-05]
Policy Loss: -0.028484, | Entropy Bonus: -0, | Value Loss: 5.4117, | Advantage Loss: 0.71192
Time elapsed (s): 1.6711785793304443
Agent stdevs: 0.26521596
--------------------------------------------------------------------------------

Step 883
++++++++ Policy training ++++++++++
Current mean reward: 1478.449634 | mean episode length: 401.600000
val_loss= 6.89082
val_loss= 7.47498
val_loss= 6.85033
val_loss= 5.47632
val_loss= 5.90982
val_loss= 7.71968
val_loss= 5.67489
val_loss= 6.53511
val_loss= 7.28428
val_loss= 6.00407
adv_loss= 0.82352
adv_loss= 3.00388
adv_loss= 3.81057
adv_loss= 1.43323
adv_loss= 1.71701
adv_loss= 1.62002
adv_loss= 0.77427
adv_loss= 0.69568
adv_loss= 2.74995
adv_loss= 0.71127
surrogate= 0.00866, entropy= 0.23959, loss= 0.00866
surrogate=-0.02160, entropy= 0.23970, loss=-0.02160
surrogate=-0.02715, entropy= 0.23944, loss=-0.02715
surrogate=-0.00445, entropy= 0.23913, loss=-0.00445
surrogate=-0.03820, entropy= 0.23898, loss=-0.03820
surrogate=-0.02729, entropy= 0.23849, loss=-0.02729
surrogate=-0.02425, entropy= 0.23846, loss=-0.02425
surrogate=-0.00077, entropy= 0.23846, loss=-0.00077
surrogate=-0.05384, entropy= 0.23839, loss=-0.05384
surrogate=-0.03820, entropy= 0.23850, loss=-0.03820
std_min= 0.21242, std_max= 0.30816, std_mean= 0.26510
val lr: [2.3565573770491817e-05], policy lr: [2.8278688524590176e-05]
Policy Loss: -0.038198, | Entropy Bonus: -0, | Value Loss: 6.0041, | Advantage Loss: 0.71127
Time elapsed (s): 1.661503553390503
Agent stdevs: 0.26510057
--------------------------------------------------------------------------------

Step 884
++++++++ Policy training ++++++++++
Current mean reward: 1776.597875 | mean episode length: 477.750000
val_loss= 6.72676
val_loss= 6.69118
val_loss= 4.16919
val_loss= 5.84094
val_loss= 4.36344
val_loss= 4.37254
val_loss= 6.59363
val_loss= 4.61030
val_loss= 5.81866
val_loss= 4.37469
adv_loss= 0.65842
adv_loss= 0.89705
adv_loss= 1.11594
adv_loss= 0.88199
adv_loss= 1.12389
adv_loss= 0.82780
adv_loss= 0.81751
adv_loss= 1.10137
adv_loss= 0.69756
adv_loss= 0.72361
surrogate=-0.00875, entropy= 0.23783, loss=-0.00875
surrogate=-0.02048, entropy= 0.23711, loss=-0.02048
surrogate= 0.02113, entropy= 0.23666, loss= 0.02113
surrogate=-0.01690, entropy= 0.23613, loss=-0.01690
surrogate=-0.02299, entropy= 0.23583, loss=-0.02299
surrogate=-0.02484, entropy= 0.23522, loss=-0.02484
surrogate=-0.02190, entropy= 0.23478, loss=-0.02190
surrogate= 0.00018, entropy= 0.23455, loss= 0.00018
surrogate=-0.01038, entropy= 0.23417, loss=-0.01038
surrogate=-0.02418, entropy= 0.23377, loss=-0.02418
std_min= 0.21182, std_max= 0.30807, std_mean= 0.26472
val lr: [2.3309426229508183e-05], policy lr: [2.797131147540982e-05]
Policy Loss: -0.024184, | Entropy Bonus: -0, | Value Loss: 4.3747, | Advantage Loss: 0.72361
Time elapsed (s): 1.6602635383605957
Agent stdevs: 0.2647227
--------------------------------------------------------------------------------

Step 885
++++++++ Policy training ++++++++++
Current mean reward: 2055.861476 | mean episode length: 567.666667
val_loss= 7.85671
val_loss=134.25490
val_loss=44.31437
val_loss=365.28137
val_loss=701.44122
val_loss=1536.82483
val_loss=25.19754
val_loss=336.13412
val_loss=824.44611
val_loss=978.37219
adv_loss= 0.80429
adv_loss= 1.08439
adv_loss= 1.10977
adv_loss= 1.97202
adv_loss= 1.32582
adv_loss= 1.62709
adv_loss= 1.01515
adv_loss= 1.19153
adv_loss= 0.98461
adv_loss= 0.73161
surrogate=-0.00546, entropy= 0.23375, loss=-0.00546
surrogate= 0.01750, entropy= 0.23375, loss= 0.01750
surrogate=-0.00549, entropy= 0.23365, loss=-0.00549
surrogate=-0.00137, entropy= 0.23378, loss=-0.00137
surrogate= 0.00014, entropy= 0.23395, loss= 0.00014
surrogate=-0.01149, entropy= 0.23393, loss=-0.01149
surrogate=-0.01258, entropy= 0.23350, loss=-0.01258
surrogate=-0.01104, entropy= 0.23340, loss=-0.01104
surrogate=-0.01600, entropy= 0.23317, loss=-0.01600
surrogate=-0.02109, entropy= 0.23296, loss=-0.02109
std_min= 0.21193, std_max= 0.30759, std_mean= 0.26462
val lr: [2.305327868852458e-05], policy lr: [2.7663934426229494e-05]
Policy Loss: -0.021091, | Entropy Bonus: -0, | Value Loss: 978.37, | Advantage Loss: 0.73161
Time elapsed (s): 1.6671380996704102
Agent stdevs: 0.26462153
--------------------------------------------------------------------------------

Step 886
++++++++ Policy training ++++++++++
Current mean reward: 1526.517995 | mean episode length: 406.333333
val_loss=11.87571
val_loss= 8.80566
val_loss= 5.01633
val_loss= 5.65439
val_loss= 5.82840
val_loss= 7.36543
val_loss= 4.10009
val_loss= 4.47757
val_loss= 3.25180
val_loss= 4.02809
adv_loss= 0.59629
adv_loss= 0.86035
adv_loss= 2.61555
adv_loss= 0.85013
adv_loss= 1.01661
adv_loss= 1.41342
adv_loss= 0.62429
adv_loss= 1.54501
adv_loss= 1.17840
adv_loss= 0.93904
surrogate= 0.00588, entropy= 0.23254, loss= 0.00588
surrogate= 0.00020, entropy= 0.23220, loss= 0.00020
surrogate=-0.00009, entropy= 0.23156, loss=-0.00009
surrogate=-0.00976, entropy= 0.23104, loss=-0.00976
surrogate=-0.01006, entropy= 0.23065, loss=-0.01006
surrogate=-0.02211, entropy= 0.23050, loss=-0.02211
surrogate=-0.03005, entropy= 0.23001, loss=-0.03005
surrogate=-0.01689, entropy= 0.22977, loss=-0.01689
surrogate=-0.02715, entropy= 0.22932, loss=-0.02715
surrogate=-0.01718, entropy= 0.22916, loss=-0.01718
std_min= 0.21171, std_max= 0.30726, std_mean= 0.26428
val lr: [2.279713114754098e-05], policy lr: [2.7356557377049172e-05]
Policy Loss: -0.017178, | Entropy Bonus: -0, | Value Loss: 4.0281, | Advantage Loss: 0.93904
Time elapsed (s): 1.6403372287750244
Agent stdevs: 0.2642833
--------------------------------------------------------------------------------

Step 887
++++++++ Policy training ++++++++++
Current mean reward: 1976.809145 | mean episode length: 536.000000
val_loss= 6.62804
val_loss= 5.76244
val_loss= 6.97094
val_loss= 8.02276
val_loss= 6.30083
val_loss= 4.69396
val_loss= 5.00677
val_loss= 6.05841
val_loss= 5.98422
val_loss= 8.20767
adv_loss= 0.81390
adv_loss= 0.79021
adv_loss= 1.69696
adv_loss= 2.39428
adv_loss= 1.13293
adv_loss= 0.97382
adv_loss= 0.59139
adv_loss= 0.86802
adv_loss= 0.51355
adv_loss= 1.15863
surrogate= 0.00352, entropy= 0.22893, loss= 0.00352
surrogate=-0.00952, entropy= 0.22887, loss=-0.00952
surrogate=-0.01054, entropy= 0.22877, loss=-0.01054
surrogate=-0.01352, entropy= 0.22866, loss=-0.01352
surrogate=-0.02293, entropy= 0.22865, loss=-0.02293
surrogate= 0.00438, entropy= 0.22875, loss= 0.00438
surrogate= 0.00013, entropy= 0.22894, loss= 0.00013
surrogate=-0.00880, entropy= 0.22899, loss=-0.00880
surrogate= 0.00321, entropy= 0.22909, loss= 0.00321
surrogate=-0.00941, entropy= 0.22927, loss=-0.00941
std_min= 0.21148, std_max= 0.30728, std_mean= 0.26432
val lr: [2.2540983606557376e-05], policy lr: [2.7049180327868846e-05]
Policy Loss: -0.0094064, | Entropy Bonus: -0, | Value Loss: 8.2077, | Advantage Loss: 1.1586
Time elapsed (s): 1.6771283149719238
Agent stdevs: 0.26431772
--------------------------------------------------------------------------------

Step 888
++++++++ Policy training ++++++++++
Current mean reward: 3580.761988 | mean episode length: 1000.000000
val_loss=803.57977
val_loss=22.62022
val_loss=394.85861
val_loss=45.86524
val_loss=54.28701
val_loss=68.99572
val_loss=56.23515
val_loss=108.03212
val_loss=26.56685
val_loss=41.09067
adv_loss=1380.70825
adv_loss= 1.88859
adv_loss= 1.16999
adv_loss= 2.99942
adv_loss= 3.61082
adv_loss= 1.02530
adv_loss= 1.18469
adv_loss= 1.11224
adv_loss= 0.94967
adv_loss= 2.08007
surrogate= 0.00005, entropy= 0.22901, loss= 0.00005
surrogate=-0.00702, entropy= 0.22906, loss=-0.00702
surrogate=-0.01308, entropy= 0.22885, loss=-0.01308
surrogate= 0.00901, entropy= 0.22873, loss= 0.00901
surrogate=-0.01221, entropy= 0.22875, loss=-0.01221
surrogate=-0.01689, entropy= 0.22846, loss=-0.01689
surrogate=-0.01123, entropy= 0.22816, loss=-0.01123
surrogate=-0.01529, entropy= 0.22774, loss=-0.01529
surrogate= 0.00864, entropy= 0.22759, loss= 0.00864
surrogate=-0.00517, entropy= 0.22750, loss=-0.00517
std_min= 0.21133, std_max= 0.30720, std_mean= 0.26417
val lr: [2.2284836065573773e-05], policy lr: [2.6741803278688524e-05]
Policy Loss: -0.0051738, | Entropy Bonus: -0, | Value Loss: 41.091, | Advantage Loss: 2.0801
Time elapsed (s): 1.681873083114624
Agent stdevs: 0.2641679
--------------------------------------------------------------------------------

Step 889
++++++++ Policy training ++++++++++
Current mean reward: 2041.999458 | mean episode length: 553.000000
val_loss=14.26423
val_loss=20.82152
val_loss=21.53207
val_loss= 7.56569
val_loss= 9.94438
val_loss=13.32512
val_loss= 9.40698
val_loss= 5.38066
val_loss=10.14819
val_loss= 6.91604
adv_loss= 1.02200
adv_loss= 1.10794
adv_loss= 1.75672
adv_loss= 0.54767
adv_loss= 0.65024
adv_loss= 0.91951
adv_loss= 0.85465
adv_loss= 0.63474
adv_loss= 0.75564
adv_loss= 0.92096
surrogate=-0.00570, entropy= 0.22743, loss=-0.00570
surrogate= 0.00448, entropy= 0.22703, loss= 0.00448
surrogate=-0.01023, entropy= 0.22641, loss=-0.01023
surrogate=-0.03532, entropy= 0.22606, loss=-0.03532
surrogate= 0.01212, entropy= 0.22589, loss= 0.01212
surrogate=-0.03104, entropy= 0.22543, loss=-0.03104
surrogate=-0.02514, entropy= 0.22521, loss=-0.02514
surrogate=-0.04836, entropy= 0.22500, loss=-0.04836
surrogate=-0.01431, entropy= 0.22468, loss=-0.01431
surrogate=-0.01716, entropy= 0.22437, loss=-0.01716
std_min= 0.21121, std_max= 0.30680, std_mean= 0.26388
val lr: [2.202868852459017e-05], policy lr: [2.64344262295082e-05]
Policy Loss: -0.017163, | Entropy Bonus: -0, | Value Loss: 6.916, | Advantage Loss: 0.92096
Time elapsed (s): 1.6440696716308594
Agent stdevs: 0.2638795
--------------------------------------------------------------------------------

Step 890
++++++++ Policy training ++++++++++
Current mean reward: 2050.008904 | mean episode length: 550.666667
val_loss=11.29376
val_loss=12.92353
val_loss= 7.81394
val_loss= 8.80080
val_loss= 5.56803
val_loss= 6.59237
val_loss=11.76447
val_loss= 7.23455
val_loss= 9.24740
val_loss= 9.74494
adv_loss= 1.14341
adv_loss= 0.84272
adv_loss= 0.60828
adv_loss= 1.48649
adv_loss= 1.40058
adv_loss= 1.10529
adv_loss= 2.44208
adv_loss= 0.63449
adv_loss= 0.47180
adv_loss= 0.81082
surrogate= 0.01884, entropy= 0.22426, loss= 0.01884
surrogate= 0.01445, entropy= 0.22416, loss= 0.01445
surrogate= 0.00320, entropy= 0.22389, loss= 0.00320
surrogate= 0.02269, entropy= 0.22404, loss= 0.02269
surrogate=-0.02083, entropy= 0.22402, loss=-0.02083
surrogate=-0.01542, entropy= 0.22377, loss=-0.01542
surrogate=-0.00831, entropy= 0.22357, loss=-0.00831
surrogate=-0.03130, entropy= 0.22332, loss=-0.03130
surrogate=-0.01910, entropy= 0.22312, loss=-0.01910
surrogate=-0.00532, entropy= 0.22302, loss=-0.00532
std_min= 0.21093, std_max= 0.30657, std_mean= 0.26378
val lr: [2.1772540983606566e-05], policy lr: [2.6127049180327876e-05]
Policy Loss: -0.0053204, | Entropy Bonus: -0, | Value Loss: 9.7449, | Advantage Loss: 0.81082
Time elapsed (s): 1.6462457180023193
Agent stdevs: 0.26377502
--------------------------------------------------------------------------------

Step 891
++++++++ Policy training ++++++++++
Current mean reward: 2207.099449 | mean episode length: 597.000000
val_loss=14.72815
val_loss= 7.15203
val_loss= 6.31768
val_loss= 6.36388
val_loss= 5.06200
val_loss= 6.15161
val_loss= 6.56080
val_loss= 6.39098
val_loss= 5.96747
val_loss= 7.44036
adv_loss= 8.22272
adv_loss= 1.19079
adv_loss= 2.32892
adv_loss= 1.26093
adv_loss= 0.69132
adv_loss= 0.60008
adv_loss= 1.90372
adv_loss= 1.18619
adv_loss= 0.66814
adv_loss= 0.92439
surrogate= 0.01064, entropy= 0.22265, loss= 0.01064
surrogate=-0.00529, entropy= 0.22215, loss=-0.00529
surrogate=-0.01078, entropy= 0.22188, loss=-0.01078
surrogate=-0.00763, entropy= 0.22169, loss=-0.00763
surrogate=-0.02646, entropy= 0.22136, loss=-0.02646
surrogate=-0.02809, entropy= 0.22119, loss=-0.02809
surrogate=-0.02038, entropy= 0.22113, loss=-0.02038
surrogate=-0.00743, entropy= 0.22088, loss=-0.00743
surrogate=-0.00960, entropy= 0.22048, loss=-0.00960
surrogate=-0.01796, entropy= 0.22017, loss=-0.01796
std_min= 0.21050, std_max= 0.30644, std_mean= 0.26355
val lr: [2.1516393442622962e-05], policy lr: [2.581967213114755e-05]
Policy Loss: -0.017957, | Entropy Bonus: -0, | Value Loss: 7.4404, | Advantage Loss: 0.92439
Time elapsed (s): 1.6409790515899658
Agent stdevs: 0.26355258
--------------------------------------------------------------------------------

Step 892
++++++++ Policy training ++++++++++
Current mean reward: 1745.507083 | mean episode length: 470.000000
val_loss= 8.36680
val_loss= 9.85830
val_loss= 7.13996
val_loss= 7.73355
val_loss= 7.55282
val_loss= 3.54517
val_loss= 4.22047
val_loss= 7.07136
val_loss= 9.69132
val_loss= 6.54182
adv_loss= 0.83949
adv_loss= 1.45097
adv_loss= 1.06286
adv_loss= 4.80509
adv_loss= 1.17684
adv_loss= 2.68875
adv_loss= 1.25459
adv_loss= 6.07807
adv_loss= 1.78292
adv_loss= 2.62988
surrogate= 0.00008, entropy= 0.21991, loss= 0.00008
surrogate= 0.00784, entropy= 0.21966, loss= 0.00784
surrogate= 0.00400, entropy= 0.21936, loss= 0.00400
surrogate=-0.00696, entropy= 0.21890, loss=-0.00696
surrogate=-0.01314, entropy= 0.21870, loss=-0.01314
surrogate=-0.02945, entropy= 0.21853, loss=-0.02945
surrogate=-0.01415, entropy= 0.21829, loss=-0.01415
surrogate=-0.02696, entropy= 0.21789, loss=-0.02696
surrogate=-0.01491, entropy= 0.21767, loss=-0.01491
surrogate= 0.01383, entropy= 0.21745, loss= 0.01383
std_min= 0.21023, std_max= 0.30640, std_mean= 0.26333
val lr: [2.126024590163936e-05], policy lr: [2.5512295081967225e-05]
Policy Loss: 0.013834, | Entropy Bonus: -0, | Value Loss: 6.5418, | Advantage Loss: 2.6299
Time elapsed (s): 1.6406071186065674
Agent stdevs: 0.2633297
--------------------------------------------------------------------------------

Step 893
++++++++ Policy training ++++++++++
Current mean reward: 2831.487050 | mean episode length: 788.000000
val_loss=56.15509
val_loss=156.70067
val_loss=52.68430
val_loss=648.66290
val_loss=809.58429
val_loss=617.35931
val_loss=1689.63538
val_loss=1042.80237
val_loss=20.28439
val_loss=901.68524
adv_loss=1843.53687
adv_loss= 1.11100
adv_loss= 1.34092
adv_loss= 3.64834
adv_loss= 0.61397
adv_loss= 1.39444
adv_loss= 0.85195
adv_loss= 0.63429
adv_loss= 0.62453
adv_loss= 0.94235
surrogate= 0.00044, entropy= 0.21727, loss= 0.00044
surrogate=-0.00917, entropy= 0.21730, loss=-0.00917
surrogate= 0.00357, entropy= 0.21751, loss= 0.00357
surrogate=-0.02103, entropy= 0.21747, loss=-0.02103
surrogate=-0.00008, entropy= 0.21762, loss=-0.00008
surrogate=-0.01522, entropy= 0.21786, loss=-0.01522
surrogate= 0.00063, entropy= 0.21791, loss= 0.00063
surrogate=-0.02644, entropy= 0.21808, loss=-0.02644
surrogate=-0.01689, entropy= 0.21813, loss=-0.01689
surrogate=-0.02860, entropy= 0.21821, loss=-0.02860
std_min= 0.21038, std_max= 0.30615, std_mean= 0.26338
val lr: [2.1004098360655728e-05], policy lr: [2.520491803278687e-05]
Policy Loss: -0.028601, | Entropy Bonus: -0, | Value Loss: 901.69, | Advantage Loss: 0.94235
Time elapsed (s): 1.6672708988189697
Agent stdevs: 0.26337686
--------------------------------------------------------------------------------

Step 894
++++++++ Policy training ++++++++++
Current mean reward: 1770.278669 | mean episode length: 477.500000
val_loss=12.83018
val_loss= 5.97788
val_loss= 8.02210
val_loss= 9.14147
val_loss= 9.25964
val_loss= 7.77556
val_loss= 6.91849
val_loss= 9.02976
val_loss= 5.60598
val_loss= 4.05850
adv_loss= 1.23708
adv_loss= 1.87102
adv_loss= 0.56641
adv_loss= 2.91375
adv_loss= 5.88247
adv_loss= 1.49098
adv_loss= 0.69607
adv_loss= 0.63260
adv_loss= 0.98612
adv_loss= 2.20518
surrogate= 0.00046, entropy= 0.21793, loss= 0.00046
surrogate= 0.00412, entropy= 0.21759, loss= 0.00412
surrogate=-0.01697, entropy= 0.21706, loss=-0.01697
surrogate= 0.01307, entropy= 0.21679, loss= 0.01307
surrogate=-0.01366, entropy= 0.21646, loss=-0.01366
surrogate=-0.02503, entropy= 0.21604, loss=-0.02503
surrogate=-0.02814, entropy= 0.21582, loss=-0.02814
surrogate=-0.03221, entropy= 0.21555, loss=-0.03221
surrogate=-0.01429, entropy= 0.21507, loss=-0.01429
surrogate=-0.00372, entropy= 0.21464, loss=-0.00372
std_min= 0.20999, std_max= 0.30614, std_mean= 0.26309
val lr: [2.0747950819672125e-05], policy lr: [2.4897540983606547e-05]
Policy Loss: -0.0037194, | Entropy Bonus: -0, | Value Loss: 4.0585, | Advantage Loss: 2.2052
Time elapsed (s): 1.6601920127868652
Agent stdevs: 0.26308995
--------------------------------------------------------------------------------

Step 895
++++++++ Policy training ++++++++++
Current mean reward: 3132.499253 | mean episode length: 848.000000
val_loss= 4.78915
val_loss=10.77972
val_loss= 3.24751
val_loss= 4.47274
val_loss= 8.06578
val_loss= 3.41223
val_loss= 7.61619
val_loss= 5.01752
val_loss= 5.53497
val_loss= 5.48398
adv_loss= 1.43923
adv_loss= 0.76603
adv_loss= 1.08006
adv_loss= 0.57756
adv_loss= 1.32028
adv_loss= 1.48805
adv_loss= 1.35023
adv_loss= 0.68061
adv_loss= 0.70391
adv_loss= 1.33544
surrogate=-0.00655, entropy= 0.21449, loss=-0.00655
surrogate=-0.00356, entropy= 0.21415, loss=-0.00356
surrogate= 0.02046, entropy= 0.21372, loss= 0.02046
surrogate=-0.00400, entropy= 0.21347, loss=-0.00400
surrogate=-0.03438, entropy= 0.21342, loss=-0.03438
surrogate=-0.01979, entropy= 0.21316, loss=-0.01979
surrogate=-0.02187, entropy= 0.21306, loss=-0.02187
surrogate=-0.03015, entropy= 0.21268, loss=-0.03015
surrogate=-0.03424, entropy= 0.21229, loss=-0.03424
surrogate=-0.01458, entropy= 0.21206, loss=-0.01458
std_min= 0.21002, std_max= 0.30540, std_mean= 0.26283
val lr: [2.049180327868852e-05], policy lr: [2.459016393442622e-05]
Policy Loss: -0.014579, | Entropy Bonus: -0, | Value Loss: 5.484, | Advantage Loss: 1.3354
Time elapsed (s): 1.6679191589355469
Agent stdevs: 0.26282525
--------------------------------------------------------------------------------

Step 896
++++++++ Policy training ++++++++++
Current mean reward: 2405.988931 | mean episode length: 671.500000
val_loss=287.59732
val_loss=21.58358
val_loss=1391.73547
val_loss=120.77581
val_loss=736.72803
val_loss=92.62191
val_loss=120.76248
val_loss=1404.04407
val_loss=231.07983
val_loss=134.97272
adv_loss= 2.48063
adv_loss= 0.88274
adv_loss= 0.57374
adv_loss= 1.16783
adv_loss= 7.75457
adv_loss= 1.22911
adv_loss= 0.68688
adv_loss= 2.02714
adv_loss= 3.28745
adv_loss= 1.33942
surrogate=-0.00889, entropy= 0.21193, loss=-0.00889
surrogate= 0.02424, entropy= 0.21175, loss= 0.02424
surrogate= 0.00757, entropy= 0.21138, loss= 0.00757
surrogate= 0.00004, entropy= 0.21106, loss= 0.00004
surrogate=-0.01008, entropy= 0.21090, loss=-0.01008
surrogate=-0.01585, entropy= 0.21067, loss=-0.01585
surrogate=-0.01051, entropy= 0.21054, loss=-0.01051
surrogate= 0.00256, entropy= 0.21043, loss= 0.00256
surrogate=-0.01470, entropy= 0.21040, loss=-0.01470
surrogate= 0.00889, entropy= 0.21026, loss= 0.00889
std_min= 0.20977, std_max= 0.30523, std_mean= 0.26268
val lr: [2.0235655737704918e-05], policy lr: [2.4282786885245896e-05]
Policy Loss: 0.008892, | Entropy Bonus: -0, | Value Loss: 134.97, | Advantage Loss: 1.3394
Time elapsed (s): 1.653547763824463
Agent stdevs: 0.26268163
--------------------------------------------------------------------------------

Step 897
++++++++ Policy training ++++++++++
Current mean reward: 3500.080107 | mean episode length: 1000.000000
val_loss=572.95154
val_loss=1331.70178
val_loss=1971.17017
val_loss=29.64221
val_loss=23.92255
val_loss=125.07364
val_loss=467.06778
val_loss=238.73401
val_loss=280.40848
val_loss=2867.77393
adv_loss=1870.24744
adv_loss= 1.74818
adv_loss= 2.03026
adv_loss= 1.75224
adv_loss= 0.68223
adv_loss= 0.41646
adv_loss= 1.41446
adv_loss= 1.73712
adv_loss= 1.41092
adv_loss= 1.70074
surrogate= 0.01700, entropy= 0.21024, loss= 0.01700
surrogate=-0.01442, entropy= 0.21025, loss=-0.01442
surrogate= 0.01179, entropy= 0.21020, loss= 0.01179
surrogate=-0.00709, entropy= 0.21034, loss=-0.00709
surrogate=-0.02349, entropy= 0.21030, loss=-0.02349
surrogate= 0.06193, entropy= 0.21040, loss= 0.06193
surrogate=-0.01639, entropy= 0.21033, loss=-0.01639
surrogate=-0.01391, entropy= 0.21024, loss=-0.01391
surrogate= 0.00581, entropy= 0.21023, loss= 0.00581
surrogate=-0.00270, entropy= 0.21028, loss=-0.00270
std_min= 0.20957, std_max= 0.30533, std_mean= 0.26271
val lr: [1.9979508196721314e-05], policy lr: [2.3975409836065574e-05]
Policy Loss: -0.0027032, | Entropy Bonus: -0, | Value Loss: 2867.8, | Advantage Loss: 1.7007
Time elapsed (s): 1.6446709632873535
Agent stdevs: 0.26270738
--------------------------------------------------------------------------------

Step 898
++++++++ Policy training ++++++++++
Current mean reward: 3212.819714 | mean episode length: 887.500000
val_loss=40.20543
val_loss=36.75419
val_loss=237.06238
val_loss=96.35062
val_loss=181.50551
val_loss=672.08948
val_loss=35.72228
val_loss=116.24466
val_loss=30.85127
val_loss=62.33612
adv_loss= 2.22708
adv_loss=725.68701
adv_loss= 1.74195
adv_loss= 1.70459
adv_loss= 4.13312
adv_loss= 1.54258
adv_loss= 1.49868
adv_loss= 2.17423
adv_loss= 1.45535
adv_loss= 1.20357
surrogate= 0.02482, entropy= 0.21020, loss= 0.02482
surrogate= 0.00868, entropy= 0.21017, loss= 0.00868
surrogate= 0.01674, entropy= 0.21008, loss= 0.01674
surrogate= 0.00791, entropy= 0.20996, loss= 0.00791
surrogate= 0.01943, entropy= 0.20984, loss= 0.01943
surrogate=-0.00973, entropy= 0.20987, loss=-0.00973
surrogate=-0.01841, entropy= 0.20982, loss=-0.01841
surrogate=-0.02275, entropy= 0.20987, loss=-0.02275
surrogate= 0.00742, entropy= 0.20972, loss= 0.00742
surrogate=-0.00562, entropy= 0.20967, loss=-0.00562
std_min= 0.20947, std_max= 0.30551, std_mean= 0.26267
val lr: [1.972336065573771e-05], policy lr: [2.3668032786885248e-05]
Policy Loss: -0.0056158, | Entropy Bonus: -0, | Value Loss: 62.336, | Advantage Loss: 1.2036
Time elapsed (s): 1.648489236831665
Agent stdevs: 0.2626677
--------------------------------------------------------------------------------

Step 899
++++++++ Policy training ++++++++++
Current mean reward: 2271.156163 | mean episode length: 618.333333
val_loss=11.82996
val_loss=10.01415
val_loss= 7.88167
val_loss= 5.69491
val_loss= 7.48347
val_loss= 7.12653
val_loss= 7.53252
val_loss= 4.45153
val_loss= 7.37683
val_loss= 4.49785
adv_loss= 1.78070
adv_loss= 1.27653
adv_loss= 2.50180
adv_loss= 1.52391
adv_loss= 0.69923
adv_loss= 1.37576
adv_loss= 1.31361
adv_loss= 0.59521
adv_loss= 1.52220
adv_loss= 0.64688
surrogate=-0.00866, entropy= 0.20983, loss=-0.00866
surrogate=-0.01440, entropy= 0.20958, loss=-0.01440
surrogate=-0.03016, entropy= 0.20934, loss=-0.03016
surrogate=-0.02112, entropy= 0.20878, loss=-0.02112
surrogate=-0.01068, entropy= 0.20871, loss=-0.01068
surrogate=-0.00209, entropy= 0.20827, loss=-0.00209
surrogate=-0.00336, entropy= 0.20786, loss=-0.00336
surrogate=-0.00812, entropy= 0.20760, loss=-0.00812
surrogate=-0.01824, entropy= 0.20736, loss=-0.01824
surrogate=-0.00946, entropy= 0.20706, loss=-0.00946
std_min= 0.20935, std_max= 0.30540, std_mean= 0.26244
val lr: [1.9467213114754107e-05], policy lr: [2.3360655737704926e-05]
Policy Loss: -0.0094554, | Entropy Bonus: -0, | Value Loss: 4.4978, | Advantage Loss: 0.64688
Time elapsed (s): 1.6512126922607422
Agent stdevs: 0.2624383
--------------------------------------------------------------------------------

Step 900
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2904.5
++++++++ Policy training ++++++++++
Current mean reward: 2081.130227 | mean episode length: 578.666667
val_loss=103.62305
val_loss=15.28151
val_loss=30.29637
val_loss=112.38815
val_loss=825.81775
val_loss=13.79003
val_loss=11.18470
val_loss=25.86035
val_loss=10.35725
val_loss=691.19220
adv_loss= 1.25768
adv_loss= 1.04149
adv_loss= 1.26266
adv_loss= 0.60866
adv_loss= 1.75015
adv_loss= 0.74825
adv_loss= 0.94092
adv_loss= 0.65487
adv_loss= 1.25448
adv_loss= 2.33347
surrogate= 0.00465, entropy= 0.20678, loss= 0.00465
surrogate=-0.00661, entropy= 0.20676, loss=-0.00661
surrogate= 0.02317, entropy= 0.20669, loss= 0.02317
surrogate=-0.01116, entropy= 0.20678, loss=-0.01116
surrogate=-0.00018, entropy= 0.20666, loss=-0.00018
surrogate= 0.00986, entropy= 0.20660, loss= 0.00986
surrogate= 0.02494, entropy= 0.20664, loss= 0.02494
surrogate=-0.02405, entropy= 0.20662, loss=-0.02405
surrogate=-0.02320, entropy= 0.20661, loss=-0.02320
surrogate=-0.01255, entropy= 0.20641, loss=-0.01255
std_min= 0.20936, std_max= 0.30479, std_mean= 0.26236
val lr: [1.9211065573770504e-05], policy lr: [2.30532786885246e-05]
Policy Loss: -0.012545, | Entropy Bonus: -0, | Value Loss: 691.19, | Advantage Loss: 2.3335
Time elapsed (s): 1.6772687435150146
Agent stdevs: 0.26235667
--------------------------------------------------------------------------------

Step 901
++++++++ Policy training ++++++++++
Current mean reward: 2445.868707 | mean episode length: 672.333333
val_loss=18.28893
val_loss=164.17845
val_loss=1662.08472
val_loss=68.43611
val_loss=90.32359
val_loss=17.31765
val_loss= 8.40221
val_loss=22.77595
val_loss=49.79504
val_loss=26.48378
adv_loss= 1.43174
adv_loss= 0.79663
adv_loss= 1.62574
adv_loss= 1.24007
adv_loss= 0.86702
adv_loss= 1.13751
adv_loss= 0.86013
adv_loss= 1.11679
adv_loss=1841.65735
adv_loss= 2.68441
surrogate= 0.00203, entropy= 0.20600, loss= 0.00203
surrogate= 0.01453, entropy= 0.20581, loss= 0.01453
surrogate= 0.00653, entropy= 0.20546, loss= 0.00653
surrogate=-0.03275, entropy= 0.20514, loss=-0.03275
surrogate= 0.01813, entropy= 0.20473, loss= 0.01813
surrogate=-0.02263, entropy= 0.20424, loss=-0.02263
surrogate=-0.00142, entropy= 0.20404, loss=-0.00142
surrogate=-0.02443, entropy= 0.20379, loss=-0.02443
surrogate= 0.01218, entropy= 0.20334, loss= 0.01218
surrogate=-0.01061, entropy= 0.20319, loss=-0.01061
std_min= 0.20919, std_max= 0.30430, std_mean= 0.26206
val lr: [1.8954918032786873e-05], policy lr: [2.2745901639344244e-05]
Policy Loss: -0.010607, | Entropy Bonus: -0, | Value Loss: 26.484, | Advantage Loss: 2.6844
Time elapsed (s): 1.6481273174285889
Agent stdevs: 0.2620641
--------------------------------------------------------------------------------

Step 902
++++++++ Policy training ++++++++++
Current mean reward: 2365.798249 | mean episode length: 635.333333
val_loss=10.96538
val_loss=16.39148
val_loss= 9.46749
val_loss= 9.25264
val_loss= 9.45600
val_loss= 5.83166
val_loss= 5.19297
val_loss= 7.77154
val_loss= 6.40908
val_loss= 5.72509
adv_loss= 1.02260
adv_loss= 1.30986
adv_loss= 0.73234
adv_loss= 0.79445
adv_loss= 1.39837
adv_loss= 1.22112
adv_loss= 1.94238
adv_loss= 0.91225
adv_loss= 1.06806
adv_loss= 0.91271
surrogate= 0.00071, entropy= 0.20322, loss= 0.00071
surrogate=-0.00679, entropy= 0.20310, loss=-0.00679
surrogate=-0.03985, entropy= 0.20301, loss=-0.03985
surrogate=-0.00844, entropy= 0.20300, loss=-0.00844
surrogate=-0.00770, entropy= 0.20277, loss=-0.00770
surrogate=-0.02005, entropy= 0.20287, loss=-0.02005
surrogate= 0.00427, entropy= 0.20295, loss= 0.00427
surrogate= 0.00163, entropy= 0.20277, loss= 0.00163
surrogate=-0.01670, entropy= 0.20279, loss=-0.01670
surrogate=-0.01169, entropy= 0.20267, loss=-0.01169
std_min= 0.20907, std_max= 0.30456, std_mean= 0.26204
val lr: [1.869877049180327e-05], policy lr: [2.243852459016392e-05]
Policy Loss: -0.011691, | Entropy Bonus: -0, | Value Loss: 5.7251, | Advantage Loss: 0.91271
Time elapsed (s): 1.683784008026123
Agent stdevs: 0.2620391
--------------------------------------------------------------------------------

Step 903
++++++++ Policy training ++++++++++
Current mean reward: 2468.516208 | mean episode length: 670.000000
val_loss=12.58398
val_loss= 9.76977
val_loss= 9.30240
val_loss=10.73723
val_loss= 8.40448
val_loss= 7.10771
val_loss=13.66833
val_loss= 7.46076
val_loss=11.22682
val_loss= 6.53068
adv_loss= 2.59466
adv_loss= 1.05838
adv_loss= 0.58404
adv_loss= 1.62821
adv_loss= 0.87405
adv_loss= 2.44381
adv_loss= 1.29464
adv_loss= 1.49369
adv_loss= 0.88474
adv_loss= 0.72864
surrogate= 0.00043, entropy= 0.20286, loss= 0.00043
surrogate= 0.00616, entropy= 0.20290, loss= 0.00616
surrogate=-0.01123, entropy= 0.20273, loss=-0.01123
surrogate=-0.00811, entropy= 0.20275, loss=-0.00811
surrogate=-0.01036, entropy= 0.20274, loss=-0.01036
surrogate=-0.01075, entropy= 0.20252, loss=-0.01075
surrogate=-0.01864, entropy= 0.20272, loss=-0.01864
surrogate=-0.03238, entropy= 0.20272, loss=-0.03238
surrogate=-0.01114, entropy= 0.20253, loss=-0.01114
surrogate=-0.00322, entropy= 0.20255, loss=-0.00322
std_min= 0.20910, std_max= 0.30438, std_mean= 0.26202
val lr: [1.8442622950819666e-05], policy lr: [2.2131147540983597e-05]
Policy Loss: -0.0032224, | Entropy Bonus: -0, | Value Loss: 6.5307, | Advantage Loss: 0.72864
Time elapsed (s): 1.6510982513427734
Agent stdevs: 0.2620178
--------------------------------------------------------------------------------

Step 904
++++++++ Policy training ++++++++++
Current mean reward: 2221.053709 | mean episode length: 600.666667
val_loss= 9.99996
val_loss= 5.56204
val_loss= 6.56242
val_loss= 6.00366
val_loss= 7.66895
val_loss= 6.01276
val_loss= 4.85781
val_loss= 6.56328
val_loss= 4.13886
val_loss= 3.77096
adv_loss= 1.13594
adv_loss= 0.98022
adv_loss= 1.28314
adv_loss= 0.97281
adv_loss= 0.87270
adv_loss= 0.38105
adv_loss= 1.08186
adv_loss= 1.02018
adv_loss= 0.68338
adv_loss= 0.65476
surrogate= 0.02260, entropy= 0.20254, loss= 0.02260
surrogate=-0.00306, entropy= 0.20238, loss=-0.00306
surrogate= 0.00403, entropy= 0.20215, loss= 0.00403
surrogate= 0.00465, entropy= 0.20217, loss= 0.00465
surrogate=-0.01244, entropy= 0.20184, loss=-0.01244
surrogate=-0.01426, entropy= 0.20167, loss=-0.01426
surrogate=-0.00727, entropy= 0.20141, loss=-0.00727
surrogate=-0.01067, entropy= 0.20126, loss=-0.01067
surrogate=-0.00558, entropy= 0.20109, loss=-0.00558
surrogate=-0.00571, entropy= 0.20086, loss=-0.00571
std_min= 0.20883, std_max= 0.30423, std_mean= 0.26189
val lr: [1.8186475409836063e-05], policy lr: [2.182377049180327e-05]
Policy Loss: -0.0057109, | Entropy Bonus: -0, | Value Loss: 3.771, | Advantage Loss: 0.65476
Time elapsed (s): 1.6310172080993652
Agent stdevs: 0.26188597
--------------------------------------------------------------------------------

Step 905
++++++++ Policy training ++++++++++
Current mean reward: 2427.476212 | mean episode length: 673.333333
val_loss=20.78107
val_loss=51.05802
val_loss=39.40839
val_loss=189.28560
val_loss=60.34541
val_loss=21.50577
val_loss=305.26215
val_loss=1650.83765
val_loss=38.02633
val_loss=22.63712
adv_loss= 0.81041
adv_loss= 0.80304
adv_loss= 1.02755
adv_loss= 1.00248
adv_loss= 2.09093
adv_loss= 1.18339
adv_loss= 1.92877
adv_loss= 0.66858
adv_loss= 1.28986
adv_loss= 0.69586
surrogate=-0.00391, entropy= 0.20049, loss=-0.00391
surrogate=-0.00251, entropy= 0.20014, loss=-0.00251
surrogate=-0.01069, entropy= 0.19982, loss=-0.01069
surrogate=-0.01167, entropy= 0.19957, loss=-0.01167
surrogate=-0.03339, entropy= 0.19925, loss=-0.03339
surrogate=-0.02529, entropy= 0.19892, loss=-0.02529
surrogate=-0.01626, entropy= 0.19845, loss=-0.01626
surrogate=-0.00220, entropy= 0.19814, loss=-0.00220
surrogate=-0.01597, entropy= 0.19776, loss=-0.01597
surrogate=-0.02657, entropy= 0.19733, loss=-0.02657
std_min= 0.20867, std_max= 0.30418, std_mean= 0.26158
val lr: [1.793032786885246e-05], policy lr: [2.151639344262295e-05]
Policy Loss: -0.026572, | Entropy Bonus: -0, | Value Loss: 22.637, | Advantage Loss: 0.69586
Time elapsed (s): 1.637117624282837
Agent stdevs: 0.26158014
--------------------------------------------------------------------------------

Step 906
++++++++ Policy training ++++++++++
Current mean reward: 1639.769303 | mean episode length: 448.250000
val_loss= 8.01804
val_loss=14.11337
val_loss=14.05676
val_loss=12.33982
val_loss=14.46039
val_loss=10.22412
val_loss=10.07541
val_loss= 6.74118
val_loss=10.97311
val_loss= 6.64288
adv_loss= 3.54253
adv_loss= 1.00921
adv_loss= 0.58284
adv_loss= 1.40005
adv_loss= 2.43578
adv_loss= 0.64097
adv_loss= 4.00251
adv_loss= 2.55855
adv_loss= 1.28768
adv_loss= 2.01901
surrogate=-0.00686, entropy= 0.19735, loss=-0.00686
surrogate=-0.01459, entropy= 0.19708, loss=-0.01459
surrogate= 0.00560, entropy= 0.19707, loss= 0.00560
surrogate= 0.02535, entropy= 0.19694, loss= 0.02535
surrogate=-0.01902, entropy= 0.19691, loss=-0.01902
surrogate=-0.01754, entropy= 0.19689, loss=-0.01754
surrogate= 0.00199, entropy= 0.19680, loss= 0.00199
surrogate= 0.00882, entropy= 0.19679, loss= 0.00882
surrogate=-0.01586, entropy= 0.19662, loss=-0.01586
surrogate= 0.00241, entropy= 0.19662, loss= 0.00241
std_min= 0.20836, std_max= 0.30417, std_mean= 0.26155
val lr: [1.7674180327868855e-05], policy lr: [2.1209016393442623e-05]
Policy Loss: 0.0024056, | Entropy Bonus: -0, | Value Loss: 6.6429, | Advantage Loss: 2.019
Time elapsed (s): 1.6579341888427734
Agent stdevs: 0.2615478
--------------------------------------------------------------------------------

Step 907
++++++++ Policy training ++++++++++
Current mean reward: 2148.404978 | mean episode length: 581.000000
val_loss= 9.05867
val_loss= 6.69699
val_loss= 6.90513
val_loss= 4.61290
val_loss= 4.01051
val_loss= 8.38258
val_loss= 6.29519
val_loss= 5.73002
val_loss= 4.09684
val_loss= 3.12432
adv_loss= 2.10488
adv_loss= 1.40934
adv_loss= 1.22636
adv_loss= 1.29930
adv_loss= 0.42855
adv_loss= 1.06859
adv_loss= 0.44763
adv_loss= 0.34104
adv_loss= 0.67453
adv_loss= 1.31556
surrogate= 0.00827, entropy= 0.19690, loss= 0.00827
surrogate=-0.01894, entropy= 0.19693, loss=-0.01894
surrogate=-0.01232, entropy= 0.19688, loss=-0.01232
surrogate= 0.00126, entropy= 0.19695, loss= 0.00126
surrogate=-0.00634, entropy= 0.19699, loss=-0.00634
surrogate= 0.00425, entropy= 0.19710, loss= 0.00425
surrogate=-0.00856, entropy= 0.19713, loss=-0.00856
surrogate=-0.03157, entropy= 0.19711, loss=-0.03157
surrogate=-0.00772, entropy= 0.19714, loss=-0.00772
surrogate=-0.02906, entropy= 0.19717, loss=-0.02906
std_min= 0.20834, std_max= 0.30435, std_mean= 0.26161
val lr: [1.7418032786885252e-05], policy lr: [2.09016393442623e-05]
Policy Loss: -0.029062, | Entropy Bonus: -0, | Value Loss: 3.1243, | Advantage Loss: 1.3156
Time elapsed (s): 1.635119915008545
Agent stdevs: 0.261606
--------------------------------------------------------------------------------

Step 908
++++++++ Policy training ++++++++++
Current mean reward: 2442.910672 | mean episode length: 665.000000
val_loss= 8.25097
val_loss=12.62809
val_loss= 3.09131
val_loss=10.21425
val_loss= 4.31587
val_loss= 4.88509
val_loss= 2.29806
val_loss= 5.08005
val_loss= 4.63651
val_loss= 3.44702
adv_loss= 2.02272
adv_loss= 0.74172
adv_loss= 1.58705
adv_loss= 1.53753
adv_loss= 0.72071
adv_loss= 0.44855
adv_loss= 1.00566
adv_loss= 0.49128
adv_loss= 3.17113
adv_loss= 1.05304
surrogate=-0.00718, entropy= 0.19720, loss=-0.00718
surrogate=-0.02736, entropy= 0.19726, loss=-0.02736
surrogate=-0.00226, entropy= 0.19712, loss=-0.00226
surrogate=-0.00937, entropy= 0.19693, loss=-0.00937
surrogate=-0.02200, entropy= 0.19670, loss=-0.02200
surrogate=-0.01591, entropy= 0.19668, loss=-0.01591
surrogate=-0.00564, entropy= 0.19637, loss=-0.00564
surrogate= 0.00199, entropy= 0.19621, loss= 0.00199
surrogate=-0.02635, entropy= 0.19609, loss=-0.02635
surrogate=-0.00451, entropy= 0.19598, loss=-0.00451
std_min= 0.20823, std_max= 0.30425, std_mean= 0.26151
val lr: [1.716188524590165e-05], policy lr: [2.0594262295081975e-05]
Policy Loss: -0.0045147, | Entropy Bonus: -0, | Value Loss: 3.447, | Advantage Loss: 1.053
Time elapsed (s): 1.6258184909820557
Agent stdevs: 0.26150516
--------------------------------------------------------------------------------

Step 909
++++++++ Policy training ++++++++++
Current mean reward: 2525.440971 | mean episode length: 682.500000
val_loss= 4.27316
val_loss= 3.88664
val_loss= 5.43820
val_loss= 6.27488
val_loss= 4.13828
val_loss= 4.28087
val_loss= 3.85488
val_loss= 6.69151
val_loss= 4.90742
val_loss= 6.33749
adv_loss= 0.35670
adv_loss= 0.51825
adv_loss= 1.13133
adv_loss= 2.75676
adv_loss= 1.17381
adv_loss= 0.83254
adv_loss= 0.50953
adv_loss= 0.62342
adv_loss= 0.68204
adv_loss= 0.49353
surrogate= 0.01013, entropy= 0.19596, loss= 0.01013
surrogate=-0.00101, entropy= 0.19604, loss=-0.00101
surrogate= 0.00080, entropy= 0.19605, loss= 0.00080
surrogate=-0.00874, entropy= 0.19581, loss=-0.00874
surrogate=-0.02475, entropy= 0.19569, loss=-0.02475
surrogate=-0.02969, entropy= 0.19568, loss=-0.02969
surrogate=-0.02278, entropy= 0.19564, loss=-0.02278
surrogate=-0.02118, entropy= 0.19556, loss=-0.02118
surrogate=-0.00335, entropy= 0.19554, loss=-0.00335
surrogate= 0.00656, entropy= 0.19548, loss= 0.00656
std_min= 0.20817, std_max= 0.30411, std_mean= 0.26146
val lr: [1.6905737704918045e-05], policy lr: [2.0286885245901653e-05]
Policy Loss: 0.0065559, | Entropy Bonus: -0, | Value Loss: 6.3375, | Advantage Loss: 0.49353
Time elapsed (s): 1.644500732421875
Agent stdevs: 0.26146138
--------------------------------------------------------------------------------

Step 910
++++++++ Policy training ++++++++++
Current mean reward: 3469.146579 | mean episode length: 959.000000
val_loss= 5.53940
val_loss= 9.03706
val_loss=10.29505
val_loss=1058.36084
val_loss=25.43758
val_loss=471.75375
val_loss=22.79860
val_loss=399.81247
val_loss=404.41794
val_loss=51.59654
adv_loss= 0.95210
adv_loss=1141.79407
adv_loss= 0.68719
adv_loss= 1.09185
adv_loss= 1.50378
adv_loss= 1.23882
adv_loss= 0.93359
adv_loss= 0.50738
adv_loss= 0.59248
adv_loss= 0.71230
surrogate=-0.01277, entropy= 0.19560, loss=-0.01277
surrogate=-0.00981, entropy= 0.19547, loss=-0.00981
surrogate=-0.01091, entropy= 0.19559, loss=-0.01091
surrogate=-0.01863, entropy= 0.19557, loss=-0.01863
surrogate=-0.01082, entropy= 0.19563, loss=-0.01082
surrogate=-0.00605, entropy= 0.19558, loss=-0.00605
surrogate=-0.01119, entropy= 0.19551, loss=-0.01119
surrogate= 0.01169, entropy= 0.19531, loss= 0.01169
surrogate=-0.00243, entropy= 0.19538, loss=-0.00243
surrogate= 0.03697, entropy= 0.19536, loss= 0.03697
std_min= 0.20827, std_max= 0.30371, std_mean= 0.26143
val lr: [1.6649590163934414e-05], policy lr: [1.9979508196721294e-05]
Policy Loss: 0.036965, | Entropy Bonus: -0, | Value Loss: 51.597, | Advantage Loss: 0.7123
Time elapsed (s): 1.6311874389648438
Agent stdevs: 0.2614267
--------------------------------------------------------------------------------

Step 911
++++++++ Policy training ++++++++++
Current mean reward: 1880.334261 | mean episode length: 507.333333
val_loss= 6.93719
val_loss=10.88521
val_loss= 6.35179
val_loss= 6.44496
val_loss= 6.80875
val_loss= 5.99352
val_loss=14.23155
val_loss= 6.22963
val_loss= 7.93860
val_loss= 9.06403
adv_loss= 1.40132
adv_loss= 2.66149
adv_loss= 1.20501
adv_loss= 0.75631
adv_loss= 1.01827
adv_loss= 0.55886
adv_loss= 1.06710
adv_loss= 1.05559
adv_loss= 0.90551
adv_loss= 0.76672
surrogate= 0.00045, entropy= 0.19515, loss= 0.00045
surrogate=-0.01985, entropy= 0.19479, loss=-0.01985
surrogate=-0.02567, entropy= 0.19432, loss=-0.02567
surrogate=-0.00585, entropy= 0.19386, loss=-0.00585
surrogate= 0.00378, entropy= 0.19346, loss= 0.00378
surrogate=-0.01971, entropy= 0.19306, loss=-0.01971
surrogate=-0.00446, entropy= 0.19277, loss=-0.00446
surrogate=-0.00556, entropy= 0.19256, loss=-0.00556
surrogate=-0.01465, entropy= 0.19220, loss=-0.01465
surrogate=-0.01017, entropy= 0.19180, loss=-0.01017
std_min= 0.20801, std_max= 0.30330, std_mean= 0.26112
val lr: [1.639344262295081e-05], policy lr: [1.967213114754097e-05]
Policy Loss: -0.01017, | Entropy Bonus: -0, | Value Loss: 9.064, | Advantage Loss: 0.76672
Time elapsed (s): 1.6476991176605225
Agent stdevs: 0.26111546
--------------------------------------------------------------------------------

Step 912
++++++++ Policy training ++++++++++
Current mean reward: 2489.081879 | mean episode length: 675.500000
val_loss=10.67053
val_loss=11.85855
val_loss= 4.51681
val_loss= 9.48161
val_loss= 6.18985
val_loss=10.00662
val_loss= 8.27913
val_loss= 4.71356
val_loss=12.36840
val_loss= 8.63178
adv_loss= 1.23945
adv_loss= 1.21457
adv_loss= 0.63683
adv_loss= 1.62911
adv_loss= 0.53984
adv_loss= 0.51696
adv_loss= 0.92837
adv_loss= 1.05194
adv_loss= 0.84988
adv_loss= 0.48569
surrogate= 0.00744, entropy= 0.19154, loss= 0.00744
surrogate=-0.00576, entropy= 0.19150, loss=-0.00576
surrogate= 0.00894, entropy= 0.19123, loss= 0.00894
surrogate=-0.03205, entropy= 0.19086, loss=-0.03205
surrogate=-0.00165, entropy= 0.19067, loss=-0.00165
surrogate=-0.01928, entropy= 0.19045, loss=-0.01928
surrogate=-0.00489, entropy= 0.19031, loss=-0.00489
surrogate=-0.01908, entropy= 0.19009, loss=-0.01908
surrogate=-0.01559, entropy= 0.18972, loss=-0.01559
surrogate=-0.03419, entropy= 0.18952, loss=-0.03419
std_min= 0.20796, std_max= 0.30278, std_mean= 0.26090
val lr: [1.6137295081967207e-05], policy lr: [1.9364754098360646e-05]
Policy Loss: -0.034185, | Entropy Bonus: -0, | Value Loss: 8.6318, | Advantage Loss: 0.48569
Time elapsed (s): 1.679917812347412
Agent stdevs: 0.2608965
--------------------------------------------------------------------------------

Step 913
++++++++ Policy training ++++++++++
Current mean reward: 1355.752712 | mean episode length: 373.000000
val_loss=681.72681
val_loss=12.46483
val_loss=51.90074
val_loss=49.31140
val_loss=18.31932
val_loss=636.00671
val_loss=1720.06580
val_loss=1597.98828
val_loss=87.54918
val_loss=1273.14319
adv_loss= 0.67736
adv_loss= 1.75486
adv_loss= 1.24878
adv_loss= 2.69252
adv_loss= 1.17309
adv_loss= 2.55841
adv_loss= 0.99704
adv_loss= 0.94052
adv_loss= 1.23578
adv_loss= 1.30677
surrogate=-0.00655, entropy= 0.18914, loss=-0.00655
surrogate= 0.01003, entropy= 0.18909, loss= 0.01003
surrogate=-0.01660, entropy= 0.18875, loss=-0.01660
surrogate= 0.01068, entropy= 0.18877, loss= 0.01068
surrogate=-0.00857, entropy= 0.18866, loss=-0.00857
surrogate= 0.00566, entropy= 0.18856, loss= 0.00566
surrogate=-0.01826, entropy= 0.18843, loss=-0.01826
surrogate= 0.00170, entropy= 0.18859, loss= 0.00170
surrogate=-0.02473, entropy= 0.18849, loss=-0.02473
surrogate= 0.00011, entropy= 0.18831, loss= 0.00011
std_min= 0.20760, std_max= 0.30262, std_mean= 0.26082
val lr: [1.5881147540983604e-05], policy lr: [1.9057377049180324e-05]
Policy Loss: 0.00010601, | Entropy Bonus: -0, | Value Loss: 1273.1, | Advantage Loss: 1.3068
Time elapsed (s): 1.6250677108764648
Agent stdevs: 0.26081932
--------------------------------------------------------------------------------

Step 914
++++++++ Policy training ++++++++++
Current mean reward: 1410.813863 | mean episode length: 381.400000
val_loss=19.72207
val_loss=12.15960
val_loss=15.86362
val_loss=18.39789
val_loss=16.21069
val_loss=16.74470
val_loss=10.05685
val_loss=17.30031
val_loss=17.38404
val_loss=17.34540
adv_loss= 1.18459
adv_loss= 1.58862
adv_loss= 3.53486
adv_loss= 2.59986
adv_loss= 3.98550
adv_loss= 0.91921
adv_loss= 3.22386
adv_loss= 1.17027
adv_loss= 1.04661
adv_loss= 0.96189
surrogate= 0.00429, entropy= 0.18826, loss= 0.00429
surrogate=-0.00482, entropy= 0.18811, loss=-0.00482
surrogate=-0.01494, entropy= 0.18780, loss=-0.01494
surrogate=-0.01097, entropy= 0.18760, loss=-0.01097
surrogate=-0.00751, entropy= 0.18750, loss=-0.00751
surrogate=-0.01143, entropy= 0.18722, loss=-0.01143
surrogate=-0.01377, entropy= 0.18717, loss=-0.01377
surrogate=-0.00743, entropy= 0.18700, loss=-0.00743
surrogate=-0.03707, entropy= 0.18681, loss=-0.03707
surrogate=-0.01775, entropy= 0.18661, loss=-0.01775
std_min= 0.20736, std_max= 0.30258, std_mean= 0.26069
val lr: [1.5625e-05], policy lr: [1.875e-05]
Policy Loss: -0.017746, | Entropy Bonus: -0, | Value Loss: 17.345, | Advantage Loss: 0.96189
Time elapsed (s): 1.6268608570098877
Agent stdevs: 0.26068735
--------------------------------------------------------------------------------

Step 915
++++++++ Policy training ++++++++++
Current mean reward: 2019.775223 | mean episode length: 587.500000
val_loss=702.52606
val_loss=2330.33643
val_loss=62.03555
val_loss=868.78510
val_loss=178.34988
val_loss=1780.13159
val_loss=423.89880
val_loss=978.20056
val_loss=1655.89539
val_loss=44.14511
adv_loss= 0.91772
adv_loss= 0.44417
adv_loss= 0.87833
adv_loss= 3.20859
adv_loss= 2.64736
adv_loss= 0.51119
adv_loss= 0.54936
adv_loss= 0.71769
adv_loss= 1.94252
adv_loss= 6.84865
surrogate= 0.00173, entropy= 0.18661, loss= 0.00173
surrogate=-0.00484, entropy= 0.18647, loss=-0.00484
surrogate=-0.01563, entropy= 0.18673, loss=-0.01563
surrogate= 0.04354, entropy= 0.18697, loss= 0.04354
surrogate= 0.00388, entropy= 0.18718, loss= 0.00388
surrogate=-0.00074, entropy= 0.18749, loss=-0.00074
surrogate=-0.01743, entropy= 0.18755, loss=-0.01743
surrogate=-0.02761, entropy= 0.18772, loss=-0.02761
surrogate=-0.02152, entropy= 0.18794, loss=-0.02152
surrogate=-0.03483, entropy= 0.18812, loss=-0.03483
std_min= 0.20754, std_max= 0.30295, std_mean= 0.26082
val lr: [1.5368852459016397e-05], policy lr: [1.8442622950819673e-05]
Policy Loss: -0.034832, | Entropy Bonus: -0, | Value Loss: 44.145, | Advantage Loss: 6.8486
Time elapsed (s): 1.6250879764556885
Agent stdevs: 0.26081964
--------------------------------------------------------------------------------

Step 916
++++++++ Policy training ++++++++++
Current mean reward: 2454.670062 | mean episode length: 682.000000
val_loss=36.85226
val_loss=188.97984
val_loss=76.57232
val_loss=1123.78748
val_loss=294.93311
val_loss=621.64801
val_loss=532.44122
val_loss=11.75788
val_loss=42.93674
val_loss=33.39245
adv_loss=1894.26489
adv_loss= 1.17125
adv_loss= 2.52229
adv_loss= 1.56590
adv_loss= 2.57037
adv_loss=1891.63892
adv_loss= 0.74643
adv_loss= 1.87082
adv_loss= 0.49133
adv_loss= 1.04547
surrogate= 0.00812, entropy= 0.18851, loss= 0.00812
surrogate=-0.00949, entropy= 0.18867, loss=-0.00949
surrogate=-0.02280, entropy= 0.18891, loss=-0.02280
surrogate=-0.00737, entropy= 0.18921, loss=-0.00737
surrogate= 0.00415, entropy= 0.18948, loss= 0.00415
surrogate=-0.01358, entropy= 0.18947, loss=-0.01358
surrogate=-0.01519, entropy= 0.18957, loss=-0.01519
surrogate=-0.02226, entropy= 0.18982, loss=-0.02226
surrogate=-0.01500, entropy= 0.19001, loss=-0.01500
surrogate=-0.01822, entropy= 0.19024, loss=-0.01822
std_min= 0.20770, std_max= 0.30321, std_mean= 0.26100
val lr: [1.5112704918032793e-05], policy lr: [1.813524590163935e-05]
Policy Loss: -0.01822, | Entropy Bonus: -0, | Value Loss: 33.392, | Advantage Loss: 1.0455
Time elapsed (s): 1.6524269580841064
Agent stdevs: 0.26100355
--------------------------------------------------------------------------------

Step 917
++++++++ Policy training ++++++++++
Current mean reward: 1615.436542 | mean episode length: 441.250000
val_loss=1937.11768
val_loss=579.97809
val_loss=1446.53564
val_loss=12.72127
val_loss=12.92127
val_loss=606.34375
val_loss=64.64746
val_loss=631.99414
val_loss=1262.73181
val_loss=15.67835
adv_loss= 0.60529
adv_loss= 1.15085
adv_loss= 0.84688
adv_loss= 1.29307
adv_loss= 1.01220
adv_loss= 1.11532
adv_loss= 1.20095
adv_loss= 0.71185
adv_loss= 2.47513
adv_loss= 1.33180
surrogate=-0.00619, entropy= 0.19002, loss=-0.00619
surrogate=-0.00487, entropy= 0.18963, loss=-0.00487
surrogate= 0.02438, entropy= 0.18934, loss= 0.02438
surrogate= 0.00020, entropy= 0.18905, loss= 0.00020
surrogate=-0.02476, entropy= 0.18877, loss=-0.02476
surrogate=-0.00522, entropy= 0.18841, loss=-0.00522
surrogate= 0.01120, entropy= 0.18816, loss= 0.01120
surrogate=-0.02702, entropy= 0.18787, loss=-0.02702
surrogate=-0.01410, entropy= 0.18744, loss=-0.01410
surrogate=-0.01190, entropy= 0.18715, loss=-0.01190
std_min= 0.20760, std_max= 0.30283, std_mean= 0.26072
val lr: [1.485655737704919e-05], policy lr: [1.7827868852459025e-05]
Policy Loss: -0.011902, | Entropy Bonus: -0, | Value Loss: 15.678, | Advantage Loss: 1.3318
Time elapsed (s): 1.6792082786560059
Agent stdevs: 0.2607196
--------------------------------------------------------------------------------

Step 918
++++++++ Policy training ++++++++++
Current mean reward: 1592.114448 | mean episode length: 430.000000
val_loss=18.71799
val_loss=13.27034
val_loss=13.42604
val_loss= 9.61879
val_loss=15.06333
val_loss=17.02087
val_loss=12.58116
val_loss=12.58373
val_loss=11.56983
val_loss= 6.74456
adv_loss= 2.42025
adv_loss= 2.15662
adv_loss= 1.38573
adv_loss= 1.56094
adv_loss= 2.79094
adv_loss= 3.85910
adv_loss= 1.53486
adv_loss= 1.95779
adv_loss= 2.88365
adv_loss= 0.99190
surrogate=-0.00938, entropy= 0.18706, loss=-0.00938
surrogate= 0.01259, entropy= 0.18722, loss= 0.01259
surrogate= 0.00747, entropy= 0.18718, loss= 0.00747
surrogate=-0.00152, entropy= 0.18717, loss=-0.00152
surrogate=-0.00949, entropy= 0.18726, loss=-0.00949
surrogate=-0.00143, entropy= 0.18715, loss=-0.00143
surrogate=-0.01672, entropy= 0.18723, loss=-0.01672
surrogate=-0.01035, entropy= 0.18729, loss=-0.01035
surrogate=-0.02269, entropy= 0.18726, loss=-0.02269
surrogate=-0.02652, entropy= 0.18736, loss=-0.02652
std_min= 0.20748, std_max= 0.30320, std_mean= 0.26076
val lr: [1.4600409836065586e-05], policy lr: [1.7520491803278703e-05]
Policy Loss: -0.026522, | Entropy Bonus: -0, | Value Loss: 6.7446, | Advantage Loss: 0.9919
Time elapsed (s): 1.695551872253418
Agent stdevs: 0.2607648
--------------------------------------------------------------------------------

Step 919
++++++++ Policy training ++++++++++
Current mean reward: 1841.530685 | mean episode length: 523.000000
val_loss=83.31947
val_loss=298.21051
val_loss=350.16257
val_loss=1094.50269
val_loss=252.51147
val_loss=48.08763
val_loss=198.27960
val_loss=632.20587
val_loss=247.86513
val_loss=103.70000
adv_loss= 1.63235
adv_loss= 0.59441
adv_loss=1814.39417
adv_loss= 0.78181
adv_loss= 0.63197
adv_loss= 0.88585
adv_loss= 0.93777
adv_loss= 1.06776
adv_loss= 2.42943
adv_loss= 0.51035
surrogate= 0.00242, entropy= 0.18724, loss= 0.00242
surrogate=-0.00423, entropy= 0.18700, loss=-0.00423
surrogate= 0.01059, entropy= 0.18679, loss= 0.01059
surrogate=-0.01226, entropy= 0.18648, loss=-0.01226
surrogate= 0.00157, entropy= 0.18623, loss= 0.00157
surrogate=-0.02268, entropy= 0.18597, loss=-0.02268
surrogate=-0.01244, entropy= 0.18571, loss=-0.01244
surrogate=-0.00515, entropy= 0.18554, loss=-0.00515
surrogate=-0.01705, entropy= 0.18537, loss=-0.01705
surrogate=-0.02721, entropy= 0.18516, loss=-0.02721
std_min= 0.20729, std_max= 0.30301, std_mean= 0.26058
val lr: [1.4344262295081956e-05], policy lr: [1.7213114754098343e-05]
Policy Loss: -0.027214, | Entropy Bonus: -0, | Value Loss: 103.7, | Advantage Loss: 0.51035
Time elapsed (s): 1.65989089012146
Agent stdevs: 0.26057836
--------------------------------------------------------------------------------

Step 920
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1904.7
++++++++ Policy training ++++++++++
Current mean reward: 3157.140080 | mean episode length: 878.000000
val_loss=385.46878
val_loss=38.31823
val_loss=61.42279
val_loss=1581.50244
val_loss=563.37158
val_loss=99.38465
val_loss=26.63329
val_loss=29.69892
val_loss=2086.55078
val_loss=34.88688
adv_loss= 1.24447
adv_loss= 1.09746
adv_loss= 1.36231
adv_loss= 1.04787
adv_loss= 1.34545
adv_loss= 1.51480
adv_loss= 1.25452
adv_loss= 1.37586
adv_loss= 2.71906
adv_loss= 2.36669
surrogate=-0.00721, entropy= 0.18530, loss=-0.00721
surrogate= 0.01846, entropy= 0.18562, loss= 0.01846
surrogate=-0.02090, entropy= 0.18577, loss=-0.02090
surrogate=-0.02072, entropy= 0.18590, loss=-0.02072
surrogate= 0.00520, entropy= 0.18587, loss= 0.00520
surrogate=-0.00471, entropy= 0.18617, loss=-0.00471
surrogate=-0.00660, entropy= 0.18621, loss=-0.00660
surrogate=-0.02321, entropy= 0.18633, loss=-0.02321
surrogate=-0.02431, entropy= 0.18644, loss=-0.02431
surrogate= 0.00051, entropy= 0.18666, loss= 0.00051
std_min= 0.20741, std_max= 0.30341, std_mean= 0.26072
val lr: [1.4088114754098352e-05], policy lr: [1.690573770491802e-05]
Policy Loss: 0.00050822, | Entropy Bonus: -0, | Value Loss: 34.887, | Advantage Loss: 2.3667
Time elapsed (s): 1.6465890407562256
Agent stdevs: 0.26071692
--------------------------------------------------------------------------------

Step 921
++++++++ Policy training ++++++++++
Current mean reward: 1651.211656 | mean episode length: 449.500000
val_loss=27.04936
val_loss=37.64291
val_loss=18.58065
val_loss=37.93128
val_loss=19.62589
val_loss=18.58112
val_loss=12.21129
val_loss=21.06345
val_loss=14.08706
val_loss=18.88846
adv_loss= 2.30165
adv_loss= 3.37335
adv_loss= 5.12375
adv_loss= 4.88917
adv_loss= 3.32047
adv_loss= 1.98938
adv_loss= 2.55213
adv_loss= 6.31062
adv_loss= 3.27395
adv_loss= 2.37766
surrogate=-0.00468, entropy= 0.18662, loss=-0.00468
surrogate=-0.01319, entropy= 0.18635, loss=-0.01319
surrogate=-0.02039, entropy= 0.18596, loss=-0.02039
surrogate=-0.02291, entropy= 0.18557, loss=-0.02291
surrogate=-0.00920, entropy= 0.18524, loss=-0.00920
surrogate=-0.01366, entropy= 0.18513, loss=-0.01366
surrogate=-0.02038, entropy= 0.18487, loss=-0.02038
surrogate=-0.00918, entropy= 0.18467, loss=-0.00918
surrogate=-0.01478, entropy= 0.18444, loss=-0.01478
surrogate= 0.00378, entropy= 0.18436, loss= 0.00378
std_min= 0.20742, std_max= 0.30263, std_mean= 0.26048
val lr: [1.3831967213114749e-05], policy lr: [1.6598360655737696e-05]
Policy Loss: 0.0037774, | Entropy Bonus: -0, | Value Loss: 18.888, | Advantage Loss: 2.3777
Time elapsed (s): 1.6351251602172852
Agent stdevs: 0.2604787
--------------------------------------------------------------------------------

Step 922
++++++++ Policy training ++++++++++
Current mean reward: 1544.173009 | mean episode length: 412.333333
val_loss=16.91792
val_loss=14.08350
val_loss=13.31561
val_loss= 8.67877
val_loss=12.03788
val_loss= 8.75293
val_loss= 9.77474
val_loss= 6.25495
val_loss= 5.85844
val_loss= 9.99117
adv_loss= 0.99540
adv_loss= 0.68972
adv_loss= 1.12125
adv_loss= 1.10916
adv_loss= 1.57446
adv_loss= 1.29679
adv_loss= 1.18372
adv_loss= 1.05003
adv_loss= 0.82470
adv_loss= 2.01731
surrogate=-0.00186, entropy= 0.18446, loss=-0.00186
surrogate= 0.01716, entropy= 0.18471, loss= 0.01716
surrogate= 0.00822, entropy= 0.18493, loss= 0.00822
surrogate=-0.00217, entropy= 0.18503, loss=-0.00217
surrogate= 0.01333, entropy= 0.18509, loss= 0.01333
surrogate= 0.00909, entropy= 0.18518, loss= 0.00909
surrogate=-0.00041, entropy= 0.18536, loss=-0.00041
surrogate=-0.00101, entropy= 0.18543, loss=-0.00101
surrogate=-0.01649, entropy= 0.18567, loss=-0.01649
surrogate=-0.00683, entropy= 0.18574, loss=-0.00683
std_min= 0.20785, std_max= 0.30259, std_mean= 0.26056
val lr: [1.3575819672131145e-05], policy lr: [1.6290983606557374e-05]
Policy Loss: -0.0068346, | Entropy Bonus: -0, | Value Loss: 9.9912, | Advantage Loss: 2.0173
Time elapsed (s): 1.6308879852294922
Agent stdevs: 0.26055977
--------------------------------------------------------------------------------

Step 923
++++++++ Policy training ++++++++++
Current mean reward: 1865.692743 | mean episode length: 497.000000
val_loss= 7.07618
val_loss= 6.59695
val_loss= 5.41392
val_loss= 7.59532
val_loss= 6.70712
val_loss= 3.51161
val_loss= 3.70789
val_loss= 5.20650
val_loss= 3.10659
val_loss= 2.76416
adv_loss= 0.84428
adv_loss= 0.80644
adv_loss= 1.14169
adv_loss= 2.36021
adv_loss= 1.13147
adv_loss= 0.72745
adv_loss= 2.63531
adv_loss= 5.32254
adv_loss= 0.97952
adv_loss= 1.01503
surrogate= 0.00924, entropy= 0.18603, loss= 0.00924
surrogate=-0.01883, entropy= 0.18629, loss=-0.01883
surrogate= 0.01695, entropy= 0.18638, loss= 0.01695
surrogate=-0.02314, entropy= 0.18639, loss=-0.02314
surrogate=-0.02757, entropy= 0.18646, loss=-0.02757
surrogate=-0.02829, entropy= 0.18667, loss=-0.02829
surrogate=-0.02469, entropy= 0.18688, loss=-0.02469
surrogate=-0.01635, entropy= 0.18703, loss=-0.01635
surrogate=-0.01144, entropy= 0.18705, loss=-0.01144
surrogate=-0.03967, entropy= 0.18705, loss=-0.03967
std_min= 0.20787, std_max= 0.30307, std_mean= 0.26069
val lr: [1.3319672131147542e-05], policy lr: [1.5983606557377048e-05]
Policy Loss: -0.039668, | Entropy Bonus: -0, | Value Loss: 2.7642, | Advantage Loss: 1.015
Time elapsed (s): 1.6360342502593994
Agent stdevs: 0.26069245
--------------------------------------------------------------------------------

Step 924
++++++++ Policy training ++++++++++
Current mean reward: 2067.371287 | mean episode length: 557.666667
val_loss= 8.43873
val_loss=10.88492
val_loss= 6.08660
val_loss= 6.79373
val_loss= 7.39221
val_loss= 8.70841
val_loss= 9.79164
val_loss=11.43105
val_loss= 9.24118
val_loss= 5.43343
adv_loss= 0.61030
adv_loss= 3.82463
adv_loss= 0.38137
adv_loss= 1.62781
adv_loss= 0.86696
adv_loss= 0.69614
adv_loss= 0.54753
adv_loss= 0.89606
adv_loss= 1.01367
adv_loss= 0.82325
surrogate= 0.00096, entropy= 0.18714, loss= 0.00096
surrogate=-0.00484, entropy= 0.18709, loss=-0.00484
surrogate=-0.01237, entropy= 0.18690, loss=-0.01237
surrogate=-0.00026, entropy= 0.18682, loss=-0.00026
surrogate=-0.00631, entropy= 0.18668, loss=-0.00631
surrogate=-0.01224, entropy= 0.18642, loss=-0.01224
surrogate=-0.01048, entropy= 0.18635, loss=-0.01048
surrogate=-0.01607, entropy= 0.18614, loss=-0.01607
surrogate=-0.03232, entropy= 0.18598, loss=-0.03232
surrogate=-0.03189, entropy= 0.18573, loss=-0.03189
std_min= 0.20776, std_max= 0.30318, std_mean= 0.26059
val lr: [1.3063524590163938e-05], policy lr: [1.5676229508196726e-05]
Policy Loss: -0.031891, | Entropy Bonus: -0, | Value Loss: 5.4334, | Advantage Loss: 0.82325
Time elapsed (s): 1.6666316986083984
Agent stdevs: 0.2605876
--------------------------------------------------------------------------------

Step 925
++++++++ Policy training ++++++++++
Current mean reward: 2110.208782 | mean episode length: 587.333333
val_loss= 9.39835
val_loss=167.21184
val_loss=696.73206
val_loss=1267.41150
val_loss=60.77979
val_loss=485.48624
val_loss=10.81527
val_loss=385.36221
val_loss=16.08850
val_loss=1043.70789
adv_loss= 0.77904
adv_loss= 1.34370
adv_loss= 1.07980
adv_loss= 1.57313
adv_loss= 0.56101
adv_loss=1823.89233
adv_loss=1824.58533
adv_loss= 2.14372
adv_loss= 0.86769
adv_loss= 1.23439
surrogate= 0.00846, entropy= 0.18562, loss= 0.00846
surrogate=-0.00118, entropy= 0.18538, loss=-0.00118
surrogate=-0.01302, entropy= 0.18536, loss=-0.01302
surrogate= 0.00860, entropy= 0.18542, loss= 0.00860
surrogate= 0.02168, entropy= 0.18535, loss= 0.02168
surrogate=-0.01604, entropy= 0.18529, loss=-0.01604
surrogate=-0.02039, entropy= 0.18538, loss=-0.02039
surrogate=-0.00872, entropy= 0.18540, loss=-0.00872
surrogate=-0.00755, entropy= 0.18538, loss=-0.00755
surrogate= 0.00995, entropy= 0.18548, loss= 0.00995
std_min= 0.20785, std_max= 0.30310, std_mean= 0.26055
val lr: [1.2807377049180335e-05], policy lr: [1.53688524590164e-05]
Policy Loss: 0.0099463, | Entropy Bonus: -0, | Value Loss: 1043.7, | Advantage Loss: 1.2344
Time elapsed (s): 1.6380598545074463
Agent stdevs: 0.26055303
--------------------------------------------------------------------------------

Step 926
++++++++ Policy training ++++++++++
Current mean reward: 2048.283990 | mean episode length: 565.666667
val_loss=257.36481
val_loss=2156.50439
val_loss=24.55365
val_loss=465.64951
val_loss=44.86553
val_loss=552.99438
val_loss=119.72550
val_loss=823.66144
val_loss=45.03571
val_loss=31.76665
adv_loss= 1.27542
adv_loss= 0.88598
adv_loss=1259.44629
adv_loss= 5.30652
adv_loss= 0.78673
adv_loss= 0.83655
adv_loss= 1.73217
adv_loss= 1.39433
adv_loss= 1.24339
adv_loss= 1.66117
surrogate= 0.00509, entropy= 0.18521, loss= 0.00509
surrogate=-0.02548, entropy= 0.18498, loss=-0.02548
surrogate= 0.02463, entropy= 0.18503, loss= 0.02463
surrogate= 0.00755, entropy= 0.18507, loss= 0.00755
surrogate=-0.02074, entropy= 0.18511, loss=-0.02074
surrogate=-0.00705, entropy= 0.18508, loss=-0.00705
surrogate=-0.02218, entropy= 0.18504, loss=-0.02218
surrogate=-0.04088, entropy= 0.18501, loss=-0.04088
surrogate=-0.02236, entropy= 0.18513, loss=-0.02236
surrogate=-0.00438, entropy= 0.18522, loss=-0.00438
std_min= 0.20793, std_max= 0.30313, std_mean= 0.26052
val lr: [1.2551229508196733e-05], policy lr: [1.5061475409836076e-05]
Policy Loss: -0.0043778, | Entropy Bonus: -0, | Value Loss: 31.767, | Advantage Loss: 1.6612
Time elapsed (s): 1.6442599296569824
Agent stdevs: 0.26052406
--------------------------------------------------------------------------------

Step 927
++++++++ Policy training ++++++++++
Current mean reward: 1505.208220 | mean episode length: 406.000000
val_loss=18.95217
val_loss=10.00544
val_loss=15.00148
val_loss=10.46557
val_loss=10.20048
val_loss= 9.60691
val_loss= 9.45216
val_loss= 9.30856
val_loss= 7.15556
val_loss=11.48601
adv_loss= 2.46565
adv_loss= 7.36679
adv_loss= 2.20678
adv_loss= 1.44474
adv_loss= 1.12555
adv_loss= 1.85430
adv_loss= 3.12011
adv_loss= 3.61121
adv_loss= 1.40173
adv_loss= 2.58538
surrogate=-0.00988, entropy= 0.18511, loss=-0.00988
surrogate=-0.01758, entropy= 0.18490, loss=-0.01758
surrogate=-0.01354, entropy= 0.18467, loss=-0.01354
surrogate= 0.00611, entropy= 0.18432, loss= 0.00611
surrogate= 0.00587, entropy= 0.18418, loss= 0.00587
surrogate=-0.00905, entropy= 0.18389, loss=-0.00905
surrogate= 0.00284, entropy= 0.18372, loss= 0.00284
surrogate=-0.02748, entropy= 0.18355, loss=-0.02748
surrogate=-0.01222, entropy= 0.18334, loss=-0.01222
surrogate=-0.03603, entropy= 0.18312, loss=-0.03603
std_min= 0.20756, std_max= 0.30326, std_mean= 0.26037
val lr: [1.229508196721313e-05], policy lr: [1.4754098360655752e-05]
Policy Loss: -0.036035, | Entropy Bonus: -0, | Value Loss: 11.486, | Advantage Loss: 2.5854
Time elapsed (s): 1.6352386474609375
Agent stdevs: 0.2603744
--------------------------------------------------------------------------------

Step 928
++++++++ Policy training ++++++++++
Current mean reward: 1906.428087 | mean episode length: 515.000000
val_loss=11.78427
val_loss= 6.04328
val_loss= 5.10317
val_loss= 7.65234
val_loss= 6.89066
val_loss= 8.42649
val_loss= 3.86962
val_loss= 6.43095
val_loss= 4.79272
val_loss= 6.44008
adv_loss= 1.44298
adv_loss= 2.91660
adv_loss= 1.48614
adv_loss= 2.72392
adv_loss= 1.65578
adv_loss= 2.90865
adv_loss= 0.93455
adv_loss= 1.21023
adv_loss= 0.89197
adv_loss= 3.35994
surrogate=-0.00011, entropy= 0.18277, loss=-0.00011
surrogate= 0.02268, entropy= 0.18239, loss= 0.02268
surrogate= 0.00288, entropy= 0.18192, loss= 0.00288
surrogate=-0.02017, entropy= 0.18150, loss=-0.02017
surrogate= 0.00571, entropy= 0.18107, loss= 0.00571
surrogate= 0.02965, entropy= 0.18068, loss= 0.02965
surrogate=-0.00115, entropy= 0.18020, loss=-0.00115
surrogate=-0.03041, entropy= 0.17975, loss=-0.03041
surrogate=-0.00735, entropy= 0.17929, loss=-0.00735
surrogate=-0.00079, entropy= 0.17892, loss=-0.00079
std_min= 0.20725, std_max= 0.30306, std_mean= 0.26002
val lr: [1.2038934426229497e-05], policy lr: [1.4446721311475395e-05]
Policy Loss: -0.00079254, | Entropy Bonus: -0, | Value Loss: 6.4401, | Advantage Loss: 3.3599
Time elapsed (s): 1.643726110458374
Agent stdevs: 0.26002017
--------------------------------------------------------------------------------

Step 929
++++++++ Policy training ++++++++++
Current mean reward: 1493.193074 | mean episode length: 400.750000
val_loss=11.89570
val_loss=11.14729
val_loss= 7.12310
val_loss= 8.88036
val_loss= 9.86556
val_loss= 7.85467
val_loss= 6.76804
val_loss=11.19665
val_loss= 5.97280
val_loss= 7.95144
adv_loss= 1.10325
adv_loss= 1.94993
adv_loss= 1.09087
adv_loss= 0.99970
adv_loss= 5.71096
adv_loss= 1.71962
adv_loss= 0.98181
adv_loss= 1.04309
adv_loss= 1.79031
adv_loss= 2.44108
surrogate=-0.00637, entropy= 0.17877, loss=-0.00637
surrogate= 0.02395, entropy= 0.17872, loss= 0.02395
surrogate= 0.00268, entropy= 0.17866, loss= 0.00268
surrogate=-0.01052, entropy= 0.17867, loss=-0.01052
surrogate= 0.01937, entropy= 0.17870, loss= 0.01937
surrogate=-0.02937, entropy= 0.17861, loss=-0.02937
surrogate=-0.00368, entropy= 0.17863, loss=-0.00368
surrogate=-0.01400, entropy= 0.17854, loss=-0.01400
surrogate=-0.01596, entropy= 0.17848, loss=-0.01596
surrogate=-0.02598, entropy= 0.17844, loss=-0.02598
std_min= 0.20720, std_max= 0.30309, std_mean= 0.25998
val lr: [1.1782786885245893e-05], policy lr: [1.413934426229507e-05]
Policy Loss: -0.025978, | Entropy Bonus: -0, | Value Loss: 7.9514, | Advantage Loss: 2.4411
Time elapsed (s): 1.6290500164031982
Agent stdevs: 0.25998414
--------------------------------------------------------------------------------

Step 930
++++++++ Policy training ++++++++++
Current mean reward: 2640.090262 | mean episode length: 709.500000
val_loss= 4.74055
val_loss= 5.07768
val_loss= 6.94914
val_loss= 4.34915
val_loss= 4.36065
val_loss= 2.93599
val_loss= 4.27572
val_loss= 4.27653
val_loss= 4.17306
val_loss= 3.82520
adv_loss= 1.70488
adv_loss= 0.34509
adv_loss= 0.25163
adv_loss= 0.57583
adv_loss= 0.50029
adv_loss= 0.50436
adv_loss= 0.63652
adv_loss= 0.55964
adv_loss= 0.91253
adv_loss= 1.09044
surrogate=-0.00654, entropy= 0.17844, loss=-0.00654
surrogate= 0.00217, entropy= 0.17858, loss= 0.00217
surrogate=-0.03409, entropy= 0.17868, loss=-0.03409
surrogate=-0.00171, entropy= 0.17874, loss=-0.00171
surrogate=-0.01184, entropy= 0.17864, loss=-0.01184
surrogate=-0.00966, entropy= 0.17852, loss=-0.00966
surrogate=-0.00389, entropy= 0.17849, loss=-0.00389
surrogate=-0.01337, entropy= 0.17852, loss=-0.01337
surrogate=-0.02211, entropy= 0.17831, loss=-0.02211
surrogate=-0.01071, entropy= 0.17821, loss=-0.01071
std_min= 0.20716, std_max= 0.30313, std_mean= 0.25997
val lr: [1.152663934426229e-05], policy lr: [1.3831967213114747e-05]
Policy Loss: -0.010714, | Entropy Bonus: -0, | Value Loss: 3.8252, | Advantage Loss: 1.0904
Time elapsed (s): 1.6683783531188965
Agent stdevs: 0.25996897
--------------------------------------------------------------------------------

Step 931
++++++++ Policy training ++++++++++
Current mean reward: 1362.492708 | mean episode length: 367.000000
val_loss= 6.21885
val_loss= 6.15989
val_loss= 5.86737
val_loss= 9.80491
val_loss= 5.54619
val_loss= 8.48704
val_loss= 5.82521
val_loss= 5.01576
val_loss= 7.81430
val_loss= 6.00674
adv_loss= 1.98345
adv_loss= 1.35366
adv_loss= 1.33276
adv_loss= 2.01553
adv_loss= 0.93425
adv_loss= 0.60643
adv_loss= 2.00699
adv_loss= 0.99595
adv_loss= 2.12884
adv_loss= 2.29770
surrogate=-0.00231, entropy= 0.17819, loss=-0.00231
surrogate=-0.03058, entropy= 0.17802, loss=-0.03058
surrogate=-0.01469, entropy= 0.17800, loss=-0.01469
surrogate=-0.02743, entropy= 0.17787, loss=-0.02743
surrogate= 0.00428, entropy= 0.17778, loss= 0.00428
surrogate=-0.01754, entropy= 0.17768, loss=-0.01754
surrogate=-0.01686, entropy= 0.17759, loss=-0.01686
surrogate=-0.02632, entropy= 0.17749, loss=-0.02632
surrogate=-0.01439, entropy= 0.17740, loss=-0.01439
surrogate=-0.02764, entropy= 0.17726, loss=-0.02764
std_min= 0.20684, std_max= 0.30352, std_mean= 0.25993
val lr: [1.1270491803278688e-05], policy lr: [1.3524590163934423e-05]
Policy Loss: -0.027643, | Entropy Bonus: -0, | Value Loss: 6.0067, | Advantage Loss: 2.2977
Time elapsed (s): 1.6481454372406006
Agent stdevs: 0.25993192
--------------------------------------------------------------------------------

Step 932
++++++++ Policy training ++++++++++
Current mean reward: 1989.745793 | mean episode length: 540.000000
val_loss= 7.80085
val_loss= 5.83232
val_loss= 3.02472
val_loss= 3.49242
val_loss= 6.43581
val_loss= 3.81072
val_loss= 5.13853
val_loss= 3.69693
val_loss= 2.96284
val_loss= 2.95889
adv_loss= 1.77015
adv_loss= 2.87028
adv_loss= 0.70766
adv_loss= 0.55033
adv_loss= 0.60800
adv_loss= 0.92161
adv_loss= 1.12293
adv_loss= 0.38280
adv_loss= 1.15571
adv_loss= 1.22074
surrogate=-0.00077, entropy= 0.17739, loss=-0.00077
surrogate=-0.00064, entropy= 0.17749, loss=-0.00064
surrogate=-0.00391, entropy= 0.17751, loss=-0.00391
surrogate= 0.00391, entropy= 0.17746, loss= 0.00391
surrogate=-0.01022, entropy= 0.17756, loss=-0.01022
surrogate=-0.01007, entropy= 0.17751, loss=-0.01007
surrogate=-0.00977, entropy= 0.17757, loss=-0.00977
surrogate=-0.02708, entropy= 0.17756, loss=-0.02708
surrogate=-0.02591, entropy= 0.17756, loss=-0.02591
surrogate=-0.02176, entropy= 0.17754, loss=-0.02176
std_min= 0.20682, std_max= 0.30357, std_mean= 0.25996
val lr: [1.1014344262295085e-05], policy lr: [1.32172131147541e-05]
Policy Loss: -0.021758, | Entropy Bonus: -0, | Value Loss: 2.9589, | Advantage Loss: 1.2207
Time elapsed (s): 1.6416289806365967
Agent stdevs: 0.25996062
--------------------------------------------------------------------------------

Step 933
++++++++ Policy training ++++++++++
Current mean reward: 1650.154169 | mean episode length: 441.250000
val_loss= 4.83313
val_loss= 5.69241
val_loss= 6.23452
val_loss= 4.87771
val_loss= 3.22738
val_loss= 6.23578
val_loss= 6.10525
val_loss= 4.88935
val_loss= 7.30599
val_loss= 5.69242
adv_loss= 1.40603
adv_loss= 0.94149
adv_loss= 1.29956
adv_loss= 3.10049
adv_loss= 1.29423
adv_loss= 3.97677
adv_loss= 1.95354
adv_loss= 1.09978
adv_loss= 1.38434
adv_loss= 0.55293
surrogate= 0.00623, entropy= 0.17774, loss= 0.00623
surrogate=-0.00064, entropy= 0.17769, loss=-0.00064
surrogate=-0.00498, entropy= 0.17755, loss=-0.00498
surrogate= 0.00562, entropy= 0.17754, loss= 0.00562
surrogate=-0.00240, entropy= 0.17740, loss=-0.00240
surrogate= 0.00305, entropy= 0.17727, loss= 0.00305
surrogate= 0.02417, entropy= 0.17720, loss= 0.02417
surrogate=-0.02872, entropy= 0.17710, loss=-0.02872
surrogate= 0.00006, entropy= 0.17691, loss= 0.00006
surrogate= 0.01506, entropy= 0.17681, loss= 0.01506
std_min= 0.20680, std_max= 0.30344, std_mean= 0.25989
val lr: [1.0758196721311481e-05], policy lr: [1.2909836065573775e-05]
Policy Loss: 0.015058, | Entropy Bonus: -0, | Value Loss: 5.6924, | Advantage Loss: 0.55293
Time elapsed (s): 1.640089511871338
Agent stdevs: 0.25989172
--------------------------------------------------------------------------------

Step 934
++++++++ Policy training ++++++++++
Current mean reward: 2019.782751 | mean episode length: 542.333333
val_loss= 9.89300
val_loss= 5.77344
val_loss= 6.83223
val_loss= 2.72127
val_loss= 3.47680
val_loss= 5.25822
val_loss= 4.09134
val_loss= 3.60201
val_loss= 3.64789
val_loss= 3.26741
adv_loss= 0.72166
adv_loss= 1.05834
adv_loss= 0.71634
adv_loss= 0.68245
adv_loss= 0.69142
adv_loss= 0.38656
adv_loss= 1.01350
adv_loss= 0.68863
adv_loss= 0.64877
adv_loss= 0.48054
surrogate=-0.00117, entropy= 0.17669, loss=-0.00117
surrogate=-0.00231, entropy= 0.17650, loss=-0.00231
surrogate=-0.00692, entropy= 0.17646, loss=-0.00692
surrogate=-0.00338, entropy= 0.17632, loss=-0.00338
surrogate=-0.01389, entropy= 0.17618, loss=-0.01389
surrogate=-0.03060, entropy= 0.17612, loss=-0.03060
surrogate= 0.00015, entropy= 0.17591, loss= 0.00015
surrogate= 0.01140, entropy= 0.17572, loss= 0.01140
surrogate=-0.01545, entropy= 0.17556, loss=-0.01545
surrogate=-0.03586, entropy= 0.17550, loss=-0.03586
std_min= 0.20658, std_max= 0.30334, std_mean= 0.25979
val lr: [1.0502049180327878e-05], policy lr: [1.2602459016393451e-05]
Policy Loss: -0.035864, | Entropy Bonus: -0, | Value Loss: 3.2674, | Advantage Loss: 0.48054
Time elapsed (s): 1.653740644454956
Agent stdevs: 0.2597917
--------------------------------------------------------------------------------

Step 935
++++++++ Policy training ++++++++++
Current mean reward: 1528.563396 | mean episode length: 416.500000
val_loss=314.46487
val_loss=65.09264
val_loss= 9.53891
val_loss= 9.82113
val_loss=32.56437
val_loss= 7.58213
val_loss=47.00396
val_loss=2514.29907
val_loss=148.17993
val_loss=18.67197
adv_loss= 1.13685
adv_loss= 2.01652
adv_loss= 1.92082
adv_loss=1935.78784
adv_loss= 0.60346
adv_loss= 1.33360
adv_loss= 1.45453
adv_loss= 0.72523
adv_loss= 0.90042
adv_loss= 0.90970
surrogate=-0.00079, entropy= 0.17549, loss=-0.00079
surrogate=-0.00168, entropy= 0.17551, loss=-0.00168
surrogate= 0.00687, entropy= 0.17561, loss= 0.00687
surrogate=-0.01328, entropy= 0.17566, loss=-0.01328
surrogate=-0.01581, entropy= 0.17570, loss=-0.01581
surrogate=-0.01013, entropy= 0.17566, loss=-0.01013
surrogate= 0.00501, entropy= 0.17553, loss= 0.00501
surrogate=-0.01283, entropy= 0.17546, loss=-0.01283
surrogate=-0.00004, entropy= 0.17542, loss=-0.00004
surrogate=-0.02306, entropy= 0.17538, loss=-0.02306
std_min= 0.20656, std_max= 0.30344, std_mean= 0.25979
val lr: [1.0245901639344274e-05], policy lr: [1.2295081967213128e-05]
Policy Loss: -0.023061, | Entropy Bonus: -0, | Value Loss: 18.672, | Advantage Loss: 0.9097
Time elapsed (s): 1.6294081211090088
Agent stdevs: 0.2597873
--------------------------------------------------------------------------------

Step 936
++++++++ Policy training ++++++++++
Current mean reward: 2235.428426 | mean episode length: 595.333333
val_loss= 4.11027
val_loss= 7.38945
val_loss= 6.21461
val_loss= 5.31721
val_loss= 3.57885
val_loss= 3.85556
val_loss= 3.58542
val_loss= 3.90599
val_loss= 2.92695
val_loss= 3.23529
adv_loss= 0.47942
adv_loss= 0.52101
adv_loss= 0.38888
adv_loss= 1.50083
adv_loss= 0.55937
adv_loss= 0.77942
adv_loss= 0.39250
adv_loss= 0.49351
adv_loss= 3.72725
adv_loss= 1.07828
surrogate= 0.00463, entropy= 0.17535, loss= 0.00463
surrogate=-0.00625, entropy= 0.17528, loss=-0.00625
surrogate=-0.01390, entropy= 0.17524, loss=-0.01390
surrogate=-0.01039, entropy= 0.17518, loss=-0.01039
surrogate=-0.02205, entropy= 0.17508, loss=-0.02205
surrogate=-0.01334, entropy= 0.17509, loss=-0.01334
surrogate=-0.01050, entropy= 0.17511, loss=-0.01050
surrogate=-0.01002, entropy= 0.17507, loss=-0.01002
surrogate=-0.01036, entropy= 0.17504, loss=-0.01036
surrogate=-0.01999, entropy= 0.17510, loss=-0.01999
std_min= 0.20645, std_max= 0.30350, std_mean= 0.25978
val lr: [9.989754098360644e-06], policy lr: [1.198770491803277e-05]
Policy Loss: -0.019987, | Entropy Bonus: -0, | Value Loss: 3.2353, | Advantage Loss: 1.0783
Time elapsed (s): 1.6863832473754883
Agent stdevs: 0.2597752
--------------------------------------------------------------------------------

Step 937
++++++++ Policy training ++++++++++
Current mean reward: 2155.590925 | mean episode length: 583.666667
val_loss= 7.23501
val_loss=11.93642
val_loss=10.45179
val_loss=13.54540
val_loss= 4.99950
val_loss= 9.10665
val_loss=10.07038
val_loss= 9.09833
val_loss= 4.48876
val_loss= 8.91740
adv_loss= 1.37623
adv_loss= 1.40102
adv_loss= 0.50961
adv_loss= 0.77151
adv_loss= 1.11638
adv_loss= 1.24417
adv_loss= 1.49928
adv_loss= 1.54661
adv_loss= 1.05113
adv_loss= 0.44789
surrogate=-0.00043, entropy= 0.17522, loss=-0.00043
surrogate=-0.00014, entropy= 0.17532, loss=-0.00014
surrogate=-0.00653, entropy= 0.17541, loss=-0.00653
surrogate=-0.00718, entropy= 0.17549, loss=-0.00718
surrogate=-0.01218, entropy= 0.17565, loss=-0.01218
surrogate=-0.02774, entropy= 0.17579, loss=-0.02774
surrogate=-0.01520, entropy= 0.17590, loss=-0.01520
surrogate=-0.01495, entropy= 0.17607, loss=-0.01495
surrogate=-0.01292, entropy= 0.17626, loss=-0.01292
surrogate=-0.00238, entropy= 0.17635, loss=-0.00238
std_min= 0.20640, std_max= 0.30383, std_mean= 0.25990
val lr: [9.73360655737704e-06], policy lr: [1.1680327868852446e-05]
Policy Loss: -0.0023808, | Entropy Bonus: -0, | Value Loss: 8.9174, | Advantage Loss: 0.44789
Time elapsed (s): 1.6442248821258545
Agent stdevs: 0.25990456
--------------------------------------------------------------------------------

Step 938
++++++++ Policy training ++++++++++
Current mean reward: 1352.599293 | mean episode length: 368.200000
val_loss=13.49965
val_loss=21.62831
val_loss= 9.55195
val_loss=18.67140
val_loss= 9.84742
val_loss=18.37992
val_loss=14.65783
val_loss=11.51173
val_loss=10.05070
val_loss= 8.23205
adv_loss= 1.87527
adv_loss= 2.24376
adv_loss= 0.65096
adv_loss= 2.03504
adv_loss= 3.23732
adv_loss= 0.91072
adv_loss= 1.79012
adv_loss= 1.13801
adv_loss= 1.41749
adv_loss= 6.27926
surrogate=-0.00854, entropy= 0.17624, loss=-0.00854
surrogate= 0.00142, entropy= 0.17627, loss= 0.00142
surrogate=-0.00225, entropy= 0.17626, loss=-0.00225
surrogate=-0.00418, entropy= 0.17614, loss=-0.00418
surrogate=-0.02111, entropy= 0.17605, loss=-0.02111
surrogate=-0.01181, entropy= 0.17595, loss=-0.01181
surrogate= 0.01285, entropy= 0.17584, loss= 0.01285
surrogate=-0.00660, entropy= 0.17575, loss=-0.00660
surrogate=-0.02276, entropy= 0.17571, loss=-0.02276
surrogate=-0.02348, entropy= 0.17554, loss=-0.02348
std_min= 0.20637, std_max= 0.30360, std_mean= 0.25983
val lr: [9.477459016393437e-06], policy lr: [1.1372950819672122e-05]
Policy Loss: -0.023479, | Entropy Bonus: -0, | Value Loss: 8.232, | Advantage Loss: 6.2793
Time elapsed (s): 1.6826064586639404
Agent stdevs: 0.25982657
--------------------------------------------------------------------------------

Step 939
++++++++ Policy training ++++++++++
Current mean reward: 2108.674187 | mean episode length: 583.333333
val_loss=11.26239
val_loss= 5.46896
val_loss=93.26759
val_loss=511.92075
val_loss=23.33836
val_loss=878.90302
val_loss=1542.03491
val_loss=1692.10742
val_loss=11.87460
val_loss=1061.65601
adv_loss= 0.54674
adv_loss= 0.50400
adv_loss= 0.52703
adv_loss= 0.63574
adv_loss= 0.49864
adv_loss= 0.82242
adv_loss= 0.65519
adv_loss= 0.84411
adv_loss= 0.96240
adv_loss= 3.67394
surrogate=-0.00538, entropy= 0.17557, loss=-0.00538
surrogate= 0.00818, entropy= 0.17565, loss= 0.00818
surrogate=-0.01889, entropy= 0.17574, loss=-0.01889
surrogate=-0.01774, entropy= 0.17588, loss=-0.01774
surrogate= 0.00854, entropy= 0.17591, loss= 0.00854
surrogate=-0.00631, entropy= 0.17591, loss=-0.00631
surrogate= 0.00869, entropy= 0.17594, loss= 0.00869
surrogate=-0.01210, entropy= 0.17587, loss=-0.01210
surrogate=-0.01478, entropy= 0.17598, loss=-0.01478
surrogate=-0.00102, entropy= 0.17596, loss=-0.00102
std_min= 0.20629, std_max= 0.30361, std_mean= 0.25987
val lr: [9.221311475409833e-06], policy lr: [1.1065573770491798e-05]
Policy Loss: -0.0010207, | Entropy Bonus: -0, | Value Loss: 1061.7, | Advantage Loss: 3.6739
Time elapsed (s): 1.679619312286377
Agent stdevs: 0.25987417
--------------------------------------------------------------------------------

Step 940
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 1876.2
++++++++ Policy training ++++++++++
Current mean reward: 2162.767465 | mean episode length: 585.000000
val_loss=10.68581
val_loss= 8.45302
val_loss=10.43619
val_loss= 9.40655
val_loss= 5.27994
val_loss=15.22019
val_loss= 4.17054
val_loss= 7.93505
val_loss= 8.35040
val_loss= 7.52820
adv_loss= 0.95131
adv_loss= 1.21878
adv_loss= 0.88950
adv_loss= 0.43767
adv_loss= 0.82489
adv_loss= 0.62991
adv_loss= 1.20920
adv_loss= 0.84962
adv_loss= 1.27205
adv_loss= 0.56351
surrogate=-0.00628, entropy= 0.17605, loss=-0.00628
surrogate= 0.00316, entropy= 0.17623, loss= 0.00316
surrogate=-0.02110, entropy= 0.17629, loss=-0.02110
surrogate=-0.02349, entropy= 0.17639, loss=-0.02349
surrogate=-0.00592, entropy= 0.17634, loss=-0.00592
surrogate=-0.04183, entropy= 0.17633, loss=-0.04183
surrogate=-0.03444, entropy= 0.17636, loss=-0.03444
surrogate=-0.02290, entropy= 0.17631, loss=-0.02290
surrogate=-0.02297, entropy= 0.17636, loss=-0.02297
surrogate=-0.00345, entropy= 0.17636, loss=-0.00345
std_min= 0.20637, std_max= 0.30370, std_mean= 0.25990
val lr: [8.96516393442623e-06], policy lr: [1.0758196721311474e-05]
Policy Loss: -0.0034488, | Entropy Bonus: -0, | Value Loss: 7.5282, | Advantage Loss: 0.56351
Time elapsed (s): 1.6985979080200195
Agent stdevs: 0.2599046
--------------------------------------------------------------------------------

Step 941
++++++++ Policy training ++++++++++
Current mean reward: 2004.091045 | mean episode length: 543.666667
val_loss= 6.73114
val_loss= 6.47658
val_loss= 6.76095
val_loss= 6.25357
val_loss= 5.46913
val_loss= 4.51372
val_loss= 4.12263
val_loss= 4.27167
val_loss= 3.16579
val_loss= 4.88813
adv_loss= 0.51253
adv_loss= 0.70266
adv_loss= 1.92958
adv_loss= 0.53821
adv_loss= 0.72931
adv_loss= 0.41443
adv_loss= 0.55739
adv_loss= 0.87484
adv_loss= 0.43463
adv_loss= 1.33995
surrogate=-0.00132, entropy= 0.17635, loss=-0.00132
surrogate=-0.00273, entropy= 0.17636, loss=-0.00273
surrogate= 0.01537, entropy= 0.17639, loss= 0.01537
surrogate=-0.02767, entropy= 0.17633, loss=-0.02767
surrogate=-0.00251, entropy= 0.17626, loss=-0.00251
surrogate=-0.01129, entropy= 0.17623, loss=-0.01129
surrogate= 0.00044, entropy= 0.17623, loss= 0.00044
surrogate=-0.00067, entropy= 0.17613, loss=-0.00067
surrogate= 0.00200, entropy= 0.17613, loss= 0.00200
surrogate=-0.02601, entropy= 0.17611, loss=-0.02601
std_min= 0.20638, std_max= 0.30385, std_mean= 0.25989
val lr: [8.709016393442626e-06], policy lr: [1.045081967213115e-05]
Policy Loss: -0.026013, | Entropy Bonus: -0, | Value Loss: 4.8881, | Advantage Loss: 1.34
Time elapsed (s): 1.6645739078521729
Agent stdevs: 0.25988662
--------------------------------------------------------------------------------

Step 942
++++++++ Policy training ++++++++++
Current mean reward: 2279.383979 | mean episode length: 619.666667
val_loss= 6.29650
val_loss= 7.18493
val_loss= 5.39427
val_loss= 6.42918
val_loss= 7.30208
val_loss= 6.38380
val_loss=10.75738
val_loss= 7.31111
val_loss= 3.99774
val_loss= 4.96640
adv_loss= 0.51616
adv_loss= 0.81231
adv_loss= 1.18670
adv_loss= 2.02093
adv_loss= 0.46412
adv_loss= 0.37903
adv_loss= 1.61813
adv_loss= 1.24309
adv_loss= 0.47008
adv_loss= 0.36221
surrogate=-0.00239, entropy= 0.17589, loss=-0.00239
surrogate=-0.00210, entropy= 0.17581, loss=-0.00210
surrogate=-0.00685, entropy= 0.17558, loss=-0.00685
surrogate=-0.00817, entropy= 0.17533, loss=-0.00817
surrogate=-0.01673, entropy= 0.17512, loss=-0.01673
surrogate=-0.02614, entropy= 0.17489, loss=-0.02614
surrogate=-0.02428, entropy= 0.17470, loss=-0.02428
surrogate=-0.00776, entropy= 0.17451, loss=-0.00776
surrogate=-0.01517, entropy= 0.17436, loss=-0.01517
surrogate=-0.02656, entropy= 0.17420, loss=-0.02656
std_min= 0.20615, std_max= 0.30351, std_mean= 0.25973
val lr: [8.452868852459022e-06], policy lr: [1.0143442622950827e-05]
Policy Loss: -0.026563, | Entropy Bonus: -0, | Value Loss: 4.9664, | Advantage Loss: 0.36221
Time elapsed (s): 1.7004482746124268
Agent stdevs: 0.25972572
--------------------------------------------------------------------------------

Step 943
++++++++ Policy training ++++++++++
Current mean reward: 3244.267483 | mean episode length: 899.000000
val_loss=1290.97913
val_loss=1213.70166
val_loss= 7.13936
val_loss= 9.54095
val_loss=1275.36230
val_loss=733.32629
val_loss=16.16346
val_loss=1712.93848
val_loss=54.23022
val_loss=72.02466
adv_loss= 0.60294
adv_loss= 0.39824
adv_loss= 0.86281
adv_loss= 0.77581
adv_loss= 0.44648
adv_loss= 0.56203
adv_loss= 0.52273
adv_loss= 0.60132
adv_loss= 0.56874
adv_loss= 0.87437
surrogate= 0.00381, entropy= 0.17405, loss= 0.00381
surrogate= 0.00284, entropy= 0.17409, loss= 0.00284
surrogate=-0.01321, entropy= 0.17400, loss=-0.01321
surrogate=-0.01129, entropy= 0.17400, loss=-0.01129
surrogate=-0.01542, entropy= 0.17401, loss=-0.01542
surrogate=-0.00526, entropy= 0.17393, loss=-0.00526
surrogate= 0.00836, entropy= 0.17386, loss= 0.00836
surrogate=-0.02030, entropy= 0.17378, loss=-0.02030
surrogate= 0.01115, entropy= 0.17368, loss= 0.01115
surrogate=-0.01928, entropy= 0.17363, loss=-0.01928
std_min= 0.20624, std_max= 0.30329, std_mean= 0.25966
val lr: [8.196721311475419e-06], policy lr: [9.836065573770501e-06]
Policy Loss: -0.01928, | Entropy Bonus: -0, | Value Loss: 72.025, | Advantage Loss: 0.87437
Time elapsed (s): 1.6666522026062012
Agent stdevs: 0.25965667
--------------------------------------------------------------------------------

Step 944
++++++++ Policy training ++++++++++
Current mean reward: 2160.664221 | mean episode length: 584.000000
val_loss= 8.77254
val_loss=10.84699
val_loss= 8.78035
val_loss= 6.11858
val_loss= 8.89570
val_loss= 6.44871
val_loss= 7.72350
val_loss= 5.76215
val_loss= 6.52558
val_loss= 6.59910
adv_loss= 0.81535
adv_loss= 0.86900
adv_loss= 0.43647
adv_loss= 0.78278
adv_loss= 0.65899
adv_loss= 0.64016
adv_loss= 0.86675
adv_loss= 0.49996
adv_loss= 0.58608
adv_loss= 0.65279
surrogate= 0.00269, entropy= 0.17371, loss= 0.00269
surrogate=-0.00155, entropy= 0.17381, loss=-0.00155
surrogate=-0.00655, entropy= 0.17399, loss=-0.00655
surrogate=-0.00662, entropy= 0.17406, loss=-0.00662
surrogate=-0.02748, entropy= 0.17419, loss=-0.02748
surrogate=-0.02117, entropy= 0.17435, loss=-0.02117
surrogate=-0.00074, entropy= 0.17444, loss=-0.00074
surrogate=-0.02385, entropy= 0.17456, loss=-0.02385
surrogate= 0.00499, entropy= 0.17462, loss= 0.00499
surrogate=-0.02203, entropy= 0.17471, loss=-0.02203
std_min= 0.20639, std_max= 0.30345, std_mean= 0.25974
val lr: [7.940573770491815e-06], policy lr: [9.528688524590177e-06]
Policy Loss: -0.022025, | Entropy Bonus: -0, | Value Loss: 6.5991, | Advantage Loss: 0.65279
Time elapsed (s): 1.6650323867797852
Agent stdevs: 0.2597446
--------------------------------------------------------------------------------

Step 945
++++++++ Policy training ++++++++++
Current mean reward: 2482.192522 | mean episode length: 670.000000
val_loss= 7.59969
val_loss=10.02701
val_loss= 5.99142
val_loss= 4.32515
val_loss= 7.52350
val_loss= 5.40724
val_loss= 4.73322
val_loss= 5.63844
val_loss= 6.64820
val_loss= 4.47992
adv_loss= 0.74629
adv_loss= 0.46705
adv_loss= 2.23894
adv_loss= 0.28804
adv_loss= 0.61327
adv_loss= 0.43841
adv_loss= 0.97983
adv_loss= 0.48559
adv_loss= 0.60153
adv_loss= 0.69135
surrogate= 0.00464, entropy= 0.17468, loss= 0.00464
surrogate= 0.01093, entropy= 0.17469, loss= 0.01093
surrogate=-0.02049, entropy= 0.17472, loss=-0.02049
surrogate=-0.01168, entropy= 0.17469, loss=-0.01168
surrogate=-0.00350, entropy= 0.17463, loss=-0.00350
surrogate=-0.00469, entropy= 0.17466, loss=-0.00469
surrogate=-0.00149, entropy= 0.17456, loss=-0.00149
surrogate=-0.01192, entropy= 0.17454, loss=-0.01192
surrogate=-0.00338, entropy= 0.17450, loss=-0.00338
surrogate=-0.01289, entropy= 0.17445, loss=-0.01289
std_min= 0.20644, std_max= 0.30348, std_mean= 0.25972
val lr: [7.684426229508185e-06], policy lr: [9.221311475409821e-06]
Policy Loss: -0.012885, | Entropy Bonus: -0, | Value Loss: 4.4799, | Advantage Loss: 0.69135
Time elapsed (s): 1.667834758758545
Agent stdevs: 0.25971684
--------------------------------------------------------------------------------

Step 946
++++++++ Policy training ++++++++++
Current mean reward: 3429.082778 | mean episode length: 933.000000
val_loss=11.35446
val_loss=15.28481
val_loss= 8.64592
val_loss= 7.85590
val_loss= 4.31997
val_loss= 5.07054
val_loss= 8.23967
val_loss= 9.07783
val_loss=10.98063
val_loss= 5.73799
adv_loss= 0.69144
adv_loss= 0.72614
adv_loss= 0.62556
adv_loss= 1.57653
adv_loss= 1.02743
adv_loss= 0.91576
adv_loss= 0.81809
adv_loss= 0.51733
adv_loss= 2.16074
adv_loss= 1.78238
surrogate=-0.00287, entropy= 0.17440, loss=-0.00287
surrogate= 0.00267, entropy= 0.17440, loss= 0.00267
surrogate=-0.00374, entropy= 0.17438, loss=-0.00374
surrogate=-0.01081, entropy= 0.17435, loss=-0.01081
surrogate= 0.00540, entropy= 0.17424, loss= 0.00540
surrogate=-0.01719, entropy= 0.17415, loss=-0.01719
surrogate= 0.00243, entropy= 0.17410, loss= 0.00243
surrogate= 0.00189, entropy= 0.17403, loss= 0.00189
surrogate=-0.01709, entropy= 0.17391, loss=-0.01709
surrogate=-0.00102, entropy= 0.17381, loss=-0.00102
std_min= 0.20636, std_max= 0.30343, std_mean= 0.25967
val lr: [7.428278688524581e-06], policy lr: [8.913934426229497e-06]
Policy Loss: -0.0010153, | Entropy Bonus: -0, | Value Loss: 5.738, | Advantage Loss: 1.7824
Time elapsed (s): 1.677269458770752
Agent stdevs: 0.2596661
--------------------------------------------------------------------------------

Step 947
++++++++ Policy training ++++++++++
Current mean reward: 2465.362866 | mean episode length: 672.000000
val_loss= 6.95492
val_loss=102.50909
val_loss=53.99953
val_loss=19.76578
val_loss=15.70502
val_loss=82.60411
val_loss=12.55466
val_loss= 9.73044
val_loss=12.46953
val_loss=11.57175
adv_loss= 0.43872
adv_loss= 1.04573
adv_loss= 0.64868
adv_loss= 0.79439
adv_loss= 1.60721
adv_loss= 1.69891
adv_loss= 0.52635
adv_loss= 0.41867
adv_loss= 0.97335
adv_loss= 0.41693
surrogate=-0.00035, entropy= 0.17369, loss=-0.00035
surrogate=-0.00258, entropy= 0.17350, loss=-0.00258
surrogate=-0.00596, entropy= 0.17342, loss=-0.00596
surrogate=-0.00875, entropy= 0.17328, loss=-0.00875
surrogate=-0.00520, entropy= 0.17309, loss=-0.00520
surrogate=-0.01823, entropy= 0.17294, loss=-0.01823
surrogate=-0.00770, entropy= 0.17281, loss=-0.00770
surrogate=-0.02020, entropy= 0.17265, loss=-0.02020
surrogate=-0.00373, entropy= 0.17255, loss=-0.00373
surrogate=-0.00284, entropy= 0.17242, loss=-0.00284
std_min= 0.20632, std_max= 0.30302, std_mean= 0.25953
val lr: [7.172131147540978e-06], policy lr: [8.606557377049172e-06]
Policy Loss: -0.0028402, | Entropy Bonus: -0, | Value Loss: 11.572, | Advantage Loss: 0.41693
Time elapsed (s): 1.681006669998169
Agent stdevs: 0.25952902
--------------------------------------------------------------------------------

Step 948
++++++++ Policy training ++++++++++
Current mean reward: 1478.057799 | mean episode length: 406.400000
val_loss= 8.97292
val_loss=26.29396
val_loss=17.23309
val_loss=25.35333
val_loss= 9.84637
val_loss=22.93096
val_loss= 8.37941
val_loss= 3.83800
val_loss=12.18164
val_loss=10.04796
adv_loss= 0.90276
adv_loss= 1.21842
adv_loss= 2.13796
adv_loss= 1.62404
adv_loss= 1.39867
adv_loss= 1.02029
adv_loss= 2.52825
adv_loss= 2.90597
adv_loss= 2.77148
adv_loss= 2.96043
surrogate= 0.00119, entropy= 0.17228, loss= 0.00119
surrogate=-0.01090, entropy= 0.17216, loss=-0.01090
surrogate=-0.01357, entropy= 0.17208, loss=-0.01357
surrogate=-0.00776, entropy= 0.17199, loss=-0.00776
surrogate=-0.01062, entropy= 0.17192, loss=-0.01062
surrogate=-0.00433, entropy= 0.17174, loss=-0.00433
surrogate=-0.00959, entropy= 0.17154, loss=-0.00959
surrogate=-0.01178, entropy= 0.17141, loss=-0.01178
surrogate=-0.02343, entropy= 0.17132, loss=-0.02343
surrogate=-0.02023, entropy= 0.17113, loss=-0.02023
std_min= 0.20611, std_max= 0.30298, std_mean= 0.25943
val lr: [6.915983606557374e-06], policy lr: [8.299180327868848e-06]
Policy Loss: -0.020227, | Entropy Bonus: -0, | Value Loss: 10.048, | Advantage Loss: 2.9604
Time elapsed (s): 1.7382802963256836
Agent stdevs: 0.25943354
--------------------------------------------------------------------------------

Step 949
++++++++ Policy training ++++++++++
Current mean reward: 1698.261918 | mean episode length: 456.333333
val_loss=10.46791
val_loss=20.96268
val_loss= 9.00557
val_loss= 8.34907
val_loss= 9.49268
val_loss= 8.04343
val_loss=14.04508
val_loss= 9.17494
val_loss= 5.68848
val_loss= 9.88048
adv_loss= 0.43164
adv_loss= 1.04105
adv_loss= 1.79925
adv_loss= 0.55810
adv_loss= 0.66557
adv_loss= 0.71957
adv_loss= 0.81982
adv_loss= 1.00452
adv_loss= 0.54582
adv_loss= 0.63590
surrogate=-0.00047, entropy= 0.17108, loss=-0.00047
surrogate=-0.00086, entropy= 0.17112, loss=-0.00086
surrogate= 0.00008, entropy= 0.17107, loss= 0.00008
surrogate= 0.00124, entropy= 0.17105, loss= 0.00124
surrogate=-0.00378, entropy= 0.17108, loss=-0.00378
surrogate=-0.00924, entropy= 0.17098, loss=-0.00924
surrogate=-0.02212, entropy= 0.17090, loss=-0.02212
surrogate= 0.01677, entropy= 0.17087, loss= 0.01677
surrogate= 0.01049, entropy= 0.17081, loss= 0.01049
surrogate=-0.01043, entropy= 0.17078, loss=-0.01043
std_min= 0.20618, std_max= 0.30274, std_mean= 0.25939
val lr: [6.659836065573771e-06], policy lr: [7.991803278688524e-06]
Policy Loss: -0.010433, | Entropy Bonus: -0, | Value Loss: 9.8805, | Advantage Loss: 0.6359
Time elapsed (s): 1.6672003269195557
Agent stdevs: 0.259387
--------------------------------------------------------------------------------

Step 950
++++++++ Policy training ++++++++++
Current mean reward: 2164.754195 | mean episode length: 600.666667
val_loss=284.01190
val_loss=257.05875
val_loss=11.50515
val_loss=460.33847
val_loss=21.19776
val_loss=79.62376
val_loss=168.72885
val_loss=12.10634
val_loss=2300.86768
val_loss=1253.56653
adv_loss= 0.47412
adv_loss= 0.67861
adv_loss= 1.87223
adv_loss= 0.82730
adv_loss= 0.62799
adv_loss= 1.66829
adv_loss= 0.60663
adv_loss= 2.91541
adv_loss= 0.87082
adv_loss= 1.26045
surrogate=-0.00431, entropy= 0.17064, loss=-0.00431
surrogate= 0.00194, entropy= 0.17047, loss= 0.00194
surrogate=-0.00104, entropy= 0.17036, loss=-0.00104
surrogate=-0.00990, entropy= 0.17023, loss=-0.00990
surrogate=-0.01441, entropy= 0.17018, loss=-0.01441
surrogate=-0.00065, entropy= 0.17008, loss=-0.00065
surrogate= 0.00393, entropy= 0.16996, loss= 0.00393
surrogate=-0.01316, entropy= 0.16990, loss=-0.01316
surrogate= 0.00145, entropy= 0.16983, loss= 0.00145
surrogate=-0.02656, entropy= 0.16979, loss=-0.02656
std_min= 0.20620, std_max= 0.30252, std_mean= 0.25929
val lr: [6.403688524590167e-06], policy lr: [7.6844262295082e-06]
Policy Loss: -0.026565, | Entropy Bonus: -0, | Value Loss: 1253.6, | Advantage Loss: 1.2604
Time elapsed (s): 1.6845955848693848
Agent stdevs: 0.2592877
--------------------------------------------------------------------------------

Step 951
++++++++ Policy training ++++++++++
Current mean reward: 2274.519459 | mean episode length: 641.500000
val_loss=1308.04431
val_loss=1371.04126
val_loss=17.16186
val_loss=495.40543
val_loss=44.13952
val_loss=615.20776
val_loss=24.86900
val_loss=181.10201
val_loss=1380.81812
val_loss=101.73315
adv_loss= 0.42207
adv_loss= 3.54241
adv_loss= 3.19848
adv_loss= 0.86290
adv_loss= 0.70668
adv_loss= 1.06925
adv_loss= 1.12181
adv_loss= 0.61055
adv_loss= 0.76917
adv_loss= 1.60794
surrogate= 0.00117, entropy= 0.16965, loss= 0.00117
surrogate=-0.00455, entropy= 0.16951, loss=-0.00455
surrogate=-0.00604, entropy= 0.16934, loss=-0.00604
surrogate= 0.00379, entropy= 0.16926, loss= 0.00379
surrogate= 0.00564, entropy= 0.16922, loss= 0.00564
surrogate=-0.00390, entropy= 0.16907, loss=-0.00390
surrogate=-0.01055, entropy= 0.16901, loss=-0.01055
surrogate= 0.01485, entropy= 0.16891, loss= 0.01485
surrogate=-0.00795, entropy= 0.16892, loss=-0.00795
surrogate=-0.01326, entropy= 0.16876, loss=-0.01326
std_min= 0.20604, std_max= 0.30243, std_mean= 0.25921
val lr: [6.147540983606565e-06], policy lr: [7.377049180327876e-06]
Policy Loss: -0.013257, | Entropy Bonus: -0, | Value Loss: 101.73, | Advantage Loss: 1.6079
Time elapsed (s): 1.6801347732543945
Agent stdevs: 0.25920817
--------------------------------------------------------------------------------

Step 952
++++++++ Policy training ++++++++++
Current mean reward: 2362.646125 | mean episode length: 662.500000
val_loss=944.57965
val_loss=1091.59888
val_loss=1775.45435
val_loss=16.93106
val_loss=19.89605
val_loss=254.01419
val_loss=25.64753
val_loss=47.95647
val_loss=31.18623
val_loss=1280.85779
adv_loss= 0.75732
adv_loss= 0.38591
adv_loss= 1.65254
adv_loss= 1.22767
adv_loss= 2.63121
adv_loss= 0.68671
adv_loss= 0.57612
adv_loss= 1.15987
adv_loss=1967.96216
adv_loss= 0.47820
surrogate= 0.00028, entropy= 0.16874, loss= 0.00028
surrogate=-0.00406, entropy= 0.16868, loss=-0.00406
surrogate=-0.00291, entropy= 0.16862, loss=-0.00291
surrogate= 0.00226, entropy= 0.16852, loss= 0.00226
surrogate=-0.01000, entropy= 0.16845, loss=-0.01000
surrogate=-0.00915, entropy= 0.16841, loss=-0.00915
surrogate=-0.00250, entropy= 0.16840, loss=-0.00250
surrogate=-0.01312, entropy= 0.16841, loss=-0.01312
surrogate=-0.01823, entropy= 0.16837, loss=-0.01823
surrogate=-0.00985, entropy= 0.16834, loss=-0.00985
std_min= 0.20593, std_max= 0.30250, std_mean= 0.25918
val lr: [5.891393442622961e-06], policy lr: [7.069672131147552e-06]
Policy Loss: -0.0098454, | Entropy Bonus: -0, | Value Loss: 1280.9, | Advantage Loss: 0.4782
Time elapsed (s): 1.6689403057098389
Agent stdevs: 0.25918433
--------------------------------------------------------------------------------

Step 953
++++++++ Policy training ++++++++++
Current mean reward: 2270.732945 | mean episode length: 640.500000
val_loss=23.73752
val_loss=13.36027
val_loss=297.67624
val_loss=26.39808
val_loss=796.02155
val_loss=28.19125
val_loss=24.04749
val_loss=28.54266
val_loss=354.37766
val_loss=15.15372
adv_loss= 2.71796
adv_loss= 1.60705
adv_loss= 0.86850
adv_loss= 0.63773
adv_loss= 0.95486
adv_loss= 1.06348
adv_loss= 0.46793
adv_loss= 2.96717
adv_loss= 1.00920
adv_loss= 0.59376
surrogate=-0.00244, entropy= 0.16836, loss=-0.00244
surrogate= 0.00371, entropy= 0.16834, loss= 0.00371
surrogate=-0.00691, entropy= 0.16837, loss=-0.00691
surrogate=-0.00877, entropy= 0.16833, loss=-0.00877
surrogate=-0.02500, entropy= 0.16839, loss=-0.02500
surrogate=-0.01213, entropy= 0.16843, loss=-0.01213
surrogate=-0.00090, entropy= 0.16839, loss=-0.00090
surrogate=-0.01403, entropy= 0.16848, loss=-0.01403
surrogate= 0.02924, entropy= 0.16844, loss= 0.02924
surrogate= 0.02884, entropy= 0.16847, loss= 0.02884
std_min= 0.20595, std_max= 0.30245, std_mean= 0.25919
val lr: [5.635245901639358e-06], policy lr: [6.7622950819672285e-06]
Policy Loss: 0.028845, | Entropy Bonus: -0, | Value Loss: 15.154, | Advantage Loss: 0.59376
Time elapsed (s): 1.6671338081359863
Agent stdevs: 0.25919282
--------------------------------------------------------------------------------

Step 954
++++++++ Policy training ++++++++++
Current mean reward: 2330.899516 | mean episode length: 640.000000
val_loss=1052.68262
val_loss=1018.87970
val_loss= 7.89660
val_loss= 9.95709
val_loss=1490.68994
val_loss=331.59631
val_loss=18.17277
val_loss=12.12641
val_loss=81.53191
val_loss=14.85252
adv_loss= 1.70923
adv_loss=1878.60986
adv_loss= 0.65531
adv_loss= 1.13934
adv_loss= 0.45750
adv_loss= 0.83249
adv_loss= 1.21765
adv_loss= 1.09335
adv_loss= 3.08425
adv_loss= 0.50382
surrogate= 0.00733, entropy= 0.16850, loss= 0.00733
surrogate=-0.00070, entropy= 0.16859, loss=-0.00070
surrogate= 0.00225, entropy= 0.16862, loss= 0.00225
surrogate=-0.00018, entropy= 0.16870, loss=-0.00018
surrogate=-0.00486, entropy= 0.16878, loss=-0.00486
surrogate=-0.01675, entropy= 0.16878, loss=-0.01675
surrogate=-0.01928, entropy= 0.16883, loss=-0.01928
surrogate=-0.00909, entropy= 0.16886, loss=-0.00909
surrogate=-0.02151, entropy= 0.16890, loss=-0.02151
surrogate=-0.03267, entropy= 0.16897, loss=-0.03267
std_min= 0.20594, std_max= 0.30253, std_mean= 0.25924
val lr: [5.379098360655726e-06], policy lr: [6.454918032786871e-06]
Policy Loss: -0.032669, | Entropy Bonus: -0, | Value Loss: 14.853, | Advantage Loss: 0.50382
Time elapsed (s): 1.700385332107544
Agent stdevs: 0.25924113
--------------------------------------------------------------------------------

Step 955
++++++++ Policy training ++++++++++
Current mean reward: 2428.861181 | mean episode length: 674.000000
val_loss=1676.04260
val_loss=53.52872
val_loss=1449.34827
val_loss=23.77888
val_loss=17.57767
val_loss=892.95227
val_loss=24.28396
val_loss=87.36086
val_loss=43.86131
val_loss=1300.39319
adv_loss= 0.93545
adv_loss= 1.82080
adv_loss= 0.59585
adv_loss= 1.02845
adv_loss= 0.86717
adv_loss= 0.94206
adv_loss= 1.20261
adv_loss= 0.66748
adv_loss= 0.71696
adv_loss= 0.49188
surrogate= 0.00191, entropy= 0.16894, loss= 0.00191
surrogate=-0.00036, entropy= 0.16891, loss=-0.00036
surrogate= 0.00142, entropy= 0.16893, loss= 0.00142
surrogate=-0.02342, entropy= 0.16896, loss=-0.02342
surrogate= 0.00449, entropy= 0.16897, loss= 0.00449
surrogate= 0.01115, entropy= 0.16895, loss= 0.01115
surrogate=-0.00602, entropy= 0.16898, loss=-0.00602
surrogate=-0.02857, entropy= 0.16902, loss=-0.02857
surrogate= 0.01410, entropy= 0.16904, loss= 0.01410
surrogate=-0.03138, entropy= 0.16907, loss=-0.03138
std_min= 0.20595, std_max= 0.30253, std_mean= 0.25925
val lr: [5.122950819672123e-06], policy lr: [6.147540983606547e-06]
Policy Loss: -0.031382, | Entropy Bonus: -0, | Value Loss: 1300.4, | Advantage Loss: 0.49188
Time elapsed (s): 1.6688752174377441
Agent stdevs: 0.2592486
--------------------------------------------------------------------------------

Step 956
++++++++ Policy training ++++++++++
Current mean reward: 3348.541233 | mean episode length: 914.000000
val_loss=15.28156
val_loss=58.05922
val_loss=25.86567
val_loss=16.44769
val_loss=14.52273
val_loss= 8.54332
val_loss=16.38517
val_loss=45.12862
val_loss=43.92497
val_loss=44.32697
adv_loss= 0.85792
adv_loss= 0.62860
adv_loss= 1.16376
adv_loss= 1.24424
adv_loss= 0.67842
adv_loss= 1.30833
adv_loss= 1.28434
adv_loss= 0.92823
adv_loss= 0.93902
adv_loss= 1.23194
surrogate=-0.00841, entropy= 0.16907, loss=-0.00841
surrogate=-0.01312, entropy= 0.16915, loss=-0.01312
surrogate=-0.01269, entropy= 0.16919, loss=-0.01269
surrogate= 0.00430, entropy= 0.16920, loss= 0.00430
surrogate=-0.03474, entropy= 0.16924, loss=-0.03474
surrogate=-0.01047, entropy= 0.16923, loss=-0.01047
surrogate=-0.01999, entropy= 0.16923, loss=-0.01999
surrogate=-0.01050, entropy= 0.16922, loss=-0.01050
surrogate=-0.00781, entropy= 0.16925, loss=-0.00781
surrogate=-0.03079, entropy= 0.16933, loss=-0.03079
std_min= 0.20593, std_max= 0.30258, std_mean= 0.25928
val lr: [4.86680327868852e-06], policy lr: [5.840163934426223e-06]
Policy Loss: -0.030788, | Entropy Bonus: -0, | Value Loss: 44.327, | Advantage Loss: 1.2319
Time elapsed (s): 1.66508150100708
Agent stdevs: 0.25927624
--------------------------------------------------------------------------------

Step 957
++++++++ Policy training ++++++++++
Current mean reward: 2637.016363 | mean episode length: 744.000000
val_loss=94.90015
val_loss=19.07095
val_loss=307.78729
val_loss=322.96021
val_loss=152.39067
val_loss=180.41261
val_loss=184.17932
val_loss=92.05964
val_loss=127.37550
val_loss=62.53940
adv_loss=10.89836
adv_loss= 0.93599
adv_loss= 5.24504
adv_loss= 1.19240
adv_loss= 4.14728
adv_loss= 2.34216
adv_loss= 7.92421
adv_loss= 1.85712
adv_loss= 2.91927
adv_loss= 1.70289
surrogate=-0.00531, entropy= 0.16934, loss=-0.00531
surrogate= 0.01864, entropy= 0.16932, loss= 0.01864
surrogate=-0.00740, entropy= 0.16938, loss=-0.00740
surrogate=-0.01491, entropy= 0.16947, loss=-0.01491
surrogate=-0.01540, entropy= 0.16950, loss=-0.01540
surrogate= 0.00261, entropy= 0.16955, loss= 0.00261
surrogate=-0.01336, entropy= 0.16958, loss=-0.01336
surrogate=-0.00577, entropy= 0.16963, loss=-0.00577
surrogate=-0.02300, entropy= 0.16962, loss=-0.02300
surrogate=-0.01054, entropy= 0.16967, loss=-0.01054
std_min= 0.20592, std_max= 0.30263, std_mean= 0.25931
val lr: [4.6106557377049165e-06], policy lr: [5.532786885245899e-06]
Policy Loss: -0.010536, | Entropy Bonus: -0, | Value Loss: 62.539, | Advantage Loss: 1.7029
Time elapsed (s): 1.7046864032745361
Agent stdevs: 0.2593093
--------------------------------------------------------------------------------

Step 958
++++++++ Policy training ++++++++++
Current mean reward: 1743.308921 | mean episode length: 483.000000
val_loss=158.62323
val_loss=20.69884
val_loss=87.06937
val_loss=573.54041
val_loss=52.81889
val_loss=21.33350
val_loss=20.32600
val_loss=166.97032
val_loss=50.94712
val_loss=16.32283
adv_loss= 0.99583
adv_loss= 1.08489
adv_loss= 2.17200
adv_loss= 1.41601
adv_loss= 1.41519
adv_loss= 1.38294
adv_loss= 2.35453
adv_loss= 1.29559
adv_loss= 0.81428
adv_loss= 2.21192
surrogate= 0.00080, entropy= 0.16968, loss= 0.00080
surrogate=-0.01126, entropy= 0.16971, loss=-0.01126
surrogate=-0.00570, entropy= 0.16967, loss=-0.00570
surrogate=-0.02067, entropy= 0.16970, loss=-0.02067
surrogate= 0.02355, entropy= 0.16967, loss= 0.02355
surrogate= 0.00315, entropy= 0.16963, loss= 0.00315
surrogate=-0.00405, entropy= 0.16959, loss=-0.00405
surrogate=-0.02110, entropy= 0.16954, loss=-0.02110
surrogate=-0.00658, entropy= 0.16954, loss=-0.00658
surrogate=-0.02639, entropy= 0.16953, loss=-0.02639
std_min= 0.20593, std_max= 0.30255, std_mean= 0.25929
val lr: [4.354508196721313e-06], policy lr: [5.225409836065575e-06]
Policy Loss: -0.026388, | Entropy Bonus: -0, | Value Loss: 16.323, | Advantage Loss: 2.2119
Time elapsed (s): 1.6952476501464844
Agent stdevs: 0.25929347
--------------------------------------------------------------------------------

Step 959
++++++++ Policy training ++++++++++
Current mean reward: 1427.017261 | mean episode length: 384.800000
val_loss=16.51699
val_loss= 8.06282
val_loss=10.92990
val_loss=16.42594
val_loss= 9.98479
val_loss= 9.25009
val_loss= 7.73648
val_loss= 7.70480
val_loss=11.88388
val_loss=12.74434
adv_loss= 3.31997
adv_loss= 3.26582
adv_loss= 0.85917
adv_loss= 1.14935
adv_loss= 2.37360
adv_loss= 0.95828
adv_loss= 1.07308
adv_loss= 0.38661
adv_loss= 2.97376
adv_loss= 0.50197
surrogate=-0.00497, entropy= 0.16947, loss=-0.00497
surrogate=-0.00829, entropy= 0.16948, loss=-0.00829
surrogate=-0.01831, entropy= 0.16944, loss=-0.01831
surrogate= 0.00297, entropy= 0.16941, loss= 0.00297
surrogate= 0.00305, entropy= 0.16936, loss= 0.00305
surrogate=-0.00942, entropy= 0.16935, loss=-0.00942
surrogate=-0.02345, entropy= 0.16929, loss=-0.02345
surrogate=-0.01107, entropy= 0.16922, loss=-0.01107
surrogate=-0.02458, entropy= 0.16917, loss=-0.02458
surrogate=-0.02108, entropy= 0.16911, loss=-0.02108
std_min= 0.20587, std_max= 0.30259, std_mean= 0.25926
val lr: [4.0983606557377095e-06], policy lr: [4.9180327868852505e-06]
Policy Loss: -0.021085, | Entropy Bonus: -0, | Value Loss: 12.744, | Advantage Loss: 0.50197
Time elapsed (s): 1.6973326206207275
Agent stdevs: 0.25926316
--------------------------------------------------------------------------------

Step 960
Saving checkpoints to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805 with reward 2316.9
++++++++ Policy training ++++++++++
Current mean reward: 1635.961775 | mean episode length: 452.500000
val_loss=403.74637
val_loss=214.29033
val_loss=271.20218
val_loss=544.21759
val_loss=23.44923
val_loss=46.12096
val_loss=25.94135
val_loss=342.70239
val_loss=20.46418
val_loss=194.35141
adv_loss=42.00621
adv_loss= 5.47560
adv_loss= 3.12637
adv_loss= 1.44874
adv_loss= 2.67924
adv_loss= 7.23942
adv_loss= 1.88464
adv_loss= 2.54378
adv_loss= 3.62409
adv_loss= 1.93231
surrogate=-0.00069, entropy= 0.16915, loss=-0.00069
surrogate=-0.00357, entropy= 0.16917, loss=-0.00357
surrogate=-0.00797, entropy= 0.16923, loss=-0.00797
surrogate= 0.00663, entropy= 0.16933, loss= 0.00663
surrogate= 0.00871, entropy= 0.16940, loss= 0.00871
surrogate= 0.00742, entropy= 0.16951, loss= 0.00742
surrogate=-0.01650, entropy= 0.16961, loss=-0.01650
surrogate= 0.00416, entropy= 0.16972, loss= 0.00416
surrogate=-0.02583, entropy= 0.16985, loss=-0.02583
surrogate=-0.01032, entropy= 0.16991, loss=-0.01032
std_min= 0.20594, std_max= 0.30269, std_mean= 0.25933
val lr: [3.842213114754106e-06], policy lr: [4.610655737704927e-06]
Policy Loss: -0.010316, | Entropy Bonus: -0, | Value Loss: 194.35, | Advantage Loss: 1.9323
Time elapsed (s): 1.7334544658660889
Agent stdevs: 0.25933197
--------------------------------------------------------------------------------

Step 961
++++++++ Policy training ++++++++++
Current mean reward: 2129.996264 | mean episode length: 572.666667
val_loss=20.78756
val_loss=10.91359
val_loss= 7.28317
val_loss=11.30175
val_loss=12.22250
val_loss= 7.95294
val_loss= 7.46834
val_loss=13.90160
val_loss=14.24728
val_loss=10.06194
adv_loss= 0.77924
adv_loss= 1.36907
adv_loss= 1.53954
adv_loss= 1.27632
adv_loss= 1.03782
adv_loss= 1.23980
adv_loss= 1.15438
adv_loss= 1.09048
adv_loss= 1.05235
adv_loss= 0.91914
surrogate= 0.00057, entropy= 0.16994, loss= 0.00057
surrogate=-0.00771, entropy= 0.16997, loss=-0.00771
surrogate=-0.01264, entropy= 0.16993, loss=-0.01264
surrogate= 0.01447, entropy= 0.16997, loss= 0.01447
surrogate= 0.00030, entropy= 0.16997, loss= 0.00030
surrogate= 0.00111, entropy= 0.16993, loss= 0.00111
surrogate=-0.02334, entropy= 0.16990, loss=-0.02334
surrogate=-0.02092, entropy= 0.16984, loss=-0.02092
surrogate= 0.00755, entropy= 0.16979, loss= 0.00755
surrogate=-0.01706, entropy= 0.16971, loss=-0.01706
std_min= 0.20590, std_max= 0.30268, std_mean= 0.25932
val lr: [3.586065573770503e-06], policy lr: [4.303278688524603e-06]
Policy Loss: -0.017064, | Entropy Bonus: -0, | Value Loss: 10.062, | Advantage Loss: 0.91914
Time elapsed (s): 1.6673529148101807
Agent stdevs: 0.25931743
--------------------------------------------------------------------------------

Step 962
++++++++ Policy training ++++++++++
Current mean reward: 2375.625991 | mean episode length: 636.500000
val_loss= 8.18754
val_loss=14.15899
val_loss=10.87893
val_loss= 9.76803
val_loss= 8.32662
val_loss= 6.92058
val_loss=10.06395
val_loss= 6.80896
val_loss= 8.81076
val_loss=10.64435
adv_loss= 0.65966
adv_loss= 0.66562
adv_loss= 1.06645
adv_loss= 2.25290
adv_loss= 1.08920
adv_loss= 1.24589
adv_loss= 0.54025
adv_loss= 0.67309
adv_loss= 1.14035
adv_loss= 2.10760
surrogate= 0.00099, entropy= 0.16965, loss= 0.00099
surrogate=-0.00404, entropy= 0.16963, loss=-0.00404
surrogate=-0.00580, entropy= 0.16963, loss=-0.00580
surrogate= 0.00016, entropy= 0.16957, loss= 0.00016
surrogate=-0.00962, entropy= 0.16956, loss=-0.00962
surrogate=-0.01245, entropy= 0.16951, loss=-0.01245
surrogate=-0.00290, entropy= 0.16945, loss=-0.00290
surrogate=-0.00323, entropy= 0.16945, loss=-0.00323
surrogate=-0.02053, entropy= 0.16941, loss=-0.02053
surrogate= 0.01442, entropy= 0.16938, loss= 0.01442
std_min= 0.20588, std_max= 0.30261, std_mean= 0.25929
val lr: [3.329918032786872e-06], policy lr: [3.995901639344246e-06]
Policy Loss: 0.014416, | Entropy Bonus: -0, | Value Loss: 10.644, | Advantage Loss: 2.1076
Time elapsed (s): 1.6610257625579834
Agent stdevs: 0.25928724
--------------------------------------------------------------------------------

Step 963
++++++++ Policy training ++++++++++
Current mean reward: 1920.356158 | mean episode length: 518.666667
val_loss= 9.31464
val_loss= 5.25453
val_loss= 9.18662
val_loss= 5.57108
val_loss=11.10610
val_loss=11.91319
val_loss= 8.60939
val_loss=10.27004
val_loss= 9.02649
val_loss= 8.77067
adv_loss= 0.64943
adv_loss= 0.49808
adv_loss= 0.99924
adv_loss= 0.80356
adv_loss= 2.48928
adv_loss= 1.00793
adv_loss= 0.80481
adv_loss= 1.15301
adv_loss= 0.94160
adv_loss= 1.37792
surrogate=-0.00196, entropy= 0.16938, loss=-0.00196
surrogate=-0.00033, entropy= 0.16938, loss=-0.00033
surrogate=-0.00872, entropy= 0.16937, loss=-0.00872
surrogate=-0.00444, entropy= 0.16939, loss=-0.00444
surrogate=-0.02559, entropy= 0.16940, loss=-0.02559
surrogate=-0.00389, entropy= 0.16941, loss=-0.00389
surrogate= 0.01301, entropy= 0.16943, loss= 0.01301
surrogate= 0.02163, entropy= 0.16944, loss= 0.02163
surrogate=-0.01552, entropy= 0.16943, loss=-0.01552
surrogate= 0.00867, entropy= 0.16944, loss= 0.00867
std_min= 0.20591, std_max= 0.30265, std_mean= 0.25929
val lr: [3.0737704918032684e-06], policy lr: [3.6885245901639216e-06]
Policy Loss: 0.008675, | Entropy Bonus: -0, | Value Loss: 8.7707, | Advantage Loss: 1.3779
Time elapsed (s): 1.6936571598052979
Agent stdevs: 0.25929075
--------------------------------------------------------------------------------

Step 964
++++++++ Policy training ++++++++++
Current mean reward: 2720.652823 | mean episode length: 738.000000
val_loss= 9.56897
val_loss= 7.84373
val_loss= 5.66961
val_loss= 7.93118
val_loss= 7.76746
val_loss=11.14194
val_loss= 5.78756
val_loss= 5.94905
val_loss= 7.74889
val_loss= 5.33684
adv_loss= 0.71394
adv_loss= 0.79470
adv_loss= 0.75271
adv_loss= 0.59164
adv_loss= 0.49032
adv_loss= 0.67204
adv_loss= 0.60860
adv_loss= 0.82826
adv_loss= 0.91849
adv_loss= 0.71679
surrogate=-0.00109, entropy= 0.16945, loss=-0.00109
surrogate=-0.00608, entropy= 0.16946, loss=-0.00608
surrogate=-0.01095, entropy= 0.16946, loss=-0.01095
surrogate= 0.00046, entropy= 0.16947, loss= 0.00046
surrogate=-0.02205, entropy= 0.16950, loss=-0.02205
surrogate=-0.00782, entropy= 0.16952, loss=-0.00782
surrogate=-0.02362, entropy= 0.16953, loss=-0.02362
surrogate=-0.01252, entropy= 0.16954, loss=-0.01252
surrogate=-0.00353, entropy= 0.16956, loss=-0.00353
surrogate=-0.01024, entropy= 0.16957, loss=-0.01024
std_min= 0.20594, std_max= 0.30264, std_mean= 0.25930
val lr: [2.817622950819665e-06], policy lr: [3.3811475409835977e-06]
Policy Loss: -0.010242, | Entropy Bonus: -0, | Value Loss: 5.3368, | Advantage Loss: 0.71679
Time elapsed (s): 1.6955351829528809
Agent stdevs: 0.25929898
--------------------------------------------------------------------------------

Step 965
++++++++ Policy training ++++++++++
Current mean reward: 3286.428880 | mean episode length: 914.500000
val_loss=432.41443
val_loss=528.42651
val_loss=254.74649
val_loss=96.43900
val_loss=61.12621
val_loss=1742.52344
val_loss= 8.91479
val_loss=149.04553
val_loss=185.42384
val_loss=213.10143
adv_loss= 0.45500
adv_loss= 0.44879
adv_loss= 0.44565
adv_loss= 1.01419
adv_loss= 1.24372
adv_loss= 0.69904
adv_loss= 0.66759
adv_loss= 1.26017
adv_loss= 0.52884
adv_loss= 0.63446
surrogate=-0.00390, entropy= 0.16954, loss=-0.00390
surrogate=-0.00015, entropy= 0.16957, loss=-0.00015
surrogate= 0.00036, entropy= 0.16955, loss= 0.00036
surrogate=-0.02119, entropy= 0.16955, loss=-0.02119
surrogate=-0.00963, entropy= 0.16956, loss=-0.00963
surrogate= 0.00570, entropy= 0.16959, loss= 0.00570
surrogate=-0.00809, entropy= 0.16961, loss=-0.00809
surrogate=-0.01355, entropy= 0.16961, loss=-0.01355
surrogate=-0.01962, entropy= 0.16963, loss=-0.01962
surrogate=-0.00740, entropy= 0.16965, loss=-0.00740
std_min= 0.20593, std_max= 0.30262, std_mean= 0.25931
val lr: [2.5614754098360613e-06], policy lr: [3.0737704918032734e-06]
Policy Loss: -0.007399, | Entropy Bonus: -0, | Value Loss: 213.1, | Advantage Loss: 0.63446
Time elapsed (s): 1.6973001956939697
Agent stdevs: 0.25930646
--------------------------------------------------------------------------------

Step 966
++++++++ Policy training ++++++++++
Current mean reward: 2643.357139 | mean episode length: 711.500000
val_loss= 7.53902
val_loss=10.31534
val_loss= 7.42331
val_loss= 5.22402
val_loss= 6.34115
val_loss= 5.94777
val_loss= 8.14460
val_loss= 4.47426
val_loss= 5.97144
val_loss= 5.12201
adv_loss= 0.63336
adv_loss= 0.44234
adv_loss= 0.67529
adv_loss= 1.44044
adv_loss= 0.36047
adv_loss= 1.09863
adv_loss= 0.81106
adv_loss= 0.41690
adv_loss= 0.30979
adv_loss= 0.51445
surrogate= 0.00193, entropy= 0.16963, loss= 0.00193
surrogate=-0.00641, entropy= 0.16961, loss=-0.00641
surrogate=-0.00648, entropy= 0.16960, loss=-0.00648
surrogate= 0.00406, entropy= 0.16958, loss= 0.00406
surrogate=-0.00826, entropy= 0.16959, loss=-0.00826
surrogate=-0.02908, entropy= 0.16955, loss=-0.02908
surrogate=-0.02847, entropy= 0.16954, loss=-0.02847
surrogate= 0.00603, entropy= 0.16954, loss= 0.00603
surrogate=-0.00692, entropy= 0.16954, loss=-0.00692
surrogate=-0.01648, entropy= 0.16952, loss=-0.01648
std_min= 0.20590, std_max= 0.30248, std_mean= 0.25929
val lr: [2.3053278688524583e-06], policy lr: [2.7663934426229496e-06]
Policy Loss: -0.01648, | Entropy Bonus: -0, | Value Loss: 5.122, | Advantage Loss: 0.51445
Time elapsed (s): 1.735640048980713
Agent stdevs: 0.25929272
--------------------------------------------------------------------------------

Step 967
++++++++ Policy training ++++++++++
Current mean reward: 2483.395097 | mean episode length: 682.333333
val_loss=2882.07812
val_loss=196.81502
val_loss=106.00198
val_loss=2566.33618
val_loss=1709.03699
val_loss=918.10266
val_loss=1842.68286
val_loss=1610.89331
val_loss= 9.87721
val_loss=32.47597
adv_loss=1926.18420
adv_loss= 0.63121
adv_loss= 0.76057
adv_loss= 1.11379
adv_loss= 0.62159
adv_loss= 1.56870
adv_loss= 0.72754
adv_loss= 0.98218
adv_loss= 0.52169
adv_loss= 1.35381
surrogate=-0.00033, entropy= 0.16948, loss=-0.00033
surrogate= 0.00146, entropy= 0.16949, loss= 0.00146
surrogate=-0.00173, entropy= 0.16946, loss=-0.00173
surrogate= 0.00499, entropy= 0.16948, loss= 0.00499
surrogate=-0.00087, entropy= 0.16946, loss=-0.00087
surrogate=-0.00201, entropy= 0.16946, loss=-0.00201
surrogate=-0.01401, entropy= 0.16944, loss=-0.01401
surrogate= 0.00040, entropy= 0.16942, loss= 0.00040
surrogate=-0.00322, entropy= 0.16942, loss=-0.00322
surrogate=-0.00571, entropy= 0.16942, loss=-0.00571
std_min= 0.20589, std_max= 0.30251, std_mean= 0.25929
val lr: [2.0491803278688547e-06], policy lr: [2.4590163934426253e-06]
Policy Loss: -0.0057087, | Entropy Bonus: -0, | Value Loss: 32.476, | Advantage Loss: 1.3538
Time elapsed (s): 1.708594799041748
Agent stdevs: 0.25928676
--------------------------------------------------------------------------------

Step 968
++++++++ Policy training ++++++++++
Current mean reward: 1604.013334 | mean episode length: 433.250000
val_loss= 8.16185
val_loss=15.04905
val_loss=10.62149
val_loss= 7.03811
val_loss=12.75226
val_loss= 9.53354
val_loss= 9.92149
val_loss= 4.98095
val_loss= 7.25917
val_loss= 6.88366
adv_loss= 0.74593
adv_loss= 0.92787
adv_loss= 1.26434
adv_loss= 0.68917
adv_loss= 0.45666
adv_loss= 0.81752
adv_loss= 0.81789
adv_loss= 2.01032
adv_loss= 0.71004
adv_loss= 2.81533
surrogate=-0.00055, entropy= 0.16942, loss=-0.00055
surrogate=-0.00023, entropy= 0.16938, loss=-0.00023
surrogate=-0.00632, entropy= 0.16938, loss=-0.00632
surrogate= 0.00051, entropy= 0.16935, loss= 0.00051
surrogate=-0.00164, entropy= 0.16932, loss=-0.00164
surrogate=-0.00271, entropy= 0.16930, loss=-0.00271
surrogate=-0.01711, entropy= 0.16929, loss=-0.01711
surrogate=-0.01060, entropy= 0.16926, loss=-0.01060
surrogate=-0.01281, entropy= 0.16923, loss=-0.01281
surrogate= 0.00302, entropy= 0.16920, loss= 0.00302
std_min= 0.20591, std_max= 0.30249, std_mean= 0.25926
val lr: [1.7930327868852514e-06], policy lr: [2.1516393442623014e-06]
Policy Loss: 0.0030165, | Entropy Bonus: -0, | Value Loss: 6.8837, | Advantage Loss: 2.8153
Time elapsed (s): 1.668184518814087
Agent stdevs: 0.2592639
--------------------------------------------------------------------------------

Step 969
++++++++ Policy training ++++++++++
Current mean reward: 1705.546967 | mean episode length: 456.250000
val_loss=13.00628
val_loss= 8.13259
val_loss= 9.24732
val_loss= 7.19421
val_loss= 9.34218
val_loss=11.97380
val_loss= 9.99292
val_loss= 8.52002
val_loss= 7.69054
val_loss= 7.52819
adv_loss= 5.72526
adv_loss= 0.40738
adv_loss= 2.10254
adv_loss= 1.27210
adv_loss= 2.54931
adv_loss= 0.77994
adv_loss= 0.70188
adv_loss= 1.57953
adv_loss= 1.16221
adv_loss= 1.11579
surrogate=-0.00077, entropy= 0.16917, loss=-0.00077
surrogate=-0.00078, entropy= 0.16915, loss=-0.00078
surrogate=-0.00490, entropy= 0.16912, loss=-0.00490
surrogate=-0.00043, entropy= 0.16907, loss=-0.00043
surrogate=-0.00279, entropy= 0.16905, loss=-0.00279
surrogate= 0.00001, entropy= 0.16904, loss= 0.00001
surrogate=-0.00402, entropy= 0.16901, loss=-0.00402
surrogate=-0.01979, entropy= 0.16898, loss=-0.01979
surrogate=-0.01263, entropy= 0.16896, loss=-0.01263
surrogate=-0.01722, entropy= 0.16892, loss=-0.01722
std_min= 0.20590, std_max= 0.30245, std_mean= 0.25924
val lr: [1.536885245901648e-06], policy lr: [1.8442622950819775e-06]
Policy Loss: -0.01722, | Entropy Bonus: -0, | Value Loss: 7.5282, | Advantage Loss: 1.1158
Time elapsed (s): 1.6631319522857666
Agent stdevs: 0.2592382
--------------------------------------------------------------------------------

Step 970
++++++++ Policy training ++++++++++
Current mean reward: 2226.525832 | mean episode length: 614.333333
val_loss= 8.77292
val_loss= 9.54519
val_loss=454.27072
val_loss= 8.20013
val_loss=218.91463
val_loss=50.07483
val_loss=35.85369
val_loss=17.95047
val_loss=13.42077
val_loss=21.55435
adv_loss= 0.37999
adv_loss= 1.39293
adv_loss=1340.22083
adv_loss= 1.08714
adv_loss= 1.16638
adv_loss= 0.51974
adv_loss=1339.58960
adv_loss= 0.56725
adv_loss= 2.07455
adv_loss= 2.29736
surrogate=-0.00020, entropy= 0.16892, loss=-0.00020
surrogate=-0.00016, entropy= 0.16890, loss=-0.00016
surrogate=-0.00033, entropy= 0.16888, loss=-0.00033
surrogate=-0.00568, entropy= 0.16888, loss=-0.00568
surrogate=-0.00279, entropy= 0.16886, loss=-0.00279
surrogate=-0.00378, entropy= 0.16884, loss=-0.00378
surrogate=-0.00300, entropy= 0.16882, loss=-0.00300
surrogate=-0.00303, entropy= 0.16883, loss=-0.00303
surrogate=-0.00609, entropy= 0.16882, loss=-0.00609
surrogate= 0.00214, entropy= 0.16879, loss= 0.00214
std_min= 0.20589, std_max= 0.30244, std_mean= 0.25923
val lr: [1.2807377049180446e-06], policy lr: [1.5368852459016534e-06]
Policy Loss: 0.0021414, | Entropy Bonus: -0, | Value Loss: 21.554, | Advantage Loss: 2.2974
Time elapsed (s): 1.6521000862121582
Agent stdevs: 0.25922728
--------------------------------------------------------------------------------

Step 971
++++++++ Policy training ++++++++++
Current mean reward: 2098.891599 | mean episode length: 578.666667
val_loss=141.44618
val_loss= 8.82955
val_loss= 6.49839
val_loss=1030.20032
val_loss=1442.22717
val_loss=10.65057
val_loss=19.16101
val_loss=32.54766
val_loss=131.36534
val_loss=13.61217
adv_loss= 1.51543
adv_loss= 0.44298
adv_loss= 0.71436
adv_loss= 2.61146
adv_loss= 0.94482
adv_loss= 2.90284
adv_loss= 2.65004
adv_loss= 0.64093
adv_loss= 0.39415
adv_loss= 0.75117
surrogate= 0.00144, entropy= 0.16880, loss= 0.00144
surrogate= 0.00159, entropy= 0.16878, loss= 0.00159
surrogate= 0.00100, entropy= 0.16877, loss= 0.00100
surrogate=-0.00681, entropy= 0.16876, loss=-0.00681
surrogate=-0.00847, entropy= 0.16878, loss=-0.00847
surrogate=-0.00229, entropy= 0.16877, loss=-0.00229
surrogate=-0.00859, entropy= 0.16877, loss=-0.00859
surrogate= 0.00191, entropy= 0.16877, loss= 0.00191
surrogate= 0.00204, entropy= 0.16876, loss= 0.00204
surrogate=-0.00485, entropy= 0.16878, loss=-0.00485
std_min= 0.20590, std_max= 0.30242, std_mean= 0.25922
val lr: [1.0245901639344136e-06], policy lr: [1.2295081967212961e-06]
Policy Loss: -0.0048482, | Entropy Bonus: -0, | Value Loss: 13.612, | Advantage Loss: 0.75117
Time elapsed (s): 1.673191785812378
Agent stdevs: 0.25922436
--------------------------------------------------------------------------------

Step 972
++++++++ Policy training ++++++++++
Current mean reward: 2425.834778 | mean episode length: 668.666667
val_loss=734.52008
val_loss=1236.59314
val_loss=10.81079
val_loss=996.10022
val_loss=109.05290
val_loss= 9.69161
val_loss=1602.80664
val_loss=50.48106
val_loss=126.29489
val_loss=301.18420
adv_loss=1946.30579
adv_loss= 0.74412
adv_loss= 2.67406
adv_loss= 0.26029
adv_loss= 1.66063
adv_loss= 0.31187
adv_loss= 1.09787
adv_loss= 0.68922
adv_loss= 0.80235
adv_loss= 1.20600
surrogate= 0.00050, entropy= 0.16878, loss= 0.00050
surrogate= 0.00111, entropy= 0.16878, loss= 0.00111
surrogate=-0.00071, entropy= 0.16878, loss=-0.00071
surrogate=-0.00105, entropy= 0.16879, loss=-0.00105
surrogate= 0.00277, entropy= 0.16878, loss= 0.00277
surrogate=-0.00200, entropy= 0.16879, loss=-0.00200
surrogate=-0.00153, entropy= 0.16878, loss=-0.00153
surrogate=-0.00135, entropy= 0.16878, loss=-0.00135
surrogate=-0.01197, entropy= 0.16878, loss=-0.01197
surrogate= 0.00159, entropy= 0.16877, loss= 0.00159
std_min= 0.20591, std_max= 0.30239, std_mean= 0.25922
val lr: [7.684426229508101e-07], policy lr: [9.22131147540972e-07]
Policy Loss: 0.0015943, | Entropy Bonus: -0, | Value Loss: 301.18, | Advantage Loss: 1.206
Time elapsed (s): 1.7108900547027588
Agent stdevs: 0.25922188
--------------------------------------------------------------------------------

Step 973
++++++++ Policy training ++++++++++
Current mean reward: 2283.410818 | mean episode length: 615.000000
val_loss= 7.00782
val_loss= 8.26413
val_loss= 9.27037
val_loss= 7.82828
val_loss= 8.45522
val_loss=10.11422
val_loss= 5.69451
val_loss= 7.09138
val_loss= 9.78866
val_loss=11.27705
adv_loss= 0.58070
adv_loss= 0.79546
adv_loss= 3.12785
adv_loss= 1.08772
adv_loss= 1.16522
adv_loss= 0.65918
adv_loss= 0.55928
adv_loss= 0.72616
adv_loss= 0.51850
adv_loss= 1.60704
surrogate=-0.00023, entropy= 0.16878, loss=-0.00023
surrogate=-0.00150, entropy= 0.16878, loss=-0.00150
surrogate=-0.00378, entropy= 0.16879, loss=-0.00378
surrogate=-0.00308, entropy= 0.16880, loss=-0.00308
surrogate=-0.00094, entropy= 0.16880, loss=-0.00094
surrogate=-0.00306, entropy= 0.16880, loss=-0.00306
surrogate= 0.00045, entropy= 0.16880, loss= 0.00045
surrogate=-0.00576, entropy= 0.16881, loss=-0.00576
surrogate= 0.00657, entropy= 0.16882, loss= 0.00657
surrogate=-0.00614, entropy= 0.16883, loss=-0.00614
std_min= 0.20591, std_max= 0.30239, std_mean= 0.25923
val lr: [5.122950819672068e-07], policy lr: [6.147540983606481e-07]
Policy Loss: -0.0061401, | Entropy Bonus: -0, | Value Loss: 11.277, | Advantage Loss: 1.607
Time elapsed (s): 1.6802978515625
Agent stdevs: 0.259227
--------------------------------------------------------------------------------

Step 974
++++++++ Policy training ++++++++++
Current mean reward: 2377.526067 | mean episode length: 645.333333
val_loss= 7.17471
val_loss= 8.86979
val_loss=12.03442
val_loss= 6.79763
val_loss=11.84001
val_loss=12.21213
val_loss=10.56869
val_loss= 6.61468
val_loss= 7.03799
val_loss= 8.19069
adv_loss= 1.11293
adv_loss= 0.75393
adv_loss= 0.81846
adv_loss= 0.87657
adv_loss= 0.60926
adv_loss= 0.58354
adv_loss= 1.25018
adv_loss= 0.40962
adv_loss= 0.66315
adv_loss= 1.45234
surrogate= 0.00054, entropy= 0.16883, loss= 0.00054
surrogate=-0.00056, entropy= 0.16882, loss=-0.00056
surrogate=-0.00190, entropy= 0.16881, loss=-0.00190
surrogate=-0.00062, entropy= 0.16879, loss=-0.00062
surrogate=-0.00021, entropy= 0.16879, loss=-0.00021
surrogate=-0.00367, entropy= 0.16878, loss=-0.00367
surrogate=-0.00332, entropy= 0.16877, loss=-0.00332
surrogate=-0.00263, entropy= 0.16876, loss=-0.00263
surrogate=-0.00910, entropy= 0.16875, loss=-0.00910
surrogate=-0.00001, entropy= 0.16874, loss=-0.00001
std_min= 0.20590, std_max= 0.30238, std_mean= 0.25922
val lr: [2.561475409836034e-07], policy lr: [3.0737704918032403e-07]
Policy Loss: -9.6932e-06, | Entropy Bonus: -0, | Value Loss: 8.1907, | Advantage Loss: 1.4523
Time elapsed (s): 1.680762529373169
Agent stdevs: 0.25921977
--------------------------------------------------------------------------------

Step 975
++++++++ Policy training ++++++++++
Current mean reward: 3452.401171 | mean episode length: 958.500000
val_loss=1939.23901
val_loss= 8.46787
val_loss=58.23497
val_loss=94.54098
val_loss=490.28351
val_loss=32.23590
val_loss=344.79886
val_loss= 8.84508
val_loss=10.77526
val_loss=187.32166
adv_loss= 0.96636
adv_loss= 1.37885
adv_loss= 0.89227
adv_loss= 0.75524
adv_loss= 0.68225
adv_loss= 0.57992
adv_loss= 0.66798
adv_loss= 0.36940
adv_loss= 0.58041
adv_loss= 0.78886
surrogate=-0.00007, entropy= 0.16874, loss=-0.00007
surrogate=-0.00024, entropy= 0.16874, loss=-0.00024
surrogate=-0.00011, entropy= 0.16874, loss=-0.00011
surrogate= 0.00004, entropy= 0.16874, loss= 0.00004
surrogate=-0.00057, entropy= 0.16874, loss=-0.00057
surrogate=-0.00036, entropy= 0.16874, loss=-0.00036
surrogate= 0.00068, entropy= 0.16874, loss= 0.00068
surrogate=-0.00024, entropy= 0.16874, loss=-0.00024
surrogate=-0.00210, entropy= 0.16874, loss=-0.00210
surrogate=-0.00375, entropy= 0.16874, loss=-0.00375
std_min= 0.20589, std_max= 0.30239, std_mean= 0.25922
val lr: [0.0], policy lr: [0.0]
Policy Loss: -0.0037549, | Entropy Bonus: -0, | Value Loss: 187.32, | Advantage Loss: 0.78886
Time elapsed (s): 1.685072898864746
Agent stdevs: 0.2592199
--------------------------------------------------------------------------------

Models saved to /home/xulin/Documents/robust_rl_benchmarks/NUUS/src/exps/hopper/vanilla_ppo/agents/ebe061ad-6388-41ce-98f4-4e6ebe51a805
